<html>
<head>
<title>K-Means Clustering: Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means聚类:Python中的机器学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/k-means-clustering-machine-learning-in-python-1810231efef5?source=collection_archive---------1-----------------------#2021-01-20">https://medium.datadriveninvestor.com/k-means-clustering-machine-learning-in-python-1810231efef5?source=collection_archive---------1-----------------------#2021-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="6978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过我们从学习<a class="ae kl" href="https://medium.com/ai-in-plain-english/k-nearest-neighbors-machine-learning-in-python-1c071986d260" rel="noopener">K-Nearest-Neighbors</a>-KNN(一种监督学习算法和一种懒惰学习器)中收集的知识，我们发现所有类似的东西都存在于附近。它取决于这个假设是否足够真实，算法是否有用。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/965f7300e0eedff8496084ed32492bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*bMoLHaavm0y_bHmUnXK1Pg.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image Source: <a class="ae kl" href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68#:~:text=Clustering%20is%20a%20Machine%20Learning,the%20grouping%20of%20data%20points.&amp;text=In%20theory%2C%20data%20points%20that,dissimilar%20properties%20and%2For%20features." rel="noopener" target="_blank">towardsdatascience.com</a></figcaption></figure><p id="7f1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K-Means也不例外。基于相似性或相似性的相同基本原理，K-Means也能够将数据点分类成组。在这篇博客中，我们将回顾K-Means聚类背后的数学原理，并从头开始构建一个小模型。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="dfe8" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">k-均值聚类—简介</h1><p id="cac3" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">K-Means聚类，也称为Lloyd's算法，是一种迭代的、数据分区的、无监督的学习算法，用于将<em class="mi"> n </em>个观察值分配给由质心定义的<em class="mi"> K </em>个聚类中的一个，其中<em class="mi"> K </em>是在算法开始之前选择的。</p><h2 id="e302" class="mj lg iq bd lh mk ml dn ll mm mn dp lp jy mo mp lt kc mq mr lx kg ms mt mb mu bi translated">使聚集</h2><p id="d559" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">聚类的最基本定义是将数据点组合在一起的技术。假设同一组的数据点具有相似的性质或特征，而来自不同组的数据点将具有高度的不相似性。</p><p id="d003" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚类是一种无监督的学习方法，通常用于许多领域的统计数据分析。对于聚类，我们只尝试通过对数据进行分组来研究数据的结构。</p><h1 id="4430" class="lf lg iq bd lh li mv lk ll lm mw lo lp lq mx ls lt lu my lw lx ly mz ma mb mc bi translated">算法和数学</h1><p id="c7f9" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">K均值聚类的工作方式是:</p><ul class=""><li id="e650" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated">我们随机选择<em class="mi"> K </em>个初始聚类中心(质心)。</li><li id="134e" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">计算所有观测到每个质心的点到聚类质心的距离。</li><li id="87d1" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">将每个观测值分配给质心最近的聚类。</li><li id="554b" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">计算每个聚类中观察值的平均值，以获得<em class="mi"> K </em>个新的质心位置。</li><li id="b51d" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated">重复步骤2到4，直到聚类分配不变，或者达到最大迭代次数。</li></ul><p id="1e81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K-Means聚类解决问题的方法是<strong class="jp ir">期望最大化算法</strong>。这是一种逼近最大似然函数的迭代方法。该算法分两步工作:</p><ul class=""><li id="69be" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">电子步骤<br/> </strong>它包括将数据点分配给最近的聚类。</li><li id="7fa3" class="na nb iq jp b jq nj ju nk jy nl kc nm kg nn kk nf ng nh ni bi translated"><strong class="jp ir">M步<br/> </strong>它正在计算每个簇的质心。</li></ul><p id="8263" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">K均值的目标函数是:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8adcf7cb6c30c3f238acbb3248253b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*x9UUupmZfyyXq5e-L8EKpw.png"/></div></figure><p id="2a2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<strong class="jp ir"> <em class="mi"> wᵢₖ = 1 </em> </strong>为数据点<strong class="jp ir"><em class="mi"/></strong>如果属于集群<strong class="jp ir"><em class="mi">k</em></strong>；否则，<strong class="jp ir"> <em class="mi"> wᵢₖ = 0 </em> </strong>。同样，<strong class="jp ir"><em class="mi"/></strong>是<strong class="jp ir"><em class="mi"/></strong>星团的质心。</p><p id="2797" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们通过对j相对于<strong class="jp ir"><em class="mi"/></strong>求微分来得到这两个步骤，并更新集群分配— <strong class="jp ir"> <em class="mi"> E-Step </em> </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/761fc12064bed9700963a9ff7911aca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*VGJ28a4fDdsV42Pl_BVN4w.png"/></div></figure><p id="b4af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后我们相对于<strong class="jp ir"> <em class="mi"> μₖ </em> </strong>对j进行微分，并从先前的迭代<strong class="jp ir"> <em class="mi"> M步</em> </strong>重新计算聚类分配后的质心</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/beedf274804a2f01fc89d201c8c94592.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*v0XLnoeZxeqNB0MGGpocpA.png"/></div></figure><p id="8bc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">迭代地使用这些步骤，K-均值聚类能够找到聚类质心。</p></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><h1 id="7d40" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">从头开始实施</h1><blockquote class="nr ns nt"><p id="def3" class="jn jo mi jp b jq jr js jt ju jv jw jx nu jz ka kb nv kd ke kf nw kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong>对于这个示例实现，我们将使用<code class="fe nx ny nz oa b">sklearn.datasets</code>中的<code class="fe nx ny nz oa b">make_blobs()</code>函数。</p></blockquote><ul class=""><li id="81a1" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">导入库</strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="f98e" class="mj lg iq oa b gy of og l oh oi">import numpy as np<br/>from matplotlib import pyplot as plt<br/>from sklearn.datasets import make_blobs  # Only for Dataset creation</span></pre><ul class=""><li id="0b48" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">样本数据集</strong></li></ul><blockquote class="nr ns nt"><p id="65c5" class="jn jo mi jp b jq jr js jt ju jv jw jx nu jz ka kb nv kd ke kf nw kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong>强烈建议对<code class="fe nx ny nz oa b">random_state</code>使用不同的值，以便更好地理解聚类。</p></blockquote><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="bb3f" class="mj lg iq oa b gy of og l oh oi">X,y = make_blobs(n_samples = 1500, centers = 4, random_state=3)</span></pre><ul class=""><li id="fa8e" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">绘制初始数据集</strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="0828" class="mj lg iq oa b gy of og l oh oi">plt.figure(0)<br/>plt.grid(True)<br/>plt.scatter(X[:,0],X[:,1])</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7e799d67f43009626c8a6ad31a8fc3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Zv3FBsxnvOZGp96TNu1eMA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Initial Uncategorized Clusters</figcaption></figure><ul class=""><li id="b4f4" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">设置标签、K值和聚类字典</strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="3e98" class="mj lg iq oa b gy of og l oh oi">k = 4<br/>color=['Red','Yellow','Green','Blue']<br/>clusters = {}<br/>for i in range(k):<br/>  p_mean = 10*(2*np.random.random((X.shape[1],))-1)<br/>  Xp=[]<br/>  cluster = {<br/>       "p_mean":p_mean,<br/>       "Xp":Xp,<br/>       "colors":color[i]<br/>  }<br/>  clusters[i] = cluster</span></pre><ul class=""><li id="e928" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">定义距离功能</strong></li></ul><blockquote class="nr ns nt"><p id="9eb6" class="jn jo mi jp b jq jr js jt ju jv jw jx nu jz ka kb nv kd ke kf nw kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong>我们这里用了欧几里德距离。尽管如此，我们也可以使用其他方法来计算距离。<strong class="jp ir">随意实验。</strong></p></blockquote><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="1f21" class="mj lg iq oa b gy of og l oh oi">def euclidian(p1,p2):<br/>  return np.sqrt(np.sum((p1-p2)**2))</span></pre><ul class=""><li id="26cd" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">定义集群分配功能— <em class="mi">电子步骤</em> </strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="210d" class="mj lg iq oa b gy of og l oh oi">def compare(clusters):<br/>  for i in range(X.shape[0]):<br/>    euc_dist = []<br/>    point = X[i]<br/>    for j in range(k):<br/>      dist = euclidian(point,clusters[j]['p_mean'])<br/>      euc_dist.append(dist)<br/>    clstr = np.argmin(euc_dist)<br/>    clusters[clstr]['Xp'].append(point)</span></pre><ul class=""><li id="4031" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">定义寻找聚类点平均值的函数— <em class="mi"> M步</em> </strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="25b2" class="mj lg iq oa b gy of og l oh oi">def cluster_mean(clusters):<br/>  for i in range(k):<br/>    points = np.array(clusters[i]['Xp'])<br/>    if points.shape[0]&gt;0:<br/>      next_mean = points.mean(axis=0)<br/>      clusters[i]['p_mean'] = next_mean</span></pre><ul class=""><li id="76f0" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">创建一个绘图功能</strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="e653" class="mj lg iq oa b gy of og l oh oi">def plotC(clusters):<br/>  plt.figure()<br/>  for i in range(k):<br/>    pnts = np.array(clusters[i]['Xp'])<br/>    try:<br/>      plt.scatter(pnts[:,0],pnts[:,1],alpha=0.1,c=clusters[i]['colors'])<br/>    except:<br/>      pass<br/>    c_cluster = clusters[i]['p_mean']<br/>    plt.scatter(c_cluster[0],c_cluster[1],color="black",marker="X")<br/>    clusters[i]['Xp'] = []</span></pre><ul class=""><li id="2ab0" class="na nb iq jp b jq jr ju jv jy nc kc nd kg ne kk nf ng nh ni bi translated"><strong class="jp ir">迭代寻找最佳匹配</strong></li></ul><pre class="kn ko kp kq gt ob oa oc od aw oe bi"><span id="5e8b" class="mj lg iq oa b gy of og l oh oi">for i in range(10):<br/>  compare(clusters)<br/>  cluster_mean(clusters)<br/>  plotC(clusters)</span></pre><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bc742f0822070c7fdfcaa207a41606bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*_CzVD41yOJ3I_atWRRaHnw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Final Cluster Categorization</figcaption></figure></div><div class="ab cl ky kz hu la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="ij ik il im in"><p id="e74d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi ok translated">他的博客介绍了K-Means聚类算法的基本工作原理，并探究了算法背后的数学原理。本阅读旨在快速有效地复习概念。实现是从头开始的，以单独研究每个功能。要了解更多信息，建议在互联网上单独搜索每个主题。</p><blockquote class="ot"><p id="ed35" class="ou ov iq bd ow ox oy oz pa pb pc kk dk translated">感谢阅读。<br/>别忘了点击👏！</p></blockquote></div></div>    
</body>
</html>