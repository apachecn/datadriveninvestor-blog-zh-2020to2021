<html>
<head>
<title>The basic Methods for classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类的基本方法</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/the-basic-methods-for-classification-9c10a961b0ee?source=collection_archive---------13-----------------------#2021-01-10">https://medium.datadriveninvestor.com/the-basic-methods-for-classification-9c10a961b0ee?source=collection_archive---------13-----------------------#2021-01-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/86a744292fa46d97c0a23f08846de564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HRYk_XEgKNJGn-Ue"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@timtrad?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Trad</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="eb2d" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">机器学习理论</h2><div class=""/><div class=""><h2 id="7651" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">线性分类方法</h2></div><p id="98d0" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当我们尝试将我们的数据分类到不同的组中时，我们的预测器<strong class="lg jq"> G(x) </strong>在离散集合<strong class="lg jq"> ζ </strong>中取值，我们可以将输入空间划分为根据分类标记的区域集合。对于线性方法，我们的意思是预测类之间的决策边界是线性的。</p><h1 id="9723" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">指标矩阵的线性回归</h1><p id="c2f7" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">所有响应类别都有一个指示变量。这样如果<strong class="lg jq"> ζ </strong>有<strong class="lg jq"> K </strong>类，就会有<strong class="lg jq"> K </strong>这样的指标<strong class="lg jq"> Yk: k = 1，…，k </strong>与<strong class="lg jq"> Yk=1 </strong> if <strong class="lg jq"> G=K </strong>，else <strong class="lg jq"> 0 </strong>，这些就叫做哑变量。所有这些变量一起收集在一个向量<strong class="lg jq"> Y = (Y1，Y2，…，Yk) </strong>中，并形成一个<strong class="lg jq"> NxK </strong>矩阵，其中<strong class="lg jq"> N </strong>是训练实例的数量。</p><p id="4625" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，我们同时对所有Y列进行线性回归拟合:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/addfcd13e7767faf6f07dbf41b11fca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*BA57u5TorVj-z7yX9CJOpg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Fitted vector, self-generated.</figcaption></figure><p id="3f5a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一个新的观察结果分类如下:</p><ul class=""><li id="8e62" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated">使用拟合的线性回归计算输出向量</li><li id="5d85" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">确定最大的组成部分，并相应地进行分类。</li></ul><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7631aede8e1cf221f683a3811f9ae129.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*rfHFmlBv7Ul-OE9K0IPCuw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The largest component, self-generated.</figcaption></figure><p id="cfd7" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用这个我们为每个类构造一个目标<strong class="lg jq"> <em class="nr"> tk </em> </strong>，其中<strong class="lg jq"> <em class="nr"> tk </em> </strong>是<strong class="lg jq"> <em class="nr"> KxK </em> </strong>单位矩阵的<strong class="lg jq"> <em class="nr"> kth </em> </strong>列。所以我们必须重现每一次观察的目标。我们可以使用最小二乘法求解:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/05f234371afbabfd9f0a87c245954e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ATYCfRJD7SDWhqYvebvcTQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Linear model and closes the target classification function, self-generated.</figcaption></figure><p id="29b6" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正如你可能注意到的，最小化问题与我们用于多重响应线性回归的问题完全相同。</p><p id="8299" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里我们发现一个问题，当K&gt;2时，模型的刚性暗示着类很容易被别人屏蔽，而且随着K的增长它变大。为了解决这个问题，我们可以尝试对我们的输入变量进行多项式变换，但这有时会奏效，并非总是如此。这个模型会变得非常复杂。</p><h1 id="7ea4" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">线性判别分析(LDA)</h1><p id="c8be" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">假设<strong class="lg jq"> f <em class="nr"> k </em> (x) </strong>是类 <strong class="lg jq"> <em class="nr">中<strong class="lg jq"> X </strong> <em class="nr">的类条件密度<strong class="lg jq">G = K</strong></em></em></strong><em class="nr">，设</em> <strong class="lg jq"> <em class="nr"> πk </em> </strong>是类<strong class="lg jq">的先验概率<strong class="lg jq"> <em class="nr"> K </em> </strong>贝叶斯定理的一个简单应用给出:</strong></p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/eef3a697ff7df3756cb4e3c8bc2d5fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*xoc0xievQgwoqPGxpg0R5A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Applying Bayes theorem, self-generated.</figcaption></figure><p id="df4a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所以，拥有<strong class="lg jq"> fk(x) </strong>几乎等同于拥有<strong class="lg jq"> Pr(G=k|X=x) </strong>。许多技术都基于类密度:</p><ul class=""><li id="1fdf" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated">线性和二次判别分析使用高斯密度。</li><li id="64cc" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">高斯混合模型允许非线性决策边界。</li><li id="822d" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">朴素贝叶斯假设每个类密度都是边际密度的乘积，每个类中的输入都是条件独立的。</li></ul><p id="2726" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果我们将每个类密度建模为多元高斯:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/657e49e0298f4a1d2af2c7ed9a95d6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*PgbwPHkjEBNU3YOLNI004w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Class density as multivariate Gaussian density, self-generated.</figcaption></figure><p id="8234" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">线性判别分析(LDA)假设类别具有共同的协方差矩阵<strong class="lg jq"> <em class="nr"> ∑k = ∑∀k </em> </strong>。在比较两个类别<strong class="lg jq"> <em class="nr"> k </em> </strong>和<strong class="lg jq"> <em class="nr"> l </em> </strong>时，查看对数比就足够了，并且:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/60c899723ba68c2a1bf51e3d6b093431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*anH-A9e_QGRBdbkHQ2j_Hw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LDA log-ratio, self-generated.</figcaption></figure><p id="a510" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">相等的协方差矩阵使得归一化因子以及指数中的二次部分相消。该线性对数优势函数意味着两个类别之间的判定边界在<strong class="lg jq"> <em class="nr"> x </em> </strong>中在<strong class="lg jq"> <em class="nr"> p </em> </strong>维度上是线性的。所有类别之间都是如此，因此所有决策边界都是线性的。</p><p id="e5ff" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">线性判别函数</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/10d06457275b97354d48f9073e3ec6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*upo0cdOJRWcpMXh0MK6FHQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LDA functions, self-generated</figcaption></figure><p id="20f6" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">等同于具有<strong class="lg jq"><em class="nr">G(x)= arg max _ kδ_ k(x)</em></strong>的决策规则，在实践中我们将需要使用我们的训练数据来估计高斯分布:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b251dc9d181bc93a7049e6a63bfb918c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*RBXrL0m6xuEJPKLsKqLwow.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Estimating gaussian distribution from data, self-generated.</figcaption></figure><p id="18c6" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在2个类的情况下，由于通过最小二乘法的LDA方向的推导没有对特征使用高斯假设，所以它可以扩展到高斯数据之外，但是截距仍然需要该假设。对于两个以上的类别，LDA不同于类别指示矩阵的线性回归，并且避免了掩蔽问题。</p><p id="6aad" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果不假设<strong class="lg jq"> <em class="nr"> ∑k </em> </strong>相等，那么我们用来简化<strong class="lg jq"> <em class="nr"> fk </em> </strong>的抵消就不会发生，我们得到二次判别分析(QDA):</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/82c0ee05cb4db47efb40f4da1655b0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atQREJ0rrTnqbujbBsP-3w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Quadratic discriminant analysis, self-generated.</figcaption></figure><p id="42f9" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，类之间的决策边界变成二次的<strong class="lg jq"> <em class="nr"> {x:δ_k(x)=δ_l(x)}。</em> </strong></p><h2 id="a6f0" class="nz mb jg bd mc oa ob dn mg oc od dp mk ln oe of mm lr og oh mo lv oi oj mq jm bi translated">正则判别分析</h2><p id="f47c" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">该技术提出了LDA和QDA的组合，允许像LDA中那样将QDA的单独系数向公共协方差收缩。正则化的协方差矩阵如下所示:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5cd93064530a532ab3ef48204b71c624.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XBX0A_ap0qx9hEwzg8kyIw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Regularized covariance matrices, self-generated.</figcaption></figure><p id="e493" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中估计的<strong class="lg jq"> <em class="nr"> ∑ </em> </strong>是LDA中的汇集协方差矩阵，并且<strong class="lg jq"><em class="nr">α∈【0，1】</em></strong>允许LDA和QDA之间的所有模型。</p><p id="1d32" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">另一个选项是允许<strong class="lg jq"> <em class="nr"> ∑ </em> </strong>估计值向标量协方差收缩:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/95195d7cd6b99bf47707454db2897e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*F1cJJAzgGydjlaHI_cIOww.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Regularized shrunk covariance matrices, self-generated.</figcaption></figure><p id="5426" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中<strong class="lg jq"> <em class="nr"> γ∈[0，1]。</em> </strong>用<strong class="lg jq"><em class="nr">【∑^(γ】</em></strong>替换估计的<strong class="lg jq"> <em class="nr"> ∑ </em> </strong>导致更一般的协方差<strong class="lg jq"><em class="nr">【∑^(α,γ】</em></strong>由协方差对索引。这导致更好的一般化，并允许更容易地解释系数。</p><h2 id="0ee3" class="nz mb jg bd mc oa ob dn mg oc od dp mk ln oe of mm lr og oh mo lv oi oj mq jm bi translated">降秩LDA</h2><p id="d818" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">该限制将允许我们查看数据的信息性低维投影。<strong class="lg jq"> <em class="nr"> k </em> </strong>质心un<strong class="lg jq"><em class="nr">P</em></strong>-维输入空间位于一个维数≤ <strong class="lg jq"> <em class="nr"> K-1 </em> </strong>的附属子空间中，如果P远大于<strong class="lg jq"> <em class="nr"> k </em> </strong>，这将是一个相当大的维数下降。此外，在定位最近的质心时，我们可以忽略与该子空间正交的距离，因为它们对每个类的贡献是相等的。这样，我们将维度投影<strong class="lg jq"> <em class="nr"> X* </em> </strong>降低到子空间<strong class="lg jq"><em class="nr">【H _(k-1)</em></strong>中，在这里进行距离比较。</p><ul class=""><li id="4efc" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated">如果<strong class="lg jq"> <em class="nr"> k = 3 </em> </strong>，我们可以在一个<strong class="lg jq"> <em class="nr">二维</em> </strong>表示中查看数据，添加颜色或尺寸的第三维。</li><li id="fcd7" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">在<strong class="lg jq"> <em class="nr"> k &gt; 3 </em> </strong>的情况下，我们要求一个<strong class="lg jq"> <em class="nr"> L &lt; k-1 </em> </strong>维空间<strong class="lg jq"><em class="nr">h _ l</em></strong><strong class="lg jq"><em class="nr">⊆k _(k-1)</em></strong>最优LDA。最优定义为尽可能分散质心的<strong class="lg jq"> <em class="nr"> H_L </em> </strong>。</li></ul><p id="2d12" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">维度是有序的，因此我们可以按顺序计算额外的维度。寻找LDA的最佳子空间序列包括:</p><ul class=""><li id="00e3" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated">计算类质心的<strong class="lg jq"> <em class="nr"> K x p </em> </strong>矩阵<strong class="lg jq"> <em class="nr"> M </em> </strong>和公共协方差矩阵<strong class="lg jq"> <em class="nr"> W </em> </strong>。</li><li id="104e" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">使用<strong class="lg jq"><em class="nr"/></strong>的本征分解计算<strong class="lg jq"> <em class="nr"> M* = MW^(1/2) </em> </strong></li><li id="0b45" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">计算<strong class="lg jq"> <em class="nr"> B* </em> </strong>，<strong class="lg jq"> <em class="nr"> M* </em> </strong>的协方差矩阵及其本征分解<strong class="lg jq"><em class="nr">b * = v*d_bv*^t.</em></strong><strong class="lg jq"><em class="nr">ve *</em><strong class="lg jq"><em class="nr">【v *</em></strong>的列从第一个到最后一个顺序定义最优子空间的坐标。</strong></li></ul><p id="3e83" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第1个判别变量由下式给出:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8d98f9412dafe77deaab83b92354b7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*nmxhWn2XYfZ5aCSA7nZd5g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">lth discriminant variable, self-generated.</figcaption></figure><p id="9932" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">The between-class variance is the variance of the class means of <strong class="lg jq"><em class="nr">Z</em></strong>, and the within-class variance is the pooled variance about the means.</p><p id="1565" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Although we separate the most variance variables, our data will still overlap. By taking the covariance into account, a direction with minimum overlap is found.</p><p id="4ca9" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">The between-class variance of <strong class="lg jq"><em class="nr">Z</em></strong> is <strong class="lg jq"><em class="nr">a^TBa</em></strong>, and the within-class variance <strong class="lg jq"><em class="nr">a^TWa </em></strong>where <strong class="lg jq"><em class="nr">W </em></strong>is defined earlier and<strong class="lg jq"><em class="nr"> B</em></strong> is the covariance matrix of the centroid matrix <strong class="lg jq"><em class="nr">M</em></strong>. <strong class="lg jq"><em class="nr">B+W=T</em></strong>, where <strong class="lg jq"><em class="nr">T</em></strong> is the total covariance matrix of<strong class="lg jq"><em class="nr"> X</em></strong>, ignoring class information.</p><p id="fac0" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Then the problem amounts to maximizing the Rayleigh quotient,</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d7142361f8ffa7f3f4f1ebd551a3ce30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*asFaDuLGw-f5ZlsidJXWMg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Rayleigh quotient maximization, self-generated.</figcaption></figure><p id="81ac" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">This is a generalized eigenvalue problem, with a given by the largest eigenvalue of <strong class="lg jq"><em class="nr">W^(-1)B</em></strong>. Then the first direction <strong class="lg jq"><em class="nr">a1 </em></strong>is identical to <strong class="lg jq"><em class="nr">v1 </em></strong>and <strong class="lg jq"><em class="nr">a2 </em></strong>is the orthogonal direction to <strong class="lg jq"><em class="nr">a1 </em></strong>such that <strong class="lg jq"><em class="nr">a2^(T)Ba2/a2^(T)Wa2 </em></strong>is maximized.</p><p id="922f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Instead of being created for data variable reduction, this technique works well in classification problems. It’s performed by limiting the distance to centroids. The centroids of a gaussian classification can be shown to lie in an L-dimensional subspace <strong class="lg jq"><em class="nr">R^p. </em></strong>Fitting this by maximum likelihood and then constructing the posterior probabilities using Bayes theorem rounds it.</p><p id="1088" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Gaussian classification dictates the <strong class="lg jq"><em class="nr">log</em></strong> <strong class="lg jq"><em class="nr">π_k </em></strong>correction in distance calculation, that is because the misclassification rate is based on the area of centroids that overlap and if <strong class="lg jq"><em class="nr">π_k </em></strong>are equal, both classes will receive the same amount of errors. If not, we can end up with misclassification errors caused by getting class centroids near the cut points.</p><h1 id="9a91" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">Logistic Regression</h1><p id="8576" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">The objective of logistic regression is to model the posterior probabilities of K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0,1].</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b2f6827cb090df32b4956638a0d3daef.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*3KV6VPlx2VumfEvp7LZSzg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Logistic regression, self-generated.</figcaption></figure><p id="b684" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Here K-1 is the number of logit transformations(log-odds). The last class is used as the denominator in the log-odds ratio. We can obtain <strong class="lg jq"><em class="nr">Pr(G=k|X=x) </em></strong>using exponents</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/664153ebc35de68dc244e322a5fc3738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RA9ho2kWWhVIidC-rmR3Ng.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Obtaining <strong class="bd mc"><em class="oq">Pr(G=k|X=x), </em></strong><em class="oq">self-generated.</em></figcaption></figure><p id="028d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了简化符号，我们将使用θ作为整个参数集θ=<strong class="lg jq"><em class="nr"/></strong>，从而表示概率<strong class="lg jq"><em class="nr">pr(g = k | x = x)= PK(x:</em></strong>θ<strong class="lg jq"><em class="nr">)。</em>T11】</strong></p><div class="ip iq gp gr ir or"><a href="https://www.datadriveninvestor.com/2020/11/19/how-machine-learning-and-artificial-intelligence-changing-the-face-of-ecommerce/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd jq gy z fp ow fr fs ox fu fw jp bi translated">机器学习和人工智能如何改变电子商务的面貌？|数据驱动…</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">电子商务开发公司，现在，整合先进的客户体验到一个新的水平…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ix or"/></div></div></a></div><h2 id="32b8" class="nz mb jg bd mc oa ob dn mg oc od dp mk ln oe of mm lr og oh mo lv oi oj mq jm bi translated">拟合逻辑回归模型</h2><p id="f1af" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">逻辑模型通常由最大似然拟合，使用G给定X的条件似然。由于P(G|X)指定了条件分布，因此多项式分布是合适的。N次观察的对数似然为:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3c15ebf83df6bec3f4fd8171aef47c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*g-P50lV4y2dCs2iikyFFbQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Log-likelihood for N observations, self-generated.</figcaption></figure><p id="0c3e" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中<strong class="lg jq"><em class="nr">Pk(Xi:</em></strong>θ<strong class="lg jq"><em class="nr">)= Pr(G = k | X = Xi，</em></strong>θ<strong class="lg jq"><em class="nr">)。</em> </strong></p><p id="c421" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在两个类的情况下，算法很简单，通过一个<strong class="lg jq"> <em class="nr"> 0，1 </em> </strong>响应<strong class="lg jq"> <em class="nr"> yi </em> </strong>来编码两个类<strong class="lg jq"> <em class="nr"> gi = {1，2} </em> </strong>其中:</p><ul class=""><li id="6e23" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated"><strong class="lg jq"><em class="nr">yi = 1</em></strong>if<strong class="lg jq"><em class="nr">gi = 1</em></strong></li><li id="3b7c" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated"><strong class="lg jq"><em class="nr">yi = 0</em></strong>if<strong class="lg jq"><em class="nr">gi = 2</em></strong></li></ul><p id="4526" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">设<strong class="lg jq"><em class="nr">P1(x:</em></strong>θ<strong class="lg jq"><em class="nr">)= p(x:</em></strong>θ<strong class="lg jq"><em class="nr">)</em></strong>和<strong class="lg jq"><em class="nr">p2(x:</em></strong>θ<strong class="lg jq"><em class="nr">)= 1-p(x:</em></strong>θ<strong class="lg jq"><em class="nr">)。</em> </strong>对数似然可以写成</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/0a1e3360cd35835fc69f3540fa05b379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJ6QAsfmE7CiS1ihPIQaLg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Log-likelihood for N observations, self-generated.</figcaption></figure><p id="3a7d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里<strong class="lg jq"> <em class="nr"> β = {β_01，β_1}，</em> </strong>并且我们假设输入向量<strong class="lg jq"><em class="nr"/></strong>包含常数项<strong class="lg jq"> 1 </strong>以容纳截距。为了最大化对数似然，我们将导数设为<strong class="lg jq"> 0 </strong>。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/f5625e2ece3d0427ed7e728323696bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*qdRgqkQobvnKuEUjyrHNjA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The first derivative, self-generated.</figcaption></figure><p id="4e2c" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些是在<strong class="lg jq"> <em class="nr"> β中非线性的p+1个方程。</em> </strong>求解他们的方程组，我们可以使用牛顿-拉夫森算法，这种算法需要二阶导数或者海森矩阵。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/d6ba9cf312b5edbcfc20ca3fbf6d4b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*jzPQ9yiBaHT8F4lPc0n8_g.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The second derivative, self-generated.</figcaption></figure><p id="97ed" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从<strong class="lg jq">开始<em class="nr">β旧</em>开始</strong>，单次牛顿更新是:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/01202139ddad83839f9452ca7205c2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*35g-qQg_PXjw2EZxMTKfxg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd mc"><em class="oq">βold</em></strong><em class="oq"> Newton update, self-generated</em><strong class="bd mc"><em class="oq">.</em></strong></figcaption></figure><p id="e012" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中导数在<strong class="lg jq"> <em class="nr"> βold处评估。</em> </strong>在矩阵符号中，我们可以把它写成:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/70b7b25371bb5ee77afc89375c93fd62.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*jRCH30OeRvCacINKaF_1Rg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Derivatives in matrix notation, self-generated</figcaption></figure><p id="b15f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，牛顿步骤看起来像这样:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/e56699f82cf98df42ad90a67a9d842fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPjtbpC_lAinAxbi2N0JXg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Newton steps in matrix notation, self-generated.</figcaption></figure><p id="7ffc" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">z称为调整后的响应。该模型是一个迭代的加权最小二乘(IRLS ),因为在每次迭代中，它解决了加权最小二乘问题:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/9ccfba321234de85b554917aa3554345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*eqcvH7OoGiNLJCkNWMn3Qw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">IRLS maximization problem, self-generated.</figcaption></figure><p id="b0ac" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">通常<strong class="lg jq"> <em class="nr"> β=0 </em> </strong>是迭代过程的良好起点。</p><h2 id="a673" class="nz mb jg bd mc oa ob dn mg oc od dp mk ln oe of mm lr og oh mo lv oi oj mq jm bi translated">二次近似和推理</h2><p id="5105" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">最大似然参数估计值<strong class="lg jq"> <em class="nr">、β^ </em> </strong>满足自洽关系，它们是加权最小二乘拟合的系数，其中响应为:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi po"><img src="../Images/c89f7d877fccd2db856b901b51f3e05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*qz7s9Pm0QKuBDXG1v7MZLQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">The weighted least-squares response, self-generated.</figcaption></figure><p id="f5fd" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">权重分别是<strong class="lg jq"> <em class="nr"> wi = pi^(1-pi^) </em> </strong>，两者都取决于<strong class="lg jq"><em class="nr"/></strong>本身。与最小二乘法的联系为我们提供了很多:</p><ul class=""><li id="66ce" class="nc nd jg lg b lh li lk ll ln ne lr nf lv ng lz nh ni nj nk bi translated">加权残差平方和是我们熟悉的皮尔逊卡方统计量。</li><li id="84e6" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">渐近似然理论说，如果模型是正确的，<strong class="lg jq"> <em class="nr"> β </em> </strong>是一致的，所以收敛到真的<strong class="lg jq"> <em class="nr"> β </em>。</strong></li><li id="597d" class="nc nd jg lg b lh nl lk nm ln nn lr no lv np lz nh ni nj nk bi translated">中心极限定理表明，<strong class="lg jq"> <em class="nr"> β^ </em> </strong>的分布收敛于<strong class="lg jq"> <em class="nr"> N(β,(X^TWX)^(-1)).</em> </strong></li></ul><p id="6352" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于需要迭代，逻辑回归模型的建模成本可能很高，因此存在一些流行的捷径来避免迭代，其中一些是Rao分数检验到Wald检验，它们基于当前模型的最大似然拟合。</p><h2 id="8786" class="nz mb jg bd mc oa ob dn mg oc od dp mk ln oe of mm lr og oh mo lv oi oj mq jm bi translated">L1正则化逻辑回归</h2><p id="b96f" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">L1罚函数可用于任何线性回归模型。对于逻辑回归，我们可以将其添加为:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pp"><img src="../Images/ae0e5f21d86191cf515c14c463970dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*D-PU20EQtvxq4LhvBQiEzw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">L1-regularization for logistic regression, self-generated.</figcaption></figure><p id="2a48" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以用IRLS通过二次近似来解决这个问题。非零系数变量的得分方程具有以下形式:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/012ff5594663f26111558fd094a91639.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*ydEsnGFaGUlY6JYpAd_hNw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Non-zero coefficient variables, self-generated.</figcaption></figure><p id="719f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">它推广了LAR。lasso的LAR路径算法更困难，因为系数是逐段平滑的，而不是线性的。然而，可以用二次近似法来估计进展。</p><h1 id="490e" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">逻辑回归与线性判别分析</h1><p id="42a9" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">这两个模型似乎是相同的，它们具有完全相同的形式，但是逻辑回归更通用，因为它做出的假设更少。</p><p id="2006" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">逻辑回归采用密度函数<strong class="lg jq"> <em class="nr"> Pr(x) </em> </strong>，并通过最大化条件似然来拟合<strong class="lg jq"> <em class="nr"> P(G|X) </em> </strong>的参数。同时，LDA在先验密度的基础上，通过最大化全对数似然来确定参数。</p><p id="ee90" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">通过依赖更多的假设，逻辑回归有更多关于参数的信息，并且可以获得更低的估计方差。</p><p id="4517" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">远离决策边界的观测值对于协方差矩阵估计是重要的，这使得LDA对显著异常值不具有鲁棒性。</p><p id="1bd3" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果一个平面上的数据可以用一条线分开，那么logistic回归就不会收敛到最优解，否则LDA就会做到。</p><h1 id="471b" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">结论</h1><p id="a419" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">逻辑回归和线性判别分析都是执行简单分类的好方法。逻辑回归是一个更稳健的模型，因为它的假设更少，但在实践中，这两个模型表现相似。</p></div><div class="ab cl pr ps hu pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="ij ik il im in"><p id="2762" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是我的第37篇文章，我将在GitHub、Twitter和Medium ( <a class="ae jd" href="https://medium.com/u/48c8d3ce491d?source=post_page-----7053db93ba6----------------------" rel="noopener"> Adrià Serra </a>)上发布这个挑战的进展。</p><p id="365a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">https://twitter.com/CrunchyML<a class="ae jd" href="https://twitter.com/CrunchyML" rel="noopener ugc nofollow" target="_blank"/></p><p id="4c09" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg jq">访问专家视图— </strong> <a class="ae jd" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="lg jq">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>