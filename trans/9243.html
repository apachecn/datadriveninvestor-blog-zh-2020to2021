<html>
<head>
<title>K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k均值聚类</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/k-means-clustering-c92463d5fa0e?source=collection_archive---------5-----------------------#2021-02-04">https://medium.datadriveninvestor.com/k-means-clustering-c92463d5fa0e?source=collection_archive---------5-----------------------#2021-02-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a707d75921e102b3e2f54a5a85d65c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EiFZ8PNUDmfJcOl3.png"/></div></div></figure><p id="d762" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">简介</strong></p><p id="a641" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">K-Means聚类是一种无监督的机器学习算法，用于解决机器学习中的聚类问题。在现实世界的场景中，可能存在未标记的数据来解决问题。在这种情况下，K-means算法在解决问题中起着至关重要的作用。由此将未标记的数据分成子组。其中组可以被称为集群。可以通过具有相似大小、形状、度量、特征的数据来进行分组。最后，将数据分组到单独的群集中。这里是K值，需要创建多少个集群。有一个单独的技术来确定k值。我们将在本文的后面讨论。</p><p id="69de" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">什么是K-Means算法？</strong></p><p id="6723" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">K-Means聚类是一种无监督学习算法，它将未标记的数据集分组到不同的簇中。这里K定义了在这个过程中需要创建的预定义的集群的数量，如果K=2，将有两个集群，对于K=3，将有三个集群，以此类推。</p><p id="3dba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它是一种迭代算法，将未标记的数据集划分为k个不同的聚类，使得每个数据集只属于一个具有相似属性的组。</p><p id="9fc2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它允许我们将数据聚类到不同的组中，并且提供了一种方便的方法来发现未标记数据集中的组的类别，而不需要任何训练。</p><p id="30a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一种基于质心的算法，其中每个聚类都与一个质心相关联。该算法的主要目标是最小化数据点和它们对应的聚类之间的距离之和。</p><p id="8216" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该算法将未标记的数据集作为输入，将数据集划分为k个聚类，并重复该过程，直到没有找到最佳聚类。在这个算法中，k的值应该是预先确定的。</p><p id="0f8e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">k-means聚类算法主要执行两项任务:</p><ul class=""><li id="9be8" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">通过迭代过程确定K个中心点或质心的最佳值。</li><li id="e780" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">将每个数据点分配到其最近的k中心。靠近特定k中心的那些数据点创建一个聚类。</li></ul><p id="29a5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，每个聚类都有一些共性的数据点，并且远离其他聚类。</p><p id="ef9e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下图解释了K均值聚类算法的工作原理:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lk"><img src="../Images/7df4250ca676df48caf387d9ebaefc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*-fhpd7yMDfC1hO3_.png"/></div></div></figure><p id="2d80" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">K-Means算法是如何工作的？</strong></p><p id="c17c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们的目标是在数据集中找到几个相似的组，比如:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/0beae80f777e17bd729a38df39533cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*JD9tdUdhph32Ytik.png"/></div></figure><p id="d77a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">K-Means从k个随机放置的质心开始。质心，顾名思义，是星团的中心点。例如，这里我们添加了四个随机质心:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/a11263954e1a9dbf68d277871f21aa9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*qBrQqkhwAfsmQlOf.png"/></div></figure><p id="edaf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，我们将每个现有数据点分配到其最近的质心:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/9085bf87541534f038ab330bb9e17d5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*dpXvcl5aoi8uZKvy.png"/></div></figure><p id="b06d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在指定之后，我们移动质心到指定给它的点的平均位置。请记住，质心应该是群集的中心点:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/4d733c6a50d890765077892b92790fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*kDcmtLOFehU9PkaP.png"/></div></figure><p id="5eb1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每次重新定位质心后，当前的迭代就会结束。我们重复这些迭代，直到多个连续迭代之间的赋值停止变化:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/cba825c46a8857db7045febe845ac2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*pRrCfBZdFoCR_FDz.png"/></div></figure><p id="e82f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当算法终止时，这四个聚类如预期的那样被找到。</p><p id="5c7c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> K均值聚类方法:</strong></p><p id="5b2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果给定了K，K-means算法可以按以下步骤执行:</p><ul class=""><li id="8d07" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">将对象划分成k个非空子集</li><li id="fb5e" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">识别当前分区的聚类质心(平均点)。</li><li id="7b1a" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">将每个点分配给特定的聚类</li><li id="83f2" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">计算每个点的距离，并将点分配到距质心距离最小的聚类中。</li><li id="0a16" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">重新分配点后，找到新形成的群集的质心。</li></ul><p id="d755" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">循序渐进的过程:</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/6d6c9342810c94fccd257deb292f9da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*xFhQdPTGJRiArT4w.png"/></div></figure><p id="ad6b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">确定集群的‘k’个数量</strong></p><p id="c393" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">肘方法是寻找最佳聚类数的最流行的方法之一。这种方法使用了WCSS值的概念。WCSS代表聚类平方和内的<strong class="ka ir">，它定义了一个聚类内的总变化量。计算WCSS值(针对3个集群)的公式如下所示:</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ls"><img src="../Images/b511c5de2c505506a540b59300ef80d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a7DregBsjkGcPVTK.png"/></div></div></figure><p id="9af7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在WCSS的上述公式中，</p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/4eb89b49542f669e50f47f8b561fb459.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/0*NqnSuozxxlGxaUUC.png"/></div></figure><p id="2069" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它是聚类1中每个数据点与其质心之间距离的平方和，其他两项也是如此。</p><p id="310e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了测量数据点和质心之间的距离，我们可以使用任何方法，如欧几里德距离或曼哈顿距离。</p><p id="41c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们开始使用Python实现确定K值。这里使用Google collab来实现。</p><p id="6e43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">导入库</strong></p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="7b86" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">import</strong> pandas <strong class="lv ir">as</strong> pd<br/><strong class="lv ir">import</strong> numpy <strong class="lv ir">as</strong> np<br/><strong class="lv ir">from</strong> google.colab <strong class="lv ir">import</strong> files<br/>uploaded = files.upload()</span></pre><p id="6d4f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">加载数据集</strong></p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="aec7" class="lz ma iq lv b gy mb mc l md me">import io<br/>train_data = pd.read_csv(io.StringIO(uploaded['Mall_Customers.csv'].decode('utf-8')))</span></pre><p id="388c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">显示数据集中的前5条记录</strong></p><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/5730ee6b557a20cc43865afb83cea72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*sRDK1bvD-9-pCLNB.png"/></div></figure><p id="d563" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">确定X值</strong></p><p id="fcf1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">计算年收入和支出分数。</p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="0875" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">X</strong>= train_data.iloc[:,3:]<br/><strong class="lv ir">X</strong></span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/93b61a813d6f93da635004871a18cf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/0*yCatG2o02chxAz9y.png"/></div></figure><p id="5b2f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">确定K值</strong></p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="8a28" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">from</strong> sklearn.cluster <strong class="lv ir">import</strong> KMeans<br/><strong class="lv ir">from</strong> matplotlib <strong class="lv ir">import</strong> pyplot <strong class="lv ir">as</strong> plot <br/>wcss = []</span><span id="37f5" class="lz ma iq lv b gy mh mc l md me"><strong class="lv ir">for</strong> i <strong class="lv ir">in</strong> range (1, 15):<br/>  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)<br/>  kmeans.fit(X)<br/>  wcss.append(kmeans.inertia_)<br/>plot.plot(range(1,15), wcss)<br/>plot.title('The Elbow Method')<br/>plot.xlabel('Number of clusters')<br/>plot.ylabel('WCSS')<br/>plot.show()</span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/97a08e2a171ad03f591ae63154741b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/0*MxEWMhWnwCsFe4UQ.png"/></div></figure><p id="b702" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">弯曲的尖点或图中的点看起来像一只手臂，那么该点被认为是K的最佳值。因此，最佳K值是5。</p><p id="446d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们继续应用K值。</p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="84ea" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">kmeans</strong> = KMeans(n_clusters=5, <strong class="lv ir">init</strong>='k-means++',  random_state=42)<br/><strong class="lv ir">y_kmeans</strong>= kmeans.fit_predict(X)<br/><strong class="lv ir">y_kmeans</strong></span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/a07efc91907fd54efbbf38aa7e4dc9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/0*qmNokm1WgeR0fzyR.png"/></div></figure><p id="0979" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">可视化</strong></p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="4c03" class="lz ma iq lv b gy mb mc l md me">plot.scatter(X[y_kmeans == 0]['Annual Income (k$)'], X[y_kmeans == 0]['Spending Score (1-100)'], s = 100, c = 'red', label = 'Cluster 1')<br/>plot.scatter(X[y_kmeans == 1]['Annual Income (k$)'], X[y_kmeans == 1]['Spending Score (1-100)'], s = 100, c = 'blue', label = 'Cluster 2')<br/>plot.scatter(X[y_kmeans == 2]['Annual Income (k$)'], X[y_kmeans == 2]['Spending Score (1-100)'], s = 100, c = 'green', label = 'Cluster 3')<br/>plot.scatter(X[y_kmeans == 3]['Annual Income (k$)'], X[y_kmeans == 3]['Spending Score (1-100)'], s = 100, c = 'cyan', label = 'Cluster 4')<br/>plot.scatter(X[y_kmeans == 4]['Annual Income (k$)'], X[y_kmeans == 4]['Spending Score (1-100)'], s = 100, c = 'magenta', label = 'Cluster 5')<br/>plot.scatter(kmeans.cluster<em class="mk">_centers_</em>[:, 0], kmeans.cluster<em class="mk">_centers_</em>[:, 1], s = 300, c = 'yellow', label = 'Centroids')<br/>plot.title('Clusters of customers')<br/>plot.xlabel('Annual Income (k$)')<br/>plot.ylabel('Spending Score (1-100)')<br/>plot.legend()<br/>plot.show()</span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/73ddb325028b746bd42e8fe32dd80aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/0*OwlCUbsx9_13Qhd5.png"/></div></figure><p id="ac7a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出图像清楚地显示了五个不同颜色的簇。聚类是在数据集的两个参数之间形成的；客户年收入和支出。我们可以根据要求或选择改变颜色和标签。我们还可以从上述模式中观察到一些要点，如下所示:</p><ul class=""><li id="4bba" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">聚类1显示平均工资和平均支出的客户</li><li id="960f" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">聚类2显示客户收入高但支出低</li><li id="57af" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">集群3显示了低收入和低支出</li><li id="909a" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">聚类4显示了高消费的低收入客户</li><li id="b18e" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">Cluster5显示了高收入和高支出的客户，因此可以将他们归类为目标客户，这些客户可能是商场所有者最有利可图的客户。</li></ul><p id="73f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在展示我们如何使用轮廓值方法来找到“k”的值。</p><pre class="ll lm ln lo gt lu lv lw lx aw ly bi"><span id="a82c" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">from</strong> sklearn.metrics <strong class="lv ir">import</strong> silhouette_samples, silhouette_score<br/><strong class="lv ir">import</strong> seaborn <strong class="lv ir">as</strong> sns <br/>clusters_range = range (2,15)<br/>results = []<br/><strong class="lv ir">for</strong> i <strong class="lv ir">in</strong> clusters_range:<br/>  cluster = KMeans(n_clusters=i, init='k-means++', random_state=42)<br/>  cluster_labels= cluster.fit_predict(X)<br/>  silhouette_avg =  silhouette_score(X, cluster_labels)<br/>  results.append([i, silhouette_avg])<br/>result =  pd.DataFrame(results, columns=['n_clusters','silhouette_score'])<br/>pivot = pd.pivot_table(result,index='n_clusters', values='silhouette_score')<br/>plot.figure()<br/>sns.heatmap(pivot, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm.rocket_r)<br/>plot.tight_layout()</span></pre><figure class="ll lm ln lo gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/000649a662e1aee14951492d429a8106.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/0*uNV0sBCNrH4ywBny.png"/></div></figure><p id="71a6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果我们观察，我们得到n = 5时的最佳聚类数，因此我们可以最终选择k = 5的值。</p><p id="a691" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">使用k均值聚类的优势</strong></p><ul class=""><li id="a483" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">容易实现。</li><li id="0662" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">对于大量的变量，K-Means在计算上可能比层次聚类更快(如果K很小)。</li><li id="ec06" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">k-Means可以产生比层次聚类更高的聚类。</li></ul><p id="df56" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">使用k-means聚类的缺点</strong></p><ul class=""><li id="1108" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">难以预测群集的数量(K值)。</li><li id="c504" class="kw kx iq ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">初始种子对最终结果有很大影响。</li></ul><p id="af8d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">希望这篇文章能让你更好的理解K-Means机器学习模型。</p><p id="bad1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">另一篇有趣的文章再见。</p><p id="9477" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">快乐学习:)</p></div></div>    
</body>
</html>