<html>
<head>
<title>Understanding Core Data Science Algorithms: k-means and k-medoids clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解核心数据科学算法:k-means和k-medoids聚类</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/understanding-core-data-science-algorithms-k-means-and-k-medoids-clustering-12c4df80964b?source=collection_archive---------10-----------------------#2021-01-20">https://medium.datadriveninvestor.com/understanding-core-data-science-algorithms-k-means-and-k-medoids-clustering-12c4df80964b?source=collection_archive---------10-----------------------#2021-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8fb5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">数据科学算法在数据科学家的生活中扮演着重要的角色。获取算法方面的知识和技能被认为是解决手边任何一种任务的核心技能。</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/c8db2d8681056766c114768631736ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2UNzSpvailCbfQ8ZX57_g.jpeg"/></div></div></figure><p id="0fa3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi ky translated"><span class="l kz la lb bm lc ld le lf lg di"> C </span>聚类是统计数据分析的主要技术之一。</p><p id="2a34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">顾名思义，“聚类”被定义为将相似的对象聚集到不同的组中或将数据集分布到具有定义的距离度量的子集的过程。</p><p id="1321" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl"> k-means </em>聚类被吹捧为每个数据科学家工具箱中应该有的基础算法。由于其非凡的特性，该算法在<a class="ae lh" href="https://www.dasca.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">数据科学行业</strong> </a>中的受欢迎程度扩大。</p><ul class=""><li id="0717" class="li lj iq jp b jq jr ju jv jy lk kc ll kg lm kk ln lo lp lq bi translated">简单的</li><li id="5c02" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">快的</li><li id="eb1e" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">高效的</li></ul><h1 id="4f1f" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated"><strong class="ak">如何运作？</strong></h1><p id="e4af" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated"><em class="kl"> k-means和k-medoids是在分区聚类算法中使用的</em>方法，其功能基于指定组的初始数量，或者更准确地说，通过在组之间重新分配对象来迭代工作。</p><p id="5264" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该算法首先将所有点分成已经选定数量的聚类。该过程通过测量点和每个聚类中心之间的距离来进行。并且因为k-means只能在欧几里得空间中起作用，所以算法的功能是有限的。尽管该算法存在缺陷或不足，但k-means仍然被证明是用于聚类的最强有力的工具之一。这些应用广泛应用于多维领域——物理科学、自然语言处理(NLP)和医疗保健。</p><p id="08fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k-means算法的扩展包括其k-中心的更智能的起始位置，这进一步允许更多可变的聚类大小。当这种情况发生时，产生的距离将大于欧几里德距离。</p><p id="1dce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将讨论不同的其他方法，如CLARANS、CLARA和PAM，这些方法有助于整合超过欧几里德距离的测量距离。</p><p id="7e92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，在讨论其他方法之前，让我们先解决k-means聚类的缺点。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/1272491d6fc49cef1d1cdb773555324f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*m5EGpP-ZiH7lfxrt.jpg"/></div></figure><p id="b83d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最常见的情况是，异常值是由欺诈行为、人为错误和机械故障引起的。也可以在k-means聚类中看到。首先，需要在数据集中应用k-means聚类算法，然后您可以从每个聚类开始<a class="ae lh" href="https://www.dasca.org/world-of-big-data/article/identifying-and-removing-outliers-using-python-packages" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">识别离群值</strong> </a>。用于识别或检测数据集中异常值和异常值的基于距离的方法和基于聚类的方法。</p><p id="41ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主要目的是首先检测异常值，然后将其移除，使聚类更加可靠。</p><h1 id="4521" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">下面是预测k均值聚类失败的几点:</h1><ul class=""><li id="c0e2" class="li lj iq jp b jq mu ju mv jy na kc nb kg nc kk ln lo lp lq bi translated">当群集的大小和密度不同时，无法正常工作。</li><li id="3975" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">预测精确的质心数量来划分数据变得困难。</li><li id="8bec" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">k形心的初始位置会影响结果。</li><li id="77a1" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">质心是数据集中的一个假想点，因此可能价值较小。</li><li id="e13c" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">对维度的比例敏感，因此重新调整数据可能会变得困难。</li><li id="16ac" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">分割点时利用欧几里得距离。然而，它在高维设置中可能变得无效，因为所有点彼此之间的距离变得相等。</li><li id="7d71" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">即使分区毫无意义，该算法也会对空间进行分区。</li></ul><h1 id="2d6f" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">【medoids周围划分(PAM)算法</h1><p id="4528" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">除了聚类的平均值，您还可以使用medoid进行分区，或者使用位于聚类中心点的数据点。据说medoid与一个簇中的所有点的相异度最小。Medoid对数据集中的异常值不太敏感。</p><p id="2190" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae lh" href="https://medium.com/swlh/top-five-methods-to-identify-outliers-in-data-2777a87dd7fe" rel="noopener">机器学习(ML) </a>中的无监督学习下，聚类算法证明了自己。k-means背后的一个主要思想是，我们希望向我们已经拥有的数据添加新的点(k)——这些点中的每一个都被称为质心。k-means算法是最简单的<strong class="jp ir">数据科学算法</strong>之一，每个数据科学家的工具箱中都必须有。</p><p id="8ce9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，这些分区可以使用任意距离，而不总是依赖欧几里德距离。这是《帕姆，克拉拉和克拉拉》中最关键的一点。</p><h1 id="90f7" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">以下是PAM中涉及的步骤:</h1><ul class=""><li id="0c7c" class="li lj iq jp b jq mu ju mv jy na kc nb kg nc kk ln lo lp lq bi translated">给定k</li><li id="d5ac" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">现在选择随机的k作为初始的medoid</li><li id="e2d6" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">这些实例中的每一个都需要分配给最近的medoid (x)</li><li id="5116" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">然后计算目标函数，即每个实例与最接近的medoids的相异度之和</li><li id="ba94" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">选择任意随机实例(y)</li><li id="14a4" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">当发生这种情况并且交换或替换降低了功能时，用y替换x</li><li id="9bb8" class="li lj iq jp b jq lr ju ls jy lt kc lu kg lv kk ln lo lp lq bi translated">然后重复(3–6 ),直到没有进一步的变化</li></ul><p id="c680" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CLARA(大型应用程序群集)是PAM的一个更快版本，有助于在算法中启用循环的嵌套顺序。我们需要一个更快的PAM版本，以防PAM算法的时间复杂度比k-means算法慢。</p><p id="706d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="kl">尽管k-means聚类算法存在诸多缺点，如易受离群值影响、依赖欧几里德距离以及收集的质心不能代表真实数据点，但PAM、CLARA和CLARANS在解决这个问题上发挥了重要作用。</em>T9】</strong></p></div></div>    
</body>
</html>