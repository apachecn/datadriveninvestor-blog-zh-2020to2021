<html>
<head>
<title>How I collected an Arabic DataSet of more than 5000 articles?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何收集超过5000篇文章的阿拉伯语数据集？</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-i-collected-an-arabic-dataset-of-more-than-5000-articles-7f5de65e8ca2?source=collection_archive---------18-----------------------#2021-02-02">https://medium.datadriveninvestor.com/how-i-collected-an-arabic-dataset-of-more-than-5000-articles-7f5de65e8ca2?source=collection_archive---------18-----------------------#2021-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d4d1e2d6953f90e297be7fa68be26d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ANky0hnlupa6o4XZyPz6LA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Screenshot from kaggle.com</figcaption></figure><p id="a66a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">自然语言处理(NLP)是研究最多的机器学习领域之一。近年来已经取得了很大的进展，这使得该领域在几个领域中进入大规模使用。如今，NLP广泛应用于社交网络、搜索引擎、翻译工具、聊天助手和许多其他情况。</p><p id="5261" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然而，不同语言的进度和结果是不同的。大多数机器学习模型优先处理英语，而放弃其他语言，特别是阿拉伯语。造成这种情况的主要原因是缺乏数据集。因此，我将在本文中介绍的所有数据集都很有趣。因此，我创建了一个数据集，收集了来自半岛电视台网站的+5000篇阿拉伯语新闻文章。</p><p id="d277" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">该数据集可在Kaggle上通过以下链接获得:<a class="ae ld" href="https://www.kaggle.com/arhouati/arabic-news-articles-from-aljazeeranet" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/arhouati/Arabic-news-articles-from-al jazeeranet</a></p><p id="c345" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先，我需要定义从每篇文章中提取哪些信息。经过简单的分析，我决定收集以下信息:</p><ul class=""><li id="ba00" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">出版日期</li><li id="d42e" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">标题</li><li id="3b73" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">描述</li><li id="7020" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">环</li><li id="45a7" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">内容</li><li id="e6eb" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">图像</li><li id="ef65" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">参考</li><li id="04c7" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">标签</li></ul><p id="72e0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是一个来自半岛电视台的网页的例子，有各种各样的信息。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/c0fe1083c1632e0ab2144fca563c64d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wBniitP_5bhc4ETQ8dkFOA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Taged screenshot from Aljazeera.net website</figcaption></figure><p id="de37" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在下一节中，我将一步一步地详细介绍我是如何创建数据集的。</p><h1 id="3243" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">1.获取URL</h1><p id="795e" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky mz la lb lc im bi translated">获取所有文章URL的一种可能性是应用网络爬虫技术。但是，我决定使用一个更简单的方法，通过RSS提要。这种方法很容易实现，但有一些主要的不便之处。首先，我们将只获得较新的文章，其次，我们的脚本必须运行很长时间才能获得更多的数据。</p><p id="701a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，我创建了一个python类来获取文章:</p><figure class="lt lu lv lw gt ju"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="7725" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这个类使用两个主要库:</p><ul class=""><li id="305a" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">“urllib.request.urlopen”以获取RSS源的内容</li><li id="9026" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">“xmltodict”解析RSS提要并获取项目</li></ul><h1 id="64af" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">2.数据收集</h1><p id="6cd1" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky mz la lb lc im bi translated">除了文章的URL之外，RSS提要还提供了更多的信息，如发表日期、标题、描述和链接。为了收集其余的信息，我们需要从URL中检索页面，并浏览HTML代码以准确提取缺失的信息。为此，我们使用了最强大的python库进行HTML解析:“BeautifulSoup”。</p><p id="0eb0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">BeautifulSoup有一个非常好的方法叫做“选择”。这种方法使得使用CSS选择获取任何HTML标签成为可能，这对于任何web开发人员来说都是非常自然的。然而，对目标HTML的任何更新，例如，添加新的CSS类或更改旧类的名称，都会误导元素的选择</p><p id="118a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，我们实现了下面的类:<br/> -调用RSS Feed类从Aljazeera RSS Feed中获取条目<br/> -对于每个条目，获取HTML页面，解析它并检索缺失的信息。</p><figure class="lt lu lv lw gt ju"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="2648" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">3.数据清理</h1><p id="7282" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky mz la lb lc im bi translated">虽然数据是可用的并且结构良好，因为它来自严格认可的新闻网站，但是仍然有必要应用包括以下任务的清理步骤:删除空格、删除拉丁字符，并且特别要确保它确实是阿拉伯语。</p><p id="facc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是用于清理数据的类，也可从以下位置获得:</p><figure class="lt lu lv lw gt ju"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="2c46" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结论</h1><p id="a87a" class="pw-post-body-paragraph kf kg it kh b ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky mz la lb lc im bi translated">为了获得超过5000篇文章，我在GCP环境下运行了5个月的脚本。数据集有三种格式:CSV、Json和*。SQLite数据库的db。</p><figure class="lt lu lv lw gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/8351071636b23e675635c95be2a8b74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*jK94k84_VDrQT4uulGUzxQ.png"/></div></figure><p id="c35d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我希望这个数据集能帮助你探索更多关于阿拉伯语的机器学习和NLP模型。在Kaggle上可以通过以下链接获得:<a class="ae ld" href="https://www.kaggle.com/arhouati/arabic-news-articles-from-aljazeeranet" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/arhouati/Arabic-news-articles-from-al jazeeranet</a></p><p id="2be2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于下一步，我将在以后的文章中解释我如何使用这个数据集来实现和训练一个应用于阿拉伯文本的摘要模型。</p></div></div>    
</body>
</html>