<html>
<head>
<title>Outlier Detection with K-means Clustering in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中基于K-means聚类的离群点检测</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/outlier-detection-with-k-means-clustering-in-python-ee3ac1826fb0?source=collection_archive---------2-----------------------#2021-02-01">https://medium.datadriveninvestor.com/outlier-detection-with-k-means-clustering-in-python-ee3ac1826fb0?source=collection_archive---------2-----------------------#2021-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6557" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用k-means聚类检测异常值以一种非常简单的形式解释。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/3025c39923e354a6e2cef48b02b2558d.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*kytY3LJQchJovM6JJEWp8Q.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Data with outliers detected by Author</figcaption></figure><p id="8f1b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当您想要将数据聚类到k个组中时，使用K-means聚类。我会告诉你如何捕捉远离这些群体的离群值。我们将通过确定一个阈值比率来实现。对于每个聚类，超出阈值比率的数据将被视为异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/e1ffa7199126d2da22865fe66cf4a96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*KOX1AKojZUPXU2qI2CqtxA.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Data to be used by Author</figcaption></figure><p id="9682" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当你看剧情的时候，更容易看出我们旨在抓住哪些点。在黄色聚类中没有异常值，而在绿色和紫色聚类中分别有一个和两个异常值。因此，我们的目标是捕捉这个数据集中的三个异常值。</p><p id="eceb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们首先导入必要的库并组合数据。然后，通过设置k = 3来预测k-均值聚类。最后，我们通过运行这段代码得到上面的图。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="bea1" class="lw lx it ls b gy ly lz l ma mb">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans<br/>from scipy.spatial.distance import cdist</span><span id="8f5a" class="lw lx it ls b gy mc lz l ma mb"># composing data set<br/>data = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 9], [7, 9], [7, 7], [12,10], [25, 24], [24, 24], [24, 25], [25, 25], [25,20], [20,25]])</span><span id="3e8a" class="lw lx it ls b gy mc lz l ma mb"># kmeans model, setting k = 3<br/>km = KMeans(n_clusters = 3)<br/>clusters=km.fit_predict(data)</span><span id="8101" class="lw lx it ls b gy mc lz l ma mb"># plotting data set<br/>plt.scatter(*zip(*data),c=clusters,marker = “x”)</span></pre><p id="8e06" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将找到聚类的中心，然后计算每个点到其聚类中心的距离。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="adda" class="lw lx it ls b gy ly lz l ma mb"># obtaining the centers of the clusters<br/>centroids = km.cluster_centers_</span><span id="a977" class="lw lx it ls b gy mc lz l ma mb"># points array will be used to reach the index easy<br/>points = np.empty((0,len(data[0])), float)</span><span id="05d2" class="lw lx it ls b gy mc lz l ma mb"># distances will be used to calculate outliers<br/>distances = np.empty((0,len(data[0])), float)</span><span id="1cef" class="lw lx it ls b gy mc lz l ma mb"># getting points and distances<br/>for i, center_elem in enumerate(centroids):<br/>    # cdist is used to calculate the distance between center and other points<br/>    distances = np.append(distances, cdist([center_elem],data[clusters == i], 'euclidean')) <br/>    points = np.append(points, data[clusters == i], axis=0)</span></pre><p id="5868" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能会问，我们使用哪种算法来计算距离，我们可以选择其他算法吗？正如您可能认识到的，在<em class="md"> cdist </em>函数中，我们将距离类型作为一个‘欧几里得’参数给出。你可以用<a class="ae me" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html" rel="noopener ugc nofollow" target="_blank"> <em class="md"> cdist接受</em></a><em class="md"/>的其他任何一个来代替</p><p id="3477" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在获得点的距离之后，现在，我们将决定一个阈值比率作为百分位数，并找出异常值。</p><p id="de57" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当您决定阈值比率<em class="md"> th </em>时，您正在对所有点的所有距离(到它们自己的中心)进行排序，然后说我希望这些点是高于百分点<em class="md"> th </em>的异常值。我把它设置为80，但你可以随意摆弄。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="6ebc" class="lw lx it ls b gy ly lz l ma mb">percentile = 80<br/># getting outliers whose distances are greater than some percentile<br/>outliers = points[np.where(distances &gt; np.percentile(distances, percentile))]</span></pre><p id="8871" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以，我们完了！我们剩下的唯一一件事就是可视化我们检测到异常值的数据。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="d535" class="lw lx it ls b gy ly lz l ma mb">fig = plt.figure()</span><span id="0be1" class="lw lx it ls b gy mc lz l ma mb"># plotting initial data<br/>plt.scatter(*zip(*data),c=clusters,marker = “x”) </span><span id="9d98" class="lw lx it ls b gy mc lz l ma mb"># plotting red ovals around outlier points<br/>plt.scatter(*zip(*outliers),marker=”o”,facecolor=”None”,edgecolor=”r”,s=70);</span><span id="736a" class="lw lx it ls b gy mc lz l ma mb"># plotting centers as blue dots<br/>plt.scatter(*zip(*centroids),marker=”o”,facecolor=”b”,edgecolor=”b”,s=10);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/3025c39923e354a6e2cef48b02b2558d.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*kytY3LJQchJovM6JJEWp8Q.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Data with outliers detected by Author</figcaption></figure><p id="8a02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图中的蓝点代表聚类的中心。簇的颜色已经改变，但这并不重要。异常值用红色椭圆标记。</p><p id="2d8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您希望使用此算法来检测所有数据之外的异常值，而不是聚类，则需要选择k = 1。</p><pre class="kj kk kl km gt lr ls lt lu aw lv bi"><span id="e1f3" class="lw lx it ls b gy ly lz l ma mb"># setting k = 1<br/>km = KMeans(n_clusters = 1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/9e61cabe6860871d0bf487e31a40fb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*q7QJbvFuzkLbubasCrVo6g.png"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Outliers caught after setting k = 1 by Author</figcaption></figure><p id="b1b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，为该数据集选择k = 1没有多大意义，但当我们这样做时，我们会看到一个蓝点位于所有数据的中心，最远的点用红色椭圆圈出。</p></div></div>    
</body>
</html>