<html>
<head>
<title>Word Embedding: CBOW &amp; Skip-gram</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入:CBOW和Skip-gram</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/word-embedding-cbow-skip-gram-8262e22fa7c?source=collection_archive---------7-----------------------#2021-01-11">https://medium.datadriveninvestor.com/word-embedding-cbow-skip-gram-8262e22fa7c?source=collection_archive---------7-----------------------#2021-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c169" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗨伙计们！在这篇博客中，我想分享我对这篇研究论文'<a class="ae kl" href="https://arxiv.org/pdf/1301.3781v3.pdf" rel="noopener ugc nofollow" target="_blank">Vector Space</a>中单词表示的有效估计'的评论，这篇论文提出了使用深度学习的CBOW和Skip-gram新架构。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/824d0577b69cfd9971d1ff07475550fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAaLLj9PzezIfu4Yw_3kSQ.png"/></div></div></figure><h1 id="9f24" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">1.介绍</h1><p id="61d9" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">单词嵌入意味着将单词表示成连续的(或数字的)矢量表示。</p><p id="ce3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个简单的模型，<strong class="jp ir">统计语言建模</strong>表示(如词袋(BoW)、TF-IDF(词频-逆文档频率)、N-gram模型)，具有合理的良好选择——简单性、鲁棒性和观察性。然而，它有一定的局限性——</p><ul class=""><li id="e4c9" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">缺乏语义意义。例如，<em class="mk">好吃</em>和<em class="mk">好吃</em>会考虑不同，但是，意思是一样的。</li><li id="802d" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">用于自动语音识别的相关域内数据量是有限的。</li><li id="9dfa" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">按比例增加单词(像包含几十亿单词的语料库)不会导致任何有意义的进步。</li></ul><p id="442f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们需要先进的技术，以及深度学习如何进入这个阶段，它也赋予语义意义。比如Glove，lda2vec等等。此外，我们知道，如今，深度学习在大多数研究领域(如对象检测、语言建模、文本到语音转换等)占据主导地位，其结果优于ML。</p><h1 id="fa8f" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">2.目标</h1><ul class=""><li id="ed67" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk mg mh mi mj bi translated">介绍可用于从拥有数十亿单词和数百万词汇的高数据集中学习<em class="mk">高质量单词向量</em>的技术。</li><li id="4f38" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">用于测量所得向量表示的质量，期望不仅相似的单词倾向于彼此更接近，而且具有<em class="mk">倍</em> <em class="mk">相似度</em>【1】。例如，男性/女性关系是自动学习的，通过诱导的向量表示，“国王-男人+女人”产生非常接近“女王”的向量。</li></ul><p id="4acf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">目:</strong>通过开发保持单词间线性规律的新结构，最大化这些向量运算的准确性。</p><h1 id="acf3" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">3.模型架构</h1><ul class=""><li id="211e" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk mg mh mi mj bi translated">专注于通过神经网络学习的单词的分布式表示(显示它们在保持单词之间的线性规则方面比<a class="ae kl" href="https://medium.com/acing-ai/what-is-latent-semantic-analysis-lsa-4d3e2d18417a" rel="noopener">潜在语义分析(LSA) </a>表现得好得多)</li><li id="dcc5" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated"><a class="ae kl" href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2" rel="noopener" target="_blank">在大型数据集上，潜在的狄利克雷分配</a> (LDA)在计算上变得非常昂贵。</li></ul><p id="7aca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是以前做过的相关工作…</p><h2 id="750f" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">3.1前馈神经网络语言模型(NNLM) [2]</h2><p id="5b91" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">(如果你有时间，可以从第1141页开始阅读参考文献，因为其中涉及许多公式和数学。我将在这里总结它实际上是什么。)</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nf"><img src="../Images/ad47e201cf87431bfac4dee199862489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCh-359oshF3B9LqpdGdsg.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">From research paper [2]</figcaption></figure><ul class=""><li id="9bf7" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">它由输入、投影、隐藏和输出层组成。</li><li id="c9ba" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">在输入层，使用1-of- <em class="mk"> V </em>编码对<em class="mk"> N </em>个先前单词进行编码，其中<em class="mk"> V </em>是词汇的大小。</li><li id="5cd5" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">然后使用共享投影矩阵将输入层投影到维度为<em class="mk"> N X D </em>的投影层<em class="mk"> C </em>。</li></ul><blockquote class="nk nl nm"><p id="0d07" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">根据研究论文[2]，</p><p id="3df4" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">映射<strong class="jp ir"> C </strong>的参数就是特征向量本身，用一个<strong class="jp ir"> |V| × m </strong>矩阵表示；C的<strong class="jp ir">行‘I’</strong>是针对<strong class="jp ir">单词‘I’</strong>的特征向量<strong class="jp ir"> C(i) </strong>和<strong class="jp ir">‘m’</strong>是维度。</p><p id="c1ff" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">这里我用D来代替m，以便于理解，因为D对应于每个单词向量的D维。</p></blockquote><ul class=""><li id="10e3" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">对于投影和隐藏层之间的计算来说，NNLM架构变得复杂，因为投影层中的值是密集的。</li><li id="594d" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">隐藏层用于<strong class="jp ir"> <em class="mk">计算词汇表</em> </strong>中所有单词的概率分布，产生一个维数为<em class="mk"> V </em>的输出层。</li><li id="76f4" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">函数<strong class="jp ir"> <em class="mk"> g </em> </strong>可以通过前馈或递归神经网络或其他参数化函数实现，参数<strong class="jp ir"> <em class="mk"> ω </em> </strong>。整体参数设置为<strong class="jp ir"> θ = (C，ω) </strong>。为上图编写的符号如下，其中<em class="mk"> g </em>为神经网络，<em class="mk"> C(i) </em>为第I个单词特征向量。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7da210b0518441c7d19cba34daca238e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*_V6iZoMi8wCbnXjumkcuPw.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Notation of above figure</figcaption></figure><p id="1030" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">培训— </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nr"><img src="../Images/8e92f37337a00503e7c4994f6f463f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*donNOKlWN0USNNkLvLPxpw.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">From research paper [2], page no. 1142</figcaption></figure><p id="22f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种架构的常见选择:</p><ul class=""><li id="7bb5" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated"><em class="mk"> N </em> =10</li><li id="91e4" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">投影层，<em class="mk"> C </em>，—可能在500到2000之间</li><li id="73cc" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">隐藏层，<em class="mk"> H </em>，—通常为500到1000个单位</li></ul><p id="7597" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于这个架构的问题—</p><ol class=""><li id="a367" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk ns mh mi mj bi translated">每个训练样本的每个训练的计算复杂度是:</li></ol><blockquote class="nt"><p id="dfb3" class="nu nv iq bd nw nx ny nz oa ob oc kk dk translated"><em class="od"> Q = N X D + N X D X H + H X V </em></p></blockquote><blockquote class="nk nl nm"><p id="c42a" class="jn jo mk jp b jq oe js jt ju of jw jx nn og ka kb no oh ke kf np oi ki kj kk ij bi translated">请记住，每个单词w(I)对应于“V”维，因为它是1/V编码。因此，对于N个单词，输入层的尺寸是N X V。但是，如果我们也看到，词汇的大小将有一个有限的和固定的整个网络，而训练，对不对？我们剩下的是可以输入到输入层的单词数，即“N”，它是变量和超参数(以找到正确的值)。这就是为什么它在训练复杂性时忽略了“V”项。我正根据研究论文跟进这篇评论。</p></blockquote><p id="0ae5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一项是从输入图层到投影图层-N X D</p><p id="57db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第三项是从隐藏层到输出层— H X V</p><p id="e7b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二项是从投影到隐藏层，这是D X H，而且因为我们有N个前面的单词，所以需要执行“N”次以上-N X D X H(这一项是最复杂的)。</p><p id="5b67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.由于输出单元具有与词汇<em class="mk"> V. </em>相同的大小，通常，V不是小而是大量的词汇，这导致性能降低。</p><p id="b599" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(第二)点的解决方案—</p><ul class=""><li id="2a16" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">通过词汇表的二叉树表示，需要评估的输出单元的数量可以下降到log_base2(V)左右，即使用<em class="mk">分层softmax </em>，其中词汇表表示为<strong class="jp ir">霍夫曼二叉树</strong>。</li><li id="e6d3" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">基于Huffman树的层次化softmax需要大约log _ base 2(<em class="mk">Unigram _ perfusion</em>(<em class="mk">V</em>))。例如，当V = 100万字时，结果是评估中的2倍加速。</li></ul><p id="2426" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)点的解决方案—</p><ul class=""><li id="a50b" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">计算瓶颈<em class="mk"> N X D X H </em>，他们后来提出的架构<em class="mk">没有隐藏层</em>，因此严重依赖于softmax规范化的效率(即输出层)。</li></ul><h2 id="07e3" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">3.2递归神经网络语言模型(RNNLM)</h2><ul class=""><li id="82ea" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk mg mh mi mj bi translated">基于递归神经网络的语言模型用于克服以前模型的局限性，例如需要指定上下文长度(<em class="mk"> N </em>)。此外，该模型没有投影层；只有输入、隐藏和输出层。</li><li id="efaf" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">我们已经知道RNN的特点是使用延时连接将隐藏层连接到其自身，这允许形成短期记忆(因为来自过去的信息可以由隐藏层状态来表示，该隐藏层状态基于当前输入的<em class="mk">和前一时间步中隐藏层</em>的<em class="mk">状态得到更新</em></li></ul><p id="4112" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RNN模型的每个训练样本的复杂度是:</p><blockquote class="nt"><p id="f60d" class="nu nv iq bd nw nx oj ok ol om on kk dk translated">Q = H X H + H X V</p></blockquote><p id="4c3d" class="pw-post-body-paragraph jn jo iq jp b jq oe js jt ju of jw jx jy og ka kb kc oh ke kf kg oi ki kj kk ij bi translated">词表征(维度)——<em class="mk">D，</em>隐层(<em class="mk"> H </em>)。</p><p id="d8ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与之前型号相同的问题——</p><ol class=""><li id="3264" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk ns mh mi mj bi translated">通过使用分层softmax，输出层<em class="mk"> V </em>可以减少到log( <em class="mk"> V </em>)。于是，第二项变成<em class="mk"> H X log(V) </em>。</li><li id="9108" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk ns mh mi mj bi translated">计算量最大的是第一项，即<em class="mk">H X H</em>——这是RNN隐层的递归。</li></ol><h1 id="0b50" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">4.建议—新的对数线性模型</h1><ul class=""><li id="c67e" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk mg mh mi mj bi translated">从以前的工作中主要观察到，大部分复杂性是由模型中的非线性隐藏层引起的。</li></ul><h2 id="2223" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">4.1连续词袋模型</h2><ul class=""><li id="abd1" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk mg mh mi mj bi translated">第一个提出的架构类似于前馈NNLM(3.1节)，其中<strong class="jp ir">非线性隐藏层被移除</strong>并且<strong class="jp ir">投影层被所有单词</strong>共享；所有单词被投影到相同的位置(它们的向量被平均)。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi oo"><img src="../Images/1317be11cfc2e469c309c302064cfcb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pNPRuA0jDmI6l_Xd2qt5rA.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">From research paper [3], page no. 5</figcaption></figure><ul class=""><li id="db63" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">称这个架构为<em class="mk">单词袋</em>模型，因为历史中的<strong class="jp ir">单词顺序不会影响投影。</strong></li><li id="f0d4" class="mb mc iq jp b jq ml ju mm jy mn kc mo kg mp kk mg mh mi mj bi translated">你需要知道的两件事:<em class="mk">语境</em>和<em class="mk">焦点</em>词。</li></ul><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ecd863020dcde7023b148952112be333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*C61vsnEnJH8kwf3OQGn4-A.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 1. Word notation</figcaption></figure><p id="f425" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mk">焦点</em>单词是该模型需要从给定的<em class="mk">上下文</em>单词中预测的那些单词(输出)。因此，从上面的例子中，当时间步长<em class="mk"> t </em> =3时，w_3为；同样，对于给定的时间步长t，我们可以写成:</p><p id="520f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上下文词= w _(<em class="mk">t</em>-2)w _(<em class="mk">t</em>-1)w _(<em class="mk">t</em>+1)w _(<em class="mk">t</em>+2)→两个历史和两个未来词from(焦点词w_(t))当前时间<em class="mk"> t. </em></p><p id="576d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">焦点词=w( <em class="mk"> t </em></p><ul class=""><li id="0eda" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">它发现，通过用f <strong class="jp ir">我们的未来和输入的四个历史单词</strong>构建对数线性分类器，在任务上获得了最佳性能。</li></ul><h2 id="d44c" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">更深入的</h2><p id="4427" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">根据图1中给出的相同符号。，建筑的插图看起来像什么</p><p id="1747" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">w_1、w_2、w_4、w_5是上下文单词，被馈送到输入层；w3是需要在输出层预测的焦点字。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2af496930ff02f99e40b67ca0f28b6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*pqxgkGdUYxagW-u1QAOLRg.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 2. Notation in CBOW architecture</figcaption></figure><p id="0b9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更深入地看这个建筑，它看起来像这样</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi or"><img src="../Images/d2aff2f55ecabc8d389854ced9313458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6z7uv80d_jN46xFl0SmEQ.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Deep Learning Illustration of Figure 2.</figcaption></figure><blockquote class="nk nl nm"><p id="b820" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">在上图中，我写下了权重w4<em class="iq">x</em>D，因为输入了4个输入。如果N个输入被馈入，那么权重变成W_N <em class="iq"> x </em> D. <br/>同样，这里我也说N；一般是W_N <em class="iq"> x </em> V <em class="iq"> x </em> D，由于V在训练网络中是定长的，所以我们将有W_N <em class="iq"> x </em> D. <br/> **所以，你想怎么理解就看你自己了。N或N <em class="iq"> x </em> V **</p></blockquote><p id="1396" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">理想情况下，箭头表示为“完全连接”。然而，根据研究论文，它表示为分层softmax。</p><p id="e841" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让放大更多，在' D '层和输出层之间，看看如何分层softmax看起来像。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1091c5e990dde785f10edec909d0b6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*7nl8D2L8_05xbdSChD-KNQ.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 3. Magnified between ‘D’ layer and output layer.</figcaption></figure><p id="b6e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，说重点，在训练阶段，</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ot"><img src="../Images/7c3cb24cad32b35042cc4dc204098659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kts5ap0TEC8CRJ3AIvrGGg.png"/></div></div></figure><p id="77e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于5个词，在网络训练时，每一个不同的焦点词作为一个输出，需要重复训练5次。所以，如果我们有几百万个单词，我们需要训练几百万次。</p><p id="ffe9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练复杂度— </strong></p><blockquote class="nt"><p id="04be" class="nu nv iq bd nw nx oj ok ol om on kk dk translated">Q = N X D + D X log_base2(V)</p></blockquote><p id="920f" class="pw-post-body-paragraph jn jo iq jp b jq oe js jt ju of jw jx jy og ka kb kc oh ke kf kg oi ki kj kk ij bi translated">第一项来自输入层(N)和投影层(N)— N X D</p><p id="9454" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第二项来自投影层(D)到输出层(V)—D X log _ base 2(V)；[不是“V ”,因为它没有完全连接；它是层次结构，即霍夫曼编码，如果你知道霍夫曼编码的时间复杂度是多少？是O(n*log(n))。这和第二项非常相似，对吗？</p><p id="e8c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意，输入层和投影层之间的权重矩阵以与NNLM中相同的方式为所有单词位置共享。</strong></p><div class="ou ov gp gr ow ox"><a href="https://www.datadriveninvestor.com/2020/11/27/deep-learning-amid-increased-physician-administrative-workload/" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd ir gy z fp pc fr fs pd fu fw ip bi translated">医生管理工作量增加时的深度学习|数据驱动的投资者</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">行政工作量是我们这个时代大多数医生所经历的众多负担之一。医生，尤其是…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl kw ox"/></div></div></a></div><h2 id="ac2b" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">4.2连续跳格模型</h2><p id="43ac" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">第二种架构类似于CBOW，但它不是基于上下文来预测当前单词，而是将焦点单词作为具有连续投影层的对数线性分类器的输入，并预测上下文单词之前(历史)和之后(未来)一定范围内的单词。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/0bc287c36abbc816fcb0628cf85357cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*f8BWWj4vew1BD9sYT1fRuw.png"/></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">From research paper [3], page no. 5</figcaption></figure><p id="b357" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，这一次，我们将焦点词(w(t))输入到体系结构中，得到一定范围内的预测历史(即w(t-2)，w(t-1))和未来上下文(即w(t+1)，w(t+2))词</p><h2 id="10af" class="mt kz iq bd la mu mv dn le mw mx dp li jy my mz lm kc na nb lq kg nc nd lu ne bi translated">更深入的</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pn"><img src="../Images/6c8e49a90c606e125174fd393b878481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g5fFVgH01o8XMIabgzNQhg.png"/></div></div><figcaption class="ng nh gj gh gi ni nj bd b be z dk">Figure 4.</figcaption></figure><p id="eb44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练复杂度— </strong></p><p id="bf04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数人会认为这种体系结构的训练复杂性是</p><blockquote class="nt"><p id="7b64" class="nu nv iq bd nw nx oj ok ol om on kk dk translated">Q = D + D x log_base2(V)</p></blockquote><p id="ac12" class="pw-post-body-paragraph jn jo iq jp b jq oe js jt ju of jw jx jy og ka kb kc oh ke kf kg oi ki kj kk ij bi translated">然而，那不对！让我告诉你为什么。参见图4中的相同例子。；</p><p id="711e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">跟平常一样；网络最初将进入前向阶段，即每个焦点单词w3被馈送到输入层，并过渡到投影层，最终得到所有4个上下文单词(w1)，w2，M4，w5)的输出。</p><p id="b60a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，在反向阶段，我们需要反向传播来更新权重。会不会发生从所有输出层来更新投影权重的情况？(请参见下图进行说明)</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi po"><img src="../Images/5026d94469a1173b31792fc0e7d5b469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmdoQwVdIbGa1yM5T7RXxA.png"/></div></div></figure><p id="d218" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不行，不能更新。所以，我们需要重复执行4次来更新投影层。</p><p id="219e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先从w _→w _；在后向阶段，投影层流从w1→D→w3更新</p><p id="5c61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其次从w _(3)→w _(2)；在后向阶段，投影层流动从w(2)→D→w(3)更新</p><p id="217a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第三从w(3)→w(4)；在后向阶段，投影层流动从w4→D→w3更新</p><p id="995d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第四个从w(3)→w(5)；在后向阶段，投影层流动从w5→D→w3更新</p><p id="c2ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，你需要重复C次，其中C是上下文单词的数量(包括历史单词和未来单词)。在上面的例子中，我们取了4个单词，所以C=4，重复' C '次。</p><p id="a08e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练复杂度中加上‘C’次，我们会得到:<strong class="jp ir"> Q = C × (D + D × log_base2(V )) </strong>，其中C是单词的最大距离。</p><blockquote class="nk nl nm"><p id="e9e1" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">摘自研究论文[3]第4页第4.2节最后一段</p><p id="f74a" class="jn jo mk jp b jq jr js jt ju jv jw jx nn jz ka kb no kd ke kf np kh ki kj kk ij bi translated">因此，如果我们选择C = 5，对于每个训练单词，我们将在&lt;1 and C&gt;之间的范围内随机选择一个数字R，然后使用来自当前单词的历史的R个单词和来自当前单词的未来的R个单词作为正确的标签(即，这将要求我们进行R × 2单词分类，将当前单词作为输入，将R + R个单词中的每一个作为输出。)</p></blockquote><ul class=""><li id="3838" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">在这个实验中，它使用C = 10。</li></ul><h1 id="0e27" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">5.结果</h1><ol class=""><li id="d5c1" class="mb mc iq jp b jq lw ju lx jy mq kc mr kg ms kk ns mh mi mj bi translated">具有不同大小词汇‘V’，</li></ol><p id="63ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练数据由几个LDC语料库组成，在(320万字，82K词汇量)中有详细描述。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pp"><img src="../Images/b9e46a24a911bcf898a47d234f9c526e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FE2tWUutnldwVfCOXwfzwA.png"/></div></div></figure><p id="afa7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.选择D=640，与所有其他模型相比，</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pq"><img src="../Images/eb5b6be88b6908ccf270daca716c20a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_P4xl2bUj8Ka_6nQx7o4g.png"/></div></div></figure><p id="8414" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CBOW架构在语法任务上比NNLM更好，在语义任务上也差不多。最后，Skip-gram架构在句法任务上比CBOW模型稍差(但仍比NNLM好)，在测试的语义部分比所有其他模型好得多。</p><p id="1f43" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.评估仅使用一个CPU训练的模型，并将结果与公开可用的词向量(如谷歌新闻)进行比较，</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi pr"><img src="../Images/e1b3ecfbe4c1cc6f111e3994e25ce6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v567fWEYwjMnN87-3fBx0g.png"/></div></div></figure><p id="aaea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.使用小批量异步梯度下降和称为Adagrad的自适应学习速率程序，在Google News 6B数据集上训练的几个模型的结果。他们在训练中使用了50到100个模型复制品。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ps"><img src="../Images/8b040c276d0a81e914a072a069f46bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kosTNLkMhsyz3k_N39YvQw.png"/></div></div></figure><h1 id="38a9" class="ky kz iq bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">参考</h1><p id="a193" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated">[1] <a class="ae kl" href="https://www.aclweb.org/anthology/N13-1090.pdf" rel="noopener ugc nofollow" target="_blank">连续空间词表征的语言规律</a></p><p id="a11e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] <a class="ae kl" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">一种神经概率语言模型</a></p><p id="caeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] <a class="ae kl" href="https://arxiv.org/pdf/1301.3781v3.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a></p></div><div class="ab cl pt pu hu pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="ij ik il im in"><p id="a516" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就是这样。希望您已经了解了CBOW和Skip-gram架构是如何工作的。</p><p id="6d73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的LinkedIn个人资料<a class="ae kl" href="https://www.linkedin.com/in/sahil-40a621168/" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><p id="116b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">谢谢你，祝你愉快，:D</p><p id="5d46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">访问专家视图— </strong> <a class="ae kl" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>