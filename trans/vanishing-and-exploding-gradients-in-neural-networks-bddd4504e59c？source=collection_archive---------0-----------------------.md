# 神经网络中的消失梯度和爆炸梯度

> 原文：<https://medium.datadriveninvestor.com/vanishing-and-exploding-gradients-in-neural-networks-bddd4504e59c?source=collection_archive---------0----------------------->

![](img/f3994653cc661dcf5652ecacb23c88ab.png)

*Image* [*Source*](https://unsplash.com/photos/GjCnF8InuXc)

在这篇博客中，你会明白为什么渐变消失和爆炸的问题会发生。什么是消失和爆炸梯度问题，为什么会发生。

***什么是渐变？***

梯度只不过是损失函数相对于权重的导数。它用于在神经网络的反向传播过程中更新权重以最小化损失函数。

***什么是消失渐变？***

*在反向传播过程中，当我们对每一层进行反向传播时，导数或斜率会变得越来越小，这时就会出现消失梯度。*

*当权值更新很小或指数级小时，训练时间过长，在最坏的情况下，这可能会完全停止神经网络训练。*

*由于 sigmoid 和 tanh 激活函数的导数介于 0 到 0.25 和 0–1 之间，因此 sigmoid 和 tanh 激活函数会出现消失梯度问题。因此，更新的权重值很小，并且新的权重值与旧的权重值非常相似。这导致消失梯度问题。我们可以使用 ReLU 激活函数来避免这个问题，因为对于负输入和零输入，梯度为 0，对于正输入，梯度为 1。*

***什么是爆炸渐变？***

*在反向传播过程中，当我们随着每一层向后移动，导数或斜率变得越来越大时，就会出现爆炸梯度。这种情况与消失梯度正好相反。*

*这个问题的发生是因为权重，而不是因为激活功能。由于高权重值，导数也将更高，因此新的权重与旧的权重变化很大，梯度将永远不会收敛。所以可能会导致在极小点附近振荡，永远不会达到全局极小点。*

**结论:**

*在深度神经网络的反向传播期间，由于 sigmoid 和 tan 激活函数而出现消失梯度问题，并且由于大的权重而出现爆炸梯度问题。*

如果你喜欢这个博客或者觉得它有帮助，请留下你的掌声！

***谢谢。***