<html>
<head>
<title>Decision-Tree Model Building Metrics Explained in Detail</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树模型构建指标详细解释</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/decision-tree-model-building-metrics-explained-in-detail-940eed167b06?source=collection_archive---------5-----------------------#2021-01-18">https://medium.datadriveninvestor.com/decision-tree-model-building-metrics-explained-in-detail-940eed167b06?source=collection_archive---------5-----------------------#2021-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="331d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何计算用于构建决策树模型的指标</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7a6da75447d2650892cd9ecb93e6e8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4qrqmzKUhUxqWVm0"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@greg_rosenke?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Greg Rosenke</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a6c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你学习你的第一个数据科学机器学习算法时，我确信决策树是你学习的第一个模型之一。毕竟，这是迄今为止最简单也是最强大的模型。</p><p id="19df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么出名？我能想到几个原因；它们是:</p><ol class=""><li id="5b74" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">使用方便</li><li id="3a22" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">学习过程很快。</li><li id="ede7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">人们可以解释这个模型(不像黑盒模型)</li><li id="08b7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">它是许多著名机器学习模型的基础模型。</li></ol><p id="5739" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于那些仍然不理解树模型的人，让我给你看一个简单的<strong class="lb iu">决策树的例子。</strong></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="b1fe" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">决策图表</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/9e023afc55422f05e8022d09287202ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6nmHHFinSvdgKgFqjwY_A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Decision Tree example| Image by Author</figcaption></figure><p id="e518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是基于树的算法中每个变体的基础模型，它的工作方式如上图所示。直觉上，它看起来像一棵颠倒的树，根在上面，叶子在底部。</p><p id="ea72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型通过创建一系列条件来从数据中得出一个结果，其中通过从序列的上部(根)向下直到到达底部(叶)来做出决策。例如，在上面的图像中，当“颜色绿色”为真时，我们将进入“圆形”决策节点；否则，我们就进入“品尝甜蜜”的决策节点。然后再走一步，直到我们到达底部(树叶)。</p><p id="6dbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我做一个更简单的解释，<strong class="lb iu">决策树是一个倒置的树，其中每个节点代表变量或特征，每个决策节点或分支代表一个决策，每个叶子代表一个结果或结果</strong>。</p><p id="c684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树模型用于分类和回归问题，这意味着我们可以预测分类和连续结果，尽管不能同时预测两种结果。</p><p id="84c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想到的下一个问题是，我们如何为每个节点决定哪些特性？我们如何决定这个特性是最好的？</p><p id="1838" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用一些算法指标来构建决策树，但是我们将只讨论两个最著名的指标；<strong class="lb iu">基尼指数</strong>和<strong class="lb iu">熵和信息增益</strong>指标。</p><p id="a70b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种方法度量之间的基本思想是相似的。下面让我给你看一个例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f165e440eb2e6686b18255fa311e268b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_9VnHrmgctoCwHGJQoZFw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="f11e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两个特征X1和X2，其中绿色和蓝色是你要预测的类别。我们希望在决策树中找到最好的特征以及它们的值，以便最好地将绿色和蓝色类别分开。</p><p id="8f4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是什么是最好的分割器呢？最佳拆分器可以解释为在用于拆分时产生最同质(所有类别都相似)结果的要素和值。</p><p id="0404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么如何计算同质性呢？这是我们使用我之前提到的指标的时候。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/3dd38a72cca9c7a2fa86f8b4b6ac8c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38s_8n19XFyaDBuhYTQbkQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="4b8e" class="nl mr it bd ms nm nn dn mw no np dp na li nq nr nc lm ns nt ne lq nu nv ng nw bi translated">基尼指数</h2><p id="d5ad" class="pw-post-body-paragraph kz la it lb b lc nx ju le lf ny jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">先说<strong class="lb iu">基尼指数</strong>。基尼指数是一个分数，它通过划分的两组中各个阶层的混合程度来评估划分的好坏。基尼指数可以具有在<strong class="lb iu">值0 </strong>和<strong class="lb iu"> 1 【T5，<strong class="lb iu"> </strong>之间的分数，其中0是当所有观察值属于一个类时，1是元素在类内的随机分布。在这种情况下，我们希望基尼系数尽可能低。</strong></p><p id="a490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基尼指数方程用下面的图像方程表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/cae777cc7b9d5169ed50990469d76773.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*q69nY85iyPN3l2mC34Z9Sg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Gini Index|Image created by Author</figcaption></figure><p id="688d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的符号中，Pi是不同类(在上面的例子中，绿色和蓝色)的分裂结果的观察概率，n是类的数量(在我们的例子中，是2)。顺便说一下，上面的符号是一个<strong class="lb iu">负和符号，</strong>所以我们用减法代替加法。</p><p id="f29b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们可以尝试计算X1的基尼指数&lt;2 instances.</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ecb3d9fc59740db6bdc6c479bbda4e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*t_krhgLuQKcePGMpQTdJ6w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="00a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The Gini Index for the X1&lt;2 instance is 0.465. Let’s try to calculate for every instance Gini Index.</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dffbe93211ff900fba0b6fcaa4f67935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Bmy4taqoB7zaVNDB7wjIAg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="61a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">To find the best splitter by Gini Index, we take the Gini Index weighted average for both instances and choose the lowest Gini Index.</p><p id="8778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">In this case, X1 =2 weighted average Gini index is</p><p id="3a8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">((19/29) * 0.465 + (10/29) * 0.18) = 0. 367</p><p id="3ca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">and X1= 3 Gini index is</p><p id="1e2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">((24/29) * 0.497+ (5/29) *0) = 0.412</p><p id="f1f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">We would choose X1 = 2 as the best splitter because it has the least Gini Index.</p><p id="ee5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The steps are repeated until all the nodes have a Gini Index equal to zero (mean only one class found in the leaves), or until it reaches some criteria, we set to stop.</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="dbb6" class="nl mr it bd ms nm nn dn mw no np dp na li nq nr nc lm ns nt ne lq nu nv ng nw bi translated">Entropy and Information Gain</h2><p id="f65c" class="pw-post-body-paragraph kz la it lb b lc nx ju le lf ny jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">What is Entropy and Information Gain metrics? These metrics are two separate metrics that we use to build our decision tree but differ from the Gini Index.</p><p id="58d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">As a starter, <strong class="lb iu">熵</strong>被定义为数据集内杂质或不确定性的度量。这意味着熵测量混合在数据中的类观察的数量。让我用下面的一个图像样本来展示给你看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/e9a43811cc5626f019cb64b1ae161041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UzlekxnzyvgjDEuyo-f9xA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="142f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当组不纯(类混合)时，熵将接近1，但是当组纯(数据中只有一个类)时，熵将接近0。</p><p id="c209" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熵可以用下面的等式来表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/71ca58aaaf528e23d08d6cc3a9a7721e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*7zxyi15NY6AU_T3lQAGxZA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="1944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中S是数据集，p(c)是每个类的概率。</p><p id="e1dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，那什么是<strong class="lb iu">信息增益</strong>？该度量与熵相关，因为根据定义，信息增益是由特征分割之前和之后的熵差。换句话说，<strong class="lb iu">信息增益测量分裂后杂质减少</strong>。</p><p id="711a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息增益可以用下面的等式表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/bc76730662c3b5b13fb2eecb6440c3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*CdjFCsHLVe2bEizcTnbtew.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="fab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">A是用于分裂的特征，T是来自特征A中分裂的整个数据集子集，p(t)是数据子集T中的类的概率，H(t)是子集的熵。</p><p id="62c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息增益测量分裂后杂质减少；这意味着我们想要的是最高的信息增益分数，因为最高的信息增益意味着分裂导致更同质的结果。</p><p id="5697" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，根据这些指标，我们如何计算哪种功能可以提供最佳拆分器呢？我们需要完成几个步骤，我将在下面详细概述每个步骤。</p><p id="8779" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要计算数据集的熵。让我们使用之前的示例数据集(16个绿色和13个蓝色)来计算熵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/45e3a5c3d97391a57fad5cded96df986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*jIMT5joid6VCJ8QA6gJjeQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by AuthorAfter</figcaption></figure><p id="6975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获取数据集熵后，下一步是测量分割特征后的信息增益。在上面的示例中，我们将X1 = 2和X1 = 3作为分割特征。让我们通过计算熵子集X1 = 2来测量信息增益。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/dd0abe7151f291e99e61d2464fa3db62.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*XW3bQLAi12-Ry72VTsw7FQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="2985" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经获得了两个子集熵，但是我们仍然需要将我们的两个子集熵与子集概率相加，如果你还记得前面的信息增益方程的话。如果我们将所有数字放入信息增益等式，X1 = 2时的信息增益为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/50ae1ea4b70943f392e0af89161e3f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*G3x1d-I3ZMHL8pSC3c78BQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image created by Author</figcaption></figure><p id="34f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用X1 = 2特性作为分离器，我们将杂质减少了0.208。接下来，我们还计算X1 = 3特征的信息增益。我现在要展示信息增益结果，X1 = 3的结果是0.168。因为X1 = 2具有最高的信息增益，这意味着我们选择X1 = 2作为我们的最佳分路器。</p><p id="a721" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些步骤一直进行到不再可能分离，或者一些标准停止了我们预先设定的计算(类似于基尼指数)。</p><p id="36fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，基尼指数、熵和信息增益度量都是在算法中用来创建决策树的度量。这两种算法都使用贪婪函数来查找最佳特征，这意味着数据集越大，查找时间就越长。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="01ad" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="bd19" class="pw-post-body-paragraph kz la it lb b lc nx ju le lf ny jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">决策树是分类和回归问题中最常用的机器学习模型之一。有几种算法可用于创建决策树模型，但决策树模型创建中著名的方法是应用:</p><ol class=""><li id="de8f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">基尼指数，或</li><li id="3da5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">熵和信息增益</li></ol><p id="7e6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种方法都测量用于分割的特征的杂质，但是方式不同。基尼指数计算二进制分裂杂质结果，信息增益衡量分裂结果前后的熵。</p><p id="7ea9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望有帮助！</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="07b2" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">如果您喜欢我的内容，并希望获得更多关于数据或数据科学家日常生活的深入知识，请考虑在此订阅我的<a class="ae ky" href="https://cornellius.substack.com/welcome" rel="noopener ugc nofollow" target="_blank">简讯。</a></h1><blockquote class="ol"><p id="207e" class="om on it bd oo op oq or os ot ou lu dk translated">如果您没有订阅为中等会员，请考虑通过<a class="ae ky" href="https://cornelliusyudhawijaya.medium.com/membership" rel="noopener">我的介绍</a>订阅。</p></blockquote></div></div>    
</body>
</html>