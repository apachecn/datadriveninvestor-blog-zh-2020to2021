<html>
<head>
<title>How to Improve K-Nearest Neighbors?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何提高K近邻？</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-to-improve-k-nearest-neighbors-1e9170fb1a89?source=collection_archive---------5-----------------------#2021-01-19">https://medium.datadriveninvestor.com/how-to-improve-k-nearest-neighbors-1e9170fb1a89?source=collection_archive---------5-----------------------#2021-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1ac6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">K近邻指南</h2><div class=""/><div class=""><h2 id="1d72" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">第3节:用Python调优模型</h2></div><p id="0e22" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">参考</em> <a class="ae ll" href="https://kopaljain95.medium.com/how-to-implement-k-nearest-neighbors-4c46da0396fb" rel="noopener"> <strong class="kq ja"> <em class="lk">如何实现K近邻？第2节:在继续… </em>之前，用Python  </strong> </a> <em class="lk">构建模型</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/78352383902c25daca87f0cd12228c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Inia2RtsOhxZVhh4N9l5A.png"/></div></div></figure><blockquote class="ly lz ma"><p id="8edb" class="ko kp lk kq b kr ks ka kt ku kv kd kw mb ky kz la mc lc ld le md lg lh li lj ij bi translated"><strong class="kq ja"><em class="iq">【10】定义网格搜索参数</em> </strong></p></blockquote><pre class="ln lo lp lq gt me mf mg mh aw mi bi"><span id="ea1c" class="mj mk iq mf b gy ml mm l mn mo">param_grid_knn = {<br/>    'n_neighbors': [2, 5, 10, 15],                                   <br/>    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],          <br/>    'metric': ['minkowski', 'euclidean', 'manhattan', 'chebyshev']<br/>}</span></pre><ul class=""><li id="96a1" class="mp mq iq kq b kr ks ku kv kx mr lb ms lf mt lj mu mv mw mx bi translated"><code class="fe my mz na mf b">n_neighbors</code>是“k”的值-最近邻。</li><li id="5a3e" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">algorithm</code>是计算最近邻的算法。</li><li id="61b9" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">metric</code>是寻找距离的算法。</li></ul><p id="9c25" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi ng translated"><span class="l nh ni nj bm nk nl nm nn no di"> W </span> hy这一步:设置所选择的参数用来寻找最佳组合。通过参考<a class="ae ll" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">sk learn . neighbors . kneighborsclassifier</a>文档，您可以找到可用于网格搜索功能的完整参数列表。</p><blockquote class="ly lz ma"><p id="6de3" class="ko kp lk kq b kr ks ka kt ku kv kd kw mb ky kz la mc lc ld le md lg lh li lj ij bi translated"><strong class="kq ja"><em class="iq">【11】超参数调整使用训练数据</em> </strong></p></blockquote><pre class="ln lo lp lq gt me mf mg mh aw mi bi"><span id="2dd0" class="mj mk iq mf b gy ml mm l mn mo">from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import GridSearchCV</span><span id="7c1c" class="mj mk iq mf b gy np mm l mn mo">kNNModel_grid = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid_knn, verbose=1, cv=10, n_jobs=-1)</span><span id="6c34" class="mj mk iq mf b gy np mm l mn mo">kNNModel_grid.fit(X_train, y_train)</span><span id="903c" class="mj mk iq mf b gy np mm l mn mo">print(kNNModel_grid.best_estimator_)</span><span id="86ea" class="mj mk iq mf b gy np mm l mn mo">...</span><span id="90e9" class="mj mk iq mf b gy np mm l mn mo"><em class="lk">Fitting 10 folds for each of 64 candidates, totalling 640 fits</em></span><span id="bd06" class="mj mk iq mf b gy np mm l mn mo"><em class="lk">KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform')</em></span></pre><p id="adeb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注:由于<code class="fe my mz na mf b">cv</code>被定义为10，有64个候选值，因此拟合总数为640(<code class="fe my mz na mf b">n_neighbors</code>有4个定义参数，<code class="fe my mz na mf b">algorithm</code>有4个定义参数，<code class="fe my mz na mf b">metric</code>有4个定义参数)。因此，总拟合次数的计算→ 10 x [4 x 4 x 4] = 640。</p><ul class=""><li id="7d5f" class="mp mq iq kq b kr ks ku kv kx mr lb ms lf mt lj mu mv mw mx bi translated"><code class="fe my mz na mf b">estimator</code>是感兴趣的机器学习模型，假设该模型具有评分功能；在这种情况下，分配的模型是KNeighborsClassifier()。</li><li id="3347" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">param_grid</code>是一个字典，参数名(字符串)作为关键字，参数设置列表作为值；这使得能够搜索任何参数设置序列。</li><li id="9d47" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">verbose</code>是详细度:越高，消息越多；在这种情况下，它被设置为1。</li><li id="8329" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">cv</code>是交叉验证生成器还是iterable，在这种情况下，有10重交叉验证。</li><li id="a767" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><code class="fe my mz na mf b">n_jobs</code>是并发运行的工人的最大数量；在这种情况下，它被设置为-1，这意味着使用了所有的CPU。</li></ul><p id="6fc5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi ng translated"><span class="l nh ni nj bm nk nl nm nn no di"> W </span>通过这个步骤:找到一个<strong class="kq ja">超参数</strong>的最佳组合，该组合最小化一个预定义的损失函数以给出更好的结果。</p><blockquote class="ly lz ma"><p id="9fca" class="ko kp lk kq b kr ks ka kt ku kv kd kw mb ky kz la mc lc ld le md lg lh li lj ij bi translated"><strong class="kq ja"><em class="iq">【12】对测试数据进行预测</em> </strong></p></blockquote><pre class="ln lo lp lq gt me mf mg mh aw mi bi"><span id="94d0" class="mj mk iq mf b gy ml mm l mn mo">y_pred = kNNModel_grid.predict(X_test)</span><span id="5a49" class="mj mk iq mf b gy np mm l mn mo">print(y_pred)</span><span id="da21" class="mj mk iq mf b gy np mm l mn mo">...</span><span id="7f1a" class="mj mk iq mf b gy np mm l mn mo"><em class="lk">[1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0]</em></span></pre><p id="0d1d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi ng translated">通过这一步:获得对测试数据的模型预测，以评估模型的准确性和效率。</p><blockquote class="ly lz ma"><p id="f935" class="ko kp lk kq b kr ks ka kt ku kv kd kw mb ky kz la mc lc ld le md lg lh li lj ij bi translated"><strong class="kq ja"> <em class="iq">【十三】数值分析</em> </strong></p></blockquote><pre class="ln lo lp lq gt me mf mg mh aw mi bi"><span id="5092" class="mj mk iq mf b gy ml mm l mn mo">from sklearn.metrics import confusion_matrix<br/>print(confusion_matrix(y_test, y_pred), ": is the confusion matrix")</span><span id="7439" class="mj mk iq mf b gy np mm l mn mo">from sklearn.metrics import accuracy_score<br/>print(accuracy_score(y_test, y_pred), ": is the accuracy score")</span><span id="5d52" class="mj mk iq mf b gy np mm l mn mo">from sklearn.metrics import precision_score<br/>print(precision_score(y_test, y_pred), ": is the precision score")</span><span id="81c8" class="mj mk iq mf b gy np mm l mn mo">from sklearn.metrics import recall_score<br/>print(recall_score(y_test, y_pred), ": is the recall score")</span><span id="c39d" class="mj mk iq mf b gy np mm l mn mo">from sklearn.metrics import f1_score<br/>print(f1_score(y_test, y_pred), ": is the f1 score")</span><span id="a757" class="mj mk iq mf b gy np mm l mn mo">...</span><span id="7e5f" class="mj mk iq mf b gy np mm l mn mo"><em class="lk">[[83 32]<br/>[20 73]] : is the confusion matrix <br/><br/>0.75 : is the accuracy score<br/>0.6952380952380952 : is the precision score<br/>0.7849462365591398 : is the recall score<br/>0.7373737373737373 : is the f1 score</em></span></pre><p id="11f2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意:使用混淆矩阵，可以提取真阳性、假阳性、假阴性和真阴性值，这将有助于计算准确度分数、精确度分数、回忆分数和f1分数:</p><ul class=""><li id="e2e9" class="mp mq iq kq b kr ks ku kv kx mr lb ms lf mt lj mu mv mw mx bi translated"><strong class="kq ja">真正</strong> = 83</li><li id="3402" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><strong class="kq ja">假阳性</strong> = 32</li><li id="44da" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><strong class="kq ja">假阴性</strong> = 20</li><li id="38b8" class="mp mq iq kq b kr nb ku nc kx nd lb ne lf nf lj mu mv mw mx bi translated"><strong class="kq ja">真负值</strong> = 73</li></ul><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nq"><img src="../Images/26e4f1261eb17819c55ee2fbef5092cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rB5CecD6UPllcQhtdIx6Q.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">Equations for Accuracy, Precision, Recall, and F1.</figcaption></figure><p id="411e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi ng translated"><span class="l nh ni nj bm nk nl nm nn no di"> W </span>通过这一步:评估调优分类模型的性能。正如您所看到的，通过调整在第2节中创建的基本K-最近邻模型，准确度、精确度、召回率和F1分数都得到了提高。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><div class="ln lo lp lq gt oc"><a href="https://github.com/kopaljain95/import-data.science-classification/blob/main/KNearestNeighbors%5B2%5D/KNearestNeighbors.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ja gy z fp oh fr fs oi fu fw iz bi translated">kopaljain 95/import-data . science-分类</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">在GitHub上创建一个帐户，为kopaljain 95/import-data . science-classification的开发做出贡献。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">github.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq lw oc"/></div></div></a></div><p id="7cf3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">接下来— </em> <a class="ae ll" href="https://kopaljain95.medium.com/why-use-k-nearest-neighbors-7535fa8df2c3" rel="noopener"> <em class="lk">为什么要用K近邻呢？第4部分:评估模型权衡</em> </a> <em class="lk"> … </em></p></div></div>    
</body>
</html>