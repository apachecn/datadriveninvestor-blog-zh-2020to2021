<html>
<head>
<title>Spark Architecture and Application Lifecycle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark架构和应用生命周期</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/spark-architecture-and-application-lifecycle-77e347badcbe?source=collection_archive---------0-----------------------#2021-01-11">https://medium.datadriveninvestor.com/spark-architecture-and-application-lifecycle-77e347badcbe?source=collection_archive---------0-----------------------#2021-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b6d24b424f22b40fe2fad6ce6693a2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyHEu13MoeyNwQK3Pc-9_g.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@amir_v_ali?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">amirali mirhashemian</a> on <a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="b857" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是Apache Spark准备系列的Databricks认证助理开发人员的第二部分。在<a class="ae jd" href="https://medium.com/datadriveninvestor/databricks-certified-associate-developer-for-apache-spark-preparation-series-41e66dc4165b?sk=295a1092b9a76bb387fac28d2daa84ce" rel="noopener">第一部分</a>中，我们讨论了考试细节、先决条件和建议的准备工作。</p><p id="1326" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们遵循Databricks推荐的准备材料(包含在<a class="ae jd" href="https://medium.com/datadriveninvestor/databricks-certified-associate-developer-for-apache-spark-preparation-series-41e66dc4165b?sk=295a1092b9a76bb387fac28d2daa84ce" rel="noopener">第一部分</a>)。因此，在本文中，我们将从Spark的架构开始，并尝试从认证的角度来介绍它。我也会尽可能地添加有用的资源。</p><p id="5183" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">基础架构</strong></p><p id="0167" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Apache Spark是一个分布式处理引擎。由于其内存中的并行计算框架，它非常快。请记住，Spark只是处理引擎，它需要一个单独的存储(如HDFS)来永久写入数据。典型的Spark应用程序运行在一个机器集群(也称为节点)上。它将一个较大的任务分解成多个较小的任务，并在这些机器之间分配。然而，这给协调和管理这些机器上的工作带来了挑战。Spark内置了这一功能，其架构旨在处理大型分布式应用程序。有几个核心组件和角色被分配给这些组件和角色来帮助执行这种分布式工作。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="ab gu cl lf"><img src="../Images/59d2b45d379da5bc4de19759a0f357ea.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QPdR-rZGWgRmKipIlT3Nmw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1: A Spark Cluster</figcaption></figure><p id="91f9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">驱动</strong></p><p id="70c4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">驱动程序(或驱动程序)是在一台机器上运行的进程，负责整个应用程序的生命周期。它维护关于Spark应用程序的信息。它在执行器(工作机)上分析、分配和调度任务(要完成的处理)。它还在应用程序成功/失败后对用户做出响应。驱动器类似于管弦乐队。像管弦乐队一样，驱动程序不执行任何计算，它只是管理整个应用程序生命周期(图1)。</p><p id="d007" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">遗嘱执行人</strong></p><p id="4186" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">执行器是运行在集群中其他机器上的工作进程。同一台机器上可以运行多个执行器。每个执行器都有一个数据集合，也称为分区，由驱动程序分配给它们执行代码。他们还向驾驶员报告分配工作的状态(成功/失败)(图1)。</p><p id="af6c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">插槽</strong></p><p id="c3fe" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据用户为Spark应用程序指定的内核，每个执行器可以有多个插槽用于一个任务(由驱动程序分配)。这意味着有两个级别的并行性:首先，工作在执行器之间分配，然后一个执行器可能有多个插槽来进一步分配工作(图1)。</p><p id="707c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">任务</strong></p><p id="5bbd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任务是要在分区(行的集合)上完成的工作。这些由驱动程序分配给执行者。这些将在executor机器上可用的插槽中运行(图1)。</p><p id="ae8b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">乔布斯</strong></p><p id="00ba" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作业是Spark中的并行操作。由驱动程序维护的spark应用程序可以包含多个作业。</p><p id="5349" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">火花会</strong></p><p id="cdc0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SparkSession是控制Spark应用程序的驱动程序进程。它是Spark所有功能的入口点。对于Databricks笔记本和REPL环境，它已经在变量“spark”下创建了(图2)。然而，当编写应用程序时，你必须自己创建它，如下所示。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/4d80ecaa095a35fbc587602a36976a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*ix7ABPDC9HKPmJ_zkjw3Tw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2: SparkSession found as spark in Databricks’ notebook</figcaption></figure><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="f966" class="lm ln jg li b gy lo lp l lq lr">## Create a SparkSession when building applications</span><span id="3e5e" class="lm ln jg li b gy ls lp l lq lr">from pyspark.sql import SparkSession</span><span id="72e0" class="lm ln jg li b gy ls lp l lq lr">spark = SparkSession.builder.master("local")\<br/>                            .appName("My First App")\<br/>                            .config("config.name","config.value")\<br/>                            .getOrCreate()</span></pre><p id="3172" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建SparkSession后，您将能够运行pyspark代码。SparkSession是在Spark 2.x版本中添加的。在早期版本中，我们有两个独立的上下文:SparkContext和sqlContext，它们分别提供了与rdd和Dataframes/Hive交互的功能。这两个功能现在已经合并到SparkSession中，但是SparkContext和sqlContext还没有被弃用，仍然可以单独访问(图3)。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lt"><img src="../Images/1b17a9f325d054a473de121f1d1fe6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3ti0DybtejTldWs9w7xVw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3: SparkContext and sqlContext</figcaption></figure><p id="880e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">转换、动作和执行</strong></p><p id="0899" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">转换是帮助您实现逻辑的功能。人们可以使用这些来修改底层数据。转换的例子有<strong class="kf jh">选择、过滤、分组、排序、限制</strong>等。转换在本质上是<em class="lu">惰性的</em>，这意味着直到一个动作被调用时处理才被执行。Spark只是构建一个谱系，并等待调用一个动作来执行该谱系中的转换(图4)。惰性求值使得并行执行操作变得容易，并允许各种优化。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lv"><img src="../Images/b8ba65f8c55d425ba17aac78b1d2d6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*IEkH-B2Wox860rSAC3m-BA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4: Spark lineage</figcaption></figure><p id="7143" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">转换本质上可以是狭义的或广义的。窄转换不会导致混洗(数据在网络上的机器间移动)，即计算结果所需的数据最多驻留在一个分区上。另一方面，大范围转换会导致混乱，因为底层数据驻留在许多分区中，并且需要跨机器重新分布。</p><p id="f4c6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">动作</strong></p><p id="1b31" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">动作是当遇到时触发来自沿袭的计算的语句。虽然变革本质上是懒惰的，但行动是热切的。操作的例子有show()、count()、collect()等。例如</p><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="c8b3" class="lm ln jg li b gy lo lp l lq lr">1. Data_file = spark.read.csv(“/home/users/some_random_data.csv”)</span><span id="55a9" class="lm ln jg li b gy ls lp l lq lr">2. Selection_df = data_file.select(“first column”, “second column”)</span><span id="cd89" class="lm ln jg li b gy ls lp l lq lr">3. Filter_df = selection_df.where(“second column is not NULL”)</span><span id="2a43" class="lm ln jg li b gy ls lp l lq lr">4. Group_df = filter_df.groupBy(“first column”)</span><span id="dd72" class="lm ln jg li b gy ls lp l lq lr">5. Count_df = Group_df.count()</span></pre><p id="301c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的代码示例展示了一个典型的spark应用程序。对于步骤1-4，我们从csv文件中读取数据并应用一系列转换。Spark只是在内存中为这些步骤构建一个谱系，并不执行实际的处理。步骤5是一个计数动作，当Spark到达时，它将追溯到步骤1，执行所有处理并输出计数。</p><p id="da21" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">流水线作业</strong></p><p id="b1c8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每当Spark应用程序遇到洗牌时，数据就会被写入执行器磁盘。但是，洗牌操作之前的所有步骤都可以放在一起，并且一次执行。这就是所谓的流水线技术，结合Spark在内存中进行处理而不会立即溢出到磁盘的事实，它可以使查询更快。</p><p id="d6fd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">催化剂优化器</strong></p><p id="d3dd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark数据框架——构建在Spark SQL之上——使用底层catalyst optimizer获得性能速度。Catalyst optimizer找到最有效的方法来应用您的转换和操作。Catalyst optimizer是数据帧性能优于rdd的原因。</p><p id="76f4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:rdd不包括在Databricks Spark认证中，所以我们在这里不讨论它。</p><p id="2d13" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每当我们使用Spark SQL(也可以是pySpark中的dataframe代码)运行一个查询时，在转换成物理计划并执行之前，它都要经历几个计划阶段。使用Dataframes和Spark SQL意味着您依赖catalyst optimizer来优化您的查询计划，而不是使用rdd并自己完成。例如，</p><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="52f2" class="lm ln jg li b gy lo lp l lq lr">### Example By: <a class="ae jd" href="https://www.linkedin.com/in/bigdatabysumit" rel="noopener ugc nofollow" target="_blank">Sumit M</a> <br/>### For example, you are trying to calculate average salary of employees by age</span><span id="6045" class="lm ln jg li b gy ls lp l lq lr">### using RDDs<br/>fileRdd = sc.textFile(“/employeeData.csv”)<br/>fileRdd.map( x =&gt; {val fields = x.split(",");\<br/>       (fields(1), fields(2)) }) \ <br/>       .map(x =&gt; (x._1, (x._2, 1))) \ <br/>       .reduceByKey((x,y) =&gt; (x._1 + y._1, x._2 + y._2))\<br/>       .map( x =&gt; (x._1, x._2._1 / x._2._2))</span><span id="e8c7" class="lm ln jg li b gy ls lp l lq lr">### using Dataframes<br/>data_df = fileRdd.toDF(“username”, “age”, “salary”)<br/>data_df.groupBy($”age”).agg(avg(“salary”))</span></pre><p id="3d60" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">在RDD进场:</strong></p><p id="0960" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用Lambda函数准确地告诉Spark“如何做”, Spark不能优化它。它直接将这些函数发送给执行器来处理数据。如果有任何可能的优化，我们必须自己做。</p><p id="096d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">在数据帧方法中:</strong></p><p id="298f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用声明的方式告诉Spark“做什么”,而将“如何做”的部分留给Spark的优化器。这使得通过Catalyst optimizer优化数据帧成为可能。</p><p id="4525" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">催化剂优化器工作</strong></p><p id="4d95" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们使用Spark SQL提交查询时，它会经历以下步骤。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lw"><img src="../Images/eee6d1ce1beef4d338245164f964f8c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sC8GqsoLQ7m1W6xk.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5: Catalyst Optimizer working (Image courtesy: <a class="ae jd" href="https://databricks.com/glossary/catalyst-optimizer" rel="noopener ugc nofollow" target="_blank">Databricks blog</a>)</figcaption></figure><ul class=""><li id="e344" class="lx ly jg kf b kg kh kk kl ko lz ks ma kw mb la mc md me mf bi translated">它创建一个未解析的逻辑计划，并检查列名和表名等的有效性。</li><li id="662b" class="lx ly jg kf b kg mg kk mh ko mi ks mj kw mk la mc md me mf bi translated">之后，创建一个已解析的逻辑计划。在这一步，命令可能会被重新组织以优化性能。</li><li id="54ed" class="lx ly jg kf b kg mg kk mh ko mi ks mj kw mk la mc md me mf bi translated">Catalyst optimizer可能在此阶段生成至少一个物理计划来执行查询。这个阶段代表了Spark在应用优化后实际要做的事情。</li><li id="fa3a" class="lx ly jg kf b kg mg kk mh ko mi ks mj kw mk la mc md me mf bi translated">如果有多个物理计划，则使用成本模型评估每个计划的成本。选择具有最佳性能的计划，编译成java字节码，然后执行。</li></ul><p id="4aab" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">缓存</strong></p><p id="15b7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Spark可以在计算期间将数据存储在内存中。这是进一步加快查询速度的好方法。我们知道Spark是一个内存处理引擎，但是在开始处理之前，它必须从磁盘读取一次数据。例如</p><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="933b" class="lm ln jg li b gy lo lp l lq lr">Data_file = spark.read.csv(“/home/users/some_random_data.csv”)</span></pre><p id="c734" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是第一次从磁盘读取，并且该读取所属的每个沿袭都必须从磁盘读取。但是您可以使用缓存将它存储在内存中以加快处理速度。</p><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="8e80" class="lm ln jg li b gy lo lp l lq lr">df_cached = Data_file.cache()<br/>df_persisted = Data_file.persist()</span></pre><p id="8c32" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">cache()和persist()是用于内存存储的内置Spark函数。cache()仅存储默认值(仅限内存),然而，persist有几个选项，例如内存和磁盘持久性。更多细节可以在<a class="ae jd" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="d054" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以对应用程序逻辑中经常访问的任何数据进行同样的操作。在反复使用相同数据集的应用程序中，缓存是最强大的优化技术之一。当您缓存一个数据帧时，它的每个分区都将临时存储在它的执行器的内存中，这将使即将到来的读取更快。</p><p id="96d2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">Spark应用的生命周期</strong></p><p id="b51e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经讨论了Spark应用程序的核心组件。让我们看看应用程序本身的生命周期。假设您创建了一个pyspark应用程序<code class="fe ml mm mn li b">my_first_app.py</code>并提交给集群。</p><pre class="lb lc ld le gt lh li lj lk aw ll bi"><span id="5752" class="lm ln jg li b gy lo lp l lq lr">spark-submit \<br/>--master &lt;url&gt; \ <br/>--deploy-mode cluster \ <br/>--conf &lt;some_key&gt; = &lt;some_value&gt; \ <br/>my_first_app.py \</span></pre><p id="1450" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，这个应用程序将与资源管理器通信，并请求运行资源。如果请求成功，在其中一个节点上启动驱动程序(图6)。由于这是一个打包的应用程序，代码中的第一件事应该是在SparkSession上创建。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9be158f7a3093fb100078503cde45d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*iNnyDlGhe5qYkE2rdhHlfA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 6: Driver Launch</figcaption></figure><p id="8cff" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建SparkSession后，它将与集群管理器通信，请求在集群中启动Spark executor进程。请记住，执行器数量和相关配置(内核、RAM等)是由用户在提交应用程序时设置的。集群管理器通过启动执行器进程来响应请求，并将相关信息发送给驱动程序进程。之后，驱动程序和执行器进行通信，移动数据，驱动程序将任务调度到每个执行器上(图7)。每个执行者用状态(成功或失败)和结果来响应。在图7中，我们总共有三个节点。在节点01上，驱动程序进程正在运行。在Worker 01上，两个执行器正在运行，而Worker 02上只有一个执行器进程在运行。</p><figure class="lb lc ld le gt is gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6b4b3cb2b5ad9e786e4ed3bd6284a662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*GshdAP_46o4st1eW2qX3dQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 7: Drivers and Executors in action</figcaption></figure><p id="9c6c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完成后，驱动程序以成功或失败退出，集群管理器关闭集群中的执行器。</p><p id="10fc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您在<code class="fe ml mm mn li b">my_first_app.py</code>中编写的实际代码定义了您的Spark应用程序，每个应用程序可以有一个或多个作业。</p><p id="36c2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，一个动作有一个火花工作。Spark作业被分解为一系列阶段。阶段表示可以一起执行的一组任务，例如Select后跟Where等。每当数据需要在执行器之间转移时(例如在连接查询中)，Spark就会创建一个新的阶段。每个阶段由几个任务组成，这些任务在执行器的可用槽中运行。</p><p id="fe45" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">总结:</strong></p><p id="e325" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">驱动程序是一个向多个执行者发送任务的程序。Spark性能的秘密在于并行性，即将工作分配给多个虚拟机的能力。</p><p id="4946" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，Spark不会在读取操作后立即执行操作(惰性评估)。相反，它建立了一个将应用于源数据的数据转换计划。只有当你调用一个动作时，这个计划才会被执行。</p><p id="1acf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Catalyst Optimizer旨在为应用您在代码中调用的转换和操作找到最有效的计划。</p><p id="854d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">恭喜你！您已经了解了Spark架构的基础知识。还有一些事情，特别是关于Spark UI的，我将在以后的某个时间讨论。在本系列接下来的文章中，我将重点介绍使用数据框架的Spark编程，因为它是认证的最大部分。</p><p id="2db5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我鼓励你在LinkedIn 上关注<a class="ae jd" href="https://www.linkedin.com/company/justenough-spark/" rel="noopener ugc nofollow" target="_blank">just suffith Spark(顺便说一句，这不是我的页面:p ),了解关于Spark和数据块的日常mcq。</a></p><p id="8be9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快乐学习:)</p></div></div>    
</body>
</html>