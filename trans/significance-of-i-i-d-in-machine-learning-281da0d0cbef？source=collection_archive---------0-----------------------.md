# I.I.D .在机器学习中的重要性

> 原文：<https://medium.datadriveninvestor.com/significance-of-i-i-d-in-machine-learning-281da0d0cbef?source=collection_archive---------0----------------------->

## 独立同分布假设是几乎所有机器学习算法的核心，也是大多数统计推断中的一个显式假设

![](img/ab8375099bb19b4a63617b63e09f1ac5.png)

Photo by [Edge2Edge Media](https://unsplash.com/@edge2edgemedia?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

让我们试着理解它是什么，为什么它在机器学习和统计学中如此重要

独立同分布是指当分布良好时，既独立又同分布。让我们试着进一步细分。

# 什么使变量独立？

所谓独立，我们是指从单个随机变量中抽取的样本是相互独立的。从随机变量中抽取的样本之间不包含任何内部相关性

让我们看看独立分布和非独立分布的简单例子:

# 独立事件

*   想象一下抛硬币。如果你在第一次试验中得到正面，在下一次试验中得到正面或反面的概率不会改变(仍然是 50-50 的概率)。每次抛硬币都是相互独立的。另一点要注意的是，不管你掷的是公平硬币还是不公平硬币，每个样本都是独立于其他样本的。类似地，如果你掷骰子，结果和这些结果的样本是相互独立的。

如果我们想把掷硬币和掷骰子组合成一个样本，比如说我想分别从掷硬币和掷骰子组合中得到(H，2 ),它们仍然保持相互独立。

在这种情况下:

![](img/3265c155466e8dd61c6cb92aa56089ce.png)

# 相依事件

*   由骰子决定移动的蛇梯游戏就是一个依赖事件的例子。这个特殊的游戏也被称为一阶马尔可夫链，其中唯一重要的是棋盘的当前状态，下一个状态由当前状态和下一次掷骰子决定。任何马尔可夫序列都可以被认为是非独立(或相关)分布，我们可以清楚地看到一个状态或样本对其先前状态的潜在依赖性

# 是什么让发行版完全相同？

理解同分布有不同的方法。让我们看看几种理解它的方法:

**数学上:**

*   如果我们以相同的方式从相同的基础数学函数中采样，则样本是同分布的
*   样本中的所有项目都取自同一个概率分布

**总的来说:**

*   如果样本来自同一个随机变量，则分布是相同的
*   它也可以解释为:对于所有考虑的样本，生成数据的基本机制必须是相同的

这种情况的一个例子是选择样本偏差，其中您有来自一个子群或人口阶层的更多训练数据，并希望将其推广到整个人口

注意:同分布并不意味着相关的随机变量需要有相同或相似的概率

现在我们对 I.I.D .有了一个很好的概念，让我们试着理解是什么让它在机器学习中如此重要。

# I.I.D .在机器学习中的重要性

*   让我们举一个监督学习的例子。这里，我们将数据集分为训练和测试数据集，在训练数据集上进行训练，在测试数据集上测试模型性能。
    在将数据划分为训练-验证-测试集时，一个内置的假设是独立身份假设。如果训练集和测试集之间的分布不同，或者如果存在内置的采样依赖关系，则算法在部署/运行后将无法泛化。
    需要注意的另一点是，我们还假设数据分布在部署后不会改变。如果它发生变化(由于环境不稳定，称为数据集转移)，我们可能需要重新训练模型，或者使用主动学习/在线学习技术来保持我们的模型最新。
*   支配这一想法的基本原则被称为**经验风险最小化(ERM)** ，它是许多机器学习和数据挖掘算法的核心。ERM 本身值得一篇单独的深入文章，但简言之，它传达了不可能计算与将特征向量 X 映射到标签 Y 的假设 h 相关联的真实风险，因为我们不知道该算法将处理的完整数据的真实分布。因此，我们可以通过对训练数据的损失函数进行平均来计算经验风险，并专注于选择最佳假设来最小化经验风险
*   独立同分布假设也是**大数定律**的核心，该定律指出，来自大样本人群的观察样本平均值将接近真实总体平均值，并且随着样本量的增加，它将更接近真实总体平均值
*   独立同分布假设也是数据科学中最广泛使用的定理之一的核心，**中心极限定理(CLT)** 是假设检验的核心。CLT 指出，如果我们从总体中随机抽取足够大的样本，那么样本均值将近似呈正态分布。正如你所注意到的，随机抽取的样本不能是相关的，随机变量的分布也不能随着时间而改变

因此，在某种程度上，I.I.D .的假设有助于简化训练机器学习算法，因为它假设数据分布不会随时间或空间而改变，样本不会以任何方式相互依赖。这最终帮助我们将训练限制在人群的子集，并最终部署我们的模型来预测进一步的数据集。

在接下来的几篇文章中，我们将更深入地探讨这个假设被打破的场景，以及我们如何使我们的模型对不同类型的数据集变化更具弹性。

您可以使用几个链接来深入了解这个主题:

[数据科学实验室](https://datasciences.org/non-iid-learning/)

[经验风险最小化](https://en.wikipedia.org/wiki/Empirical_risk_minimization)

[独立同分布的随机变量](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)

很高兴听到你的反馈。你可以通过 [LinkedIn](https://www.linkedin.com/in/sundaresh-prasanna-chandran-17825811a/) 联系我

喜欢我的文章？给我买杯咖啡

 [## sundaresh 正在创作与数据科学相关的文章，并且热爱教学

### 嘿👋我刚刚在这里创建了一个页面。你现在可以给我买杯咖啡了！

www.buymeacoffee.com](https://www.buymeacoffee.com/sundaresh)