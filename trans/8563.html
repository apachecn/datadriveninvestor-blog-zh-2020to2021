<html>
<head>
<title>Decision Tree and Random Forest: Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林:Python中的机器学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/decision-tree-and-random-forest-machine-learning-in-python-b62b7c82db87?source=collection_archive---------14-----------------------#2021-01-14">https://medium.datadriveninvestor.com/decision-tree-and-random-forest-machine-learning-in-python-b62b7c82db87?source=collection_archive---------14-----------------------#2021-01-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ffcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决定，决定，决定…我们每天都在做无数的决定；不知不觉地或有意识地，有时不费吹灰之力就自动做到了，有时却为另一件事苦恼了几个小时。要是有一种方法能绘制出通往结论的路径就好了。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/8c702de964c6cadbd3c9bbc1e7d059e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OV9Z62PDs7QqoKOR2_oGSA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Sample Decision Tree</figcaption></figure><p id="10c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树是最流行和最强大的分类和回归工具之一。顾名思义，它是一个树状结构的流程图。正是决策树的这一特性使得它们易于理解和解释。</p><h1 id="a2c3" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">决策树的剖析</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ma"><img src="../Images/bc715b411877a187fbb887a85bca6a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-RCKG8mY-XQkY0qrWb3tw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Image Source: <a class="ae mb" href="https://www.researchgate.net/figure/The-General-Anatomy-of-a-Classification-or-Regression-Tree_fig1_5246556" rel="noopener ugc nofollow" target="_blank">researchgate.net</a></figcaption></figure><blockquote class="mc md me"><p id="5e16" class="jn jo kl jp b jq jr js jt ju jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj kk ij bi translated">研究论文:<a class="ae mb" href="https://www.researchgate.net/publication/5246556_Machine_Learning_Methods_Without_Tears_A_Primer_for_Ecologists" rel="noopener ugc nofollow" target="_blank">奥尔登，朱利安&amp;劳勒，约书亚&amp;波夫，N..(2008).没有眼泪的机器学习方法:生态学家入门。生物学季刊。83.171–93.10.1086/587826.</a></p></blockquote><ul class=""><li id="f15c" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated"><strong class="jp ir">根节点:</strong>树结构的第一个节点就是根节点。</li><li id="19b4" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">父节点:</strong>任何有子节点的节点称为父节点或内部节点。</li><li id="412b" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">子节点:</strong>给定节点的任何子节点称为子节点。</li><li id="367d" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">叶节点:</strong>没有任何子节点的节点是叶节点或终端节点。</li></ul><h1 id="6b27" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">构建决策树的算法</h1><p id="d6bb" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">有几种方法可以建立决策树。算法的选择取决于数据中目标变量的类型。这些算法包括:</p><ul class=""><li id="8f21" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated"><strong class="jp ir">ID3——D3算法</strong> <br/>的扩展，代表<strong class="jp ir">迭代分割器3 </strong>，基本上意味着算法<strong class="jp ir">在每一步重复地将特征</strong>分成两组或更多组。这是一种自上而下的贪婪方法。仅用于带有名义特征的<strong class="jp ir">分类问题</strong>。</li><li id="499a" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">C4.5—ID3</strong>的继承者<br/>也称为<strong class="jp ir">统计分类器</strong>，c 4.5是ID3算法的扩展。它比ID3有一些优势；能够<strong class="jp ir">处理连续值和离散值</strong>；<strong class="jp ir">是否允许标记为<em class="kl">的缺失属性值</em></strong>？；它<strong class="jp ir">提供创建后的修剪</strong>。</li><li id="7413" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir"> CART —分类和回归树</strong><br/>CART算法的结构类似于一系列跟进<strong class="jp ir"> ' <em class="kl">问题</em> ' </strong>。我们基于这些“<em class="kl">问题</em>”来确定节点的分割。该算法对分类和回归问题都有效。结果是一个<strong class="jp ir">二叉树</strong>结构。</li><li id="4bda" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir"> CHAID —卡方自动交互检测器<br/> </strong>该算法可以处理名义数据、序数数据和连续数据。它会创建每个分类预测值的所有可能的交叉表，直到获得最佳结果，并且不可能再进行拆分。</li><li id="0856" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir"> MARS —多元自适应回归样条<br/>T5<strong class="jp ir">复杂非线性回归问题的算法</strong>。该算法发现一组简单的<strong class="jp ir">分段线性函数</strong>，并在<strong class="jp ir">集合中使用它们进行预测</strong>。在某种意义上，该模型可以被视为线性函数</strong>的<strong class="jp ir">集合。</strong></li></ul><h1 id="0086" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">属性选择度量</h1><p id="f44d" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">对于N个属性的数据集，我们必须定义一个条件来决定将哪个属性作为树的根节点或父节点。为了解决这个问题，研究人员研究并设计了一些解决方案:</p><ul class=""><li id="5a0e" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk mn mo mp mq bi translated"><strong class="jp ir">熵</strong> <br/>它是随机性的<strong class="jp ir">度量</strong>。熵越高，就越难从信息中得出任何结论。</li><li id="740e" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">信息增益</strong> <br/>一种统计属性，衡量给定属性根据目标分类将训练样本分离的程度。当<strong class="jp ir"> <em class="kl"> IG </em>高而熵</strong>低时，选择一个特征。我们假设属性是<strong class="jp ir">分类的</strong>。</li><li id="176e" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">基尼指数</strong> <br/>与<strong class="jp ir">分类目标变量</strong>一起工作，并且只执行<strong class="jp ir">二元分割</strong>。基尼系数越高，表明同质性越高。它通常与<strong class="jp ir">购物车算法</strong>一起使用。我们假设<strong class="jp ir">连续属性</strong>。</li><li id="b37b" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">增益比</strong> <br/> <strong class="jp ir"> C4.5算法</strong>使用增益比——信息增益的一种修改<strong class="jp ir">减少其偏差</strong>，通常是最佳选择。它克服了信息增益的缺点，因为它考虑了在进行分割之前可能产生的分支数量。</li><li id="d7f2" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">方差减少</strong> <br/>用于<strong class="jp ir">连续/回归问题</strong>。该算法使用<strong class="jp ir">标准方差公式</strong>作为分裂条件。</li><li id="fcf6" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk mn mo mp mq bi translated"><strong class="jp ir">卡方</strong> <br/> <strong class="jp ir"> CHAID算法</strong>使用卡方方法分割特征。它也适用于<strong class="jp ir">分类目标变量</strong>。它可以执行两次或多次拆分。较高的卡方值表明子节点和父节点之间的差异具有较高的统计显著性。</li></ul><h1 id="8eef" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">从头开始实现决策树</h1><blockquote class="mc md me"><p id="ddfe" class="jn jo kl jp b jq jr js jt ju jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong>本例使用的数据集:<a class="ae mb" href="https://github.com/itsDV7/Internity-Practice-Notebooks/blob/main/Day-09" rel="noopener ugc nofollow" target="_blank"> haberman.cs </a> v来自<a class="ae mb" href="https://www.kaggle.com/gilsousa/habermans-survival-data-set" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></blockquote><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h1 id="3b00" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">决策树的缺点</h1><p id="6f75" class="pw-post-body-paragraph jn jo iq jp b jq mw js jt ju mx jw jx jy my ka kb kc mz ke kf kg na ki kj kk ij bi translated">如果对决策树的增长没有设置约束，它们往往会使训练数据过拟合。如果没有约束，树可能会完全“记住”训练数据。这将为您提供100%的训练数据准确性，但无法使用新数据提供这样的结果。</p><p id="c504" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种方法可以消除过度拟合:</p><ol class=""><li id="8820" class="mi mj iq jp b jq jr ju jv jy mk kc ml kg mm kk nd mo mp mq bi translated">修剪决策树。</li><li id="aa03" class="mi mj iq jp b jq mr ju ms jy mt kc mu kg mv kk nd mo mp mq bi translated">随机森林。</li></ol><h2 id="528e" class="ne ld iq bd le nf ng dn li nh ni dp lm jy nj nk lq kc nl nm lu kg nn no ly np bi translated">修剪决策树</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d69b72eec415d52b9420cd31a6901fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*b2UjS8hFp4V0CRPNQaDERA.gif"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Image Source: <a class="ae mb" href="https://cdn.edureka.co/blog/wp-content/uploads/2015/01/Decision_blog_animation_01-1.gif" rel="noopener ugc nofollow" target="_blank">edureka.co</a></figcaption></figure><p id="6285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在修剪的时候，我们通过移除那些使我们的树过度拟合的树枝来修剪树，而不会降低精确度。这是借助于将原始数据集分割成测试训练数据，并根据从测试数据获得的准确度来优化树来完成的。</p><h2 id="94f1" class="ne ld iq bd le nf ng dn li nh ni dp lm jy nj nk lq kc nl nm lu kg nn no ly np bi translated">随机森林</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/16426f474d65409bb03bcaa2487954bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/1*Oe2dOP4eerBqPDG4CeBJ2w.gif"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Image Source: <a class="ae mb" href="https://towardsdatascience.com/why-random-forests-outperform-decision-trees-1b0f175a0b5" rel="noopener" target="_blank">towardsdatascience.com</a></figcaption></figure><p id="05ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机森林是集成学习的一个例子，其中我们组合多个决策树以获得更好的预测性能。随机森林通常用“Bagging法”——Bootstrap聚集法来训练。Bagging是一种元算法，旨在提高机器学习算法的稳定性和准确性。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="01fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">本文涵盖了各种决策树算法和其他参数的基本定义。建议对这些定义进行更多的搜索，并尝试使用不同的数据集。</em></p><blockquote class="nz"><p id="e1fc" class="oa ob iq bd oc od oe of og oh oi kk dk translated">感谢阅读。<br/>别忘了点击👏！</p></blockquote></div></div>    
</body>
</html>