<html>
<head>
<title>Ensemble Techniques— Bagging (Bootstrap aggregating)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成技术——Bagging(引导聚集)</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/ensemble-techniques-bagging-bootstrap-aggregating-c7a7e26bdc13?source=collection_archive---------4-----------------------#2021-01-28">https://medium.datadriveninvestor.com/ensemble-techniques-bagging-bootstrap-aggregating-c7a7e26bdc13?source=collection_archive---------4-----------------------#2021-01-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d5814fc338fa32c13a39148339349c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hc_NprJ6uugN4zG_"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@daseine?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Fitore F</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="b32e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">集成机器学习可以主要分类为打包和提升。bagging技术对回归和统计分类都有用。Bagging与决策树一起使用。</h2></div><p id="20b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Bootstrap aggregating也称为bagging，是一种机器学习集成元算法，旨在提高统计分类和回归中使用的机器学习算法的稳定性和准确性。它还减少了方差，有助于避免过度拟合。虽然它通常应用于决策树方法，但它可以用于任何类型的方法。Bagging是模型平均方法的一个特例。</p><p id="2209" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们想要减少决策树的方差(过度拟合)时，使用Bagging (Bootstrap Aggregation)。装袋包括以下步骤:</p><p id="7829" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">引导取样:</strong></p><p id="7aa0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过替换，可以从随机选择的训练数据中获得几个数据子集。这个数据集合将用于训练决策树。Bagging将使用训练数据的bootstrap采样来构建n个决策树。因此，我们最终会得到不同模型的集合。</p><p id="cb3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">打包由两部分组成:聚合和引导。Bootstrapping是一种采样方法，使用替换方法从一组样本中选择一个样本。然后，对选择的样本运行学习算法。</p><p id="8452" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自举技术使用替换采样来使选择过程完全随机。当选择一个没有替换的样本时，变量的后续选择总是依赖于先前的选择，因此使标准非随机。</p><p id="dbfc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">聚集:</strong></p><p id="18f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来自所有独立模型的输出被聚集成单个预测，作为最终模型的一部分。就回归而言，输出只是预测结果值的平均值。在分类方面，选择具有最高频率输出的类别。</p><p id="64b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与boosting不同，bagging涉及以并行的方式训练一堆单独的模型。使用Bootstrap聚合的优点是，它允许通过对从总体数据的随机样本测量的多个估计值进行平均来减少模型的方差。</p><p id="1bcf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型预测经过聚合，将它们组合成最终预测，以考虑所有可能的结果。可以基于结果的总数或者基于从过程中每个模型的引导中导出的预测的概率来进行聚集。</p><p id="b861" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">集成机器学习可以主要分类为打包和提升。bagging技术对回归和统计分类都有用。Bagging与决策树一起使用，它在减少方差和提高准确性方面显著提高了模型的稳定性，从而消除了过度拟合的挑战。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/a00cf65e00ef69a3fd33aaac5386ef32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WeG7SJyxSaeJ5isOV73d2A.png"/></div></div></figure><p id="b66c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lz"> Bagging </em>(也称为<a class="ae jg" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank"> Bootstrap aggregation </a>)是第一个也是最基本的集成技术之一。它是由<a class="ae jg" href="https://en.wikipedia.org/wiki/Leo_Breiman" rel="noopener ugc nofollow" target="_blank">利奥·布雷曼</a>在1994年提出的。Bagging基于<a class="ae jg" href="https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29" rel="noopener ugc nofollow" target="_blank">自举</a>的统计方法，使得复杂模型的很多统计量的评估变得可行。</p><p id="e465" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自举方法如下。设有一个大小为N的样本X，我们可以从原始样本中随机均匀地抽取N个元素，用替换的方法，从原始样本中制作一个新样本。换句话说，我们从原始大小的样本中选择一个随机元素，并这样做N次。所有元素被选择的可能性相等，因此每个元素以1/N的相等概率被绘制</p><p id="ea89" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们一次从一个袋子里抽出一个球。在每一步，所选的球被放回袋中，以便等概率地进行下一次选择，即从相同数量的球n中进行选择。请注意，因为我们将球放回袋中，所以新样本中可能会有重复的球。我们姑且称这个新样品为<em class="lz"> X1 </em>。</p><p id="fafd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过重复这个过程M次，我们创建M个<em class="lz">引导样本</em> X1，… XM。最后，我们有足够数量的样本，可以计算原始分布的各种统计数据。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ma"><img src="../Images/51f56283d99aaa503f1f4b71677ef04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FKUvC_NQ5w7eEX6l.png"/></div></div></figure><p id="7cb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于我们的例子，我们将使用熟悉的telecom_churn数据集。数据集可以在Kaggle网站上找到。让我们将数据可视化，看看这个特征的分布。</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="mb mc l"/></div></figure><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/1d99bad4b1b2b66543170b59ba2559ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*s6IraGw4afVILr-XqFZlqA.png"/></div></figure><p id="3576" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来忠诚的顾客打电话给客服的次数比那些最终离开的顾客少。现在，估计每组客户服务电话的平均数量可能是个好主意。由于我们的数据集很小，我们不能通过简单地计算原始样本的平均值来得到一个好的估计。应用自举方法会更好。让我们从原始总体中生成1000个新的bootstrap样本，并生成平均值的区间估计。</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="mb mc l"/></div></figure><p id="8dc5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于置信区间的解释，你可以参考<a class="ae jg" href="https://www.graphpad.com/guides/prism/7/statistics/stat_more_about_confidence_interval.htm?toc=0&amp;printWindow" rel="noopener ugc nofollow" target="_blank">本</a>简明笔记或任何统计学课程。说置信区间包含95%的值是不正确的。请注意，忠诚客户的时间间隔更窄，这是合理的，因为与那些一直打电话直到厌倦并决定更换提供商的烦躁客户相比，他们打电话的次数更少(0、1或2次)。</p><h1 id="fd74" class="me mf jj bd mg mh mi mj mk ml mm mn mo kp mp kq mq ks mr kt ms kv mt kw mu mv bi translated">装袋:</h1><p id="dfb2" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">现在你已经掌握了自举的概念，我们可以继续进行<em class="lz">打包</em>。</p><p id="eefa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们有一个训练集x。使用自举，我们生成样本X1，…，Xm。现在，对于每个引导样本，我们训练它自己的分类器<em class="lz"> ai(x) </em>。最终分类器将对所有这些单独分类器的输出进行平均。在分类的情况下，这种技术对应于投票:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9ce541138c474ce7d14c32e00b8876b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*ftMpx6JjqkfbhmuEe1BtAg.png"/></div></figure><p id="0e51" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图说明了这种算法:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/192853d28c1f252d090524e8084b0075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x2Ea8WqMO2Ir4nq6.png"/></div></div></figure><p id="6631" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们考虑一个使用基本算法<em class="lz"> b1(x) </em>，…，<em class="lz"> bn(x) </em>的回归问题。假设存在为所有输入定义的真实答案y(x)的理想目标函数，并且定义了分布p(x)。然后，我们可以将每个回归函数的误差表示如下:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/87953da5ce0d56f0037c32bdd5c15ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*EyMVITR3wV3ir7nFZnuN_g.png"/></div></figure><p id="8385" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以及均方误差的期望值:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/d4867fc928c7f0a045c855caee704cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*ZE0UsCv6XK_ecPJbKyCFUQ.png"/></div></figure><p id="035f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，所有回归函数的平均误差将如下所示:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/294074c0aa270be2fb2405eb9655200f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*_8lmzSMNZKnXpsh3QiNNYw.png"/></div></figure><p id="a7e2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们假设误差是无偏和不相关的，即:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/436dc39d6707c748e49bb8e7c4965085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*o9KKlDJm3kvk9k3ojCIWrQ.png"/></div></figure><p id="11c0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们构建一个新的回归函数，它将对各个函数的值进行平均:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c80d0dae09756bf6a72f5aef7c02d1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*hRvaCYfoSF9OtKQvTOyw1Q.png"/></div></figure><p id="9ef0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们找出它的均方误差:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e7f75ea666ce05b083942dc4e8ddf016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*XvkeW6DJMbt6DTfHxfxehQ.png"/></div></figure><p id="3b70" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，通过平均个人答案，我们将均方差降低了n倍。</p><p id="eafa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上一课中，让我们回忆一下构成总样本外误差的成分:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/65e4d2adf0b1eb0daf00a2fe5483e02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*m2qle3YlUn5vk9JmO0oMEA.png"/></div></figure><p id="5b81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们在不同数据集上训练模型时，Bagging通过减少误差的差异来减少分类器的方差。换句话说，套袋可以防止过度拟合。bagging的效率来自于这样的事实，即由于不同的训练数据，各个模型非常不同，并且它们的误差在投票期间相互抵消。此外，在一些训练bootstrap样本中，异常值可能会被忽略。</p><p id="b968" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe nj nk nl nm b">scikit-learn</code>库支持用元估计器<code class="fe nj nk nl nm b">BaggingRegressor</code>和<code class="fe nj nk nl nm b">BaggingClassifier</code>打包。您可以使用大多数算法作为基础。</p><p id="47d7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看bagging在实践中是如何工作的，并将其与决策树进行比较。为此，我们将使用来自<a class="ae jg" href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py" rel="noopener ugc nofollow" target="_blank"> sklearn文档</a>的一个例子。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/0cdbd613368f8be00b896174bc7d5623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qYWEnxolWjV_gjpwhgEEiw.png"/></div></div></figure><p id="80f0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树的错误:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4129b9b19e4ca8e1eb1a0ff3dc3da827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*pNF9WgOdxx1vVqetw2CS2w.png"/></div></figure><p id="b628" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用装袋时的错误:</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/14285e35e204917f1abab3cf2b6934cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*3JNTyDFI6DBQs3ONDkCzeg.png"/></div></figure><p id="a655" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上图可以看出，装袋的误差方差要低得多。记住，我们已经从理论上证明了这一点。</p><p id="0b13" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Bagging在小数据集上是有效的。即使丢弃一小部分训练数据，也会导致构建本质上不同的基本分类器。如果您有一个大型数据集，您将生成一个小得多的引导样本。</p><p id="84a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的例子不太可能适用于任何实际工作。这是因为我们做了一个强有力的假设，即我们的个体误差是不相关的。通常，这对于现实世界的应用程序来说过于乐观了。当这一假设为假时，误差的减少将不会如此显著。在接下来的讲座中，我们将讨论一些更复杂的集成方法，这些方法可以在现实世界的问题中实现更准确的预测。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="cb6f" class="nx mf jj bd mg ny nz dn mk oa ob dp mo lh oc od mq ll oe of ms lp og oh mu oi bi translated">免责声明:</h2><p id="7a96" class="pw-post-body-paragraph ky kz jj la b lb mw kk ld le mx kn lg lh my lj lk ll mz ln lo lp na lr ls lt im bi translated">作者:<a class="ae jg" href="https://www.linkedin.com/in/vitaliyradchenk0/" rel="noopener ugc nofollow" target="_blank">维塔利·拉德琴科</a>和<a class="ae jg" href="https://yorko.github.io/" rel="noopener ugc nofollow" target="_blank">尤里·卡什尼茨基</a>。<a class="ae jg" href="https://www.linkedin.com/in/christinabutsko/" rel="noopener ugc nofollow" target="_blank">克里斯蒂娜·布茨科</a>、<a class="ae jg" href="https://www.linkedin.com/in/egor-polusmak/" rel="noopener ugc nofollow" target="_blank">叶戈尔·波鲁斯马克</a>、<a class="ae jg" href="https://www.linkedin.com/in/anastasiamanokhina/" rel="noopener ugc nofollow" target="_blank">阿纳斯塔西娅·马诺奇娜</a>、<a class="ae jg" href="http://linkedin.com/in/anna-shirshova-b908458b" rel="noopener ugc nofollow" target="_blank">安娜·希尔绍娃</a>、<a class="ae jg" href="https://www.linkedin.com/in/yuanyuanpao/" rel="noopener ugc nofollow" target="_blank">鲍媛媛</a>翻译编辑。</p><p id="6760" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本材料受<a class="ae jg" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank">知识共享CC BY-NC-SA 4.0 </a>许可的条款和条件约束。允许出于任何非商业目的免费使用。</p></div></div>    
</body>
</html>