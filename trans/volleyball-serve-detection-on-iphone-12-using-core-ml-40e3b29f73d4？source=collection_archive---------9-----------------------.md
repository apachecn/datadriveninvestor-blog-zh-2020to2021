# 基于 Core ML 的 iPhone 12 排球发球检测

> 原文：<https://medium.datadriveninvestor.com/volleyball-serve-detection-on-iphone-12-using-core-ml-40e3b29f73d4?source=collection_archive---------9----------------------->

![](img/0a2bb9d4b0df6130b05f829002ab8c2d.png)

Source: Unsplash

机器学习和人工智能解决方案将移动应用开发推向了一个新的高度。集成了机器学习技术的应用程序可以成功地识别和分类图像、人的声音和动作，从图像中识别文本并进行翻译。这个清单可以很容易地继续下去。然而，工程师们面临的主要问题仍然是在不损失处理速度的情况下，将具有数百万个连接的巨大模型传输到手机上，最重要的是，质量仍然保持不变。

![](img/9f8013875f86f51383010c306f532f78.png)

# 移动设备上 ML 的主要优势

让我们来看看在移动设备上实现的 ML 模型的主要优势:

1)接近实时的处理。不需要进行 API 调用来发送数据并等待模型提供响应结果。对于处理来自设备上摄像机的视频流的应用程序来说，这可能是一个关键点。

2)离线可用性。不再需要连接到任何网络来使用这种应用程序。

3)隐私。您的数据永远不会离开设备。这意味着您不需要将数据发送到任何地方进行处理。所有计算都在设备上进行。

4)成本低。应用程序在没有网络连接的情况下运行，并且没有 API 调用。这使您可以在任何地方使用应用程序，而没有任何脊约束。

# 移动设备上 ML 的主要缺点

尽管移动设备上的机器学习看起来很有前景，但它也有缺点。

1)应用程序大小。向移动应用程序添加机器学习模型会显著增加该应用程序的大小。

2)系统利用率。移动设备上的预测涉及大量计算资源，这会增加电池消耗。

3)模特培训。通常，在移动设备上工作的模型应该在该设备之外被训练。当我们需要重新训练我们的模型时，这个问题也会重复出现。

苹果团队创建了一个名为 Core ML 的框架，旨在解决包含机器学习模型的移动应用程序开发过程中可能出现的问题。该工具是为移动设备上的机器学习模型集成而创建的，用于所有苹果产品，通过轻松集成预训练模型来提供快速预测。这使您可以开发在 Apple 设备上实时处理实时图像或视频的应用程序。

# 核心 ML 的主要可能性

–实时图像识别

–人脸检测

–文本预测

–扬声器识别

–行动分类

–对表格数据的预测

–不同的定制型号

# 它是如何工作的？

Core ML 使用的是在外部某个地方(你的本地电脑或者云平台等)训练过的机器学习模型。)然后转换成 Core ML 合适的格式。为了开发应用程序，包括源代码创建和模型集成，使用了 Apple 兼容平台的 IDE，称为 Xcode。

![](img/18765e611dec596603bb716f824d1bf4.png)

# 核心 ML 如何训练模型？

为了简化模型开发阶段，您可以使用 Create ML 应用程序构建和训练您的模型。这个工具已经和 Xcode 捆绑在一起了。使用 Create ML 训练的模型在核心 ML 中合适(。mlmodel)格式，并准备好在您的应用中使用。

Create ML 支持五种不同类别的模型:

1)图像—分类和检测模型
2)声音—分类
3)运动—分类
4)文本—分类和标记
5)表格—分类、回归和推荐模型。

为 Core ML 训练模型的另一种方法是使用 coremltools，这是一个 python 包，用于在。mlmodel 格式。目前，它可用于解决以下任务:

–将来自流行机器学习工具的训练模型转换为核心 ML 格式(。mlmodel)

–使用核心 ML 框架进行预测。

使用 pip 安装软件包:pip 安装 coremltools。这个简短的代码示例演示了如何将 tf.keras 模型转换为。适合核心 ML 的 mlmodel 格式。我们在项目中使用了 coremltools 的 4.0 版本。

![](img/8ef000e471620fb25702621c4afddcd1.png)

# 任务内容

在我们的研究中，我们试图解决体育分析的一个典型问题——动作检测。主要条件——机型要集成到 iPhone 12 中。我们选择排球作为目标运动，选择“发球”作为目标动作。为了简化问题，我们假设从球场两边发出的球都应该被归类为发球。作为训练数据集，我们使用了公开的[排球活动数据集](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/vb14/)，其中包含七个具有挑战性的排球活动类别，由奥地利排球联盟(2011/12 赛季)的专业人士在六个视频中进行了注释。我们还通过自己收集的 25 个视频扩展了这个数据集。

主要的挑战是，有时我们看不到发球的球员(他在摄像机后面)。所以这个模型对于可见的和隐藏的服务都应该足够好。

# 我们尝试创建 ML

## 动作分类器

我们的第一个假设是来自 createML 的 ActionClassifier 应该可以解决我们的问题。我们受到了一些使用它的教程的启发。只是为了简单的概述，你可以从官方页面查看这个[视频](https://developer.apple.com/videos/play/wwdc2020/10043/)。

简而言之，动作分类器将一组身体关键点(通常为 19 个)作为输入，并提供一类动作作为输出。窗口大小通常由开发者决定，但是对于我们的问题，我们将窗口大小设为 30 帧(大约 1 秒的持续时间)。

在实验过程中，我们面临的问题是，当只有一个人出现在屏幕上时，动作分类器可以完美地工作。我们没有找到一个很好的方法来使用苹果的 MLActionClassifier 进行多人动作分类，这就是为什么我们在这个领域的研究没有成功。

## 图像分类器

我们的第二个方法是使用 createML 的另一个工具——image classifier。如前所述，我们有一个包含可见和不可见服务的数据集，这就是为什么我们决定尝试事件分类而不是动作分类。换句话说，我们计划根据所有玩家的位置对图像进行分类。在发球的情况下，这并不困难——除了一个人之外，所有的球员都站在球网附近，试图不让对手看到自己的发球。对方球队也有一个强大的模式——所有球员都是静止的，准备接球。

在 CreateML 中训练一个图像分类模型真的很容易——你不需要写任何一行代码。算法如下:

1)组织训练数据集目录。每一类图像都应该位于一个特定的目录中。

2)组织测试数据集目录。对训练集执行相同的操作。

3)创建图像分类项目。

4)配置训练集，设置扩充，并设置历元数。

5)根据图像的数量、大小和其他一些参数，等待一段时间。

6)保存模型并在 XCode 项目中使用它。

然而，我们在这一步也面临一些问题，即:

–图像分类器的精确度不够高。我们获得了大约 70%的发球检测准确率。

–训练模型的时间真的很长——在 Macbook 上使用约 15000 个样本的数据集超过 10 个小时。

# 我们是如何解决这些问题的？

我们决定继续处理分类问题，但现在，我们准备了一个数据集，其中每个样本是一组 30 个连续的灰度帧。照常输出——类的标签。在使用 tf.keras 框架对模型进行训练后，我们将其转换为. mlmodel。最终得分约为 0.87 (f1 分)。

# 如何整合。mlmodel 进入项目？

一旦你训练完毕。mlmodel 把它集成到你的 Xcode 项目中真的很容易——你只需要把它复制到项目的目录中。如果你点击一个带有模型的文件，你会看到一个窗口，里面有关于它的所有可用信息(见下面的截图)。

在第一个选项卡上，您可以看到有关神经网络内部各层的信息。

![](img/1f690b8e38551aded5a8308431f6a25c.png)

在第二个选项卡上，您可以检查输入和输出数据类型和维度。

![](img/793402fafb63faebb5c2688448b20627.png)

还有一件更有用的事情——您可以点击“自动生成的 Swift 模型类”,查看为类生成的模型的所有定义、输入和输出。这些信息应该有助于您将模型集成到代码中。

# 结论

Core ML 是一个非常强大的工具，它允许您将任何复杂的模型集成到您的移动设备中。为了简化集成过程，苹果团队开发了 CreateML 框架，允许人们用最少的代码编写创建不同的 ML 模型。然而，我们做的项目表明 CreateML 并不适合所有的任务。在我们的案例中，我们在排球发球检测领域进行了研究，但我们无法使用该框架获得良好的结果。总的来说，我们不能使用 CreateML 框架为多人实现一个动作分类器和一个分数足够高的图像分类器。

因此，我们应用了使用 tf.keras 的另一种方法，并进一步转换为. ml 模型。因此，排球比赛的发球检测模型以实时速度(26 FPS)工作，F1 得分接近 87%。