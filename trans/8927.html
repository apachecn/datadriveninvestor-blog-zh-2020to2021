<html>
<head>
<title>Research on Neural-Backed Decision Trees Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经支持的决策树算法研究</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/research-on-neural-backed-decision-trees-algorithms-111af8e2f92c?source=collection_archive---------8-----------------------#2021-01-26">https://medium.datadriveninvestor.com/research-on-neural-backed-decision-trees-algorithms-111af8e2f92c?source=collection_archive---------8-----------------------#2021-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/a4f26fba77f552a268156dbcd3f537b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*oK3q4kAf7UmXvUyn.png"/></div></figure><p id="9952" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">来自先宇科技的监利</em></p><h1 id="b605" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">背景</h1><p id="3297" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">仙芋上很多业务场景都需要算法来实现分类，比如图像分类、组件识别、产品分层、纠纷类别预测等。在这些情况下，通常需要算法模型来提供可解释的识别结果。换句话说，模型需要识别类别，并在识别过程中使用明显的类别层次结构和来源。因此，<strong class="jw ir">实现可解译影像分类</strong>成为项目开发需求。为此，我对神经支持的决策树(NBDT)算法进行了调查。</p><p id="3f0f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> NBDT </strong>是加州大学伯克利分校和波士顿大学在2020年4月发表的一篇论文中介绍的一个模型。你可能已经注意到，字母“B”在NBDT不代表“助推”，而是代表“支持”。因此，熟悉GBDT的读者不应该把NBDT误解为一种新型的梯度推进决策树。NBDT只是一棵决策树，而不是多棵。</p><h1 id="d2f9" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h1><p id="6d25" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">NBDT的特点是在决策树之前集成了一个神经网络(NN)。神经网络通常是卷积神经网络(CNN)。据我所知，NBDT的结构大致由CNN和决策树(DT)组成。</p><p id="16ff" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">目前，NBDT用于图像分类。它的优势不在于准确。在笔者的实验中，其准确率略低于前面的CNN。其真正的优势在于其平衡<strong class="jw ir">模型准确性</strong>和<strong class="jw ir">模型可解释性</strong>的能力。具体来说，NBDT可以通过稍微牺牲CNN的准确性来实现比任何树模型高得多的分类准确性。此外，有了决策树，NBDT还可以明确地逐步展示模型推理的基础。例如，NBDT不仅能识别一只狗的图像，还能让识别过程中的每一步都清晰可见。起初，NBDT以99.49%的概率将图像识别为“动物”。然后，它以99.63%的概率将图像识别为“脊索动物”。接下来，它以99.4%的概率将图像识别为“食肉动物”。最后以99.88%的概率将图像识别为“狗”。这种推理方法增强了模型的可解释性。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi lw"><img src="../Images/71c8a9058aa2a618751d72cf6d9bb4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XTUs0D99yZKFfTD9.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 1: Dog Classification (Referenced From Official Demo)</em></figcaption></figure><p id="33eb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">详细原则</p><p id="b299" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">NBDT采用预培训+微调的框架。整个过程大致分为以下三个步骤:</p><h2 id="a295" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">第一步:预先训练一个CNN模型，将CNN的最后一层权值作为每一类的隐向量。</h2><p id="bea2" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">例如，使用cifar10(具有10个类别的图像分类数据集，包括“猫”和“狗”)来训练一个resnet18 CNN。这种CNN的最后一层通常是全连接(FC)层。假设倒数第二层输出的向量维数为d，那么，FC层W的维数为W，这样，W的每个列向量正好对应一个类别，可以看作每个类别的隐藏向量。这种方法类似于Word2Vec。</p><h2 id="0f6d" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">第二步:使用类别的隐向量进行层次聚类，使用WordNet形成层次树结构。</h2><p id="1627" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">在本文中，这种树形结构被称为“诱导层次结构”。首先，我们对类别的隐藏向量实现层次聚类，这可以在源代码中通过直接调用sklearn模块的<code class="fe mw mx my mz b">AgglomerativeClustering</code>类来实现。建立聚类层次结构后，我们会遇到两个问题:</p><ol class=""><li id="94a8" class="na nb iq jw b jx jy kb kc kf nc kj nd kn ne kr nf ng nh ni bi translated">两个子节点可以通过聚类算法来聚类，并且两个子节点都表示实体的类别。但是，它们的父节点没有实体描述。</li><li id="624a" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr nf ng nh ni bi translated">我们需要一种方法来表示集群子节点的父节点的隐藏向量。</li></ol><p id="174b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了解决第一个问题，作者使用了<a class="ae no" href="https://en.wikipedia.org/wiki/WordNet" rel="noopener ugc nofollow" target="_blank"> WordNet </a>，这是一个包含名词之间上下位关系的词网。在Python中，WordNet模块可以直接导入到NLTK模块中，在那里可以调用它。由于一个叶节点有一个实体描述，比如cifar10的10个类别，所以可以通过WordNet找到两个叶节点最近的共同祖先。WordNet中猫狗最近的共同祖先是食肉动物这个词。所以把食肉动物作为猫和狗这两个词的父节点。根据层次聚类的结果，可以自下而上地命名父节点，直到只有一个根节点。这样就形成了一个诱导层次，如图2的<strong class="jw ir">步骤1 </strong>所示。诱导层次也是图1中用于狗分类的决策树。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi np"><img src="../Images/7f1ed7396aadb82ebe3274a23033fe0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3zgKj64r81UrYK6e.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 2: Training and Inference (Referenced From the Original Paper)</em></figcaption></figure><p id="f268" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了解决第二个问题，作者使用子节点隐藏向量的平均值来表示父节点的隐藏向量。<em class="ks">参见图3中</em> <strong class="jw ir"> <em class="ks">步骤C </em> </strong> <em class="ks">的描述。</em></p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi np"><img src="../Images/ed70c03c3c1486d2a173215af0df00a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PxMiLwB43s3UVMBp.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 3: Building Induced Hierarchies (Referenced From the Original Paper)</em></figcaption></figure><h2 id="bccb" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">步骤3:将诱导层次的分类损失添加到总损失中，并微调模型。</h2><p id="1a84" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">诱导层次结构(树形结构，以下简称DT)建立后，完整的模型不再是CNN，而是CNN + DT。为了迫使模型基于从根节点到叶节点的树结构来预测新样本，我们需要将树结构的分类损失添加到总损失中，并对模型进行微调。</p><p id="7864" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们需要了解完整模型使用的预测方法:我感觉作者的思路直击事情的本质。当一个新的样本(图像)进入模型时，图像首先到达CNN。在图像到达最后的FC层W之前，CNN向图像输出d维向量x。</p><p id="601b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然后，对x和W(本质上是与每个列向量的内积)执行矩阵乘法，以获得样本在每个类别中的logits分布。如果设置了softmax参数，则可以获得概率分布。因为W的每个列向量表示DT叶节点的隐藏向量，所以这个DT可以完全替换W。这意味着不直接对x和W执行矩阵乘法，而是从根节点开始遍历DT，以依次计算x和DT的每个子节点的内积。遍历DT节点有两种方式:<strong class="jw ir">硬</strong>和<strong class="jw ir">软</strong>。假设DT是一棵二叉树。如果使用硬模式，则分别计算x和左右两个子节点的内积。每次，x被分类到具有较大内积的子节点中，直到x到达最后一个叶节点，这表示x所属的最终类别。如果使用软模式，则遍历所有中间节点，并计算x的内积。叶节点的最终概率是x到达叶节点的路径上所有中间节点的概率积。最终，基于所有叶节点的最终概率的比较来确定x所属的类别。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi np"><img src="../Images/680711bc6b9bcc4635ddce8337fcd75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b9iI5UyndLCA4xWy.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 4: Node Probability Calculation (Referenced From the Original Paper)</em></figcaption></figure><p id="be61" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在了解了完整模型的预测细节后，我们可以解释诱导层次(树结构)的分类损失。相应的，损失函数也有硬和软两种模式，如图5所示。如果使用硬模式，损失函数将样本沿着DT中的真实路径所属的叶节点的分类损失(以特定权重)相加。不沿着真实路径的节点(图A中虚线w3和w4)不被考虑。基于交叉熵计算每个节点的分类损失。如果使用软模式，则叶节点上的最终概率分布和真实的独热分布之间的交叉熵被直接计算为损失。<strong class="jw ir">简而言之，硬模式下的损失函数计算路径的交叉熵，而软模式下的损失函数计算叶节点的交叉熵。</strong>在Pytorch中，交叉熵是这样计算的:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi lw"><img src="../Images/79256bfffeaa081fc4bace295acf8937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OV1wT-FDPGorCFlY.png"/></div></div></figure><p id="803d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">CrossEntropy(x,class)=-log⁡(exp⁡(x[class])Σjexp⁡(x[j]))=-x[class]+log⁡(Σjexp⁡(x[j]))</code></p><p id="85e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最终模型的总损失还考虑了原CNN的分类损失(Lossoriginal)。因此，需要微调的总损耗如下:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/63d70b65cf9da36cd5e1b5e044bac76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/0*kGfvomqgB6fPg3V9.png"/></div></figure><p id="6694" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><code class="fe mw mx my mz b">Losstotal=Lossoriginal+Losshard or soft</code></p><p id="fcca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">根据我对源代码的理解，当损失函数执行反向传播(BP)时，它仍然优化了CNN的网络权重。损失函数迫使前一个CNN的输出满足后一个DT的期望。这样，样本按照DT的推理路径输出的预测类别就尽可能符合真实类别。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi np"><img src="../Images/839e1127df99e0fe16a9df8e22541780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ulC7kpQVBgxzE-o4.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 5: Loss in Hard and Soft Modes (Referenced From the Original Paper)</em></figcaption></figure><h1 id="a41d" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">源代码分析</h1><p id="554f" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated"><a class="ae no" href="https://github.com/alvinwan/neural-backed-decision-trees" rel="noopener ugc nofollow" target="_blank">NBDT的Python代码在GitHub </a>上开源。一般来说，代码是用Pytorch和Networkx实现的。总共有4000多行代码，四个核心脚本分别是<strong class="jw ir"> model.py、loss.py、graph.py、hierarchy . py</strong>代码几乎没有注释和参数解释，不容易读懂。下图显示了对几个核心代码段的分析。</p><h2 id="280b" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">构建诱导层次</h2><p id="0a0e" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">核心函数是<strong class="jw ir"> build_induced_graph </strong>，用于输入叶子节点的WordNet IDs和CNN模型。然后，该函数从CNN模型中获取FC权重，执行层次聚类，并使用WordNet来“命名”聚类结果。这形成了一个DT，它的节点具有实体意义。该函数对应于本文详细原理一节中的要点2。该功能描述如下:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nr"><img src="../Images/e28811716c0961255ca177c4606f0835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h3HUL_2pNGeOH1Mg.jpeg"/></div></div></figure><h2 id="215c" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">正向计算节点概率</h2><p id="a899" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">如前所述，新样本进入模型后，首先到达CNN。在样本到达FC层之前，模型输出d维向量x，然后计算x和DT节点的隐向量的内积。DT节点的隐藏向量等于其子节点的隐藏向量的平均值。<strong class="jw ir"> get_node_logits </strong>方法优化:<strong class="jw ir">由于向量均值的内积等于向量</strong>的内积均值(如下式所示)，所以不需要显式求解隐向量，计算其内积。相反，一个节点的所有子节点的logit的平均值可以用作该节点本身的logit。具体代码如下:</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ns"><img src="../Images/51536928243dd7c86e59e1feb2f16a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XQXnRgAZi4viDQkH.png"/></div></div></figure><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nt"><img src="../Images/702828ebc78c9baade9f3983a936b7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2WHhB3TBbHh52zVj.jpeg"/></div></div></figure><h2 id="b913" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">总损失函数</h2><p id="bb09" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">如前所述，总损耗等于原始CNN损耗和树形结构损耗之和。让我们以硬模式为例。下面的代码解释了如何计算沿着DT路径的树结构的损失，然后将损失合并到总损失中。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nu"><img src="../Images/4c37c341f37cf27714571de26a9f5a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M6RklSwucOudfHWO.jpeg"/></div></div></figure><h2 id="bf58" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">实验</h2><p id="c248" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">作者在多个数据集上将原始CNN (WiderResnet28×10)与多个可解释神经网络模型进行了比较。下表显示，NBDT的精确度略低于原来的CNN，但远远超过其他模型。这表明NBDT已经达到了最先进的状态。<strong class="jw ir">在NBDT，使用软模式时的得分高于硬模式。</strong>这很容易理解，因为软模式考虑的是全局优化，而硬模式考虑的是连续的局部优化。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi nv"><img src="../Images/584d0d853e9b8a8302774180e8bd45e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sLX3mFnGPr2_i8S-.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk"><em class="mj">Figure 6: Experimental Results (Referenced From the Original Paper)</em></figcaption></figure><h2 id="09c3" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">使用</h2><blockquote class="nw nx ny"><p id="49f5" class="ju jv ks jw b jx jy jz ka kb kc kd ke nz kg kh ki oa kk kl km ob ko kp kq kr ij bi translated"><em class="iq">有关安装和使用的更多信息，请参见GitHub官方页面。本文仅总结常用方法。</em></p></blockquote><h2 id="1c00" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">在命令行中预测</h2><p id="f719" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">您可以直接运行nbdt命令，后跟图像路径(URL或本地路径)。在第一次执行期间，您必须下载WordNet和官方的预培训模型。预训练模型适用于cifar10数据集。因此，最好输入10个类别之一的图像。输出显示预测是逐步执行的。</p><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oc"><img src="../Images/c51e6c56b4958ef819f507ca8c44ea2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w7TGUg3U3NqC3JB1.jpeg"/></div></div></figure><h2 id="cf40" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">用Python预测</h2><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi od"><img src="../Images/396a250bcb971f1e9dc7c3a77d45d751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fOoJeMFRcs9XfUr3.jpeg"/></div></div></figure><h2 id="c301" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">完全使用</h2><figure class="lx ly lz ma gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi oe"><img src="../Images/1a411fdf541f99b8c525b3c16d412a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4QSjj5TYmAejzfom.jpeg"/></div></div></figure><h1 id="df36" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">后续计划</h1><p id="2edf" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">NBDT研究的目标是找到一种方法，使分类可以解释。这种可解释的方法可以用在分类期间需要DT的任何场景中。尽管这篇文章关注的是一个图像分类场景，但是任何分类过程都可以使用NBDT来解释，只要前面的CNN被其他网络所取代。例如，在仙寓的高质量产品分层项目中，我们可以根据我们的业务知识建立产品的诱导层次。例如，第一层可以将输入分类为专业销售者和个人销售者，第二层可以将输入分类为高、中、低销售率。然后，最后一层可以将投入分为不同级别的高质量产品。然后，我们可以基于这些层次训练NBDT来执行分类。</p><p id="9419" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">另一个典型的例子是图片分类，一个卖家上传一张图片给仙宇。在这里，卖家希望算法能够自动识别出他想要销售的产品的类别。卖家可能会上传一张椅子或桌子的图片，但正确的类别是“家具”基于层次结构的NBDT可以自动将销售者发布的产品识别为“家具”或者，NBDT也可以提供推荐选项，允许卖家选择他们的类别。这消除了手动填充的需要。未来，NBDTs可能会用于其他任务。</p><h2 id="81c0" class="mk ku iq bd kv ml mm dn kz mn mo dp ld kf mp mq lh kj mr ms ll kn mt mu lp mv bi translated">参考</h2><ul class=""><li id="e512" class="na nb iq jw b jx lr kb ls kf of kj og kn oh kr oi ng nh ni bi translated"><a class="ae no" href="https://arxiv.org/abs/2004.00221" rel="noopener ugc nofollow" target="_blank">论文</a></li><li id="22f8" class="na nb iq jw b jx nj kb nk kf nl kj nm kn nn kr oi ng nh ni bi translated"><a class="ae no" href="https://github.com/alvinwan/neural-backed-decision-trees" rel="noopener ugc nofollow" target="_blank">源代码</a></li></ul><h1 id="2114" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">原始来源:</h1><div class="oj ok gp gr ol om"><a href="https://www.alibabacloud.com/blog/research-on-neural-backed-decision-trees-algorithms_597094" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">神经支持的决策树算法研究</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">先宇科技2021年1月4日292作者:先宇科技监利先宇上的很多商业场景都需要算法…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">www.alibabacloud.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa js om"/></div></div></a></div></div></div>    
</body>
</html>