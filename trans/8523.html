<html>
<head>
<title>Separating Hyperplanes for classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分离分类超平面</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/separating-hyperplanes-for-classification-52de4a1c5321?source=collection_archive---------14-----------------------#2021-01-13">https://medium.datadriveninvestor.com/separating-hyperplanes-for-classification-52de4a1c5321?source=collection_archive---------14-----------------------#2021-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/47d908eb78489db2ccee4e47e6e2d73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IWhGw3K28m9hWzcg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@mihaly_koles?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mihály Köles</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="c374" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph">机器学习理论</h2><div class=""/><div class=""><h2 id="046f" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">深度学习和支持向量机的起源</h2></div><p id="dcd7" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">分离超平面过程构造线性决策边界，该边界显式地尝试尽可能将数据分成不同的类。有了它们，我们将定义支持向量分类器。</p><p id="d65a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">有时，在上一篇文章中解释的LDA和逻辑回归<a class="ae jd" href="https://medium.com/datadriveninvestor/the-basic-methods-for-classification-9c10a961b0ee" rel="noopener">会产生可避免的错误，这可以使用以下方法解决。</a></p><h1 id="5513" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">罗森布拉特感知机</h1><p id="9a45" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">该算法是现代深度学习进展的前身，它试图通过最小化误分类点到决策边界的距离来寻找一个分离超平面。目标是最小化以下功能:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/835491b4625cad551516bcf3bc2ec7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*yE1sJQdSdCVy_cvYShS5-w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Function to minimize, self-generated.</figcaption></figure><p id="5d0a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中<strong class="lg jq"> <em class="nc"> M </em> </strong>索引误分类点的集合。该数量是非负的，并且与错误分类的点到由<strong class="lg jq"> <em class="nc"> β^T x + β0 = 0定义的决策边界之间的距离成比例。</em> </strong>如果我们假设M是固定的，则梯度由下式给出:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/ac31b9c417be2462825c16dcb859c64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*v0hHlc-6DR5wcOrW7aN7iQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Gradient functions, self-generated.</figcaption></figure><p id="cf85" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该算法使用随机梯度下降来最小化分段线性准则。这意味着我们在每个输入值之后在负梯度方向上计算一步，而不是在处理所有数据之后计算。因此，错误分类的数据值以某种顺序被访问，并且<strong class="lg jq"> <em class="nc"> β </em> </strong>被更新为:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/787e9b212981079ee7da708c7a4d2967.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*_NngI6estiJHzYwrYMsXRQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Beta update function, self-generated.</figcaption></figure><p id="420f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中p是学习率，必须根据数据来选择。如果存在可分离的超平面解，则该算法收敛于它，但是它有一些问题:</p><ul class=""><li id="44a6" class="nf ng jg lg b lh li lk ll ln nh lr ni lv nj lz nk nl nm nn bi translated">当数据是可分的时，有许多解，找到的解取决于起始值。</li><li id="8dac" class="nf ng jg lg b lh no lk np ln nq lr nr lv ns lz nk nl nm nn bi translated">寻找解决方案的有限步骤可能是巨大的。班级之间的差距越小，找到它的时间就越长。</li><li id="4cd5" class="nf ng jg lg b lh no lk np ln nq lr nr lv ns lz nk nl nm nn bi translated">当数据不可分时，算法将不会收敛，并开始在可能的最优值中循环。</li></ul><p id="9871" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">通过创建基函数和增加原始空间，我们可以解决第二点，通过向数据添加约束，我们可以解决第一点。</p><h1 id="7f22" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">最优分离超平面，支持向量分类器</h1><p id="a432" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">最优分离超平面将两个类分开，并最大化每个类到闭合点的距离。有了这些，我们可以通过最大化两个类之间的差额来实现唯一的解决方案。</p><p id="d42e" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">优化问题是:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/b34b75dc8ba60276b7ba970000e703d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OLXgowVSHm2Y0AJQddApew.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">SVC optimization problem, self-generated.</figcaption></figure><p id="627c" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这些条件下，我们确保所有点距离决策边界的距离至少为M。我们可以用as条件去掉<strong class="lg jq">|<em class="nc">| |β| | = 1</em></strong>约束:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/51f7bef34957d78231458dde7fab97bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*3UHzmTM5WnA1KUli8z4gqg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Constraint conditions, self-generated.</figcaption></figure><p id="8199" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于对于任何满足这些不等式的<strong class="lg jq"><em class="nc">【β】</em></strong>和<strong class="lg jq"><em class="nc">【β0】</em></strong>，任何正比例倍数也满足这些不等式，所以，我们可以设<strong class="lg jq"> <em class="nc"> ||β||= 1/M. </em> </strong></p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a5e4640a0e81d58492ac25c69ab3c5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*khja0Qjcc1bViQq_VHoVIg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Optimization problem simplified, self-generated.</figcaption></figure><p id="8def" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，我们利用拉格朗日函数来解决最小化问题:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/2eb9f67fc6ec288713eb42966905e626.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*ooompl57TiIb-EElH84_0Q.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LaGrange function of the minimization problem, self-generated.</figcaption></figure><p id="26e8" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">将导数设置为0:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b8d44421d11c58c75db96c5a53380a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*ub7ZmOkOIHjWHRsQoz7qXg.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">LeGrange derivatives, self-generated.</figcaption></figure><p id="c795" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">将它们减去Lp，我们得到Wolfe对偶:</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ccc529d5ac7012a94a9acd3220a3104a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*bNaAHpMBB5hd7NtbQ8K3Tw.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Wolfe dual, self-generated.</figcaption></figure><p id="b40e" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">通过最大化<strong class="lg jq"> <em class="nc"> L_D </em> </strong>，我们找到了解，并且必须满足卡鲁什-昆-塔克条件。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/40b169a28ff9c134a8497c07151234a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*a1fQCenSnKhhZV9QgFzInA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Karush-Khun-Tucker conditions last condition, self-generated.</figcaption></figure><p id="ee5a" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在我们可以说</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/91952d74f9c8996b89d48a499da9333e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-Shbi6J4huKR4pEcpnawg.png"/></div></div></figure><p id="4913" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">解决方案向量仅使用决策实验室中的值来定义，这些值被定义为支持点。</p><h1 id="1d7f" class="ma mb jg bd mc md me mf mg mh mi mj mk kv ml kw mm ky mn kz mo lb mp lc mq mr bi translated">结论</h1><p id="daed" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">仅使用决策实验室值，该解决方案对模型错误设定更具鲁棒性。与之前解释的模型相比，当数据不是高斯型时更好。如果是高斯分布，LDA会更快，性能更好。</p><p id="5c27" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当数据不可分时，没有可行的模型，您可以尝试使用基变换来扩大空间，但这会导致过拟合问题。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="82ee" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是我的第38篇文章，我将在GitHub、Twitter和Medium ( <a class="ae jd" href="https://medium.com/u/48c8d3ce491d?source=post_page-----7053db93ba6----------------------" rel="noopener"> Adrià Serra </a>)上发布这个挑战的进展。</p><p id="73e1" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><a class="ae jd" href="https://twitter.com/CrunchyML" rel="noopener ugc nofollow" target="_blank">https://twitter.com/CrunchyML</a></p></div></div>    
</body>
</html>