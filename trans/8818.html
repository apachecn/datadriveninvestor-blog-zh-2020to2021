<html>
<head>
<title>Contribution of ‘Padding’ in Python for NLP Projects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的“填充”对NLP项目的贡献</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/padding-used-in-nlp-are-they-improvers-2f4613bd3648?source=collection_archive---------1-----------------------#2021-01-22">https://medium.datadriveninvestor.com/padding-used-in-nlp-are-they-improvers-2f4613bd3648?source=collection_archive---------1-----------------------#2021-01-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="79f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python-tensor flow，我们可以在NLP阶段使用促进填充进行句子分段和分析。那怎么做呢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0317fa20c1a97f3ece58791b1abeca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*154DOv7tLmafVumXOFSiDw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/de-de/@joao-vitor-heinrichs-862489?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">João Vítor Heinrichs</a> from <a class="ae ky" href="https://www.pexels.com/de-de/foto/bestand-an-holzstammen-mit-kratzern-5022456/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="e537" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP告诉我们，要按顺序做很多类似的事情。由于TensorFlow和Keras库，这些非常实用，可以很容易地集成到系统中。所以第一件事就是进口它们。事实上，我们的工作是能够使用现有的API。因为轮子已经发明了！</p><p id="d09e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP是一组操作，旨在帮助机器在向量的帮助下对句子结构进行有意义的编码。当我们从这些现有的基础出发时，我们可以在小规模和类似的工作中看到标准。但是，随着数据的增长，不可避免地需要一些补丁。</p><p id="7965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我们指的是什么？</strong></p><p id="465c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，当你进入作品的时候，是的，很多基本表达上的相似之处在句子编号的时候是非常容易的。例如，如下所示:</p><p id="b9bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">'我喜欢这个游戏'<br/>'你喜欢打网球'<br/>'我和你一样喜欢'</p><p id="fdaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这三个句子总共有12个单词，但当我们除去常见的，只有8个保持独特。所以在用记号赋予器引导它们之前，在NLP过程中很容易枚举它们。以下是代码:</p><p id="4fa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要导入我们将使用的TensorFlow库。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="fb1f" class="ma mb it lw b gy mc md l me mf">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras.preprocessing.text import Tokenizer</span></pre><p id="02e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们把上面的例子作为代码的延续，扔给一个变量。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8e55" class="ma mb it lw b gy mc md l me mf">sentences=[<br/>'I like this game’<br/>‘You like tennis play’<br/>‘i like as you'<br/>]</span></pre><p id="bc10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们让编号过程从tokenizer变量开始。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2997" class="ma mb it lw b gy mc md l me mf">tokenizer = Tokenizer(num_words = 100)<br/>tokenizer.fit_on_texts(sentences)<br/>word_index = tokenizer.word_index</span></pre><p id="0a51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">让我们看看代码做了什么:</strong></p><p id="02d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们有几个已知的或可以在这里统计的例句，实际上，很明显我们会为此对单词进行编号。我们已经数过了，开头也说过了。然而，当处理<code class="fe mg mh mi lw b">num_words</code>更大的数据时，等式对于大的数字可以是一致的。因为这里的目标是优化训练时间，所以在给出数字时要小心。但是，我们将为num_word输入数字100,“假设”这是一个常规的默认值。</p><p id="9547" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的第二个观察中，<code class="fe mg mh mi lw b">fit_on_texts</code>获取value=sentences变量中的数据，并开始对内容进行编码。</p><p id="9c06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<code class="fe mg mh mi lw b">word_index</code>返回一个包含键值对的字典。在这里，该关键字将被赋予一个唯一的值。现在让我们看看打印的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b7323d1cba88889fad580bce13a0bdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*JA2KwZ-LBMwY5BapzV3-Rg.png"/></div></figure><p id="02c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，重复的单词被赋予了唯一的值。这就像用Python中的set参数返回列表中的unique，但是字典在表达式中带有格式key &amp; value。首先理解这一点很重要。</p><h2 id="c87e" class="ma mb it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">我们正在向需要补丁的地方前进</h2><p id="0ca9" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们上面只是想解释如何快速构建句子间有相似性的小系统。然而，这个NLP对于我们的业务来说还是太小了。</p><p id="1b5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以让我们把事情弄得更复杂一点。首先，我们的目标是用我们现在用数字表达的单词来创建向量句子结构。对此，我们可以用<code class="fe mg mh mi lw b">texts_to_sequence</code>进入神经网络创建和训练数据准备阶段。拟合不同尺寸的类似过程也用于利用图像的模型训练。在理解和使用正确的API方面，我们处于一个重要的位置。</p><p id="128a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，让我们通过检查上面的例子来继续。首先，样本三个句子由相同的数字和相似的单词组成。现在让我们来看看这个:</p><p id="dc27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">补充一句:</strong>‘你到底喜欢哪个游戏？’<br/>我们可以如下创建“序列变量”。文本中字符串表达式的标记器:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5515" class="ma mb it lw b gy mc md l me mf">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras.preprocessing.text import Tokenizer</span><span id="10e3" class="ma mb it lw b gy ng md l me mf">sentences = [<br/>    'I like this game',<br/>    'You like tennis play',<br/>    'i like as you',<br/>    'Which game do you like really?'<br/>]</span><span id="4159" class="ma mb it lw b gy ng md l me mf">tokenizer = Tokenizer(num_words = 100)<br/>tokenizer.fit_on_texts(sentences)<br/>word_index = tokenizer.word_index</span><span id="ec6b" class="ma mb it lw b gy ng md l me mf">sequences=tokenizer.texts_to_sequences(sentences)</span><span id="8215" class="ma mb it lw b gy ng md l me mf">print(word_index)<br/>print(sequences)</span></pre><p id="6bd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">输出:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/1aac3a959500a79d7d499fae0e03d7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DWDtJDGIzT4yqTkt5x7Ww.png"/></div></div></figure><p id="66c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您所看到的，当我们将数字替换为我们指定的关键字<code class="fe mg mh mi lw b">word_index</code>时，我们的最后一句话可以像前三句话一样进行排序。文本之间的单词被机器识别、编码和排列。</p><p id="e37e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个阶段，我们可以说这是所有情感分析研究不可或缺的。因为如果我们想要从文本中提取的单词不能由相同的数字和它们在字符串中的位置来确定，那么一切都将变得毫无意义。</p><p id="abf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们根据列举的词语考虑一个新的测试数据:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7efd" class="ma mb it lw b gy mc md l me mf">test= [<br/>    'i really like this game',<br/>    'your husband likes this tennis'<br/>]</span><span id="7b70" class="ma mb it lw b gy ng md l me mf">test_sequences=tokenizer.texts_to_sequences(test)</span><span id="fa12" class="ma mb it lw b gy ng md l me mf">print(test_sequences)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2d4fc89f165e3244a3d2e57a21ce7312.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*_IlUviPUzykXk8VAUXBFVg.png"/></div></figure><p id="5739" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第一句，我们有足够的数字，但对于第二句，我们不能说同样的话。好吧，我们的问题是:</p><ul class=""><li id="1ad2" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">"我们是接受这种形式的第二句，还是忽略它？"</li></ul><p id="d893" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，我们会忽略一些东西，但我们会为此人为地填补空白。当我们遇到一个在我们的系统中不可见的值时，我们不会将其留空，而是再次输入一个特殊的值。</p><p id="ca61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">所以参数</strong>中的&lt; OOV &gt;</p><p id="d9ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个特殊的令牌执行这个功能。在tokenizer变量的参数中用<code class="fe mg mh mi lw b">oov_token("&lt;OOV&gt;"</code>表示。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9357" class="ma mb it lw b gy mc md l me mf">tokenizer=Tokenizer(num_word=100, oov_token="&lt;oov&gt;") </span></pre><p id="a584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们的记号赋予器变量获得了新的特性。然后输出相同的测试结果:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7fd1" class="ma mb it lw b gy mc md l me mf">print(test_sequences)</span><span id="994e" class="ma mb it lw b gy ng md l me mf">output:</span><span id="86e2" class="ma mb it lw b gy ng md l me mf">[[4, 12, 2, 6, 5], [1, 1, 1, 6, 7]]</span></pre><p id="6b0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，被分配了编号1，这也影响了其他现有的编号。</p><p id="9f8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为这个过程的延续，填充是我们填充数组中空白空间的材料，用于创建相同大小的向量。为此，必须首先从库中导入它:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="71e5" class="ma mb it lw b gy mc md l me mf">from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><p id="3ca1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们来看看如何显示我们在开始时举例的句子中的4个句子:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="77b3" class="ma mb it lw b gy mc md l me mf">sentences = [<br/> ‘I like this game’,<br/> ‘You like tennis play’,<br/> ‘i like as you’,<br/> ‘Which game do you like really?’]</span><span id="8468" class="ma mb it lw b gy ng md l me mf">padded=pad_sequences(sequences)</span><span id="ab89" class="ma mb it lw b gy ng md l me mf">print(word_index)<br/>print(padded)</span></pre><p id="9fe6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">输出:</strong></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="41c1" class="ma mb it lw b gy mc md l me mf">{'&lt;OOV&gt;': 1, 'like': 2, 'you': 3, 'i': 4, 'game': 5, 'this': 6, 'tennis': 7, 'play': 8, 'as': 9, 'which': 10, 'do': 11, 'really': 12}</span><span id="b3d6" class="ma mb it lw b gy ng md l me mf">[[ 0  0  4  2  6  5]     #i like this game<br/> [ 0  0  3  2  7  8]     #you like tennis play<br/> [ 0  0  4  2  9  3]     #i like as you<br/> [10  5 11  3  2 12]]    #which game do you like really</span></pre><p id="24e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果，我们达到了目的。换句话说，对列表中句子的数字表达式进行了必要的分配，我们能够创建一个行长度相同的矩阵。</p><p id="905c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然要感谢padding！</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h1 id="5afe" class="nz mb it bd mk oa ob oc mn od oe of mq jz og ka mt kc oh kd mw kf oi kg mz oj bi translated">结论</h1><p id="bd46" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在开始神经网络创建过程的训练之前，数据的符合性是模型的最重要的步骤。由于统一的尺寸，模型馈送有望给出更准确的结果。</p><p id="20f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这一切，我们试图理解导致我们“填充”的原因。我希望通过这个小例子，我们能更好地理解。其实对于很多看似无法编辑的流程，就像我们开头说的，API都在等你在合适的时间用在TensorFlow里。</p><p id="fd5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读！</p></div></div>    
</body>
</html>