<html>
<head>
<title>Adaboost Algorithm in Machine Learning — Ensemble Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的Adaboost算法——集成技术</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/a-simple-guide-to-adaboost-algorithm-in-machine-learning-ensemble-techniques-41d59b0d7447?source=collection_archive---------9-----------------------#2021-02-01">https://medium.datadriveninvestor.com/a-simple-guide-to-adaboost-algorithm-in-machine-learning-ensemble-techniques-41d59b0d7447?source=collection_archive---------9-----------------------#2021-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/25ba7422186a3b207d47ad888ee4c8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m176kujBsNWTg_hC"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@northwoodn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Li Lin</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="af8a" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">在本文中，我们将通过一个示例和AdaBoost算法的python实现来了解什么是AdaBoost算法，AdaBoost算法如何工作。</h2></div><p id="2750" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">近年来，助推算法在Kaggle竞赛中获得了巨大的人气。许多Kaggler使用这些提升算法来实现更高的性能，从而赢得了竞争。Adaboost算法是boosting算法的例子。我们将在AdaBoost算法中详细讨论。</p><h1 id="cab3" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">集合方法的类型:</h1><ul class=""><li id="4f35" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated">集成方法可以分为两组:基于基础学习者。</li></ul><h2 id="3d64" class="mx lv jj bd lw my mz dn ma na nb dp me lh nc nd mg ll ne nf mi lp ng nh mk ni bi translated"><strong class="ak">顺序学习者:</strong></h2><p id="56d5" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">在基础学习器顺序生成的情况下，例如自适应增强(AdaBoost)，基础学习器的顺序创建促进了基础学习器之间的依赖性。然后，通过给先前的学习者分配更高的权重来提高模型的性能。</p><h2 id="8c3c" class="mx lv jj bd lw my mz dn ma na nb dp me lh nc nd mg ll ne nf mi lp ng nh mk ni bi translated"><strong class="ak">平行学习者:</strong></h2><p id="8958" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">其中基本学习器<strong class="la jk">T5以并行格式生成，例如随机森林，随机森林包含许多决策树。基础学习者的并行创建促进了基础学习者之间的独立性。</strong></p><ul class=""><li id="f35b" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">基于基础学习器的类型，集成方法可以分为两类。</li><li id="8b16" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">同质集成方法在每次迭代中使用相同类型的基本学习器。</li><li id="315b" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">异构集成方法在每次迭代中使用不同类型的基学习器。</li></ul><h1 id="e498" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">什么是AdaBoost算法？</h1><ul class=""><li id="c937" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated"><strong class="la jk"> AdaBoost或自适应Boosting </strong>是由Yoav Freund和Robert Schapire于1996年提出的集成Boosting方法之一。你可以用它来解决分类和回归问题。AdaBoost是一种迭代集成方法。它通过组合多个弱性能分类器来构建强分类器。最终的分类器是几个弱分类器的加权组合。它适合不同加权训练数据上的弱学习者序列。如果使用第一个学习器的预测是不正确的，那么它给予已经被错误预测的观察更高的权重。作为一个迭代过程，它继续添加学习者，直到达到模型数量或精确度的极限。你可以在AdaBoost图中看到这个过程。</li></ul><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/c24fbfb628e819a7d193c36026d74e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*uugIVp8G1OEWFtQP.png"/></div></div></figure><ul class=""><li id="db78" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">AdaBoost可以使用任何基本分类器。这种算法不容易过拟合。AdaBoost易于实现。AdaBoost的一个缺点是它受离群值的影响很大，因为它试图完美地拟合每个点。与XGBoost相比，它的计算速度较慢。</li><li id="9663" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">简单来说，最初，AdaBoost选择一个随机训练子集，并对每个观察值赋予相同的权重。如果使用第一个学习器的预测是不正确的，那么它给予已经被错误预测的观察更高的权重。该模型通过基于上次训练的准确预测选择训练集来迭代训练。作为一个迭代过程，该模型添加多个学习器，直到模型数量或准确度达到极限。</li></ul></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><ul class=""><li id="e020" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">任何机器学习算法都可以用作基本分类器，只要它接受训练集的权重。</li><li id="d7eb" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated"><strong class="la jk"> AdaBoost </strong>应满足两个条件:</li></ul><ol class=""><li id="ddf1" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt og mu mv mw bi translated">分类器应该在各种加权的训练例子上被交互地训练。</li><li id="f12a" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">在每一次迭代中，它试图通过最小化训练误差来为这些示例提供最佳拟合。</li></ol><h1 id="f456" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">AdaBoost算法直觉:</h1><ul class=""><li id="e953" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated">它按以下步骤工作:</li></ul><ol class=""><li id="4757" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt og mu mv mw bi translated">最初，Adaboost随机选择一个训练子集。</li><li id="22dd" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">通过基于上一次训练的准确预测选择训练集来迭代地训练AdaBoost机器学习模型。</li><li id="da70" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">通过基于上一次训练的准确预测选择训练集来迭代地训练AdaBoost机器学习模型。</li><li id="684d" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">此外，它在每次迭代中根据分类器的准确性将权重分配给训练好的分类器。越准确的分类器将获得越高的权重。</li><li id="e211" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">这个过程反复进行，直到完整的训练数据没有任何误差地拟合，或者达到指定的最大估计数。</li><li id="7e2a" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">为了分类，对你建立的所有学习算法进行“投票”。</li></ol><ul class=""><li id="e35b" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">直觉可以用下图来描述:</li></ul><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e5624a1306ae73d3c52c9b5145664a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/0*SJ1bPLHVvue1P7GB.png"/></div></figure><h1 id="9dc9" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">AdaBoost工作原理的一个示例:</h1><p id="f9cd" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">首先，我们必须了解增压的工作原理。当我们训练数据时，它创建了n个决策树。当创建第一个决策树或模型时，定型示例错误地对第一个模式进行了分类，然后第一个模型具有更高的优先级。只有这些训练示例被发送作为第二模型的输入。这个过程将一直持续到我们没有提供我们想要在模型中创建的基础学习者的数量。请记住，所有增强方法都允许重复训练示例。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/b0e2f2d8e673fae7418ef3a5445ed7ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P_KCD4Xc7j31KxSK"/></div></div></figure><p id="8a5d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图显示，当创建第一个模型时，算法注意到第一个模型的错误，错误分类的训练示例将作为下一个模型的输入。这个过程反复重复，直到不满足指定的条件。当你看上图时，有n个模型是通过最小化前一个模型的误差而创建的。这就是增压的工作原理。模型1，2，3，…，N是被称为决策树的独立模型。所有的组装方法都基于相同的概念。</p><p id="2c73" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们知道了升压的概念，理解AdaBoost算法就很容易了。让我们深入了解Adaboost算法的工作原理。当使用随机森林算法时，该算法创建n棵树。它创建了完美的树，由一个起始节点和几个叶子节点组成。一些决策树可能比其他树更大，但在随机森林中没有固定的深度或长度。但有了Adaboost，情况就不一样了。在AdaBoost中，该算法只创建一个具有两片叶子的节点，这被称为决策树桩。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/d570e1cdee97553fe49b366d4f2f290e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s5wXEooeJJYMaW2G"/></div></div></figure><p id="cbf1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图代表了一个决策树桩。我们可以清楚地看到，它只有一个节点，只有两片叶子。这些决策树桩是弱学习者，boosting算法使这些小学习者成为强学习者。决策树桩的顺序在AdaBoost算法中非常重要。第一个决策树桩的错误反映了另一个决策树桩是如何创建的。让我们举一个理解的例子。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/32acfe60ebc6eadb50ba09afb1944b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/0*Bo1OGo14hohHQUFO"/></div></figure><p id="ce40" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们有一个仅包含三个要素的样本数据集，输出是二进制的。由于输出是二进制格式，这就变成了一个分类问题。在现实生活中，数据集可以有许多训练示例和数据集中的许多要素。为了便于解释，我们假设有5个训练示例。输出是二进制格式，这里是“是”或“否”。所有这些训练示例都将分配一个样本权重。使用的公式是，W=1/N，其中N是分配一些权重的记录数。在这个数据集中，只有5个训练样本，所以样本权重最初为1/5。每一行都得到相同的权重。是1/5。</p><h2 id="3c25" class="mx lv jj bd lw my mz dn ma na nb dp me lh nc nd mg ll ne nf mi lp ng nh mk ni bi translated">步骤1-创建第一个基础学员</h2><p id="c1b2" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">现在是时候创建第一个基础学习者了。该算法采用第一特征，例如特征1，并创建第一决策树桩f1。然后，它将创建与特征数量相同的决策树桩。这个案例将产生3个决策难题，因为在这个数据中只有3个变量。所有这些决策树桩将创建三个决策树和一个基于决策树桩的学习者模型。AdaBoost算法将只选择一个。在选择基学习器时，有两个属性，即基尼系数和熵。我们应该用计算决策树的方法来计算基尼系数或熵值。决策树桩是最小值将被视为第一基本学习者。在下图中，所有3个决策树桩都可以由3个变量构成。叶子下面的数字表示正确和错误分类的训练示例。具有最小熵或基尼的树桩将被选择作为基础学习者。让我们假设决策树桩一的熵指数最小。因此，让我们将决策树桩1，即特征1作为我们的第一个基本学习者。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3f9b3d07cd162d7c6d8eeb23344ca88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/0*XI6BP_x7fYp2RYkc"/></div></figure><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8e27fd78fb6b4a82b69a5585013a562e.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/0*kiNJr0KBzUWIYOlA"/></div></figure><p id="f6ad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，特征f1正确地分类了2个观察值，错误地分类了1个观察值。图中标记为红色的行分类不正确。我们将只计算这一行的总误差。</p><h1 id="33b1" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">步骤2 —计算总误差(TE)</h1><p id="0d09" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">总误差是样本权重的分类训练示例中所有误差的总和。在我们的例子中，只有1个误差，所以总误差(TE) = 1/5。</p><h1 id="159e" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">步骤3 —计算树桩的性能</h1><p id="a829" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">计算决策树桩性能的公式是:-</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4bf9959a4636052841c023d30ec430ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/0*wsZfXfLvp3GWlBOa"/></div></figure><p id="e1ff" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<strong class="la jk"> ln </strong>是自然对数，<strong class="la jk"> TE </strong>是总误差。</p><p id="010e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的例子中，总误差为1/5。通过保留上述公式中的总误差值，我们得到决策树桩的性能值为0.693。你一定在问为什么确定stump的TE和性能很重要？对，我们应该在进入下一个模型之前更新样品重量。只有错误的训练样本/错误分类的记录比正确分类的训练样本得到更多的偏好。因此，只有来自决策树或决策树桩的错误记录被传递到另一个决策树桩。在AdaBoost中，两个记录都被允许通过，错误的训练示例比正确的训练示例重复得多。我们应该增加错误分类的训练样本的样本权重，并减少正确分类的训练样本的样本权重。在下一步中，我们将根据决策树桩的性能更新权重。</p><h1 id="ae4b" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">步骤4 —更新权重</h1><p id="2291" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">对于分类不正确的训练示例，公式为:</p><p id="339a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">新样品重量=样品重量* e^(Performance) </strong></p><p id="2496" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的例子中，样本重量= 1/5所以，<strong class="la jk"> 1/5 * e^ (0.693) = 0.399 </strong></p><p id="7292" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于正确分类的训练示例，我们使用带有负号的相同公式来表示性能。与不正确的分类相比，正确分类的训练样本的权重将减少。公式是:</p><p id="a846" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">样品重量=样品重量* e^-(性能)</strong></p><p id="efa7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">放入数值，<strong class="la jk"> 1/5 * e^-(0.693) = 0.100 </strong></p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a5b893c8c4d49ffcd13207a80f93fdda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/0*ZsLNcGWai8a6KrUC"/></div></figure><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/e48819d136cf75f5d42f28f78e84427e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/0*EJWBwYNP-7T3KS1j"/></div></div></figure><p id="8cf9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图中可以看到所有训练示例的更新权重。所有权重的总和应为1。但是我们可以看到，所有训练示例的总更新权重不是1，而是0.799。为了使总和为1，我们必须将每个更新的权重除以更新的权重的总和。比如更新后的权重是0.399，我们就必须用这个除以0.799，像<strong class="la jk"> 0.399/0.799=0.50 </strong>。</p><p id="eead" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 0.50 </strong>可识别为归一化重量。我们可以在下图中看到所有的标准化权重，总和大约为1。</p><h1 id="0095" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">步骤5-创建新数据集</h1><p id="a734" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">现在，我们可以从前一个数据集创建一个新的数据集。在这个新数据集中，错误分类的训练样本的频率将高于正确分类的样本。在分析这些归一化权重时，我们必须创建一个新的数据集，该数据集将基于归一化权重。出于训练目的，它可能会选择错误的训练示例。这将是第二个决策树或决策树桩。为了基于归一化权重生成新的数据集，算法将把它划分为周期。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/613699738ad206da53638637ed3111a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/0*LxwSuagEoZtYSmpJ"/></div></figure><p id="08ad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的第一个范围是从0到0.13。第二个范围是从<strong class="la jk">0.13–0.63(0.13+0.50)。</strong>第三个范围是从<strong class="la jk">0.63–0.76(0.63+0.13)、</strong>等等。此后，该算法将运行5次迭代，以从旧数据集中选择不同的记录。想象一下，在第一次迭代中。该算法将采用随机值<strong class="la jk"> 0.46。然后</strong>它将查看值将落在哪个周期中，并在新数据集中选择该训练示例，然后它将再次选择随机值并查看它在哪个周期中，然后为新数据集选择该训练示例，相同的过程重复5次迭代。</p><p id="93d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">错误的训练样本很有可能被选择几次。这将是新的数据集。在下图中可以看到，从旧数据集中多次选择了第2行，因为该行在之前的数据集中被错误地分类。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/96789bc1e636f51798e424bca74dbc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/0*Yyx3NUPGt6_eah0a"/></div></figure><p id="b2a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于新的数据集，该算法将再次创建新的决策树或树桩，并且它将从步骤1开始重复相同的过程，直到它顺序地通过所有的决策树桩，并且发现当与我们在初始步骤中具有的归一化权重相比时，存在最小的误差。</p><h1 id="67c6" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">用Python实现AdaBoost</h1><ul class=""><li id="1460" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated">现在，我们来看看Python中AdaBoost算法的实现部分。</li><li id="3304" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">第一步是加载所需的库。</li></ul><h1 id="14c7" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">导入库</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/100ae085069f5ef53c88ff65527fc631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmx8Xwdg_lNdlWPvwRGZsA.png"/></div></div></figure><h1 id="f44f" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">加载数据集</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/2d813a50f23f4dc150c1df515dd4fcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*KLO43gAg0r6vaNqQKibmkg.png"/></div></figure><h1 id="0598" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">电子设计自动化(Electronic Design Automation)</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/4ca98a925964b56e5b80ded46890591a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCU45R0aG5E3OiYgLU0xbQ.png"/></div></div></figure><h1 id="a001" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">查看数据框的摘要</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/4f63e7cf0b6c89cedf91981d3522e21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loBTV4XwsQhtEb3gIT-SRw.png"/></div></div></figure><p id="3fa9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到数据集中没有缺失值。</p><h1 id="f9a9" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">声明特征向量和目标变量</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/602ec04eb4b396be17c5008705579d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DpK2s1ZkUYlmBQce9deWjQ.png"/></div></div></figure><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/adbe4d01746ab5437cd7c23c00e46ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*gDBng5VR-M6ntXVkY5THHQ.png"/></div></figure><h1 id="5c79" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">将数据集分为训练集和测试集</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/8ecab40aa60ab330c00786e87f5f6221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*9Q3LGdCI648VQgsianh0EQ.png"/></div></figure><h1 id="d87e" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">构建AdaBoost模型</h1><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8a698caa7bbf9ebb762f84b072eb6735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*GtQ0icJyqsUGTRwlODu_jQ.png"/></div></figure><h1 id="3a4d" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">评估模型</h1><p id="021a" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">让我们估计分类器或模型预测品种类型的准确度。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/edf648918c9c44cad1ec551a25dcf294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*c063CXwXWQ0hK7FXA9r41g.png"/></div></figure><ul class=""><li id="a406" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">在这种情况下，我们得到了86.67%的准确度，这将被认为是良好的准确度。</li></ul><h1 id="32bb" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">基于SVC估计量进一步评估</h1><ul class=""><li id="d71a" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated">为了进一步评估，我们将使用SVC作为基本估计值，如下所示:</li></ul><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/1e3973fd84f84299f0d6b5e4b7719a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*muk-8v2OqiLwRoKNAh3J8w.png"/></div></div></figure><ul class=""><li id="a240" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt mt mu mv mw bi translated">在这种情况下，我们得到了91.11%的分类率，这被认为是杰出的准确性。</li><li id="cf0d" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">在这种情况下，基于SVC的估计器比基于决策树的估计器具有更好的准确性。</li></ul><h1 id="7bf0" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">AdaBoost的优缺点</h1><p id="4ea8" class="pw-post-body-paragraph ky kz jj la b lb mo kk ld le mp kn lg lh nj lj lk ll nk ln lo lp nl lr ls lt im bi translated">其优点如下:</p><ol class=""><li id="f273" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt og mu mv mw bi translated">AdaBoost易于实现。</li><li id="d0a8" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">它迭代地纠正弱分类器的错误，并通过组合弱学习器来提高精度。</li><li id="f784" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">我们可以在AdaBoost中使用许多基本分类器。</li><li id="9f14" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">AdaBoost不容易过拟合。</li></ol><p id="ca13" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">缺点如下:</p><ol class=""><li id="6419" class="mm mn jj la b lb lc le lf lh nm ll nn lp no lt og mu mv mw bi translated">AdaBoost对噪声数据很敏感。</li><li id="7781" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">它受异常值的影响很大，因为它试图完美地拟合每个点。</li><li id="8efe" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt og mu mv mw bi translated">AdaBoost比XGBoost慢。</li></ol><h1 id="6f8c" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">结果和结论</h1><ul class=""><li id="89be" class="mm mn jj la b lb mo le mp lh mq ll mr lp ms lt mt mu mv mw bi translated">我们已经讨论了AdaBoost分类器。</li><li id="ce45" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">我们已经讨论了基础学习者是如何分类的。</li><li id="5c0b" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">然后，我们继续讨论AdaBoost分类器背后的直觉。</li><li id="80c5" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">然后，我们介绍了AdaBoost分类器在虹膜数据集上的实现。</li><li id="e817" class="mm mn jj la b lb np le nq lh nr ll ns lp nt lt mt mu mv mw bi translated">最后，我们讨论了AdaBoost分类器的优缺点。</li></ul></div></div>    
</body>
</html>