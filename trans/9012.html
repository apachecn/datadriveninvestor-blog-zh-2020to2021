<html>
<head>
<title>Intro to Principal Component Analysis (PCA) in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的主成分分析(PCA)简介</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/intro-to-principal-component-analysis-pca-in-python-c0409f9a9f1b?source=collection_archive---------10-----------------------#2021-01-28">https://medium.datadriveninvestor.com/intro-to-principal-component-analysis-pca-in-python-c0409f9a9f1b?source=collection_archive---------10-----------------------#2021-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d0a2167bad247a7baab2a54f4fc2b563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uogTxM0a-lVa__VkjFBjew.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://unsplash.com/photos/D9Zow2REm8U" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/D9Zow2REm8U</a></figcaption></figure><p id="0f9a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象你正在处理一个分类问题，你正在对苹果和橘子进行分类。您有一个包含大量要素的数据集，但您知道只有少数要素是重要的。从主观上讲，您可以手动将数据集缩减到您认为会产生良好结果的要素，同时仍然保留大量用于区分苹果和橙子的信息。或者，我们可以使用一个无偏的解决方案，数学，来降低数据集的维度。在这篇文章中，我们将回顾主成分分析(PCA)。PCA是一种用于降维的算法。</p><h1 id="c453" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是PCA，我们为什么要关心它？</h1><p id="6ac4" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">PCA是一种无监督学习技术，用于减少数据集中的维数。PCA可以减少数据集中的噪声，并通过减少数据集中的维数来增加数据的解释。现在有一股巨大的推动力去创造更容易被人类理解的模型，而创造更多可解释的模型的一种方法就是从可解释的数据开始。如果有一个包含数百个维度的数据集，很难找到可视化来理解数据，但如果我们可以将维度减少到5或10个维度，那么绘制趋势图和显示数据的可视化将会非常容易。那么，如何计算PCA呢？</p><h1 id="6369" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated"><strong class="ak">计算PCA </strong></h1><p id="7301" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">那么，现在我们知道了PCA是做什么的，我们如何在数据集上执行PCA呢？计算PCA有三个步骤。</p><p id="823e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">第一步:</strong> <strong class="kf ir">将数据标准化，计算训练数据的协方差矩阵</strong>(我们将这个矩阵称为C)。这产生了一个dxd矩阵，其中d是维数。</p><p id="39ab" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">第二步:求c的特征值和特征向量</strong></p><p id="46e9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">第三步:降低数据的维度。</strong>我们通过取n个最大特征值对应的n个特征向量来降维。当与数据相乘时，对应于最大特征值的特征向量被称为第一主元。当与数据相乘时，对应于第二大特征值的特征向量被称为第二主元，依此类推。</p><p id="fda0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">听起来很简单，但是让我们看一个PCA的例子。</p><h1 id="389b" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">PCA示例</h1><p id="e709" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在这个例子中，我们将使用MNIST数据集，一个由0到9的手写数字组成的数据集(我们将只使用数字0和1)。MNIST数据集中的每个影像都是28x 28的影像，这意味着有784个要素。在这个例子中，我将展示我们可以构建一个测试准确率为99%的分类器，同时在PCA算法的步骤中只对2个特征进行分类。</p><h2 id="d14c" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">步骤1:加载数据并执行数据预处理</h2><figure class="mq mr ms mt gt jr"><div class="bz fp l di"><div class="mu mv l"/></div></figure><figure class="mq mr ms mt gt jr"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="c523" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据预处理的一个主要步骤是标准缩放。标准化势在必行，因为PCA算法依赖于找到方差最大的特征。如果我们不标准化数据，某些特征可能仅由于特征的权重而具有不成比例的大差异。这可能会导致PCA性能不佳。</p><p id="9647" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当使用所有特征对数据进行分类时，我们得到的准确率为98.6%。让我们看看在对数据集执行降维后，当我们执行分类时会发生什么。</p><h2 id="41b5" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">第二步:利用主成分分析进行降维</h2><figure class="mq mr ms mt gt jr"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="13e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们将介绍PCA算法的步骤。我们首先计算训练数据的协方差矩阵。然后我们找到协方差矩阵的最后两个特征值和特征向量。然后，我们通过乘以训练数据来计算训练数据的第一和第二主成分。测试数据也是如此。在讨论分类算法时，我们将更深入地了解为什么不使用测试数据来计算协方差矩阵、特征值和特征向量。简单的解释是，我们需要一组数据来验证模型的结果。</p><p id="74dc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">PCA数据可视化:</strong></p><figure class="mq mr ms mt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/ea0e6987592d4dd86286082effe23c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9rThZrxdUuJ4f1h_klvyA.png"/></div></div></figure><h2 id="259e" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">第3步:使用缩减维度进行分类</h2><figure class="mq mr ms mt gt jr"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="cd03" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对降维后的数据进行分类后，准确率为99%。与其在784个维度上进行分类，我们只能在2个维度上进行相当的准确度。我并不是说这种情况会一直发生，但为了同样的准确度，我更愿意对2个特征进行分类，而不是784个特征。</p><h1 id="9167" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">结论</h1><p id="6d5d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">嗯，我希望你学到了很多！我希望您已经了解了PCA的重要性以及如何应用它。像往常一样，如果你喜欢这个帖子，一定要打破拍手按钮。如果你喜欢这个帖子，我会鼓励你看看我的其他一些帖子，然后跟着我。下次见！</p></div></div>    
</body>
</html>