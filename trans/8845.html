<html>
<head>
<title>Most Underrated component of ML: Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最被低估的ML组成部分:正规化</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/most-underrated-component-of-ml-regularization-6072048ef3a4?source=collection_archive---------4-----------------------#2021-01-23">https://medium.datadriveninvestor.com/most-underrated-component-of-ml-regularization-6072048ef3a4?source=collection_archive---------4-----------------------#2021-01-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5dcc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用正则化及其类型的数学知识强化你的机器学习模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3ad5a7012409e354bd962d452a39db6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nd9S_s1hsM_Ew5jpG3D3EA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@thisisengineering?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">ThisisEngineering RAEng</a> on <a class="ae kv" href="https://unsplash.com/s/photos/mathematics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3ad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大家好，这个博客包含了你需要知道的关于正规化的所有信息。这个博客是关于正则化背后的数学直觉及其在python中的实现。这个博客是专门为那些发现正规化难以消化的新手准备的。对于任何机器学习爱好者来说，理解数学直觉和后台工作比仅仅实现模型更重要。</p><blockquote class="ls lt lu"><p id="9c61" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">我看到很多博客都在回答为什么需要正规化，正规化有什么作用，但是没有回答如何正规化？</p></blockquote><h1 id="acdc" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">目录</h1><ol class=""><li id="29fc" class="mr ms iq ky b kz mt lc mu lf mv lj mw ln mx lr my mz na nb bi translated"><a class="ae kv" href="https://medium.com/p/6072048ef3a4#464e" rel="noopener">什么是过度拟合？</a></li><li id="cf9a" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><a class="ae kv" href="https://medium.com/p/6072048ef3a4#4043" rel="noopener">正规化</a></li><li id="890a" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><a class="ae kv" href="https://medium.com/p/6072048ef3a4#a79d" rel="noopener">实现</a></li><li id="270b" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><a class="ae kv" href="https://medium.com/p/6072048ef3a4#f4a8" rel="noopener">正则化率怎么用？</a></li><li id="21d9" class="mr ms iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated"><a class="ae kv" href="https://medium.com/p/6072048ef3a4#ec20" rel="noopener">结束注释</a></li></ol><h1 id="464e" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">什么是过度拟合？</strong></h1><p id="a29a" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">每个数据集都包含一定量的噪声。当你的模型试图拟合你的数据时，你就会陷入过度拟合。过度拟合试图达到每个噪声数据，从而增加了复杂性。过度拟合的后果是模型的训练精度太高，而测试精度太低。这意味着您的模型将无法预测新的/实时数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/9b7068464539a1ef3f74267fd70f5615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YEX7w5ZDC9jjUE2zjgPJg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by owner</figcaption></figure><p id="3b47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">避免过度拟合的一种方法是惩罚权重/系数，这正是正则化所做的。</strong></p><h1 id="4043" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">正规化</strong></h1><p id="4668" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">这是一种回归形式，将估计的系数约束或缩小到零。换句话说，<strong class="ky ir"> </strong> <em class="lv">这种技术不鼓励学习更复杂或更灵活的模型，以避免过度拟合的风险。</em></p><p id="27c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lv">主要有两种正规化:</em></p><h2 id="9f9a" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lj ns nt mn ln nu nv mp nw bi translated"><strong class="ak"> <em class="nx"> 1。</em>岭回归</strong></h2><p id="1314" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">我将通过岭回归来解释广义数学直觉，除了正则项之外，岭回归对于其他类型也是一样的。假设你正在进行简单的线性回归。</p><p id="7dc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，假设的公式是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7a1bcaf1cec3aba0f2489ce71589be98.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*waXzD9oe1E8SARGHzkoWxQ.png"/></div></figure><p id="cbc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/51360e0e27c7d6c6d630982d01733d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*rsfv4BvuenN7eTNJqPr2RA.png"/></div></figure><p id="b8d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，J(theta)/Loss是模型的损失函数。它不同于普通的损失函数，因为它加入了一个正则项来惩罚最终的系数。λ是正则化率。</p><p id="085a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，正则项是λ乘以系数平方的总和。因此，添加正则化项，与正常成本函数相比增加了成本函数</p><p id="3313" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lv">注意:正常成本函数与上图所示相同，但没有正则项，所以不要混淆。</em></p><p id="6535" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lv">在计算成本后的每个模型中，我们使用任何优化算法来提高我们的权重。在这种情况下，我们将使用梯度下降进行优化。</em></p><p id="a5dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度下降是寻找最优权重/系数的过程，该最优权重/系数又用于预测新的样本/数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/18904466792e7251f007e96ffb6adc90.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Ac9Fy6tBdQ254OHU8hussQ.png"/></div></figure><p id="d3f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在梯度下降中，当学习率(α)乘以成本函数的导数(微分)并从旧的权重中扣除时，获得新的权重。对于权重被同时优化的多次迭代，该过程发生。</p><p id="6d26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们之前看到的，添加正则化项增加了成本函数，因此当成本函数的导数从梯度下降中的权重中扣除时，新的权重将比我们使用正常成本函数(没有正则化)获得的权重小得多。</p><blockquote class="ls lt lu"><p id="7e15" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">简而言之，如果没有正则化的成本函数被称为X，而有正则化的成本函数被称为Y，则显然添加了Y&gt;X作为正则化项。此外，使用带有正则化的成本函数计算的新权重是W1，使用不带正则化的成本函数计算的新权重是W2，而W2&gt;W1。</p></blockquote><p id="429d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到岭回归，岭回归器的缺点是它可以将系数/权重降低到最小阈值，但不能将其降低到零。出现多重共线性时，使用岭回归。多重共线性意味着要素相互依赖。</p><h2 id="f89b" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lj ns nt mn ln nu nv mp nw bi translated"><strong class="ak"> 2。拉索回归(L1回归)</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7eac51392501e7bf09fbc5e05c4d10cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*dZIhU4s-BjKkWq9Lb-I-_Q.png"/></div></figure><p id="1db9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Lasso遵循与上述岭回归中相同的数学直觉。唯一的区别是正则项。正则项是正则化速率乘以权重模数的总和。套索回归克服了岭回归的缺点，它不仅惩罚系数，而且如果与因变量不相关，还将系数设置为零。</p><p id="0032" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，Lasso回归也可以用作特征选择，它属于特征选择的嵌入式方法。</p><h1 id="a79d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">实施</strong></h1><pre class="kg kh ki kj gt oc od oe of aw og bi"><span id="a2a2" class="nl ma iq od b gy oh oi l oj ok">from sklearn.linear_model import Lasso<br/>model = Lasso(alpha=0.6)<br/>model.fit(Xtrain, ytrain)<br/>ypred = model.predict(Xtest, ytest)</span></pre><p id="20b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，sklearn库为Ridge提供了功能。它还扩展到RidgeCV和LassoCV。</p><h1 id="f4a8" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">正则化率怎么用？</strong></h1><p id="d1b3" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">如果您面临以下情况，请实施这些回归技术:</p><p id="b3d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(1)过拟合→尽量提高正则化率。</p><p id="ebee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(2)欠拟合→尽量降低正则化率。</p><h1 id="ec20" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">结束注释</strong></h1><p id="5975" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">在本文中，我们对正则化有了一个大致的了解。实际上，这个概念要比这深刻得多。</p><p id="c96a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你觉得这篇文章有用吗？请在下面的评论框中告诉我们你对这篇文章的想法。</p><p id="f128" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">乡亲们，祝你们有美好的一天:)</p></div></div>    
</body>
</html>