<html>
<head>
<title>The Advantages of Self-Supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我监督学习的优势</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/the-advantage-of-self-supervised-learning-bd6fddc8f345?source=collection_archive---------15-----------------------#2021-01-28">https://medium.datadriveninvestor.com/the-advantage-of-self-supervised-learning-bd6fddc8f345?source=collection_archive---------15-----------------------#2021-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c7de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">关于为什么自我监督学习会对AI产生强烈影响的几点个人想法。从最近的NLP到计算机视觉论文</strong></p><p id="7e2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这不是一个预测，而是一个总结个人的研究和行业的发现和趋势。</p><p id="b701" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们来讨论一下自监督学习和无监督学习的区别。这两者之间是否真的有区别仍然是一个公开的讨论。<br/> <strong class="jp ir">无监督学习</strong>是在没有任何监督的情况下进行模型学习的想法。聚类算法通常是非监督学习的一个例子。对于聚类是如何形成的，没有任何监督或训练(至少对于简单的方法如K-Means来说没有)。<br/>在<strong class="jp ir">自监督学习</strong>中，我们用数据本身作为标签。我们本质上通过利用一种叫做代理任务的东西，将无监督学习转变为有监督学习。代理任务不同于下游或模型任务，因为我们对代理本身不感兴趣。</p><p id="09f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir"> NLP </strong>中，Google<a class="ae kl" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">BERT，2019 </a>等流行方法使用预训练程序，模型将根据当前句子预测一个句子或下一个句子中的缺失单词。我们可以通过简单地删除一个单词来创造一个缺少单词的句子。现在地面真相信息(我们的标签)是缺失的词。我们可以用自我监督的方式训练模型。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi km"><img src="../Images/58a311c55707bfc7f56bee137e644a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*PyNyC1q8d5nY7PoqaQZjbQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image from the original BERT paper</figcaption></figure><p id="146d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<strong class="jp ir">计算机视觉</strong>中，我们可以应用完全相同的技术来训练一个模型。我们取一张图片，去掉它的一部分(我们实际上是用一种颜色给它上色)。该模型的任务是预测丢失的像素(我们称之为图像修复)。由于我们可以访问原始图像和丢失的像素(地面真相),我们可以在监督下训练模型。论文<a class="ae kl" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">上下文编码器:通过修复进行特征学习，CVPR，2016 </a>是这种使用修复的自我监督训练程序的一个例子。不幸的是，计算机视觉中的这种方法并不奏效。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/0f234a631e977afb321ea6d8e4d1543f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*6Cm9WFWYQtzmW-xuPwd_fA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">From original paper Context Encoders. The illustration shows how the model has to reconstruct the white area in the image. To fulfill the task it needs to understand the context (needs to learn good representations).</figcaption></figure><p id="90a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">较新的方法使用图像增强。单个图像将两次通过增强管道。我们最终得到原始图像的两个新版本(我们称之为视图)。如果我们对多个图像进行同样的处理，我们可以训练一个模型来找到属于同一原始图像(增强之前)的图像对。我们基本上了解到模型对于我们选择的任何增强都是不变的。</p><p id="e430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们来看看自我监督学习可以给人工智能世界带来的优势。</p><h1 id="5adb" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">终身学习</h1><p id="ed8d" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">当我们谈论人工智能时，我们都会想到一些智能系统随着时间的推移不断学习并自我完善。不幸的是，这相当困难。监督学习系统需要新标签来训练新数据。改进系统需要不断的重新标记和重新培训。然而，通过自我监督，我们不再需要人为的标签。从<a class="ae kl" href="http://people.eecs.berkeley.edu/~efros/" rel="noopener ugc nofollow" target="_blank">阿列克谢·埃夫罗斯实验室</a>到这个方向已经有了一些伟大的工作，比如下面这篇使用自我监督学习来适应强化学习新环境的论文:<a class="ae kl" href="https://arxiv.org/pdf/1705.05363.pdf" rel="noopener ugc nofollow" target="_blank">通过自我监督预测的好奇心驱动的探索，ICML，2017 </a></p><h1 id="87e8" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">数据标记</h1><p id="b49a" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">监督学习需要真实的数据。我们称之为标签或注释，在计算机视觉等领域，它们大多由人类生成。一个标签的价格从几美分到几美元不等。这完全取决于注释任务需要多少时间以及需要多少专业知识。尽管很多人可以在汽车和行人周围画一个边界框，但很少有人能在医学图像上做同样的事情。<br/>自我监督学习有助于减少所需的标记量。一方面，我们可以在未标记的数据上预先训练模型，并在更小的标记集上对其进行微调。一个流行的例子是<a class="ae kl" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">视觉表征对比学习的简单框架，ICML 2020 </a>。顺便说一句。这篇论文的最后一位作者不是别人，正是图灵奖得主杰弗里·辛顿(Geoffrey Hinton)。提高标注效率的另一种方法是，我们可以使用从自我监督模型中获得的特征来指导选择要标注的数据的过程。一种方法是简单地挑选不同且不相似的数据样本。我们在<a class="ae kl" href="http://lightly.ai/" rel="noopener ugc nofollow" target="_blank">轻松地做这件事</a>。</p><p id="ffea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望你了解自我监督学习是如何工作的，以及为什么有一个很好的理由对此感到兴奋。如果你对计算机视觉中的自我监督学习感兴趣，别忘了在GitHub 上查看我们的<a class="ae kl" href="https://github.com/lightly-ai/lightly" rel="noopener ugc nofollow" target="_blank">开源Python框架用于自我监督学习。</a></p><p id="f903" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Igor联合创始人<br/> <a class="ae kl" href="https://lightly.ai/" rel="noopener ugc nofollow" target="_blank"> Lightly.ai </a></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="3337" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你可能会对我的其他帖子感兴趣:</p><div class="mj mk gp gr ml mm"><a href="https://towardsdatascience.com/few-shot-learning-with-fast-ai-81c66064e372" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">用fast.ai进行少拍学习</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">在少镜头学习中，我们只使用几个带标签的例子来训练模型。了解如何使用…训练您的分类器</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na ks mm"/></div></div></a></div><div class="mj mk gp gr ml mm"><a href="https://towardsdatascience.com/how-to-keep-up-with-the-latest-research-and-trends-in-ml-a45a356b1001" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">如何跟上ML的最新研究和趋势</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">对ML的大肆宣传导致该领域的研究和兴趣急剧增加。在这篇文章中，我分享了我的…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="nb l mx my mz mv na ks mm"/></div></div></a></div><div class="mj mk gp gr ml mm"><a href="https://towardsdatascience.com/the-data-you-don-t-need-removing-redundant-samples-6bfd07c1516c" rel="noopener follow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">您不需要的数据:移除多余的样本</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">在ML中有这样一句话:垃圾进，垃圾出。但是数据好坏到底意味着什么呢？</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="nc l mx my mz mv na ks mm"/></div></div></a></div></div></div>    
</body>
</html>