# 使用游戏引擎进行自动加密货币交易(没有一行代码)

> 原文：<https://medium.datadriveninvestor.com/using-the-game-engine-for-automated-stock-trading-without-a-single-line-of-code-31d46f548ab2?source=collection_archive---------1----------------------->

使用深度强化学习和可视化脚本在虚幻引擎 4 中构建一个比特币机器人

![](img/d4f0d875194f11834966b42bcf989f0a.png)

Photo by [qimono](https://pixabay.com/users/qimono-1962238/) / [CC BY](https://creativecommons.org/licenses/by/2.0/)

[ [在 Github](https://github.com/krumiaa/MindMaker/blob/master/README.md) 上下载整个项目

让我们从一个显而易见的问题开始——为什么要用游戏引擎来设计自动交易机器人？

答案是，许多活动，包括那些我们不常认为是游戏的活动，比如炒股，都可以游戏化。游戏引擎提供了一个理想的环境，用于收集各种形式的数据，并操纵它们来寻找最佳策略。虽然这个例子适合于自动股票交易，但是它也展示了可以将任何种类的数字数据集带入虚幻引擎 4 的方式以及在其中应用于它的机器学习技术。

特别是，人工智能领域称为深度强化学习，涉及一个代理寻找策略以最大化回报，[适合在游戏环境中使用](https://towardsdatascience.com/create-a-custom-deep-reinforcement-learning-environment-in-ue4-cf7055aebb3e)。深度强化学习最近在许多金融科技应用中得到展示，包括[自动股票交易](https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02)。所以这里有一个快乐的对称，我们可以利用它来构建我们的机器学习交易机器人。

在这个例子中，我们将为[虚幻引擎](https://www.unrealengine.com/en-US/release-notes?utm_source=GoogleSearch&utm_medium=Performance&utm_campaign=an*Internal_pr*UnrealEngine_ct*Search_pl*Brand_co*US_cr*broad&utm_id=6647458316&sub_campaign=UE_Broad_EN&utm_content=426ReleaseNotes&utm_term=%2Bunreal%20%2Bengine%20%2Bdownload&gclid=Cj0KCQiA0rSABhDlARIsAJtjfCfbYO_Odf09S6vVTnT6l7nkr1NU3un1093CV8I_rSHiSxo9NZ_uB7EaAonzEALw_wcB&sessionInvalidated=true)使用 [MindMaker 机器学习](https://www.unrealengine.com/marketplace/en-US/product/mindmaker-ai-plugin)插件，该插件可以从[免费下载](https://github.com/krumiaa/MindMaker)，并且具有令人满意的平坦学习曲线。事实上，这个例子不需要编写一行代码就可以完成！MindMaker 提供了一个可视化的脚本接口，用于将深度强化学习等人工智能技术应用到游戏引擎中的广泛应用中，包括机器人控制、NPC 行为、程序图形等。因为它利用了 Unreal Engine 的蓝图，所以人们可以在不了解 Tensorflow、Pytorch 或 Keras 等后端深度学习库的情况下完成所有的机器学习。这使得我们可以只使用可视化脚本来构建我们的整个项目，并使原本艰巨的任务变得简单。使用可视化脚本也降低了那些希望涉足机器学习或自动化交易的人的门槛。

我还应该在开始时声明这不是什么——这不是在股票市场或交易加密货币的万无一失的方法。然而，这可能是一个很好的切入点，让你开始设计交易机器人，或者在游戏引擎中应用机器学习技术。

没有进一步的准备，让我们开始建立我们的交易机器人。

首先，我们需要收集必要的工具和数据。特别是，我们需要获得历史交易数据，算法将从这些数据中学习交易。在这种情况下，我选择了比特币的交易数据，并计算了 RSI 和 MACD 的值——这些指标可能对交易算法有用。

接下来，我们需要使用 MindMaker AI 插件在虚幻引擎中设置我们的交易环境。为此，我们从下载虚幻引擎 4.26 的 M [indMaker AI 插件开始。在这种情况下，我们将使用](https://www.unrealengine.com/marketplace/en-US/product/mindmaker-ai-plugin) [MindMaker 的深度强化学习套件](https://unrealengine.com/marketplace/en-US/product/neurostudio-self-learning-ai)的算法，其中包括尖端的机器学习方法，如近似策略优化(PPO)、异步行动者批评(A2C)和深度 Q 学习(DQN)。一旦我们设置了交易机器人，我们就可以很容易地在这些算法之间切换，看看哪一个表现最好。

我们将首先在 Unreal Engine 中打开 MindMakerDRL starter 项目，并找到 MindMakerStarter 内容文件夹。为此，可以很容易地修改一个 MindMaker DRL 示例项目，但是使用 blank starter 项目提供了一个更清晰的基础。首要任务之一是导入股票交易数据。这是用一个虚幻的引擎数据表来完成的。要导入培训数据，我们必须创建一个自定义结构，用于在导入时对数据表进行排序。为此，创建一个名为 StockDataStructure 的新的自定义结构，并在。历史股票数据的 csv 文件。这样做之后，将 csv 文件拖到游戏引擎界面内的 UE 文件夹中。UE 将自动提示您提供用于组织数据表的结构，并且您将希望选择您刚刚创建的自定义结构。要全面了解该数据表导入过程，请观看这段有指导意义的视频:

【https://www.youtube.com/watch?v=nt1hlJO-DPo 

现在你的数据文件在 UE 中，我们可以开始设计我们的股票交易机器人。

在 starter 内容文件夹中找到名为“MindMakerObject”的 MindMaker blueprint 资产，并将其重命名为 MindMakerStockBotBP 或类似名称。这是蓝图模板，我们将用来建立我们的交易机器人。接下来，将 MindMakerStockBotBP 对象拖动到位于 starter 内容映射目录中的项目映射中。默认情况下，MindMakerStockBot 对象将作为球体出现在屏幕上。见图片。你可以把它换成其他物体，但是这个球体是所有科幻电影的帽子尖，在这些电影中，球体代表了强大的外星智慧。事实上，深度强化学习可以发现人类大脑没有完全理解的策略，这种方式有些奇怪。

![](img/4addd94f9556a57aa40570aeaaabcd6d.png)

Screen Capture by Author

继续，打开 MindMakerStockBotBP，在事件图中找到自己的位置。我们将从创建一些变量开始，这些变量将用于计算净值、利润等。如附图所示，创建以下变量。这些是除了已经包含在 MindMakerStockBotBP 中的与股票机器学习功能相关的默认变量之外的变量。

![](img/d22e523198d88f8f52494e5f94bce695.png)

Screen Capture by Author

接下来，找到 Launch MindMaker 事件节点，并向其添加以下内容，主要是设置一些开始条件，如帐户余额等。

![](img/8ae00213560418835bb913f434dc3036.png)

Screen Capture by Author

## **设定动作和观察空间:**

接下来，我们继续启动 MindMaker 节点本身，在这里我们将为我们的学习算法配置动作和观察空间。这些将从本质上定义人工智能将与之交互的环境的参数，特别是它可以采取什么行动，以及它将能够观察到环境的哪些元素。

出于我们的目的，我们将动作空间形状设置为:

"低=np.array([0，0])，高=np.array([1，1])"

我们所做的是给算法一个描述符，描述它将产生什么样的动作。在这种情况下，基本上有两个动作值，用数组[0，0]表示。数组中的每个值的最小值为 0，最大值为 1。我们将确定数组中的第一个值，以表示算法是执行买入、卖出还是持有订单。由于算法将返回 0 到 1 之间的连续范围的值，我们需要定义买入、卖出和持有订单的边界条件。在接下来的章节中，我们将任何低于 0.33 的值定义为持有，介于 0.333 和 0.666 之间的值定义为买入，高于 0.666 的值定义为卖出。现在我们只需要给出动作空间的边界，这样算法就可以开始生成随机动作，然后继续学习。

该算法将处理的第二个动作值也是一个介于 0 和 1 之间的小数，表示我们将买入或卖出的资产的百分比。因此，如果我们持有 8 股股票，数组中的第一个值表示 0.77(卖出操作)，数组中的第二个值表示 0.5，我们将卖出 0.5 或现有股票的一半(4)。如果你不确定 RL 算法如何决定它的行为，我写了一本关于强化学习的入门书，可以帮助你弄清楚这一点。现在，只要知道通过反复试验，算法会发现成功交易的要素，并随着时间的推移做更多的事情就足够了。

还有其他方法可以定义我们的交易机器人的动作空间，但这种方法相对简单和直观。不利的一面是，在定义交易策略方面，它为我们的机器人提供了巨大的空间，这可以延长学习时间。因为我们正在处理 0 和 1 之间的连续行动空间，这创造了一个巨大的探索空间，而不是限制 bot 可以交易的股份数量，比如其整个资产的 1/3、2/3 或 3/3。人们可以尝试不同的方式来划定行动空间，看看它如何影响机器人的行为。继续下一部分，我们将定义机器人的观察空间。

观察空间是机器人可以访问的关于其“环境”的数据，在这种情况下，只是来自历史交易数据的几个值。我选择了一些，总共 9 个，比如 RSI，MACD，三天或价格历史和其他一些。人们可以很容易地添加其他指标，或潜在地删除一些指标。一开始很难说这些价值观中的哪些将有助于教会机器人成功的交易策略，因此决定在人工智能的观察空间中包含哪些价值观是一种艺术形式。我们将在接下来的“进行观察”函数中提供这些参数的实际值，但现在我们只是定义将用作观察的参数的范围。为此，我们将以下内容输入到观察形状框中:

low=0，high=2147483648，shape=(1，9)，dtype=np.float32

以这种方式，当观察数据出现时，学习算法准备接收观察数据。当我们在这里时，我们还想选择我们将用来训练的学习算法，在这种情况下，A2C(演员评论家)。此外，我们将设置训练集的数量(34，608 是我发现效果很好的数量)和一些其他信息，如我们是否要在完成时保存训练的算法，以及给它取什么名称。这是一个可能的例子。

![](img/4d8913364ea4e0c69960090362c32d89.png)

Screen Capture by Author

## **定义交易环境:行动、观察和回报**

现在让我们进行交易机器人的下一个重要功能，接收动作功能。在结构上，交易机器人和相关的学习算法位于 MindMaker 学习引擎可执行文件中的虚幻引擎之外。由算法确定的策略动作在那里生成，然后通过 socketIO 连接传递给 Unreal Engine。之后，我们基于这些动作更新虚幻引擎环境，并将奖励和观察数据从 UE 发送回学习引擎(稍后将详细讨论)。然后，学习引擎使用这些奖励和观察来改进其动作选择，并且重复该过程。

“接收动作”功能是指虚幻引擎将接收来自智力制造者 DRL 学习算法的动作，并决定如何处理该动作。在这种情况下，我们需要根据算法选择的动作进行一些计算，然后根据这些计算传回一组观察结果和奖励。为了理解动作、观察和奖励如何在环境和学习算法之间传递的整体模式，熟悉 Markov 决策过程以及 OpenAI Gym 强化学习环境可能是有帮助的。MindMaker 本质上遵循与 OpenAI gym 相同的模式来处理奖励、观察和行动。

继续，打开 MindMakerStockBotBP 中的接收操作函数。这是 MindMakerObjectBP 包含的默认函数之一。内部是我们在设置股票交易环境方面进行大量计算的地方。首先，我们需要解析并存储 MindMaker 学习引擎提供给我们的动作。也就是说，我们需要确定算法选择的交易类型，是买入、卖出还是持有，以及我们投资组合的交易量。我们需要将这些数据保存在变量中。见图片。

![](img/7af9b43d66f54c52353ce3f8fb357cef.png)

Screen Capture by Author

我们还需要更新和跟踪已经过期的训练集的数量，并存储我们的净值值，该值将用于计算我们在教会机器人交易方面的总体进展。下一步是确定我们是否已经到达训练数据表的末尾。机器人本质上是在我们的训练数据表中一行一行地走，不知道接下来会发生什么，就像我们人类交易者一样，看到过去和现在发生的事情，但看不到未来。但是如果机器人已经到达数据表的末尾，它需要从头开始。我们使用一个分支节点来确定是否是这种情况，如果是，重置所有变量并从数据表的开始重新开始。如果还没有到达数据表的末尾，我们继续接收动作函数的下一部分，在该部分中，我们确定购买、出售或持有股票的计算。

![](img/ed8f4e625b788cde44a71e53d781ff8c.png)

Screen Capture by Author

但在此之前，我们需要从数据表中提取一些数据，并将其存储在变量中，如资产的当前价格、以前的价格等。

![](img/2c605652fd8a7b12047c273c2e60206f.png)

Screen Capture by Author

接下来，我们需要询问从学习引擎接收到的关于买入、卖出或持有的动作值对应于什么。这将是一个介于 0 和 1 之间的十进制数值，我们需要描述这些数值在交易中的意义。如前所述，我决定把它分成 3 组，0 到 0.333，0.333 到 0.666 和 0.666 以上。我们有几个分支节点来确定学习引擎返回了哪些值，见图。

![](img/b4e7221ea1e52e1f8e1c0a52204bef1d.png)

Screen Capture by Author

如果交易类型值小于 0.33，这相当于持有，因此实际上，不需要计算股份或余额。如果我们持有任何股票，随着资产价格的上升或下降，我们的净资产仍会波动，但这还不需要计算。在我们的 Hold conditional 的 false 分支之后，我们添加了另一个条件节点来确定它是否是一个 Buy 动作。如果动作值在 0.333 和 0.666 之间，这相当于买入，我们需要进行一系列计算，首先确定在给定账户余额的情况下买入是否可能，然后根据学习引擎返回的第二个值确定买入的数量。第二个值是我们帐户余额中用于购买的百分比。从这里，我们可以确定我们购买了多少股票/货币，并计算我们的帐户余额，持有的股份等。这可以在下面的图片中看到。

![](img/d0db67167505a238460adebee31fac5a.png)

Screen Capture by Author

我们从分支节点得到的最后一个可能性是卖出选项，它涵盖了从. 666 到 1 的动作值。这包含了许多与买入指令相似的计算，只是方向相反，根据卖出的股票数量等更新我们的账户余额。见图片。

![](img/60e6c0bbea90308a4509c963b603d24f.png)

Screen Capture by Author

一旦我们进行了与购买或出售股票相关的计算，我们必须根据我们是否购买或出售股票以及资产价值本身发生的任何价格变化来更新我们的净值。我们还想计算每集的净值，因为这可以用来衡量我们的算法学习交易的程度。我们也可以简单地使用净值来衡量这一指标，但由于净值可能存在很大的波动性，因此在我们的训练集中对其进行平均可以更清楚地了解我们的学习算法的进展情况。

![](img/96d4ebe853b67526ef5152b6e8f5bf74.png)

Screen Capture by Author

完成这些计算后，我们继续进行显示功能的延迟。这是从接收动作功能中分离出来的，因为在某些情况下，我们希望我们的 UE 环境根据代理做出的动作而改变，并且这可以在显示延迟功能中单独处理。因为显示的延迟在事件图窗口内，所以它支持像延迟这样的节点，这对于在环境发生变化时暂停学习算法是很重要的。在我们的例子中，这实际上并不重要，因为在交易时，环境并没有发生视觉上的变化，我们也不会处理任何类型的延迟。

除了更新虚幻环境之外，延迟显示功能还完成三项任务——( 1)我们将调用“进行观察”功能来收集我们希望传回学习算法的数据,( 2)我们将调用“检查奖励”功能来决定如何根据算法的动作向算法发放奖励，以及(3)我们将把所有这些信息传回 MindMaker 学习引擎，以便它可以相应地更新其策略。这是强化学习的关键，你必须把奖励、观察和行动组织成一个连贯的流程，这样学习引擎才能优化它的行为。你可以在下面的图片中看到显示功能的延迟。

![](img/51c82185b45e3bfb72f8d43223699c45.png)

Screen Capture by Author

接下来，我们将介绍 Delay 中的每一个函数，以了解其背后发生了什么。首先，让我们打开“进行观察”功能。在进行观察时，我们需要找出一些相关的交易数据。我们用一个获取数据表的行节点来做这件事，把它分成几个组成部分，并选择我们想要向学习引擎公开的元素。我决定了 MACD、RSI 和交易量等指标。这些被追加到一个字符串中，每个值用逗号分隔。这种格式很重要，因为学习引擎是专门为使用它而设计的。但是，在这里放置什么值由您决定，只要它们包含算法计算出获胜策略所必需的信息。可以提供额外的信息，因为算法应该能够在课程训练过程中从非必要数据中挑选出必要数据，但是如果您在进行观察时丢失了一些相关信息，代理将在学习策略方面受到限制。选择好的观测是至关重要的。如果经过多次尝试，你的代理似乎没有学到任何东西，这可能是因为你没有提供正确的观察。我们可以在下图中看到观察值是如何形成的。

![](img/d105116c0cffc4fe7ea280df2f4245e5.png)

Screen Capture by Author

一旦观察结果被公式化，我们就回到延迟显示功能，下一步的工作是确定给学习算法提供什么样的奖励信息。这将发生在 Check Reward 函数中，该函数在进行观察后立即被调用。打开支票奖励功能的盖子，我们需要制定如何根据学习算法刚刚采取的行动来奖励它。创建一个合适的奖励函数是一个很深的课题，很大程度上取决于它是否正确。在我们的案例中，直观的方法是使用净值作为回报，因为这最终是我们希望随着 bot 交易而增长的。然而，更有指导意义的奖励实际上是每一集的净值变化，例如前一集和这一集的净值之差。我们希望算法专注于增加净值，而不仅仅是实现高净值。你可以尝试不同的奖励来找到最有效的。在我们的案例中，净值的变化似乎很有效。您可以在下图中看到奖励节点的结构。

![](img/a60f06ba07cf5b5d5f3d6db17dd0003d.png)

Screen Capture by Author

最后，回到显示延迟函数，我们需要用我们刚刚生成的奖励和观察数据更新 MindMaker 自定义结构，并在发出函数调用中将其传递给 MindMaker 学习引擎。这将通过套接字 IO 连接将数据发送到学习引擎。如前所述，学习引擎和相关算法包含在虚幻引擎的独立可执行文件中。一旦发送了奖励和观察数据，学习引擎将相应地更新它的策略，并选择一个新的动作，理想情况下，一个更好地校准以接收奖励的动作。我们基本上是通过试错学习来教算法进行交易，就像狗通过执行一系列动作来学习接受食物奖励一样，比如坐下或翻身。随着回报和观察数据被发送回学习引擎，我们所有的计算和交易机器人的设置就完成了。学习引擎现在将选择一个新的动作，将其发送到虚幻引擎，并重复该过程。经过成千上万次的迭代，学习算法应该能够成功地根据我们提供的回报和观察数据来决定什么是成功的交易。

![](img/a82aad0aa4775ec14496480ca50c3cc1.png)

Screen Capture by Author

## 测试机器人，击败买入持有法

随着蓝图部分的完成，我们准备开始在模拟环境中测试我们的交易机器人。测试股票交易算法的方法有很多。由于我们从来没有试图制作一个天才级别的交易机器人，而是专注于这样做的广泛笔触，并看看使用 MindMaker 和 Unreal Engine 可以多么容易地做到这一点，我们将简单地使用一种天真的方法来评估交易机器人。我们基本上希望看到随着时间的推移，它在交易方面变得越来越好。一旦确定了这一点，就可以进行任何数量的改进，并设计出更彻底的测试方案。

出于测试的目的，我们将着眼于每个培训集的净值。这将与我们的奖励值本身、净值变化密切相关，但将提供跨许多集的更清晰的表现画面。基本上，随着算法学会更好地交易，我们会看到每次训练的净值逐渐增加。尽管我们的股票(比特币)本身的价值在训练过程中不断上升，但我们正在数千次地循环相同的训练数据，因此资产价值的总体变化应该被平均化，每集净值的任何长期变化都将反映算法行为，而不是训练数据。

在对学习参数进行了大量修改后，这确实是我们所看到的，随着算法的训练，每次训练的净值会随着时间的推移而稳步增加。为了确认我们确实在正确的轨道上，我们可以使用 MindMaker 的 exploit 模式，在这种模式下，对历史数据运行算法的最终、完全训练的版本。当这样做时，我们的算法表现得惊人地好，经常获得超过 10，000%初始金额的净值！这有点误导，因为在同一时期，比特币的简单买入持有方法可以实现约 5000%的净值增长。

然而，我们的算法经常击败买入持有法，这表明在引擎盖下正在进行一些有用的策略，该算法确实在发现什么样的指标组合对产生成功的交易是有用的。更稳健的测试方法是从原始训练数据中提取一些历史数据，然后在此基础上测试完全训练的算法。与此同时，我们已经创建了一个功能正常的交易机器人，并希望学习如何使用 MindMaker 和 Unreal Engine 等工具进行各种机器学习项目。

## 讨论和结论:自动化交易的未来

我听到的对使用深度强化学习进行自动交易的批评之一是，算法只是拟合一条曲线，即模型正在绘制数据的最佳拟合线，而没有发生真正的机器学习。这种批评混淆了强化学习和监督学习。人们永远无法使用监督学习过程来在像国际象棋这样的游戏中取得成功，或者走强化学习的道路。此外，监督学习无法在每次运行时都得出独特的交易模式。这正是我们在深度强化学习中看到的，如果你在相同的训练 le 数据上运行相同的完成算法，你每次都会得到不同的结果。

更准确的说法是，该算法正在发现训练数据中一组因果相关的联系，这些联系是预示股价未来走势的指标，因此有助于进行有利可图的交易，而不是拟合曲线。强化学习是一个[因果框架](https://towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525)，我们正在用我们的交易机器人构建的东西类似于经验丰富的交易员在看到特定的股票图表时获得的“直觉”，他们本能地知道股票将向哪个方向移动，但无法解释他们的推理。在许多方面，强化学习对应于人类无意识的多巴胺能学习过程，经济学家丹尼尔·卡内曼称之为系统 1 思维。强化学习算法现在可以模仿人类做出的快速、直观的决策，这表明我们正处于机器学习的分水岭时刻。因此，我们可能需要新的方法来理解和联系我们的计算机，因为它们开始以更类似于动物训练的方式被编程，而不是通过给定的离散指令集。这个问题的含义和类似的话题在相关的博客文章中有所涉及，也包括《智取:强化学习，它的希望和危险》这本书

## 作者也是

[智胜:强化学习——这是希望也是危险](https://www.amazon.com/Outsmarted-Reinforcement-Learning-Promise-Peril-ebook/dp/B08BG9FDC2)

[破解因果与强化学习](https://towardsdatascience.com/cracking-cause-and-effect-with-reinforcement-learning-f3df8dfcd525)

[用强化学习创造下一代视频游戏 AI](https://towardsdatascience.com/creating-next-gen-video-game-ai-with-reinforcement-learning-3a3ab5595d01)

[当恒等式成为一种算法](https://towardsdatascience.com/when-identity-becomes-an-algorithm-5d076162fab7)