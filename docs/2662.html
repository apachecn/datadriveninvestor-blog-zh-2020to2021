<html>
<head>
<title>Data Science : AdaBoost Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学:AdaBoost分类器</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/data-science-adaboost-classifier-4879a45c4300?source=collection_archive---------6-----------------------#2020-05-10">https://medium.datadriveninvestor.com/data-science-adaboost-classifier-4879a45c4300?source=collection_archive---------6-----------------------#2020-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/1ac9baf16404869df45c0b64e368b470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LTTIyBsn36awwrdFyE9p_Q.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">image:BBVA</figcaption></figure><div class=""/><p id="9422" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">简介:</strong></p><p id="613e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">提升是一种用于从多个弱分类器创建强分类器的集成技术。集成技术的一个众所周知的例子是随机森林，它使用多个决策树来创建一个随机森林。除了Adaboost之外，一些众所周知的增强技术是梯度增强、XGBoost等。</p><h2 id="f0b4" class="la lb jf bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated"><strong class="ak">直觉:</strong></h2><p id="c0de" class="pw-post-body-paragraph kc kd jf ke b kf lt kh ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ij bi translated"><a class="ae ly" href="https://en.wikipedia.org/wiki/AdaBoost" rel="noopener ugc nofollow" target="_blank"> AdaBoost </a>，简称<strong class="ke jg"> Adaptive </strong> <strong class="ke jg"> Boosting </strong>是第一个成功开发的用于二分类的Boosting算法。它是一种受监督的机器学习算法，用于提高任何机器学习算法的性能。它最适合像决策树这样的弱学习者。这些模型在一个分类问题上达到的准确度仅高于随机几率。</p><h2 id="4419" class="la lb jf bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated"><strong class="ak">AdaBoost的概念很简单:</strong></h2><p id="126e" class="pw-post-body-paragraph kc kd jf ke b kf lt kh ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ij bi translated">我们将把我们的数据集传递给多个基础学习者，每个基础学习者将尝试纠正被其前辈错误分类的记录。</p><p id="3dac" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将数据集(如下图所示，所有行)传递给<strong class="ke jg">基础学习者1 </strong>。<strong class="ke jg">基础学习者1 </strong>的所有误分类记录(第5、6和7行被误分类)将被传递给<strong class="ke jg">基础学习者2 </strong>，同样<strong class="ke jg">基础学习者2 </strong>的所有误分类记录将被传递给<strong class="ke jg">基础学习者3 </strong>。最后，基于每个基础学习者的大多数投票，我们将对新记录进行分类。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lz"><img src="../Images/3cfc4312ac6b620ab2c231f73765e6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEJzNS0XVKPwK5iT815pyw.png"/></div></div></figure><p id="351b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">让我们看看AdaBoost分类器的逐步实现:</strong></p><p id="b691" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤1 </strong>:取数据集，给给定数据集中的所有样本(行)分配初始权重。</p><p id="a1e9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">W=1/n =&gt;1/7，其中n是样本数。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/d2afb0d7a0e09cfef878b0f48ae23740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOlKfftqBB0NyGeFGXHKXw.png"/></div></div></figure><p id="45be" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第二步:我们将创建第一个基础学习者。在AdaBoost中，我们使用决策树作为基本学习器。这里要注意的一点是，我们将只创建深度为1的决策树，它们被称为<strong class="ke jg">决策树桩或树桩。</strong></p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/fa9b2f541f38c87d8fa4fd1d9e0c1be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*-krX3_eSGJRH1l9e4xbClA.png"/></div></figure><p id="37b5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将为数据集的每个要素创建树桩，就像在我们的例子中，我们将为每个要素创建三个树桩。</p><p id="6b43" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤3 </strong>:我们需要根据每个特征的熵值或ginni系数来选择任何一个基础学习器模型(为特征1创建的基础学习器1、为特征2创建的基础学习器2、为特征3创建的基础学习器3)。(我已经在我的决策树文章中讨论过ginni和熵)。</p><p id="051d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">熵或ginni系数值最小的基学习器，我们将选择它作为我们的第一个基学习器模型。</p><p id="7e6c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤4 </strong>:我们需要找到多少记录被正确分类，以及多少记录被我们在步骤3中选择的基本学习者模型错误分类。</p><p id="27f7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们必须找出所有错误分类的总误差。假设我们正确分类了4条记录，错误分类了1条记录</p><p id="3abc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总误差=错误分类记录的样本权重总和。</p><p id="6f4c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因为我们只有一个误差，所以总误差=1/7</p><p id="3a38" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">第五步</strong>:用下面的公式检查树桩的性能。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ca"><img src="../Images/c2f9b1e689cfa4784c73d26c8c5e46df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quzIarkrSUgoslxvURP5sA.png"/></div></div></figure><p id="1abc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当我们把总误差放入这个公式中时，我们将得到值0.895(不要偷懒放值和计算)</p><p id="0101" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过以下等式更新正确和错误分类样本(行)的样本权重。</p><p id="b71a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">错误分类样品的新样品重量= </strong></p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/f207ba6f798453b2159b670f0a58a416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BaxHRTyKtLHkVPhLRVigJg.png"/></div></div></figure><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/acbbc96d575f5e69faa2e4522dae2eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fK5M_lXOkofZ_pVLcPYBig.png"/></div></div></figure><p id="30a1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">正确分类样品的新样品重量= </strong></p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/a19d60975917c884a46304f69a1fcc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uoVYGk1vLX8U5lUlvi5grA.png"/></div></div></figure><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/050f15fdea51bfd9492e059afa2bbead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vk60Z1AjRVIzp368VUtsvw.png"/></div></div></figure><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/e645d428fa917524ea940d47188e6e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U17kufywAmLX1ZO-wfvJtA.png"/></div></div></figure><p id="d9d8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注意:我们假设第3行分类不正确</p><p id="d595" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤6 </strong>:我们在此考虑的要点是，当我们将所有样本权重相加时，其等于1，但在更新权重的情况下，总和不等于1。因此，我们将每个更新的权重除以它们的总和，我们将得到归一化值1。</p><p id="f964" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">更新后的权重之和为0.68</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mi"><img src="../Images/d8250273f6b46c9d7d5380fbc2c0701d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWUwbJKYf8JOLjXcss1-4w.png"/></div></div></figure><p id="22f8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">第7步:</strong>我们将通过删除“样本权重”和“更新权重”特性来创建数据集，并将每个样本(行)分配到一个存储桶下。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/036548b896ac5ab030cd9aa0918831a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbjIs1iQe1i02rpKVMrcLA.png"/></div></div></figure><p id="57ba" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤8 : </strong>创建一个新的数据集。为此，我们必须从0到1(对于每个样本(行))中随机选择值，并选择该样本，它落在桶下，并将该样本保存到新的数据集中。由于错误分类的记录具有更大的存储桶大小，选择该记录的概率非常高。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/ed25d793bc88f046d0e1e961e23886e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*k1qamnbQJRfxYg8-vFoXdA.png"/></div></figure><p id="8cf1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以看到，分类不正确的记录(行3)在我们的数据集中被选择了3次，因为它的存储桶大小比其他行大。</p><p id="9258" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">步骤9 : </strong>对下一个树桩(基础学习者2、基础学习者3)使用这个新数据集，并对所有特征执行步骤1到8。</p><p id="7acf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Adaboost模型通过让森林中的每棵树对样本进行分类来进行预测。然后，我们根据他们的决定把树分成组。现在对于每个组，我们把组内每棵树的重要性加起来。森林作为一个整体所做的最终分类是由具有最大总和的组确定的。下图。</p><figure class="ma mb mc md gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/16ef82e7a719e243f97d5a3d2ffd1134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7_vfVXuHXD5POkHM-pM6w.png"/></div></div></figure><p id="8579" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">Adaboost分类器的分步Python代码:</strong></p><figure class="ma mb mc md gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><h2 id="668a" class="la lb jf bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated">AdaBoost分类器的优势:</h2><ol class=""><li id="affb" class="mn mo jf ke b kf lt kj lu kn mp kr mq kv mr kz ms mt mu mv bi translated">AdaBoost可用于提高弱分类器的准确性，从而使其更加灵活。它现在已经扩展到了二元分类之外，并且在文本和图像分类中也有使用案例。</li><li id="4b04" class="mn mo jf ke b kf mw kj mx kn my kr mz kv na kz ms mt mu mv bi translated">AdaBoost具有很高的精确度。</li><li id="aaaf" class="mn mo jf ke b kf mw kj mx kn my kr mz kv na kz ms mt mu mv bi translated">不同的分类算法可以用作弱分类器。</li></ol><h2 id="1911" class="la lb jf bd lc ld le dn lf lg lh dp li kn lj lk ll kr lm ln lo kv lp lq lr ls bi translated"><strong class="ak">缺点:</strong></h2><ol class=""><li id="181c" class="mn mo jf ke b kf lt kj lu kn mp kr mq kv mr kz ms mt mu mv bi translated">助推技术循序渐进地学习，确保你有高质量的数据是很重要的。</li><li id="b03d" class="mn mo jf ke b kf mw kj mx kn my kr mz kv na kz ms mt mu mv bi translated">AdaBoost对噪声数据和异常值也非常敏感，因此如果您计划使用AdaBoost，强烈建议您消除它们。</li><li id="a3a1" class="mn mo jf ke b kf mw kj mx kn my kr mz kv na kz ms mt mu mv bi translated">AdaBoost也被证明比XGBoost慢。</li></ol><p id="9cc9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结论:在本文中，我们讨论了理解AdaBoost算法的各种方法。如果使用准确，AdaBoost就像是提高我们分类算法准确性的福音。</p><p id="f1a8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">希望你喜欢我的文章。请鼓掌(最多50次)，这将激励我写更多。</p><p id="4e95" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">想要连接:</p><p id="fd69" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">链接进来:<a class="ae ly" href="https://www.linkedin.com/in/anjani-kumar-9b969a39/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/anjani-kumar-9b969a39/</a></p><p id="5f66" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你喜欢我在Medium上的帖子，并希望我继续做这项工作，请考虑在<a class="ae ly" href="https://www.patreon.com/anjanikumar" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg"/></a>上支持我</p></div></div>    
</body>
</html>