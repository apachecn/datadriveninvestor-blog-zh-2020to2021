<html>
<head>
<title>Markov Decision Processes — Learning Some Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马尔可夫决策过程——学习一些数学知识</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/markov-decision-processes-learning-some-math-b82448169927?source=collection_archive---------4-----------------------#2020-02-21">https://medium.datadriveninvestor.com/markov-decision-processes-learning-some-math-b82448169927?source=collection_archive---------4-----------------------#2020-02-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c29d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://medium.com/datadriveninvestor/rl-explained-reinforcing-the-intuition-and-math-fd1185369186" rel="noopener"> <strong class="jp ir">之前的文章</strong> </a>对于理解强化学习架构背后的直觉是必不可少的，并探索了代理<em class="km">与其环境</em>交互的框架。代理观察奖励假设和反馈的环境，以执行行动并达到新的状态。</p><p id="6cd1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">马尔可夫决策过程</strong>进一步探索了完全可观察环境的现象以及每个策略和过程是如何工作的。大多数RL架构的特征是马尔可夫决策过程。</p><div class="kn ko gp gr kp kq"><a href="https://www.datadriveninvestor.com/2019/01/23/which-is-more-promising-data-science-or-software-engineering/" rel="noopener  ugc nofollow" target="_blank"><div class="kr ab fo"><div class="ks ab kt cl cj ku"><h2 class="bd ir gy z fp kv fr fs kw fu fw ip bi translated">数据科学和软件工程哪个更有前途？</h2><div class="kx l"><h3 class="bd b gy z fp kv fr fs kw fu fw dk translated">大约一个月前，当我坐在咖啡馆里为一个客户开发网站时，我发现了这个女人…</h3></div><div class="ky l"><p class="bd b dl z fp kv fr fs kw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="kz l"><div class="la l lb lc ld kz le lf kq"/></div></div></a></div><p id="f7c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">已经警告过你了——这个帖子确实包含了一些数学和定理(激动！！！)复杂性可能看起来令人生畏，但是不要担心——实际上没有什么能与理解ML背后的数学相提并论😍😍😍</p><h1 id="942e" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">马尔可夫性质</strong></h1><p id="4bf8" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">马尔可夫决策过程建立在对马尔可夫属性的理解上，马尔可夫属性描述了“给定现在，未来独立于过去”。</p><p id="71cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当前<strong class="jp ir">状态</strong>(一旦识别)提供了足够的信息；由于该状态包含足够的关于先前反馈的信息，不再需要<em class="km">历史</em>。这可以用下面的等式来建模。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b409fc9a96ffcd3af3b1391ebf534054.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*oA1SDgCx1yhZNsyC"/></div></figure><p id="c4a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本质上，<em class="km"> S(t) </em>包含了历史中的所有相关信息，因此<em class="km"> S(1) … S(t-1) </em>不再需要达到相同的状态转移概率来达到即将到来的状态，<em class="km"> S(t+1) </em>。</p><p id="b507" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">状态转移概率</strong>基本上是在给定当前状态的情况下，跨越任何潜在后继状态的概率分布。看起来像这样。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/609a3056a0bc670d308a323e7b677aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/0*3W3lTGKHkaJ_MpXr"/></div></figure><p id="511e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于所有的数学人(你好👋)，这个定理可以解释成一个<strong class="jp ir">状态转移矩阵</strong>。每一行表示从任何当前状态到可能的先前状态的转移概率。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/43558352102fb3d66bb1a118d07ebd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/0*ZhJXJiEPnak1yECg"/></div></figure><h1 id="5f60" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">马尔可夫过程和链</strong></h1><p id="67a9" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">这些本质上是包含马尔可夫性质的随机状态序列。所有的马尔可夫链都是存在于状态空间上的<strong class="jp ir">元组&lt; S，P&gt;T31，<em class="km"> S </em>和转移函数<em class="km"> P </em>。这个系统的性质将<em class="km"> S </em>定义为状态的有限范围，将<em class="km"> P </em>定义为状态转移概率矩阵(我们上面学过)。</strong></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/6cc3e6ce3664e4a216ebd824e2399dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*AzVH2VJm8Puv_1PB"/></div></figure><p id="90b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，网络中的每个<strong class="jp ir">节点</strong>代表婴儿行为的<em class="km"> 3个定义状态</em>，并定义与其他可能状态的行动相关的概率。在某些情况下，网络包含中的<strong class="jp ir">终端状态，其中样本剧集被终止。</strong></p><h1 id="0821" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">马氏奖励流程</strong></h1><p id="3f3e" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">我们之前谈到的马尔可夫过程没有任何与特定状态相关联的<strong class="jp ir">值</strong>——马尔可夫奖励过程将一些值的概念赋予每个状态，作为机器用来评估如何<strong class="jp ir">最大化回报的度量。</strong></p><p id="d6a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">马尔可夫奖励过程为<strong class="jp ir"> &lt; S，P，R，γ &gt;元组</strong>，其中S表示有限状态空间，<em class="km"> P </em>表示状态转移概率函数，<em class="km"> R </em>表示<strong class="jp ir">奖励函数</strong>，<em class="km"> γ </em>为<strong class="jp ir">折扣因子</strong>，如下式所示。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b042dd74724c3d3e4c4d2bf280860b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/0*Sb6wpcwrJyglHzOY"/></div></figure><p id="1cc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">γ存在于0和1的值之间，表示现在的回报相对于未来的价值。当γ= 0时，代理只关心第一个奖励，而γ = 1表示代理关心未来的奖励。</p><p id="2bfb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些奖励有两个主要原因。</p><ol class=""><li id="8923" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk mz na nb nc bi translated">它们确保算法收敛并避免无限返回</li><li id="7633" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">奖励表明短期奖励是否比长期奖励更有价值。这是至关重要的，因为代理的首要目标是最大化某种意义上的累积回报。</li></ol><p id="ae95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">状态值函数</strong>，<em class="km"> v(s) </em>输出从状态<em class="km"> S </em>开始的<em class="km">期望收益</em></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ce20a057fd87e80eab0ab56b77408f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/0*tC9tO1QAP3VE-uYi"/></div></figure><p id="5e91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于代理人想要<em class="km">最高奖励</em>计数，我们可以创建另一个<strong class="jp ir">样本集</strong>，并通过源于状态<em class="km"> S </em>的许多情况来理解预期回报。这给了代理更多关于哪条路径最有利于最高回报的信息。</p><p id="dc8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">γ = 1的这一过程在以下环境中是这样工作的。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1d72be12232090b2d02725148ea4eec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/0*HrwBu83tN2hUBwJE"/></div></figure><h1 id="2edf" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">贝尔曼方程</strong></h1><p id="c741" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">这个定理本质上是通过动态规划将价值与函数自身联系起来，以使<strong class="jp ir">最大化累积奖励的总和</strong>。</p><p id="be08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们把它分成两个部分。</p><ul class=""><li id="22d6" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk nk na nb nc bi translated"><strong class="jp ir">即时奖励</strong>——&gt;——<em class="km">rₜ₊₁</em></li><li id="c3b0" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk nk na nb nc bi translated">与后继状态相关联的<strong class="jp ir">贴现值</strong>—&gt;<em class="km">γ(sₜ₊₁)</em></li></ul><p id="2dab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们把上一个等式中的<em class="km"> G(t) </em>展开，我们会得到这样的结果。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9061d5ed39b35257f8fbf45e0bb6def1.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/0*errWmA-28qFsRG_c"/></div></figure><p id="7837" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们用时间步长代替<em class="km"> G(t+1) </em>，<em class="km"> t+1 </em>作为起点。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6315dbcde64e1bf9f3caf4d1a4486816.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*Mmcgpe_3-VoZlS-G"/></div></figure><p id="cb54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于<strong class="jp ir">贝尔曼方程</strong>实际上是<em class="km">线性</em>，我们知道𝔼(ax+by)= a𝔼(x)【+b𝔼(y】这意味着<em class="km"> G(t+1) </em>的投影<strong class="jp ir">值表示状态<em class="km"> S(t+1) </em> </strong>的值</p><p id="efe0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们用最后的公式总结一下。贝尔曼方程基本上表示状态空间中每个状态的状态值<em class="km">。</em></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5c4e8cd4dfc7a852900068bb9664224f.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*vcqWY2UA2jutxukB"/></div></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi no"><img src="../Images/61bb5ba0ac97d2077c19d01646669dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/0*PqTsaOODZzEtapSE"/></div></figure><h1 id="e66a" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">马尔可夫决策过程</strong></h1><p id="f739" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">马尔可夫决策过程基本上是带有决策的马尔可夫回报过程——这描述了每个状态都是马尔可夫的环境。马尔可夫决策过程为<strong class="jp ir">元组&lt; S，A，P，R，γ &gt; </strong>其中<em class="km"> S </em>代表状态空间，<em class="km"> A </em>指有限范围的动作，<em class="km"> P </em>为状态转移概率函数。这些是马尔可夫奖励过程的<em class="km">扩展</em>,因为它包含了代理需要做出的决定。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi np"><img src="../Images/221759c9e4543b4a167533eb7976c4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/0*OcsolWVn2Gq3qOB-"/></div></figure><p id="b3bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="km"> R </em>代表<strong class="jp ir">奖励功能</strong>:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/809bb04a72bbaf171883e27613b3f685.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*OjiPn8bThDvKoimG"/></div></figure><p id="62f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="km"> γ </em>代表<strong class="jp ir">折扣因子</strong> ∈[0，1]</p><p id="b57e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些马尔可夫决策过程对代理选择的状态<strong class="jp ir">提供了更多的控制。</strong></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/044e550f67696913921a518a7cc97bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/0*4G67NlNc5dEU6Udy"/></div></figure><h1 id="4f0c" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">政策</strong></h1><p id="3d42" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">让我们回顾一下- <strong class="jp ir">策略</strong> <em class="km"> (π) </em>是在状态上的<em class="km">分布，并且有助于规定代理的行为和他们行动背后的原因。它们基本上映射每个<strong class="jp ir">状态-动作对</strong>。马尔可夫决策策略是从<em class="km">当前状态</em>而不是从<em class="km">历史</em>构建的。</em></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/11581524730f5e51b73c589fbc417325.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/0*5wZWJyWPwY_HqQGT"/></div></figure><p id="4621" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于策略是<strong class="jp ir">固定的</strong>，一旦代理达到特定状态，代理就会采取之前由策略预先决定的<em class="km">动作。由于策略具有随机能力，代理可以在状态空间中进行探索。</em></p><h1 id="6439" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">马尔可夫决策过程价值函数</strong></h1><p id="7566" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">马尔可夫决策过程的表示为<em class="km"> v_(s) </em>的<em class="km">状态值</em>函数表示代理<strong class="jp ir">的期望输出，假设它们遵循策略，从状态<em class="km"> s </em>开始π。</strong>这个可以这么表达。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/422082303a2ff7f73738c44362de4b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/0*85qB934Yc0XLY6bC"/></div></figure><p id="eae3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">马尔可夫决策过程的<em class="km">动作值</em>函数表示为<em class="km"> q_(s，a) </em>表示代理的期望输出，假设它们<strong class="jp ir">执行动作并遵循策略，π从状态s开始</strong>这可以表示为。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0b635ee0da58bbb107fdee94fe185480.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/0*qAhYbL3kkzsqBnuJ"/></div></figure><p id="04d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">写成<strong class="jp ir">贝尔曼期望方程</strong>(见上面解释的符号)看起来像这样。</p><div class="mk ml mm mn gt ab cb"><figure class="nv mo nw nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/80c89775c5ff1d2aa434cd0419b3c83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/0*p8tiutQ_ap8B5vdM"/></div></figure><figure class="nv mo of nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/214ca1290d434555c7545ed1362ce02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/0*OOIGWaIBLNUw9-0r"/></div></figure></div><h1 id="ef9d" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">最佳值函数</strong></h1><p id="9067" class="pw-post-body-paragraph jn jo iq jp b jq me js jt ju mf jw jx jy mg ka kb kc mh ke kf kg mi ki kj kk ij bi translated">强化学习的前提是<strong class="jp ir">最大化回报</strong>——虽然基于状态和基于动作的学习对于理解RL和映射状态-动作对是必不可少的，但它们<em class="km">可能不会呈现最优路线。</em></p><p id="846a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">表示为<em class="km"> v*(s) </em>的<strong class="jp ir">最优状态值函数</strong>代表所有策略的<em class="km">最大值函数。这解释了代理人可以从其环境中获取最大可能的回报。</em></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi og"><img src="../Images/801493e608b8d4a9c7a65ca3aa7cf299.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/0*dy1ZupELNP5QS0ok"/></div></figure><p id="1365" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同时，表示为<em class="km"> q*(s，a) </em>的<strong class="jp ir">最优动作值函数</strong>代表所有策略的<strong class="jp ir">最大动作值函数。这解释了从状态<em class="km"> s </em>开始执行动作<em class="km"> a </em>时，代理可以从环境中获取的最大回报。</strong></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d872643737ffac4642c3bcd8764e5c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/0*y5uAxKzRkYkIMH-3"/></div></figure><p id="2d3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">绝对最优策略</strong>可以从<strong class="jp ir">最大化</strong>q *(s，a) 值中导出。一旦我们解决了最优状态-动作值函数，我们就可以破译采取哪个动作，输出最优状态-动作值函数，记为<em class="km"> q*(s，a)。</em></p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h1 id="b882" class="lg lh iq bd li lj op ll lm ln oq lp lq lr or lt lu lv os lx ly lz ot mb mc md bi translated">最后一件事</h1><blockquote class="ou ov ow"><p id="51e2" class="jn jo km jp b jq jr js jt ju jv jw jx ox jz ka kb oy kd ke kf oz kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="iq"> WOAH </em> </strong> <em class="iq">这里面有很多数学和定理……希望你能够进一步理解强化学习代理实际上是如何以最大化他们的回报假设为主要目标来决定执行哪些动作的。</em></p><p id="fb66" class="jn jo km jp b jq jr js jt ju jv jw jx ox jz ka kb oy kd ke kf oz kh ki kj kk ij bi translated">如果你能通过做以下事情来支持我这个作者，那对我来说将意味着一切</p></blockquote><ol class=""><li id="00a0" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk mz na nb nc bi translated">注意👏图标？给我的文章发些<strong class="jp ir">掌声</strong></li><li id="52a2" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated"><strong class="jp ir">在<a class="ae kl" href="https://www.linkedin.com/in/suraj-bansal/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</strong>和我👈</li><li id="8c9e" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated"><strong class="jp ir">跟着</strong>我上<a class="ae kl" href="https://medium.com/@bansalsuraj03" rel="noopener">梅杜姆</a> ✍️</li><li id="9f02" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated"><strong class="jp ir">查看</strong>我的<a class="ae kl" href="https://tks.life/profile/suraj.bansal" rel="noopener ugc nofollow" target="_blank">作品集</a>了解我的最新作品💪</li><li id="c870" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">关注我的旅程，订阅我的每月简讯🦄</li></ol><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="pa pb l"/></div></figure></div></div>    
</body>
</html>