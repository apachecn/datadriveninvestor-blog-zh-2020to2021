# 吴恩达关于人工超级智能的观点是错误的

> 原文：<https://medium.datadriveninvestor.com/andrew-ng-is-wrong-about-artificial-superintelligence-a023d7d04b9?source=collection_archive---------14----------------------->

## 考虑人工智能校准的正确时间就是现在

“今天担心 AI 邪恶超智能就像担心火星上的人口过剩一样。我们甚至还没有在那个星球上着陆呢！” —吴恩达

![](img/006275e10165e84e76b4dd00f3e9ed3a.png)

We control tigers because of our superior intelligence. Photo by [Sinval Carvalho](https://unsplash.com/@sinvalbmx?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@sinvalbmx?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

埃隆·马斯克警告过我们。斯蒂芬·霍金警告过我们。许多其他人对此感到担忧。人工超级智能听起来可能像科幻电影中的东西，但它很可能在本世纪成为现实。根据前面提到的思想家和我自己的观点，这可能意味着人类的终结。然而，许多人认为我们不应该担心！其中之一是，你猜对了，吴恩达。

对于那些不知道[吴恩达](https://en.wikipedia.org/wiki/Andrew_Ng)的人来说:他是谷歌大脑的联合创始人和领导者，也是百度的副总裁和首席科学家，在那里他领导着[人工智能](https://www.datadriveninvestor.com/glossary/artificial-intelligence/)团队。不仅仅称他为*一个* [机器学习](https://www.datadriveninvestor.com/glossary/machine-learning/)专家似乎是公平的:他更像是*的*机器学习专家。

吴恩达显然是一个在人工智能这个主题上有很多知识的人，但是当谈到目前仍然是理论性的人工超级智能概念时，我认为他对其潜在危险的评估是危险的错误。在对 Quora 上发布的一个问题的回应中，他说担心人工超级智能是不必要的(或者，至少，我们不应该担心*但*)。在我们深入探讨他对此事的看法之前，让我们先来定义人工超智能，并讨论它的潜在危险。

# 人工超级智能

为了讨论人工超智能，我们先从基础说起。首先，什么是智能？我喜欢莱格和赫特的定义:“智力衡量一个代理人在广泛的环境中实现目标的能力。”一个代理可以简单地是一个人，一个动物，当然也可以是一台计算机。在最后一种情况下，我们称之为[人工智能](https://medium.com/the-singularity/what-is-artificial-intelligence-6ae01b5cd5dc) (AI)。当谈到广泛的环境时，今天的人工智能远不如人类聪明:它*在特定的环境中取得了(通常是超人的)成功，比如国际象棋。在所有环境中与人类一样聪明的人工智能将会是人工通用智能(AGI)。如果一个人工智能(远远)比那个人工智能(T9)聪明，它被称为人工超级智能(ASI)。*

[](https://www.datadriveninvestor.com/2020/10/16/andrew-ng-is-wrong-about-artificial-superintelligence/) [## 吴恩达关于人工超级智能的观点是错误的

### 埃隆·马斯克警告过我们。斯蒂芬·霍金警告过我们。许多其他人对此感到担忧。人工超级智能…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/10/16/andrew-ng-is-wrong-about-artificial-superintelligence/) 

# 马斯克和其他人为什么担心？

围绕 ASI 的恐惧可能最好从一个相对普遍的观察开始解释:人类正在统治地球，因为他们比所有其他动物都聪明。因为这种优越的智力，我们发明了帮助我们统治的技术，其他动物也因此吃了不少苦头。如果智力是决定谁统治这个星球的决定性因素，那么 ASI——从定义上来说比我们聪明得多——将有能力否决我们。我们的命运将取决于这个 ASI 的欲望，这将不会默认对我们有利的工作。试想:如果我们想在一个有蚂蚁的地方建一座房子，我们会重新考虑我们的计划吗？不，无论如何我们要盖房子。对蚂蚁来说太糟糕了！

> 仅仅向 ASI 指明“不要让我们被杀”就能让它想约束我们所有人躺在医院的病床上，由机器喂饭。

请注意，我们并不(一定)讨厌蚂蚁:我们只是或多或少对它们漠不关心。我们不是邪恶的蚂蚁杀手，我们能找到的每一只蚂蚁，我们都会消灭它们；当我们发现蚂蚁会因为副作用而死去时，我们不会重新考虑我们的计划。同样的，一个 ASI 可能有杀死人类的副作用。尼克·博斯特伦提出了一个思维实验，叫做 [*回形针最大化器*](https://www.nickbostrom.com/ethics/ai.html) 。其中，有一个 AGI 的任务是最大限度地增加其收藏的回形针数量。在它升级自己的智能，成为一个 ASI 之后，它最终开始将越来越多的地球变成回形针制造设施，只是为了制造越来越多的回形针。这就是它的目标！然而，作为副作用，人类死亡了。并不是说 ASI 讨厌人类:只是它想用我们家的材料(可能还有我们自己的身体)来制造回形针。从它想要毁灭人类的意义上来说，它并不邪恶:它只是不在乎让我们活着。

# 人工智能对齐

让 ASI 做我们想做的事情的问题在形式上被称为 AI 对齐。我们不仅希望 ASI 不要杀我们；我们希望它能创造一个符合我们道德价值观的美好世界。但是这些价值观到底是什么呢？我们大多数人可能会说健康和自由之类的东西，但即使是这些“显而易见”的东西也很难准确定义。现在想起来，他们实际上在相当多的场合是互相矛盾的。你应该有不系安全带的自由吗，即使这比系安全带更危险？这可能会让你失去健康(甚至生命)。请注意，仅仅向 ASI 指明“不要让我们被杀”可能会让它想把我们都限制在医院的病床上，由机器喂养。这显然不是我们真正想要的，但是，嘿，我们还活着，对吗？任务完成了，就 ASI 而言。

让我们明确一件事:我们(还)不知道如何正式指定我们的道德价值观给一个人工智能。我们甚至不知道这些值到底是什么。我们确实知道，创造一个对我们有益的 ASI——也就是说，*与我们的价值观一致——比“仅仅”创造任何一个 ASI 都要困难得多。如果我们不首先积极尝试解决人工智能的一致性问题，那么很可能某个组织在某个时间点建立了一个与我们的价值观不一致的人工智能。那将意味着人类的灾难。*

# 吴恩达的意见

2016 年 1 月 29 日，吴恩达在 Quora 上回答了一个非常重要的问题:“AI 是对人类的存在性威胁吗？”我希望我已经清楚地回答了:“是的！”。你可以在这里阅读安德鲁的完整回答[，它是从这句话开始的:](https://www.quora.com/Is-AI-an-existential-threat-to-humanity)

“今天担心 AI 邪恶超智能就像担心火星上的人口过剩一样。我们甚至还没有在那个星球上着陆呢！”

啊，是的。人类制造第一个人工智能可能还需要一段时间。然而，就像我之前说的，在我们建立第一个 ASI 之前，我们需要弄清楚如何使 ASIs 受益。问题就在这里:我们不知道这需要多长时间。也许在第一个 ASI 真正建成之前，我们需要更多的时间。因此，我们现在需要担心人工智能对准*以便有最大的机会及时完成任务。*

*请注意，吴恩达谈论的是“邪恶的超智能”。就像我解释的，ASI 不需要邪恶到对人类有害。ASI 创造回形针并不邪恶，因为它的目标是让人类灭绝:灭绝是 ASI 追求最大化回形针数量这一实际目标的副作用。*

*由于人工超级智能(如果创造出来)将对世界产生巨大影响，人工智能排列可能是人类有史以来面临的最重要的问题。因为我们不知道解决这个问题需要多长时间，所以说我们还不需要担心这个问题是完全错误的。*

**原载于 2020 年 10 月 16 日*[](https://www.datadriveninvestor.com/2020/10/16/andrew-ng-is-wrong-about-artificial-superintelligence/)**。***

## **访问专家视图— [订阅 DDI 英特尔](https://datadriveninvestor.com/ddi-intel)**