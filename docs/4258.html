<html>
<head>
<title>Q-Learning Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q-Learning解释</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/q-learning-explained-ebb544c51ba5?source=collection_archive---------2-----------------------#2020-07-30">https://medium.datadriveninvestor.com/q-learning-explained-ebb544c51ba5?source=collection_archive---------2-----------------------#2020-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="62f1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">免费RL课程:第3部分</h2></div><p id="f8d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我的强化学习课程中的第三篇文章。上一篇文章可以在这里找到<a class="ae lb" href="https://medium.com/@NathanWeatherly/understanding-the-bellman-equation-c711e531a2e5" rel="noopener">。它涵盖了贝尔曼方程的来源，以及如何使用它来寻找最佳政策，所以如果你不熟悉这些概念，你可能应该回去读一读，然后再继续这一个。本文将介绍Q-Learning的过程，它允许代理随着时间的推移制定更好的策略，并最终掌握环境。</a></p><p id="c93e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回顾一下我们上次学到的内容，价值函数用于在遵循给定政策时为每个状态-行动对分配Q值。贝尔曼方程是一个确定的价值函数，有助于在遵循最优政策时找到状态-行动对的价值。本文将通过三个步骤展示代理如何使用这些等式:</p><ul class=""><li id="f126" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">定义奖励、行动和状态</li><li id="6597" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">制作Q表</li><li id="367d" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">为代理使用Q表</li></ul><p id="044b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步是选择一个环境，对于本文，我选择了井字游戏，因为它简单且受欢迎。因为井字游戏是一个双人游戏，所以我们需要一些方法来产生对代理的反应。这可以通过多种方式来实现，比如随机生成招式，与人类对战，或者与一个完美的人工智能对战，该人工智能被编码为做出所有最佳招式，如下所示。对于本例，我们将选择环境将做出的特定移动，以响应代理可能采取的某些操作。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/965a63c87bd97968fc468a0f88cb1d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyfzYtWkEy_Zk6rJUQ0TBQ.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Optimal Play for player X (from <a class="ae lb" href="https://en.wikipedia.org/wiki/Tic-tac-toe" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>)</figcaption></figure><p id="37be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然环境是井字游戏，我们可以很容易地定义状态和动作空间。这些动作只是数字1-9，对应于棋盘上的一个方块。代理只能选择当前未被占用的方块，这些数字将对应于如下方块:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0e4de67c5362df3a4270bda9514f896e.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*KfaTdz-utWkl7mFeHm9GNQ.png"/></div></figure><p id="aba7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态将只是一个数字1–765，因为这是井字游戏中可能的有效棋盘状态的总数。我们通常需要对每一个进行编号，以便理解每一个是什么，但是因为这只是一个例子，我们可以跳过它，只关心我们在例子中使用的状态。我们还必须定义代理在某些状态下如何获得奖励。为了简单起见，我决定使用下面的函数:</p><ul class=""><li id="d457" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">任何非终结状态:-1奖励</li><li id="d190" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">任何失败的州:-10奖励</li><li id="6163" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">任何获胜状态:+25奖励</li></ul><p id="0e09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我决定给非终结状态分配一个负奖励，因为我希望代理尽可能快地优先考虑获胜。</p><p id="7e43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经定义了Q学习算法的所有部分，现在我们需要制作一个Q表。Q表是一种策略，它使用一个值表为每个状态-动作对单独分配一个Q值，而不是使用某种将状态作为输入的函数。井字游戏的Q表示例如下:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/2329384b6c469e81f50d5b5f3c0c7b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*C4oVHyr6ASgyHsb74FM4Bw.png"/></div></figure><p id="0cc0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Q值通常用代表每个动作和状态的值的数字来填充。当策略用于在给定的状态下挑选动作时，在该状态下具有最高Q值的动作被挑选。我们在这个例子中需要的Q表将只需要3个状态，因为我们将只遍历这些状态。初始表格将如下所示:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d1426edf661ee7dfae99d6945aa59346.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*qcUCbk5uXODuj3VMR2N_rw.png"/></div></figure><p id="8c87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态2和3具有N/A值，因为在该状态下这些移动是不可能的。状态1是一个空白板，所以对代理的动作没有限制。当您看到州时，这应该是有意义的:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b8bef7a9e8cfc40f5003dd1c1c4152b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*viJLrKM_X6y6HPRGQIzxPQ.png"/></div></figure><p id="404e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们想要更新Q表时，我们将使用通过贝尔曼方程计算的值来替换现有的Q值，该方程使用代理在不同状态下的不同行为所获得的奖励。我们必须适度地这样做，因为我们希望我们的代理将它的所有学习纳入计算，而不仅仅是最近的结果。这是通过设置一个学习率来实现的，<strong class="kh ir"><em class="mk">α</em></strong>(α)，该学习率代表新Q值被合并到旧Q值中的比例。学习率甚至在Q-learning开始之前就设定好了，并且始终保持不变；然而，一些更复杂的算法可以利用动态学习率。使用和更新Q表的步骤如下所示:</p><ol class=""><li id="1065" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la ml li lj lk bi translated">将状态<strong class="kh ir"> <em class="mk"> s </em> </strong>放入策略q表<strong class="kh ir"><em class="mk"/></strong>，选择动作<strong class="kh ir"> <em class="mk"> a </em> </strong>，在指定状态下q值最大。如果多个动作达到最大Q值，那么随机选择其中一个。</li><li id="d542" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ml li lj lk bi translated">然后智能体采取行动，<strong class="kh ir"> <em class="mk"> a </em> </strong>，环境通过给予智能体奖励、<strong class="kh ir"> <em class="mk"> Rₜ ₊ </em> </strong> ₁做出响应，并更新状态，<strong class="kh ir"><em class="mk"/></strong>，到<strong class="kh ir"> <em class="mk"> s' </em> </strong>。</li><li id="b3b4" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ml li lj lk bi translated">使用贝尔曼方程计算先前状态-动作对的Q值。</li><li id="d13a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ml li lj lk bi translated">使用计算的Q值和开始Q学习之前选择的学习速率<strong class="kh ir"> <em class="mk"> α </em> </strong>更新Q表。</li><li id="0850" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ml li lj lk bi translated">重复上述步骤，但现在以<strong class="kh ir"><em class="mk">s’</em></strong>为初始状态开始。</li></ol><p id="d5aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Gamma为0.9，Alpha为0.5的示例来执行此过程，结果如下所示:</p><ul class=""><li id="3dc9" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">状态1被放入Q表，最大Q值是平局，因此选择随机动作。假设选择的行动是行动1。</li><li id="2a45" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">代理采取行动1，然后环境将状态更新为状态2，并根据先前定义的奖励函数给予代理-1的奖励。</li><li id="4b62" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">上一步中的值以及Q表用于计算状态1下动作1的Q值，如下所示:</li></ul><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mm"><img src="../Images/b401354baedb2e2b0b478c02edef6567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKF4KA1RvBsQR5KnDDGPVw.png"/></div></div></figure><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mn"><img src="../Images/1822a04d3990b1d9309bb4cb242eabd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayGSzEbzdk_TmJbflsUfHw.png"/></div></div></figure><ul class=""><li id="3a74" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">然后基于0.5的学习率更新Q表中的旧Q值，如下所示:</li></ul><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mo"><img src="../Images/b876750843261f4097217d67d2ee43e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JD10VVjTlJsf5ZgEE1_afQ.png"/></div></div></figure><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mp"><img src="../Images/57195c08d13f0de975a47a33c9460099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*goRaXES2QGEFgugznQat4Q.png"/></div></div></figure><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mq"><img src="../Images/11c530cb34c2d13266cde64afd54f4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*nyMRZu7cvqhf49BC2DuEaA.png"/></div></div></figure><p id="9432" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以对上面定义的下两个状态重复这一过程，以获得如下所示的Q表:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mq"><img src="../Images/180b0312784147690c0bff0cbfaba160.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*0oy1rqeVGiuXlwDEswWZgQ.png"/></div></div></figure><p id="f8ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以看到代理已经学会了不玩某些导致它在示例游戏中失败的移动。这通过在某些状态下某些动作的负Q值来显示。你可以看到，如果代理反复与人或计算机对弈，它会越来越好，直到学会做出所有正确的动作。在下一篇文章中，我们将学习如何使用Epsilon greedy策略来自动化和实现这个过程。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="a8b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk"> Q-Learning被认可为克里斯沃特金斯的“</em> <a class="ae lb" href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mk">从延迟奖励中学习</em></a><em class="mk"/></p></div></div>    
</body>
</html>