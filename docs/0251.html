<html>
<head>
<title>Big data &amp; small files problem in HDFS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">HDFS的大数据和小文件问题</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/big-data-small-files-problem-in-hdfs-ba8707ec46b?source=collection_archive---------6-----------------------#2020-01-20">https://medium.datadriveninvestor.com/big-data-small-files-problem-in-hdfs-ba8707ec46b?source=collection_archive---------6-----------------------#2020-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b104ff0d9e614ca271c83eafa2c017d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ascijk5Tu9TuPbEVwlTiuA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Lost person looking for answers. Image by <a class="ae kc" href="https://www.pexels.com/@freestockpro" rel="noopener ugc nofollow" target="_blank">VisionPic .net</a></figcaption></figure><p id="77b9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如今，每个人都在谈论“大数据”，但每个人都知道是什么让大数据变大了吗？试试看维基百科 <a class="ae kc" href="https://en.wikipedia.org/wiki/Big_data};" rel="noopener ugc nofollow" target="_blank">上</a><a class="ae kc" href="https://en.wikipedia.org/wiki/Big_data" rel="noopener ugc nofollow" target="_blank">“大数据”这个词的定义就知道了；</a>“一个……处理太大或太复杂而无法处理的数据集的领域……”，你在任何地方都找不到让数据变得太大或太复杂的数字。</p><p id="546a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">定义数据中的“大”</strong></p><p id="e113" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个成立一年的大数据团队的团队领导，我们没有理解大的定义，我们的组织没有！</p><p id="c775" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该项目的目的是处理来自不同来源的日志数据，以提高我们网络的安全性。日志数据主要由事件组成，平均为500字节。有的甚至是200字节。</p><p id="93eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之前，我们在Hadoop HDFS 中遇到了<a class="ae kc" href="https://medium.com/datadriveninvestor/solving-stability-problems-in-hadoop-cluster-big-data-with-small-data-ce2989d91425" rel="noopener">稳定性问题，因为我们忽略了一个基本假设；HDFS是为大文件(1 GB及以上)而设计的。当不稳定袭击我们时，我们开始重新思考是什么让我们成为大数据！毕竟，我们来自一个没有大数据的世界(和组织)，过去和将来都很难建立一个坚实的知识库。</a></p><p id="b946" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们配置Flume(我们的摄取工具)以5分钟为一批摄取数据，并将这些数据写到HDFS，每个事件500字节；输入介于15个事件/秒到10，000个事件/秒之间，我们有许多8kb到140kb的文件。当考虑来自所有输入的数据时，我们得到大约55GB/小时的输入速率，这足以被认为是大数据。</p><p id="880f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们困惑地寻找任何绝对答案的路上，我偶然发现了这个<a class="ae kc" href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html" rel="noopener ugc nofollow" target="_blank">博客</a>，虽然它语言粗糙，但它很简洁，在考虑做大(数据)时会为你节省宝贵的时间和金钱。</p><p id="e989" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该页面特别讨论了采用Hadoop的生态系统作为您的大数据平台。尽管文章提供了一些数字，让我们有了更清晰的了解，但从不同的角度来看，这些数字是不准确的，应该予以考虑。</p><p id="c8db" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，页面写于2013年，当时Hadoop在2.2.0版本。简单地说，今天我们是在3.2版本，所以事情发生了变化。</p><p id="97fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，如果我们用维基百科中的定义替换页面上的数字，我们可以说作者认为5TB+是一个太大\太复杂而无法处理的数据集。5TB当然很大，但大和复杂是相对于数字世界中的其他数据和我们的处理能力而言的形容词。以下预测将明确这些变化很快:</p><p id="f7b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.仅在一年内，累积的世界数据将增长到44 zettabytes(即44万亿千兆字节)！相比之下，今天大约是4.4 zettabytes。从<a class="ae kc" href="https://hostingtribunal.com/blog/big-data-stats/" rel="noopener ugc nofollow" target="_blank">hostingtribunal.com</a>取回。</p><p id="a5f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.摩尔定律称计算能力每两年翻一番(保持到3nm)。从<a class="ae kc" href="https://en.wikipedia.org/wiki/Moore's_law" rel="noopener ugc nofollow" target="_blank">维基百科</a>检索。</p><p id="17ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，今天的5TB可能超过20TB(使用最新技术，不考虑软件限制)。</p><p id="74ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，在大多数情况下，您可能会流式传输新数据(或每X次加载增量)，因此您的数据会有一个增长因子。这与考虑增长是否反映在您或您的客户查询的数据集中有关。</p><p id="fa2a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集的增长有几个来源。当您希望扩展查询的时间范围时，可以形成这种情况，另一个增长源可以从相同的输入(发送消息的速率)发展起来，最后，通过添加您希望处理的附加数据输入</p><p id="3b64" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理解数据集的定义是很重要的。</p><p id="07c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们更好地了解客户的数据集时，我们发现各种研究问题需要基于几天、几个月甚至几年的数据。这些数据集处理的数据从100MB到100TB不等。所以这是大事！</p><p id="ec3b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，对于Hadoop来说，小于1GB的文件是滥用，因此，我们存储数据的方式仍然存在问题。</p><p id="4fc1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">即使问题仍然存在，回答这个大问题是实质性的，需要修改以验证我们的需求没有改变。</p><p id="c844" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">为HDFS解决小文件</strong></p><p id="b6f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在项目的这一点上，已经有活跃的客户端使用存储在Hadoop上的数据，我们无法迁移到Cassandra(它比Hadoop更好地处理小文件——我认为选择Hadoop而不是Cassandra不是最适合我们最初需求的决定，但这都是事后诸葛亮)。因此，我们需要找到一个更简单的解决方案，基于与我们当前目录结构向后兼容的相同架构。</p><p id="f493" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那时，我们有了一个想法，以每小时的方式聚合数据，以充分利用当前的索引惯例(查看<a class="ae kc" href="https://medium.com/datadriveninvestor/solving-stability-problems-in-hadoop-cluster-big-data-with-small-data-ce2989d91425" rel="noopener">以前的故事</a>)，在每小时拥有我们可能拥有的最大的单个文件。</p><p id="60c5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个简单的程序，它从一个特定的(过去的)小时读取Flume写的数据，并将它们写入另一个并行目录结构中的一个Parquet文件。这种优化为我们在HDFS节省了大量空间和文件数量，从而降低了节点的RAM使用率，进而增强了HDFS的性能和稳定性。你可以查看不同格式之间的这些<a class="ae kc" href="https://conferences.oreilly.com/strata/strata-ny-2016/public/schedule/detail/51952" rel="noopener ugc nofollow" target="_blank">基准</a>，这是我们这些数据工程师必须知道的基础知识！</p><p id="662b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种简单的策略运行得非常好，这就是为什么我们扩展了它的功能，以支持不同的流应用程序输出，并通过输入/输出目录结构变得更加灵活。</p><p id="a5aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们估计这种解决方案只能维持一两年，但事实证明它的寿命超出了我们的预期。此外，由于原始数据和发布数据格式的分离，它为我们提供了灵活性。我们从纯文本和JSON格式开始，后来，我们转移到Parquet作为我们客户的最终发布数据格式，同时保持原始数据格式的灵活性(我们使用更适合输入数据类型的格式)，应用程序在两者之间进行转换。后来，当我们添加新格式时，在聚合应用程序中实现转换非常简单。</p><p id="923b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">重新思考你项目的假设和基础有利于增强你对所采取的行动和所做的决定的信念。请记住，在做这些决定时，要站在最简单最容易的一方，以保持未来的灵活性。</strong></p></div></div>    
</body>
</html>