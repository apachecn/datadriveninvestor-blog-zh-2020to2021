<html>
<head>
<title>Q-Learning: A Baby Step Towards Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q-Learning:强化学习的一小步</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/q-learning-a-baby-step-towards-reinforcement-learning-19ad4809edd4?source=collection_archive---------19-----------------------#2020-03-26">https://medium.datadriveninvestor.com/q-learning-a-baby-step-towards-reinforcement-learning-19ad4809edd4?source=collection_archive---------19-----------------------#2020-03-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f52ba6d9b9abc28309c5be83a12f6956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IjRjDm9YJRNBpCEy"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">(Picture By Jessica Rocowitz On Unsplash)</figcaption></figure><h1 id="94c1" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">强化学习简介:</h1><p id="a1a1" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">机器学习主要用于监督和非监督学习任务。在这里，我们将研究另一个有趣的学习范例，它有一个称为代理的特殊组件。代理人主要通过接收被称为奖励信号的信号来学习。一个例子可以是通过接收信号来学习向哪里移动的机器人。</p><h2 id="f6d5" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">与强化学习相关的术语:</h2><p id="2eab" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这里，我们将看到一些与强化学习相关的重要术语。</p><h2 id="fe82" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">代理人:</h2><p id="0e51" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是一个应该完成预期任务的模型。举个例子<strong class="lc ir"> <em class="mk">自动驾驶汽车</em> </strong>可以是一个代理来完成自动驾驶的任务。</p><h2 id="5d28" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">环境:</h2><p id="a670" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是一个代理执行所有动作的世界。它还负责向代理提供关于它如何工作的反馈。以自动驾驶汽车的<strong class="lc ir"> <em class="mk">路为例。</em> </strong></p><h2 id="8480" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">行动:</h2><p id="4dc3" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这基本上是代理在环境中做出的决定。例子可以是<strong class="lc ir"> <em class="mk">驾驶汽车。</em>T11】</strong></p><h2 id="23b1" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">奖励信号:</h2><p id="a5ed" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是代理由于其标量值的动作而得到的响应或反馈。</p><div class="ml mm gp gr mn mo"><a href="https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">深度学习用7个步骤解释-更新|数据驱动的投资者</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc jw mo"/></div></div></a></div><h2 id="b96b" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">观察/状态:</h2><p id="2f52" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">代理观察到的环境的描述</p><h2 id="3cd9" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">终端状态:</h2><p id="9ce1" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">这是我们无法采取进一步行动的状态。例子可以是通过自动驾驶汽车到达目的地或被困状态。</p><h1 id="d405" class="kc kd iq bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Q-学习:</h1><p id="afb2" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">了解了强化学习的基本概念后，现在让我们跳到称为q学习的技术。这是一个基本的算法，它试图学习一个使回报最大化的策略。</p><h2 id="a83f" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">创建Q表:</h2><p id="b68e" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">创建Q表是这个算法的核心。我们将这个矩阵的值初始化为零。每走一步，我们就更新并存储q值。该表被视为我们进一步行动的参考表。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="fa55" class="ly kd iq ni b gy nm nn l no np">q-table=np.zeros((state size,action size))</span></pre><h2 id="cddf" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">更新表格:</h2><p id="2c84" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">初始化表格后，我们应该更新表格。这发生在代理与环境交互并相应地更新[动作，状态]对时。</p><p id="f494" class="pw-post-body-paragraph la lb iq lc b ld nq lf lg lh nr lj lk ll ns ln lo lp nt lr ls lt nu lv lw lx ij bi translated">动作可以有两种类型，例如<strong class="lc ir">探索</strong>和<strong class="lc ir">利用</strong>。让我们理解他们。</p><h2 id="4af7" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">利用:</h2><p id="7eca" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">基于利用的动作使得代理做出在给定状态下可以利用的最大可能奖励的决定。</p><h2 id="a8e3" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">探索:</h2><p id="63cf" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">在这种方法中，我们允许代理随机选择动作，而不是做出贪婪的选择。这样，代理就能够学习新的状态，而不像以前的方法，代理的行为很大程度上依赖于奖励。</p><p id="cc0a" class="pw-post-body-paragraph la lb iq lc b ld nq lf lg lh nr lj lk ll ns ln lo lp nt lr ls lt nu lv lw lx ij bi translated">强化学习需要探索和利用。没有探索，代理将无法了解环境，而没有开发，代理将无法采取正确的决策。因此，我们会以平衡的方式选择任何一个行动。我们可以选择一个值ε来表示我们想要探索或利用的程度。</p><h2 id="1453" class="ly kd iq bd ke lz ma dn ki mb mc dp km ll md me kq lp mf mg ku lt mh mi ky mj bi translated">等式:</h2><p id="fdc0" class="pw-post-body-paragraph la lb iq lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">现在轮到我们更新q表了。这可以通过使用下面的等式来实现。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="59a0" class="ly kd iq ni b gy nm nn l no np">Q(state,action)=Q(state,action)+alpha[R(state,action)+gamma*max(Q(new state,action)- Q(state,action)]</span></pre><p id="07d4" class="pw-post-body-paragraph la lb iq lc b ld nq lf lg lh nr lj lk ll ns ln lo lp nt lr ls lt nu lv lw lx ij bi translated">这里的<strong class="lc ir"> <em class="mk">状态</em> </strong>是指当前状态。<strong class="lc ir"> <em class="mk">新状态</em> </strong>是指代理在某个动作后达到的状态。学习率是α，它表示在旧值的基础上，我们想要改变多少。还有一点需要注意的是<strong class="lc ir"> <em class="mk"> gamma </em> </strong>是用来平衡未来和眼前回报的贴现因子。在于[0，1]。值为1表示代理对当前奖励比对过去奖励更重视。我们看到的函数R基本上也是奖励，它表示代理人在完成某个动作后得到了多少。</p><p id="1f12" class="pw-post-body-paragraph la lb iq lc b ld nq lf lg lh nr lj lk ll ns ln lo lp nt lr ls lt nu lv lw lx ij bi translated">上面的等式和上面讨论的所有因素将帮助我们填写q表。</p><p id="4939" class="pw-post-body-paragraph la lb iq lc b ld nq lf lg lh nr lj lk ll ns ln lo lp nt lr ls lt nu lv lw lx ij bi translated">在下一篇文章中，我们将展示一个基于q学习的例子。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nv nw l"/></div></figure></div></div>    
</body>
</html>