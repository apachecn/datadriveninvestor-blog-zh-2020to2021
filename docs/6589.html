<html>
<head>
<title>Introduction to Scraping in Python with BeautifulSoup and Requests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BeautifulSoup和请求介绍Python中的抓取</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/introduction-to-scraping-in-python-with-beautifulsoup-and-requests-ab7b1c9bc113?source=collection_archive---------1-----------------------#2020-11-01">https://medium.datadriveninvestor.com/introduction-to-scraping-in-python-with-beautifulsoup-and-requests-ab7b1c9bc113?source=collection_archive---------1-----------------------#2020-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="838b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一旦你开始刮，互联网就成了你的数据库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1cac33b22a627342c66fa46850d47ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CxVccbFGtv6W2qlq0A4hxw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author: scraping workflow</figcaption></figure><p id="2bb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">出于各种原因，组织中不同级别的许多人可能需要从互联网上收集外部数据:分析竞争、聚合新闻源以跟踪特定市场的趋势，或者收集每日股票价格以构建预测模型…</p><p id="a045" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">无论你是数据科学家还是商业分析师，你可能会不时遇到这种情况，并问自己这个永恒的问题:<strong class="la iu"> <em class="lu">我如何才能提取这个网站的数据来进行市场分析？</em>T3】</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c0ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">提取网站数据并对其进行结构化的一种可能的免费方式是<strong class="la iu">抓取。在这篇文章中，你将学习数据抓取以及如何用python轻松构建你的第一个抓取器。</strong></p><p id="e2d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> ps*:本文支持入门视频教程</em>🎥<em class="lu">我用Python做了关于数据抓取的。有兴趣可以在这里</em>  <em class="lu">观看</em> <a class="ae mc" href="https://www.youtube.com/watch?v=7Odi2_u-yDk" rel="noopener ugc nofollow" target="_blank"> <em class="lu">。</em></a></p><p id="d4c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> ps**:如果你对源代码感兴趣，可以在这里</em>  <em class="lu">抓取</em> <a class="ae mc" href="https://github.com/ahmedbesbes/scraping-tutorial" rel="noopener ugc nofollow" target="_blank"> <em class="lu">。</em></a></p><h1 id="34b1" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">什么是数据抓取？🧹</h1><p id="189a" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">让我给你省掉冗长的定义。</p><p id="8004" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">广义地说，数据抓取是以编程方式提取网站数据并根据个人需求构建数据的过程。许多公司正在使用数据搜集来收集外部数据并支持他们的业务运营:这是当前多个领域的常见做法。</p><h2 id="73f1" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated">学习python中的数据抓取需要知道什么？</h2><p id="9108" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">不多。要构建小型刮刀，您必须对Python和HTML语法有一点熟悉。</p><p id="b3d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要构建可扩展的工业刮刀，您需要了解一两个框架，如<a class="ae mc" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"/></a><strong class="la iu"/>或<a class="ae mc" href="https://www.selenium.dev/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">硒</strong> </a> <strong class="la iu">。</strong></p><h1 id="dc9a" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">用Python构建您的第一个刮刀</h1><h2 id="adcb" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated">设置您的环境</h2><p id="6df2" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">让我们来学习如何把一个网站变成结构化数据！为此，首先需要安装以下库:</p><ul class=""><li id="f091" class="nm nn it la b lb lc le lf lh no ll np lp nq lt nr ns nt nu bi translated"><a class="ae mc" href="https://requests.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">请求</strong> </a> <strong class="la iu"> : </strong>模拟HTTP请求，如GET和POST。我们将主要使用它来访问任何给定网站的源页面。</li><li id="6148" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated"><a class="ae mc" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> BeautifulSoup </strong> </a>:非常容易解析HTML和XML数据</li><li id="7dc5" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated"><a class="ae mc" href="https://lxml.de/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> lxml </strong> </a>:提高xml文件的解析速度</li><li id="7a11" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated"><a class="ae mc" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">熊猫</strong> </a> <strong class="la iu"> : </strong>将数据组织成数据帧，并以您选择的格式(JSON、Excel、CSV等)导出。)</li></ul><p id="9194" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您使用的是Anaconda，那么您应该已经准备好了:所有这些包都已经安装好了。否则，您应该运行以下命令:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="28dc" class="na me it ob b gy of og l oh oi"><strong class="ob iu">pip install requests<br/>pip install beautifulsoup4<br/>pip install lxml<br/>pip install pandas</strong></span></pre><p id="0754" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让人们容易地跟随我的视频教程，我还使用了一个jupyter笔记本来使这个过程互动。</p><h2 id="d90f" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated"><strong class="ak">我们要刮什么网站和数据？</strong></h2><p id="dbbf" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">我的一个朋友问我能不能帮他刮这个<a class="ae mc" href="https://www.premiumbeautynews.com/fr/marches-tendances/" rel="noopener ugc nofollow" target="_blank">网站</a>。所以我决定在教程里做。</p><p id="dcc9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个网站叫做高级美容新闻。它发布美容市场的最新趋势。如果你看看首页，你会看到我们想要收集的文章被组织在一个网格中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/12c3a0c4ccc1afb6dc351542aeabeb7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnMtFlLgrK-z-wMZ1kRnOg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Article headlines</figcaption></figure><p id="9859" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多页:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/dbc3c8478409d71d5ba532ebb119474f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGMPbH8vwv5Gm2BkHUhCVg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Pagination: here’s where scraping comes in handy</figcaption></figure><p id="1fae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，我们不会只提取这些页面上出现的每篇文章的标题。我们会进入每个岗位拿走我们需要的一切:</p><p id="f552" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">标题</strong>、<strong class="la iu">日期</strong>、<strong class="la iu">摘要</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/c85aae1e8c71779e5c7ffb377fad2c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2A4fUP2-nHHU18j_jYFhg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author</figcaption></figure><p id="dd21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然还有剩余的<strong class="la iu">帖子的全部内容</strong>。</p><h1 id="a240" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">给我看看代码！</h1><p id="7143" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">因为这就是你在这里的原因，对吗？</p><p id="fd97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本进口优先:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="4b26" class="na me it ob b gy of og l oh oi"><strong class="ob iu">import requests <br/>from bs4 import BeautifulSoup <br/>import pandas as pd <br/>from tqdm import tqdm_notebook</strong></span></pre><p id="a0fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我通常定义一个函数来解析给定URL的每个页面的内容。这个函数将被多次调用。姑且称之为<strong class="la iu"> parse_url: </strong></p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="3248" class="na me it ob b gy of og l oh oi"><strong class="ob iu">def</strong> parse_url(url):     <br/>    response = requests.get(url)<br/>    content = response.content     <br/>    parsed_response = BeautifulSoup(content, "lxml")     <br/>    <strong class="ob iu">return</strong> parsed_response</span></pre><h2 id="d9b1" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated">提取每个帖子的数据和元数据</h2><p id="9d7f" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">首先，我将定义一个函数，根据给定的URL提取每个帖子的数据(标题、日期、摘要等)。然后我们将在遍历所有页面的for循环中调用这个函数。</p><p id="0430" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要构建我们的scraper，我们首先必须理解页面的底层HTML逻辑和结构。先从提取帖子标题开始。</p><p id="d589" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过在Chrome inspector上检查该元素:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/5cc76d5315deaa5369e42edeb29b4849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9NorjCOqgm49OsvOYsRpw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Title of the post</figcaption></figure><p id="e49a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们注意到标题出现在类<strong class="la iu">“文章标题”</strong>的<strong class="la iu"> h1 </strong>中。<br/>使用BeautifulSoup提取页面内容后，可以使用<strong class="la iu"> find方法提取标题。</strong></p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="d6fe" class="na me it ob b gy of og l oh oi"><strong class="ob iu">title = soup_post.find("h1", {"class": "article-title"}).text</strong></span></pre><p id="1a87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们看看日期:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/c93d08b86786a3f5b45cc687ad4af077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHx1gTLX-qjo63-wsVBFlg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Date of the post</figcaption></figure><p id="b1b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">日期出现在<strong class="la iu">跨度</strong>内，跨度本身出现在<strong class="la iu">“行子标题”类的<strong class="la iu">标题</strong>内。</strong>使用BeautifulSoup将这些内容翻译成代码非常容易:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="c98e" class="na me it ob b gy of og l oh oi"><strong class="ob iu">datetime = soup_post.find("header", {"class": "row sub-  header"}).find("span")["datetime"]</strong></span></pre><p id="c87c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于摘要:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/8c4a771fa8033022bed1131d8187208f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7E9ai931b0x6_BZZc1tnOA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Abstract of the post</figcaption></figure><p id="2c8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来它包含在类“article-intro”的一个<strong class="la iu"> h2 </strong>标签中。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="75fa" class="na me it ob b gy of og l oh oi"><strong class="ob iu">abstract = soup_post.find("h2", {"class": "article-intro"}).text</strong></span></pre><p id="17ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，帖子的全部内容呢？实际上很容易提取。该内容分布在类<strong class="la iu">“文章-文本”的<strong class="la iu"> div </strong>中的多个段落(<strong class="la iu"> p标签</strong>)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/5144f38c4e23e2cc052477610a8f4b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xiTM-6bojK48iDdJQtWjMQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — The full content of the post</figcaption></figure><p id="c0ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BeautifulSoup不需要遍历每个单独的<strong class="la iu"> p标签，</strong>提取它的文本，然后连接所有的文本，它可以用这样一种方式提取完整的文本:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="0c79" class="na me it ob b gy of og l oh oi"><strong class="ob iu">content = soup_post.find("div", {"class": "article-text"}).text</strong></span></pre><p id="f2e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们将一切打包到一个函数中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><h2 id="c354" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated">提取多个页面上的帖子URL</h2><p id="e02f" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">如果我们检查主页的源代码，在主页上，文章与标题一起显示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/6d36c1a48af14524eb2162fcf859a3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DvwQAaSJLZzofc7XgSTjtA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Homepage source</figcaption></figure><p id="2969" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将看到网格中出现的十篇文章中的每一篇都在类<strong class="la iu">“post-style 1 col-MD-6”</strong>的<strong class="la iu"> div </strong>中，而后者本身又在类<strong class="la iu">“content”的<strong class="la iu"> section </strong>中。</strong></p><p id="0e48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，提取每页的帖子非常容易:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="55ff" class="na me it ob b gy of og l oh oi"><strong class="ob iu">url = "https://www.premiumbeautynews.com/fr/marches-tendances/"</strong><br/><strong class="ob iu">soup = parse_url(url)<br/>section = soup.find("section", {"class": "content"})<br/>posts = section.findAll("div", {"class": "post-style1 col-md-6"})</strong></span></pre><p id="bb3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，对于每个单独的帖子，我们可以提取出现在一个“<strong class="la iu"> a标签”</strong>内的URL，该标签本身在一个<strong class="la iu"> h4内。我们将使用这个URL来调用我们之前定义的函数<strong class="la iu"> extract_post_data。</strong></strong></p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="c1ff" class="na me it ob b gy of og l oh oi"><strong class="ob iu">uri = post.find("h4").find("a")["href"]</strong></span></pre><h2 id="3dcb" class="na me it bd mf nb nc dn mj nd ne dp mn lh nf ng mp ll nh ni mr lp nj nk mt nl bi translated"><strong class="ak">分页</strong></h2><p id="876b" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">一旦在给定页面上提取了帖子，您可能希望转到下一个页面并重复相同的操作。</p><p id="e3fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您查看分页，您会注意到一个“下一步按钮”:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f7afd51436d73fcb43e6d5f3fb45f727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*pIz9ON2CLBOUUL7Kd-XIlg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screenshot made by the author — Pagination</figcaption></figure><p id="d2e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您到达最后一页时，此按钮变为非活动状态。<br/>换句话说，当下一页按钮激活时，你要告诉scraper抓取当前页面的帖子，移动到下一页，重复操作。当按钮变为非活动状态时，该过程应该停止。</p><p id="bf16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总结这一逻辑，它将转化为以下代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="d8df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦这个循环完成，您将拥有posts_data中的所有数据，您可以将其转换成漂亮的数据帧并导出到CSV或Excel文件。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="50fe" class="na me it ob b gy of og l oh oi"><strong class="ob iu">df = pd.DataFrame(posts_data)<br/>df.head()</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/b5e6fb113021ad0812d8e37f10c8c671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPyrTf6nFEhZWLM5OojvBg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">dataframe of the scraped data — image by the author</figcaption></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c3ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读坚持到最后！如果你对本教程的视频录制感兴趣，这里有链接:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ov or l"/></div></figure><h1 id="1341" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">从这里去哪里？</h1><p id="da4b" class="pw-post-body-paragraph ky kz it la b lb mv ju ld le mw jx lg lh mx lj lk ll my ln lo lp mz lr ls lt im bi translated">你刚刚学会如何建立你的第一个刮刀，祝贺你！</p><p id="6f89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果你想改善它，你可以考虑以下步骤:</p><ul class=""><li id="a7ee" class="nm nn it la b lb lc le lf lh no ll np lp nq lt nr ns nt nu bi translated">多重处理和多线程脚本的执行</li><li id="3cb8" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">安排刮刀在一段时间内运行，以自动化数据刮削</li><li id="3633" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">句柄错误——随着时间的推移，刮刀很难维护，因为源代码可能会改变</li><li id="d872" class="nm nn it la b lb nv le nw lh nx ll ny lp nz lt nr ns nt nu bi translated">部署一个数据库或s3存储桶来存储抓取的项目</li></ul><p id="94fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可能会让你忙上一阵子！</p><p id="0323" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据抓取快乐！</p></div></div>    
</body>
</html>