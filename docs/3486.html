<html>
<head>
<title>AI &amp; Listening Between the Lines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能&amp;字里行间的倾听</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/ai-listening-between-the-lines-e0626a71a4d8?source=collection_archive---------4-----------------------#2020-06-20">https://medium.datadriveninvestor.com/ai-listening-between-the-lines-e0626a71a4d8?source=collection_archive---------4-----------------------#2020-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/20a16530df4daf1b3b68ae4283f0f10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUTInFZk0VZHGS8aEPuHiQ.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Roan Mountain, TN, USA, photo by <a class="ae jg" href="https://unsplash.com/@tabithabrooke" rel="noopener ugc nofollow" target="_blank">@tabithabrooke</a></figcaption></figure><div class=""/><div class=""><h2 id="eb0b" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">改进语音识别中的非语义表示</h2></div><p id="4c5f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实胜于雄辩，很多时候语音识别无法捕捉上下文或您试图传达的意思。基于语义或非语义上下文采取错误的操作可能会在应用语音识别的随意或关键上下文中让您失望。</p><p id="400f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">交谈可能是一项复杂的活动。有时候，我们的意思比我们说的更多，我们的音调可能是我们传达的信息的核心部分。一个单词的不同强调可能会改变一个句子的意思。</p><p id="d11e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，考虑到这一点，自我监督如何改善语音表达和个性化模型？</p><p id="10da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">语音识别模型如何识别你在说什么？</p><p id="d324" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2020年6月18日，谷歌人工智能的一篇博客文章回答了这个问题。</p><p id="3a05" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章认为，通过大量数据可以更容易地解决许多任务，如<a class="ae jg" href="https://en.wikipedia.org/wiki/Speech_recognition" rel="noopener ugc nofollow" target="_blank">自动语音识别</a> (ASR)。</p><p id="20e1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是很方便的，例如将语音翻译成文本。</p><p id="1efa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个语义解释很有趣。</p><p id="8085" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在“非语义”任务中有一个对比。</p><p id="9b85" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些任务关注的是意义。</p><p id="d1c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，存在“副语言”任务。</p><p id="4bf6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有元交流的成分。比如对情感的识别。</p><p id="e306" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可能是认出一个说话的人。</p><p id="fb0b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">说什么语言？</p><p id="fa89" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者认为，当在小数据集上训练时，那些依赖大数据集的人可能不太成功。</p><p id="3d64" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大小之间有性能差距。</p><p id="52bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有人认为，这可以通过在大型数据集上训练表示模型，然后用较少的数据给它设置来弥补。</p><p id="2c00" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可以通过两种方式提高性能:</p><ol class=""><li id="5ffe" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">使得通过将高维数据(如图像和音频)转换到较低维度来训练小模型成为可能。表示模型也可以用作预训练。</li><li id="8451" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">此外，如果表示模型足够小，可以在设备上运行或训练，它可以通过向用户提供个性化模型的好处来以保护隐私的方式提高性能，其中原始数据永远不会离开他们的设备。</li></ol><p id="ee20" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文本域表示学习的例子可以是<a class="ae jg" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae jg" href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener ugc nofollow" target="_blank">阿尔伯特</a>。</p><p id="61a6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于图像，可以是<a class="ae jg" href="https://arxiv.org/abs/1411.1792" rel="noopener ugc nofollow" target="_blank">初始层</a>和<a class="ae jg" href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html" rel="noopener ugc nofollow" target="_blank"> SimCLR </a>。</p><p id="098b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者认为这些方法在语音领域没有得到充分利用。</p><p id="72f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">共同的基准在哪里？</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c2449303866440037b01cfa74cd1d84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*UYflE_srvL5RYbKB.jpg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Bottom:A large speech dataset is used to train a model, which is then rolled out to other environments. Top Left: On-device personalization — personalized, on-device models combine security and privacy. Top Middle: Small model on embeddings — general-use representations transform high-dimensional, few-example datasets to a lower dimension without sacrificing accuracy; smaller models train faster and are regularized. Top Right: Full model fine-tuning — large datasets can use the embedding model as pre-training to improve performance</figcaption></figure><p id="af66" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者认为，在非语义工作中，没有有用表示的标准基准。</p><p id="84b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从这个意义上说“言语表征有用”。</p><p id="c96d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">表征学习有两个进步:</p><ul class=""><li id="c301" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt mn ma mb mc bi translated"><a class="ae jg" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a>框架系统地评估文本嵌入。</li><li id="bf34" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt mn ma mb mc bi translated"><a class="ae jg" href="http://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html" rel="noopener ugc nofollow" target="_blank">视觉任务适配基准</a> (VTAB)标准化图像嵌入评测。</li></ul><p id="9ca0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些不直接评估非语义语音嵌入。</p><p id="3014" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者有一篇关于arXiv的论文，名为:“<a class="ae jg" href="https://arxiv.org/abs/2002.12764" rel="noopener ugc nofollow" target="_blank">学习语音的通用非语义表示</a>”</p><p id="d32b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这方面，他们有三个贡献。</p><ol class=""><li id="95fb" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="mo">首先，他们提出了一个用于比较语音表示的非语义语音(NOSS)基准，它包括不同的数据集和基准任务，如语音情感识别、语言识别和说话人识别。</em> <a class="ae jg" href="https://www.tensorflow.org/datasets/catalog/overview#audio" rel="noopener ugc nofollow" target="_blank"> <em class="mo">这些数据集</em> </a> <em class="mo">在</em> <a class="ae jg" href="https://www.tensorflow.org/datasets/" rel="noopener ugc nofollow" target="_blank"> <em class="mo"> TensorFlow数据集</em> </a> <em class="mo">的“音频”部分都有。</em></li><li id="735e" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><em class="mo">其次，他们创建并开源了</em> <a class="ae jg" href="https://aihub.cloud.google.com/s?q=nonsemantic-speech-benchmark" rel="noopener ugc nofollow" target="_blank"> <em class="mo">三重损耗网络</em> </a> <em class="mo"> (TRILL)，一个小到足以在设备上执行和微调的新模型，同时仍然胜过其他表示。</em></li><li id="9c1c" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><em class="mo">第三，他们执行大规模的研究，比较不同的表示，</em> <a class="ae jg" href="https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark" rel="noopener ugc nofollow" target="_blank"> <em class="mo">开源代码</em> </a> <em class="mo">用于计算新表示的性能。</em></li></ol><p id="79d7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更进一步，我建议阅读原始的博客帖子或查看他们关于arXiv的研究论文。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="ffef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里是#500daysofAI，您正在阅读的是第382条。500天来，我每天都在写一篇关于或与人工智能相关的新文章。</p></div></div>    
</body>
</html>