<html>
<head>
<title>Decision Tree in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的决策树</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-decision-tree-model-works-ce681cae10a6?source=collection_archive---------0-----------------------#2020-05-17">https://medium.datadriveninvestor.com/how-decision-tree-model-works-ce681cae10a6?source=collection_archive---------0-----------------------#2020-05-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3806" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">决策树背后的数学原理及其Python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/670cdbb09f34c5a3faa1501ed65ae7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsiNuLg5rI-IuJ-jpntb1g.jpeg"/></div></div></figure><p id="2b1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本博客涵盖以下主题:</p><p id="cf23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">1.什么是决策树？</p><p id="1187" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.特征选择</p><p id="d626" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.树代</p><p id="3210" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">4.树木修剪</p><p id="39de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">5.用Python实现</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><p id="0a01" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 1。</strong> <strong class="kw iu">什么是决策树？</strong></p><p id="994f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策树是一种监督学习，可以解决机器学习领域中的分类和回归问题。基本上，决策树将特征空间划分为一组矩形，然后通过拟合简单的模型(如组均值或模式)进行预测。一个典型的树模型由内部节点和叶节点组成。内部节点包含决策树分成的两个分支。叶节点位于分支的末端，不再分裂。</p><p id="f39c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树模型是白盒模型的一种，因此它简单且有助于解释。但是，在某些情况下，它可能无法与线性模型竞争。因此，引入bagging和boosting来提高模型性能。通过这两种方法，我们可以看到，一束树的组合可以导致预测的显著改善，但代价是解释方面的损失。</p><p id="2a6e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策树的<strong class="kw iu">优势</strong>:</p><p id="b8ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树可以生成可理解的规则，可以图形化显示。</p><p id="0fe9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树清楚地表明了哪些特征对预测最重要。</p><p id="d3c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树没有关于数据分布的假设。</p><p id="0aa8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树有效地处理共线性。</p><p id="d7da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策树的<strong class="kw iu">弱点</strong>:</p><p id="f005" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树有很高的方差，可以创建过于复杂的模型。</p><p id="5fa7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树对异常值很敏感。</p><p id="cdfd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">树在分类问题中容易出现错误，类别很多，训练样本相对较少。</p><p id="6f6e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 2。功能选择</strong></p><p id="1baa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">生长决策树包括决定选择哪些特征以及使用哪种标准来分割节点。让我们关注一下经典算法ID3，它被广泛用于建立树模型。</p><p id="1923" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在信息论中，熵用于度量随机变量的不确定性。熵越高，随机变量的不确定性越大。</p><p id="9e69" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设我们有一个离散随机变量<em class="lx"> Y </em>，概率为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/df23e0aaf7e50e0b4b6961cc3ba14c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/1*jm2Y95TsCovO6NTjBzIMLg.gif"/></div></figure><p id="c847" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么熵可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/439fc4b6ecce663ebad1987c69d79758.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*bccZOV0bPe6qxFghtU7_QA.png"/></div></figure><p id="cfd7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设<em class="lx"> i </em> =2，是二进制情况，公式可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/57ab83bc46bea0f65bdd62ff67206b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*oeGekfNScFmm0-RA7bpjPA.png"/></div></figure><p id="3615" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">熵<em class="lx"> H(p) </em>随着概率<em class="lx"> p </em>的变化如何表现，如下图所示。我们可以观察到当<em class="lx"> p </em>等于0.5时熵达到最大，当<em class="lx"> p </em>等于0或1时熵达到最小。注意，我们定义0*log(0)=0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/532a33e7536bd5a11dea2b8e0c24e775.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*19hioIfCwZGxGdhe9XZoQQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure: Generated by R</figcaption></figure><p id="009a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">条件熵<em class="lx"> H(Y|X) </em>度量随机变量<em class="lx"> Y </em>给定<em class="lx"> X. </em>的不确定性，可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/bd1e298351e596227c53b0b5b196d7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*zjK6Bxcx2koUnBnIq4lBQQ.png"/></div></figure><p id="63bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">公式中，<em class="lx">π</em>为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/53c43eb963874162100280e9f222ffce.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*z43Ee8G62pAsqEB2RdiLOw.png"/></div></figure><p id="1576" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">信息增益的公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9b36f92b6d94537fba0adbad9ac5056b.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*z7a-3atNJT3pSrHWWksiZg.png"/></div></figure><p id="d79e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个随机变量(<em class="lx"> Y </em>)的信息增益量是从其他现有变量(<em class="lx"> X) </em>观察到的不确定性的减少。</p><p id="7bb5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策树<strong class="kw iu">选择最大化信息增益的特征</strong>来分割节点。使用一个例子来解释树模型如何使用信息增益工作。假设我们有15个申请人的信息，包括年龄、工作、公寓和信用记录。根据这些信息，我们想找出哪个变量是决定申请人是否成功申请贷款的最重要的变量(<em class="lx"> Y </em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/7eabdb7e541f2df0263b95e4d3f1b862.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*WiN9ohWvyvMOYE5fZMzv8w.png"/></div></figure><p id="a737" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">根据定义，我们可以为<em class="lx"> Y </em>计算熵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/8121c9b061b7b32dc355660caa787086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*Y4YgpELtY1Bb_OIJBhEvOA.png"/></div></figure><p id="1253" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定可变年龄的<em class="lx"> Y </em>的条件熵为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a9b51f760651bac5250c7e48a22aea93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*1P-JqXKGWhiuqYp2NorB7A.png"/></div></figure><p id="a633" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们可以计算给定可变年龄的<em class="lx"> Y </em>的信息增益:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/17b8736339601fe8dd4d7cf5cec6afaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*hbRmGNYywZamcXNMHDCuZA.png"/></div></figure><p id="f639" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">类似地，我们从可变工作、自有公寓和信用记录中获得信息增益。结果如下表所示。我们选择信息增益最大化的变量own apartment作为第一个分裂的节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/cdfc9cc0c1dadae92ca275a5d77d2f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Y3edEq-Vi14a-60iwd87IA.png"/></div></figure><p id="3a04" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">确定第一个节点后，创建两个数据子集。第一个子集包含自己公寓为“是”的所有观察结果，另一个子集包含自己公寓为“否”的所有观察结果。然后，该算法将在每个子集的剩余变量上循环，以探索进一步的分割。</p><p id="b211" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在CART算法中，选择基尼系数最小的特征作为分割节点。基尼系数的公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/c4f535dc1cbfe81eb6e0cc9fc875e76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*DdFKx-44mXYRJ_NNHdSz2g.png"/></div></figure><p id="e284" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设<em class="lx"> k </em> =2，则为二进制情况，公式可写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/5791dfe2fad8b9280dffabebaf32ff28.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*d-UCBgq0K4t0MPeOvQ2Bbw.png"/></div></figure><p id="18b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下图显示了0.5 *熵、基尼系数和错误分类率的差异。所有标准在<em class="lx"> p </em>等于0.5时达到最大，在<em class="lx"> p </em>等于0或1时达到最小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/2794862f054534f6ad7eef551cae7ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*NMd7iVmNUy6G8QNQ4qg_Zg.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure: Generated by R</figcaption></figure><p id="c752" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于回归问题，树模型的目标是找到子空间R1，…Rj以最小化残差平方和(RSS)，由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6c833e26a0c0e7b17bd3a08af158059d.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*zGv5o6roHLBETz6XPQkvSQ.png"/></div></figure><p id="d301" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 3。树生成</strong></p><p id="31f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">决策树采用自上而下的贪婪方法，这就是所谓的递归二进制分裂。递归二进制分裂方法是自顶向下的，因为它从树的顶部开始，然后连续分裂特征空间；每一次分裂都通过树中更下方的两个新分支来表示。它是贪婪的，因为在树构建过程的每一步，最好的分裂都是在那个特定的步骤进行的，而不是向前看，选择一个会在未来的某个步骤中导致更好的树的分裂。因此，树模型为每个分裂寻找局部最优分支，因此它不能保证返回全局最优模型。</p><p id="9d3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 4。树木修剪</strong></p><p id="77ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了处理决策树的高变化，引入了剪枝。修剪是一种通过删除不太重要的分支来减小树模型大小的技术。因此，我们可以通过降低模型的复杂性来避免过拟合问题。</p><p id="e99f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">修剪一棵树的第一个方法是在我们种树的时候早点停下来。例如，我们可以设置一个阈值来分割节点。假设一个节点的信息增益低于阈值，我们请求树不要分裂该节点，因为这样的分裂只带来有限的好处。</p><p id="c673" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一种修剪树的方法是在树完全长大后剪去树枝。我们如何决定修剪一棵树的方法？从离开节点开始，每个内部节点都被替换为其最流行的类。如果预测精度不受影响，则保持该变化。我们可以使用交叉验证来选择具有最佳预测的最佳树模型。理论上是可行的。然而，这在实践中很麻烦，因为要考虑的子树太多了。</p><p id="2e49" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一个直观的解决方案是当树长得太大时增加惩罚，这被称为成本复杂性修剪。以回归树为例。成本函数可以修改为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/44af8eb3a0ee1aded88e8b8612b92127.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*-p-TaRNwyE_xLX_GD9o_Xg.png"/></div></div></figure><p id="aff9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lx"> T </em>表示模型中终端节点的数量。我们在成本函数中加入一个非负的调整参数<em class="lx"> a </em>。调整参数控制模型复杂性及其对训练数据的拟合优度之间的权衡。<em class="lx"> a </em>越高，分配给模型复杂度的惩罚就越大。基本上，我们可以使用交叉验证来选择最佳的调整参数。</p><p id="4ff2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 5。用Python实现</strong></p><p id="f51a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我使用Python来执行使用ID3的特性选择，并在本节中使用NumPy和scikit-learn包生成一个树模型。</p><p id="05d6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 5.1功能选择</strong></p><p id="f2a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">生成第2节中所示的数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="edde" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一步是计算响应<em class="lx"> Y </em>的熵。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="e778" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">熵测试的结果是0.971。然后计算条件熵。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2f02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">cond_entropy_test1的返回是0.888，这是就可变年龄而言的<em class="lx"> Y </em>的条件熵。下一步是计算信息增益。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="d895" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">info_gain_test1的返回是0.083。最后，将所有函数堆叠在一起，以便算法可以循环所有变量，并返回最大化信息增益的最佳函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="fd65" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">best_feature_test的回报是Own_Apartment，其对应的信息增益是0.420。</p><p id="5765" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> 5.2树生成</strong></p><p id="9e71" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将0.1定义为提前停止的阈值，以避免过拟合问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="c966" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">算法的返回是一个字典:{'label': None，' feature': 'Own_Apartment '，' tree ':{ ' label ':None，' feature': 'Work '，' tree ':{ ' label ':' No '，' feature': None，' tree': {}，' Yes': {'label': 'Yes '，' feature': None，' tree': {}}}，' Yes ':{ ' label ':' Yes ':' feature ':' Yes ':' Yes ':' feature ':' Yes ':' Yes ':' Yes ':None '，' feature ':{ ' tree '这棵树的形象化是。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/839cff960a3a26fa7676eb9b2b9c37ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZlr44PtBWDa_MJ8gpTPwg.png"/></div></div></figure><p id="eded" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">5.3 sci kit中的决策树-学习</strong></p><p id="e7a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们也可以使用完善的软件包scikit-learn来生长一个树模型。这里有一个例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="8072" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该树模型的可视化结果如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/11a8ada1574c38eecbf09642ed995fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KMAkXe5WKMm9_bkC0i3t3A.png"/></div></div></figure><p id="35ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可以查看Scikit-Learn的更多详细信息:</p><div class="mv mw gp gr mx my"><a href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">1.10.决策树-scikit-了解0.23.0文档</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">决策树(DTs)是一种用于分类和回归的非参数监督学习方法。目标是…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">scikit-learn.org</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ks my"/></div></div></a></div><p id="6198" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">结论</strong></p><p id="6560" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇博客中，我解释了决策树背后的理论和数学，举例说明了执行特征选择的模型和用Python实现的算法。希望看完这篇博客，你能对这个经典算法有更好的理解。如果您对其他博客感兴趣，请点击以下链接:</p><div class="mv mw gp gr mx my"><a href="https://towardsdatascience.com/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">机器学习和深度学习之旅</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm ks my"/></div></div></a></div><p id="ed73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">参考:</strong></p><p id="30b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1] Gareth James，Daniela Witten，Trevor Hastie，Robert Tibshirani，(2017) <em class="lx">统计学习介绍</em></p><p id="ed81" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] Christopher M. Bishop，(2009)，<em class="lx">模式识别和机器学习</em></p><p id="3f2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3]特雷弗·哈斯蒂，罗伯特·蒂布拉尼，杰罗姆·弗里德曼，(2008)，<em class="lx">统计学习的要素</em></p><p id="40e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4]https://en.wikipedia.org/wiki/ID3_algorithm</p><p id="cfc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]https://en.wikipedia.org/wiki/C4.5_algorithm</p><p id="c2c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]https://en . Wikipedia . org/wiki/Information _ gain _ in _ decision _ trees</p><p id="ec74" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7]https://scikit-learn.org/stable/modules/tree.html</p></div></div>    
</body>
</html>