<html>
<head>
<title>Visualizing Neural Networks using Saliency Maps in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中使用显著图可视化神经网络</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4?source=collection_archive---------0-----------------------#2020-01-16">https://medium.datadriveninvestor.com/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4?source=collection_archive---------0-----------------------#2020-01-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/090fbc946025cb38c22ec5c34727cf53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*vmOVSK7KplO77BSWhzzJkA.gif"/></div></div></figure><p id="e84e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">神经网络正被用于许多应用中，并且它们的用例正以惊人的速度增长。人们越来越需要神经网络能够被人类理解。理解<strong class="ka ir">神经网络在寻找什么</strong>以及它对新样本的鲁棒性有多强，不仅有助于解释它的决定，也有助于满足科学好奇心。</p><h1 id="71a3" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">什么是显著图？</h1><p id="ebe9" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">在这篇博文中，我们将讨论显著性图— <strong class="ka ir">它们是高亮显示输入图像中最容易导致输出分类的像素的热图。</strong></p><p id="6405" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们有一个用于图像分类问题的训练好的ConvNet。这个ConvNet会产生一些类分数，根据最大分数，我们会得到输入图像的一些输出类。</p><p id="2d85" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lz">顺便说一下，类得分是神经网络在softmax之前分配给类的输出层中的值，因此它们不是概率，但它们通过softmax这样的函数与概率直接相关。</em></p><div class="ma mb gp gr mc md"><a href="https://www.datadriveninvestor.com/2019/01/28/ai-creativity-deep-dream-comes-true/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">人工智能与创造力:梦想成真|数据驱动的投资者</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">人工智能总是让我着迷。不仅作为一套有用的工具，不断发展，而且作为一个…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jw md"/></div></div></a></div><p id="e5e7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，考虑输出类得分相对于输入图像像素值的梯度。<strong class="ka ir">梯度较大(正或负)的像素是需要改变最少而对类别分数影响最大的像素。人们可以预期这些像素对应于图像中物体的位置。这是显著图背后的基本思想。</strong></p><h1 id="6ae9" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">PyTorch中的显著图提取</h1><p id="1feb" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">首先，我们需要一个预训练的神经网络来进行图像分类。在这里，我们将使用预训练的VGG-19通信网。在PyTorch中，这是torchvision模块附带的。</p><p id="7160" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="lz"> VGG-19是一个卷积神经网络，它已经在ImageNet数据集的100多万张图像上进行了训练。该网络有19层深，可以将图像分为1000个对象类别，如键盘、鼠标、铅笔和许多动物。因此，该网络已经学习了各种图像的丰富特征表示。网络的图像输入大小为224×224。[4] </em></p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Imports and code for using pretrained VGG-19 model. Note that since we don’t need to find gradients with respect to the parameters of the network, so we’re setting param.requires_grad to False.</figcaption></figure><p id="82e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们需要一个输入图像来提取显著图。作为一个例子，我们将使用一只非常可爱的马耳他狗的图片。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d6947e0ce9c142d5fcadf79104d488ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*s7Xe7mnmlSMPPv3I0yl5tA.jpeg"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Image borrowed from <a class="ae nd" href="https://specials-images.forbesimg.com/imageserve/5db4c7b464b49a0007e9dfac/960x0.jpg?fit=scale" rel="noopener ugc nofollow" target="_blank">https://specials-images.forbesimg.com/imageserve/5db4c7b464b49a0007e9dfac/960x0.jpg?fit=scale</a></figcaption></figure><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Downloading and opening the image</figcaption></figure><p id="3bac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们需要对打开的图像进行预处理，这样我们就可以将它输入到ConvNet中。我们需要将图像的大小调整为所需的224 * 224，然后将图像转换为PyTorch张量。请注意，ToTensor函数将范围[0，255]中的PIL图像或NumPy数组(高度x宽度x通道)转换为范围[0，1]中的火炬形状浮动张量(通道x高度x宽度)。我们还需要使用ImageNet数据集的图像的平均值和标准偏差来归一化张量。所有这些都可以按如下方式实现。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Functions for preprocessing and displaying the image.</figcaption></figure><p id="800b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">显著图是最大得分值相对于输入图像的梯度。但是注意，输入图像具有3个通道，R、G和b。为了导出每个像素(I，j)的单个类别显著性值，<strong class="ka ir">我们在所有颜色通道上取最大值。</strong>这可以通过以下方式实现。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h1 id="5468" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结果</h1><p id="2c55" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们得到下面的热图，它显示了ConvNet在输入图像中实际寻找的位置，以预测每个图像的类别。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/9a8e4b53c8882eacbbee7d8fed9e5e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYAr7eMJfGLCyd60Z-YjdA.png"/></div></div></figure><p id="448d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们看到热图<strong class="ka ir">紧密地集中在狗</strong>身上，这令人放心，因为这意味着conv net<strong class="ka ir">只关注狗，而不是其他任何东西来做出预测</strong>。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/73b07daf30452b8764b492ebbdc21335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzRTYHcl-FppkXUvGKY3tA.png"/></div></div></figure><p id="a941" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们看到<strong class="ka ir">热图与之前的热图相比更加分散</strong>，如果在这里被照亮的其他区域没有出现在新图像中，则可能会发生ConvNet在这一类别中的表现更差的情况。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/96a9053f8749deae7e8441d000757ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6bmFVMxSgUtXmac2YwHdQ.png"/></div></div></figure><p id="32e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这一个看起来也令人放心，因为我们看到ConvNet只查看足球头盔来做出预测。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/ca591df777adb959e9d6bec92ec74807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IpmYMtJG9qw-kgszmeJEGw.png"/></div></div></figure><p id="82f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们看到，甚至背景中的一些<strong class="ka ir">树</strong>和前景中的<strong class="ka ir">喷泉基座的圆形形状</strong>也是驱动ConvNet做出预测的原因，即这幅图像是一个喷泉。这种偏见可能已经出现，因为ConvNet可能已经在许多具有圆形喷泉底座和背景树木的喷泉图像上进行了<strong class="ka ir">训练。</strong></p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/e8270836e1409900efc10ff4d4d986d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kzbp40KER73b05uVcjRP8Q.png"/></div></div></figure><p id="01bc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这张看起来也很令人欣慰，因为ConvNet正在寻找暹罗猫的面部特征来进行分类。请注意，与猫的身体相比，<strong class="ka ir">热图在猫的面部附近更亮，这意味着在预测分类器的输出时，猫的独特面部特征更重要。</strong></p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/8dee6e1e9eac83f73eead8867b025ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SejAkTS2_Y5c-7MHhbWLDA.png"/></div></div></figure><p id="d068" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">虽然这张看起来也很令人放心，<strong class="ka ir">但我们仍然看到一些扩散，这可能是因为大多数海狮的图像都是在水边拍摄的</strong>，因此ConvNet可能已经获得了这种模式，即周围的水对预测它是海狮也很重要。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/c9bd8d4e67f34a7717f024629ab71c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rND5hnVWi44Qt-7xbQQO-g.png"/></div></div></figure><p id="7fc9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这看起来也很不错，因为我们看到热图主要集中在这只美国爱斯基摩犬的身上。</p><h1 id="4313" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论</h1><p id="e335" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我们看到了显著图如何告诉我们神经网络在输入图像中寻找什么，同时预测它的输出类别。值得注意的是，显著图是使用在图像标签上训练的分类ConvNet提取的，因此不需要额外的注释。</p><p id="5115" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我们使用图像梯度来生成显著性映射。使用这些图像梯度，我们还可以通过以这样一种方式改变输入图像来产生对立的例子，以便将ConvNet的输出推向不正确的类。如果变化如此细微，以至于输出类对人类和对ConvNets来说看起来不一样，这可能会非常令人不安。</p><p id="4cdb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，使用显著图和图割算法[5]，人们还可以在这些图像中执行对象分割，而无需训练专用分割或检测模型，从而将这种类型的对象分割命名为<strong class="ka ir">弱监督</strong>。</p><h1 id="f9cf" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">参考</h1><p id="eb42" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">[1]卡伦·西蒙扬，韦达尔迪安德烈亚，和齐塞曼安德鲁。卷积网络深处:可视化图像分类模型和显著图。ICLR，2013年。https://arxiv.org/pdf/1312.6034.pdf<a class="ae nd" href="https://arxiv.org/pdf/1312.6034.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="4eb7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[2]<a class="ae nd" href="https://github.com/sijoonlee/deep_learning/blob/master/cs231n/NetworkVisualization-PyTorch.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/sijoonlee/deep _ learning/blob/master/cs 231n/network visualization-py torch . ipynb</a></p><p id="9981" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[3]<a class="ae nd" href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a</a></p><p id="8c0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae nd" href="https://in.mathworks.com/help/deeplearning/ref/vgg19.html" rel="noopener ugc nofollow" target="_blank">https://in.mathworks.com/help/deeplearning/ref/vgg19.html</a></p><p id="3af3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">[5] Y .博伊科夫和M. P .乔利N-D图像中物体的最佳边界和区域分割的交互式图切割。进行中。ICCV，第二卷，第105-112页，2001年。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nh mx l"/></div></figure></div></div>    
</body>
</html>