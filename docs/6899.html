<html>
<head>
<title>CNN Architectures from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的CNN架构</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/cnn-architectures-from-scratch-c04d66ac20c2?source=collection_archive---------2-----------------------#2020-11-15">https://medium.datadriveninvestor.com/cnn-architectures-from-scratch-c04d66ac20c2?source=collection_archive---------2-----------------------#2020-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="10b2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从Lenet到ResNet</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/50ce88afe34b02a3dc05b540c648a306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*maTLsMx-2ig63TmMPFrbmw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@padolsey?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">James Padolsey</a> on <a class="ae kv" href="https://unsplash.com/s/photos/architecture?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4068" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积神经网络是一类深度神经网络，它不仅在计算机视觉任务中，而且在其他领域如语音识别、自然语言处理等方面都取得了最先进的成果。这些CNN不仅通过堆叠层，而且通过创建定制层，已经发展了很长时间。在当前的世界里，我们用这些模型来转移学习，达到更好的效果。</p><p id="f841" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将讲述如何在Pytorch中从头开始实现最先进的CNN架构。我假设你知道这些CNN的理论，所以我不会涉及理论部分。在这篇文章的最后，我还会包括一些额外的资源来学习这个理论。</p><h1 id="bc37" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">Lenet:</h1><p id="569f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Lenet 5被认为是卷积神经网络的第一个架构，用于识别美国邮政编码中的手写数字。论文中介绍了“应用于文档识别的<a class="ae kv" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">梯度学习</a>”</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/afb0a4e7169991206518c20cba07fb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VjXufJUfN2q3B-j8.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Lenet Architecture,<a class="ae kv" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank"> Image Source</a></figcaption></figure><blockquote class="mq mr ms"><p id="b936" class="kw kx mt ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">LeNet-5架构包括两组卷积层、激活层和池层，接着是两个全连接层、tanh激活层，最后是一个softmax分类器，用于对MNIST数字进行分类[1]</p></blockquote><p id="286c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们在Pytorch中实现Lenet。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Lenet Pytorch Implementation</figcaption></figure><p id="5727" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，我们创建了一个具有<em class="mt"> tanh </em>激活函数的神经网络，两个卷积层后跟AveragePooling层，一个卷积层后跟两个带有softmax分类器的线性层[ <a class="ae kv" href="https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p><h2 id="214c" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">Alexnet:</h2><p id="8263" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">AlexNet是由Alex Krizhevsky等人在2012年开发的深度神经网络。它旨在为ImageNet [ <a class="ae kv" href="https://towardsdatascience.com/understanding-alexnet-a-detailed-walkthrough-20cd68a490aa?source=post_internal_links---------4----------------------------" rel="noopener" target="_blank"> 3 </a> ] ILSVRC-2012竞赛对图像进行分类，并取得了最先进的结果。在论文<a class="ae kv" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">中介绍了用深度卷积神经网络进行ImageNet分类</a></p><blockquote class="mq mr ms"><p id="ce96" class="kw kx mt ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">Alexnet从227 x 227 x 3图像的输入层开始，下一个卷积层由96个(11 x 11)滤波器组成，步长为4。这将它的尺寸减小了55×55。随后是具有3×3过滤器的MaxPool层以及stride 2。这一过程继续下去，最终到达具有9216个参数的全连接层和下两个各具有4096个节点的全连接层。最后，它使用具有1000个输出类的Softmax函数..Alexnet是第一个使用ReLU非线性、正则化辍学和多GPU训练的架构[3]</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/fa88a6e60b17eb80c13df29e095d1d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RT8gNyu608VSphZwLCoxIA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="http://cvml.ist.ac.at/courses/DLWT_W17/material/AlexNet.pdf" rel="noopener ugc nofollow" target="_blank">Image source</a></figcaption></figure><p id="72aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们在Pytorch中实现Alexnet。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="2757" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，我们创建了一个神经网络，第一个卷积层的内核大小= 11，步幅= 4，填充= 2，然后是LocalResponseNormalization层和池层，第二层避免了LocalResponseNormalization层，最多创建了五个卷积层，然后是三个完全连接的层，最后输出1000个类。</p><h2 id="9a37" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">VGG:</h2><p id="9f1a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">VGG可以被称为Alexnet的更深形式。这种网络比Alexnet堆叠更多的层，并使用相同的ReLU激活功能，但与Alexnet相比，参数数量较少。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/aaa65d8fe4c587a8600c2f112a5a5a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*ttVR8N2AUiTcLGij.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">VGG net architecture <a class="ae kv" href="https://www.researchgate.net/figure/VGG16-architecture-16_fig2_321829624" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><blockquote class="mq mr ms"><p id="c7e7" class="kw kx mt ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">VGG-16是一个更简单的架构模型，因为它没有使用太多的超参数。它总是在卷积层中使用跨度为1的3 x 3过滤器，并在跨度为2的合并层中使用相同的(1，1)填充。[3]</p></blockquote><p id="e2f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们在Pytorch中实现VGG。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="63e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，首先，我们创建一个层值字典，其中' M '代表最大池层，整数是输入通道和输出通道的数量，接下来。我们创建一个函数<em class="mt"> create_conv_layers，</em>在字典的帮助下创建多个卷积层，<em class="mt"> __init__ </em>方法和<em class="mt"> __forward__ </em>方法与任何其他CNN相同。</p><h2 id="35ff" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">InceptionNet(谷歌网):</h2><p id="caa2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Inception network也被认为是Googlenet，被认为是CNN历史上的一个重要里程碑。在inception net之前，所有的网络都在堆叠层，旨在获得更好的性能，但失败了。《盗梦空间网络》是Imagenet 2014挑战赛的获胜者，并在论文“<a class="ae kv" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">用卷积深入发展</a>”中进行了介绍它的主要贡献是开发了一个<em class="mt">初始模块</em>，极大地减少了网络中的参数数量(4M，相比之下AlexNet有60M)。此外，本文在ConvNet的顶部使用平均池而不是完全连接的层，消除了大量似乎不太重要的参数[ <a class="ae kv" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/c86312e6fc87c28770ebc324733e618e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CPM9_Qp8T8Lv6Aj1.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Inception network architecture <a class="ae kv" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="bab7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该架构还包含两个连接到初始模块输出的辅助分类器层(黄色块)。</p><p id="a751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们实现Inception模块的卷积块，它将是整个Inception架构的构建块。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="6410" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，我们用ReLU激活函数和BatchNorm创建了一个卷积层用于正则化。卷积层动态地获得许多输入和输出通道。</p><p id="9543" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">辅助分类器的实现如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="d183" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，我们创建了一个5*5过滤器的平均池层和1*1过滤器的卷积层，然后是线性层。</p><p id="036e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们实现先启模块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/266f40f356e7c7527f5fd7dc771c7a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VdTHme4Jguj65rUmgFcRhA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Inception Module described in the <a class="ae kv" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="041c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码基于流程图，在流程图中，我们创建了四个卷积分支并将它们连接起来，这又形成了初始模块。</p><p id="8467" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以实现完整的初始架构了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="741c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，首先，我们创建两个内核大小为3*3的卷积层，然后是Inception块。借助这个表格可以更好地理解上面的代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/1cb957d62423234827a314521630a517.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bLIJ_P_TK-jPBJjBcTB_g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><blockquote class="mq mr ms"><p id="5b21" class="kw kx mt ky b kz la jr lb lc ld ju le mu lg lh li mv lk ll lm mw lo lp lq lr ij bi translated">这些初始块的主要特征是，它们不仅更深，而且更广</p></blockquote><h2 id="c098" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">Resnet:</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/6b7a69e67e04199d471ac669c668fbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66l48ybZsnQxWLWc_qOZ1w.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image from the paper</figcaption></figure><p id="102b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Resnet被认为是一个改变游戏规则的架构，因为它被认为是一个真正更深层次的架构，有152层。在论文“<a class="ae kv" href="http://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a>中介绍了它，它赢得了Imagenet 2015竞赛，此后大多数CNNsare都是这些Resnets的变体。它之所以如此受欢迎，是因为它避免了退化问题，即当创建更深的网络时，在训练和测试集中的某个点之后，网络的损耗通常会增加，为了避免这个主要问题，作者建议使用对前一层[<a class="ae kv" href="https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278" rel="noopener" target="_blank">5</a>的引用来计算给定层的输出。在ResNet中，前一层的输出(称为残差)会添加到当前层的输出中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/31a3d355057bad89973c2bba7fb1044a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Pqm8YTTaNAy-Wir7.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Identity connection <a class="ae kv" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">Image source</a></figcaption></figure><p id="b7a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Resnet的实现如下:首先，我们创建一个层块，这有助于我们创建主网络。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="16fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码创建了一个带有ReLU激活和skip-connection的BatchNormalization的卷积层块，现在称为<em class="mt"> identity_downsample </em>。让我们用skip连接实现主Resnet层，它将在主网络中使用</p><p id="3072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它可以通过一个接一个地堆叠<code class="fe ns nt nu nv b">n</code>个块来定义，我们应该注意到第一个卷积块的跨度为2，因为"<em class="mt">我们直接通过跨度为2 </em> [ <a class="ae kv" href="https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278" rel="noopener" target="_blank"> 5 </a> ]的卷积层来执行身份下采样。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="6b60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上述函数的帮助下，我们可以从头开始创建我们的全残差网络，它遵循了论文“<a class="ae kv" href="http://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a>”中的原始实现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="6622" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的代码创建了完整的ResNet模型。这种架构的主要优点是我们可以用这种架构训练更深层次的网络</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="7cc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有代码都可以在我的<a class="ae kv" href="https://github.com/seanbenhur/papers" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。如果您有任何疑问，请随时通过<a class="ae kv" href="https://www.linkedin.com/in/sean-benhur-70535119b" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我</p><h2 id="5de7" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">其他资源:</h2><ul class=""><li id="fa3c" class="od oe iq ky b kz mk lc ml lf of lj og ln oh lr oi oj ok ol bi translated"><a class="ae kv" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank"> CS231n课程笔记</a></li><li id="ba55" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated"><a class="ae kv" href="http://coursera.org/lecture/convolutional-neural-networks/cnn-example-uRYL1" rel="noopener ugc nofollow" target="_blank">吴恩达的卷积神经网络</a></li></ul><h2 id="47e2" class="mz lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">参考资料:</h2><ul class=""><li id="8448" class="od oe iq ky b kz mk lc ml lf of lj og ln oh lr oi oj ok ol bi translated">[1] <a class="ae kv" href="https://towardsdatascience.com/understanding-and-implementing-lenet-5-cnn-architecture-deep-learning-a2d531ebc342" rel="noopener" target="_blank">理解和实现LeNet-5 CNN架构</a></li><li id="c917" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">[2]<a class="ae kv" href="https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/" rel="noopener ugc nofollow" target="_blank">LeNet—Python中的卷积神经网络</a></li><li id="4454" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">[3] <a class="ae kv" href="https://towardsdatascience.com/understanding-alexnet-a-detailed-walkthrough-20cd68a490aa?source=post_internal_links---------4----------------------------" rel="noopener" target="_blank">了解AlexNet:详细演练</a></li><li id="9bda" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">[4]<a class="ae kv" href="https://blog.csdn.net/weixin_37993251/article/details/88026138" rel="noopener ugc nofollow" target="_blank">【cv 231n】第五讲|卷积神经网络_gdtop </a></li><li id="8e2d" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">[5] <a class="ae kv" href="https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278" rel="noopener" target="_blank">剩余网络:在Pytorch中实现ResNet</a></li><li id="bb01" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">【6】<a class="ae kv" href="https://github.com/aladdinpersson/Machine-Learning-Collection" rel="noopener ugc nofollow" target="_blank">阿拉丁·皮尔森的机器学习文集</a></li></ul></div></div>    
</body>
</html>