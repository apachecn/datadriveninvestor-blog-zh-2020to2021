<html>
<head>
<title>ASAP Guide to Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ASAP线性回归指南</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/asap-guide-to-linear-regression-fda841656fbd?source=collection_archive---------11-----------------------#2020-09-22">https://medium.datadriveninvestor.com/asap-guide-to-linear-regression-fda841656fbd?source=collection_archive---------11-----------------------#2020-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="34c8" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">这是一个关于机器学习中线性回归的快速、简单、无废话的指南。你需要知道的一切，都在一个地方。我们开始吧</p></blockquote><p id="3ec5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">众所周知，线性回归是一种简单的算法，也是比较复杂模型的良好基线。在本文中，我们探索算法，理解数学，运行代码，并学习可能的线性回归<strong class="jw iu">A</strong>S<strong class="jw iu">S</strong>oon<strong class="jw iu">A</strong>S<strong class="jw iu">P</strong>。</p><h1 id="58de" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">第1部分:基础知识</h1><p id="8cac" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">线性回归(LR)是一种回归算法<em class="jv">。</em></p><p id="513f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">它用于预测输出为连续值<strong class="jw iu">的情况，例如数字7。</strong></p><p id="a56f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">它显示了一个或多个<strong class="jw iu">自变量<em class="jv"> </em> </strong> <em class="jv">(输入/数据/特征)</em>和<strong class="jw iu">因变量</strong> <em class="jv">(结果/预测)</em>之间的关系。</p><blockquote class="jq jr js"><p id="d63f" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">记住这一点的一个简单方法是输出依赖于输入。为了更好地理解其中的区别，点击<a class="ae ly" href="https://www.scribbr.com/methodology/types-of-variables/#independent-vs-dependent" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote><h1 id="ca9c" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">第二部分:数学</h1><h2 id="62b6" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">第2.1节:简单线性回归(SLR)</h2><p id="cfff" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated"><strong class="jw iu">简单线性回归</strong>是LR最简单易懂的版本。它只包含一个预测一个因变量的自变量。</p><p id="c1e8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">在学习算法和LR本身之前，理解这一点是很重要的。它也是多元和多项式线性回归的基础。如果看完这一节还是不懂单反，那就看<a class="ae ly" href="https://www.youtube.com/watch?v=IL3UCuXrUzE" rel="noopener ugc nofollow" target="_blank">这个</a>上2x速(<em class="jv"> 4分钟</em>)</p><p id="4e2d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">LR背后的等式源自我们在中学都学过的一个简单等式:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/cc6b5d1ebc0683d4a1f16a6c47f78fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*HUwQWgicSbIVX2RC5_Kmkg.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><em class="mx">Fig 1.1. This equation has many names, most commonly known as the </em><strong class="bd kx"><em class="mx">slope-intercept form</em> </strong>of a line. <a class="ae ly" href="https://mathequality.wordpress.com/2014/01/15/sharing-is-caring-linear-equations-review/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="64ec" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">SLR的方程是斜率截距形式方程的直接推导。有了这两个方程的顶部和底部，很容易看到变量重叠。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/99e47a6cf514e6932df46d9cf7c036bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*KDVtcl36CLigFwaa.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 1.2. In statistics and modeling, ŷ is pronounced as “y hat” and represents the predicted value of a function or model, or in this case, regression algorithm. <a class="ae ly" href="https://www.sharpsightlabs.com/blog/linear-regression-in-r/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><ul class=""><li id="1df4" class="mz na it jw b jx jy kb kc ks nb kt nc ku nd kr ne nf ng nh bi translated"><strong class="jw iu"> β0 </strong>是y轴截距</li><li id="8865" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated"><strong class="jw iu"> β1 </strong>是系数/斜率</li><li id="a75b" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated"><strong class="jw iu"> X </strong>是自变量</li><li id="c24f" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated">而<strong class="jw iu"> ŷ </strong>是因变量</li></ul><p id="17fc" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">虽然LR可以显示两个或多个变量之间的关系，但是SLR <em class="jv">只能显示两个变量之间的关系。这就是为什么它只有两个变量，<strong class="jw iu"> ŷ </strong>和<strong class="jw iu"> X </strong>。这使得可视化变得容易:</em></p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/0d8d2bf656a6f3abdee1a12d961433ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aFvlWGYsAJ6pwyy6.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 2.1. An example of regular linear data. <a class="ae ly" href="https://en.wikipedia.org/wiki/Simple_linear_regression" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="2183" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">这里有一个更概括的版本:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5e1a7d7002b08b276b49572295f23e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*QvXLKX8y_JJ1rlqXYJlCyg.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 2.2. E(y) = <strong class="bd kx">ŷ </strong><a class="ae ly" href="https://www.slideshare.net/atiqrehman15/regression-correlation-44070123" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h2 id="6eb2" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">第2.2节:多元线性回归(MLR)</h2><blockquote class="jq jr js"><p id="39c7" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">注意:从现在开始，特征和自变量将互换使用，因为它们代表相同的概念。</p></blockquote><p id="b93a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">虽然SLR可用于单一预测者/预测的情况，但MLR几乎可用于有多个因素决定结果的所有其他情况，例如这个<a class="ae ly" href="https://archive.ics.uci.edu/ml/datasets/Facebook+metrics" rel="noopener ugc nofollow" target="_blank">数据集</a>。它遵循与SLR几乎相同的方程，但有多个“<strong class="jw iu"> β1X </strong>”或独立变量和系数的组合。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nt"><img src="../Images/efe9745de303f576c1f9d458220475b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4H7AdqFgPALNvn0JGspasQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 3. <a class="ae ly" href="https://www.superdatascience.com/blogs/regression-classification-multiple-linear-regression/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="c4cd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">MLR为每个特征分配一个唯一的系数，并将两个值存储在一个方向的<a class="ae ly" href="https://www.mathsisfun.com/algebra/scalar-vector-matrix.html" rel="noopener ugc nofollow" target="_blank">向量</a>中，以便于修改并在运算<em class="jv">(乘法)</em>中用作<a class="ae ly" href="https://www.mathsisfun.com/algebra/scalar-vector-matrix.html" rel="noopener ugc nofollow" target="_blank">矩阵</a>函数:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nu"><img src="../Images/a4e09eac728bc5679e2b2e3066809fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RK62qjKi3vmM5LWpa2mD_w.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 4. vertical matrixes. <a class="ae ly" href="https://m325.com/linear-regression-from-scratch/#/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h2 id="600b" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">维度</h2><p id="55b6" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">有了两个变量，我们很容易将数据可视化。即使有三个变量，我们也可以在3D平面上将其可视化<em class="jv">(见图5) </em>。但是随着我们的价值增加，我们到达了维度的问题，在继续前进之前必须理解这个问题。</p><p id="6092" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">需要注意的一点是<strong class="jw iu">只有</strong>自变量的数量增加。这是因为因变量是我们预测的值，所以它总是一个值。对于利用神经网络的更复杂的人工智能模型，我们可以有多个。但是在大多数机器学习应用程序中，并且为了这篇文章，结果只有一个值。</p><p id="82f4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">假设我们有两个特征。这意味着，理论上，我们有两个x轴和一个y轴。为了映射它，我们必须向上移动维度，并像这样投影数据:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nv"><img src="../Images/222e1c3ef0bb55ea0f57276f4ccf2444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y_JK7SfJjtWpRI4c.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 5. a hyperplane regression. <a class="ae ly" href="https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression_print.html" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="550c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">在这个例子中，我们在一个三维空间中，超平面是少了一个维度的黄色子空间<em class="jv">(这就是为什么它是一个二维平面)。</em></p><p id="63b3" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">两个特征是我们仍然可以想象的，但是当我们向更高维度移动时，我们的大脑无法理解它。幸运的是，我们的电脑可以，这也是MLR工作的原因。如果你想更深入地了解，请查看这篇文章。</p><blockquote class="jq jr js"><p id="15b7" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">还有一种叫做<strong class="jw iu">降维</strong>(由于时间关系，我们不会涉及这个)的方法，可以通过不同的技术(主成分分析、线性判别分析、核PCA等)来降低维度。如果你想更深入地了解这一点，可以看看这一章。</p></blockquote><blockquote class="nw"><p id="c3fb" class="nx ny it bd nz oa ob oc od oe of kr dk translated">您可能想知道所有这些值是如何计算出来的？我们如何知道截距或系数是正确的？我们怎么知道我们的模型是否正确？</p></blockquote><p id="c345" class="pw-post-body-paragraph jt ju it jw b jx og jz ka kb oh kd ke ks oi kh ki kt oj kl km ku ok kp kq kr im bi translated">显然，我们可以用肉眼看到这条线与我们的数据是如何对齐的(<em class="jv">见图2.1 </em>)，但我们的计算机使用一种更精细、更准确的方式来做到这一点。</p><div class="ol om gp gr on oo"><a href="https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">回归树是如何工作的？数据驱动的投资者</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">之前我们谈到了决策树以及如何在分类问题中使用它们。现在我们转移我们的焦点…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc mr oo"/></div></div></a></div><h2 id="a8db" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">第2.3节:算法技术</h2><p id="9a5a" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">在我们开始之前，让我们弄清楚一些事情，<em class="jv">算法和模型之间的区别。</em></p><p id="52ba" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我在这篇<a class="ae ly" href="https://medium.com/datadriveninvestor/difference-between-an-machine-learning-algorithm-and-model-14879f4aec7b" rel="noopener">文章</a>但是<em class="jv"> tl中深入地谈到了这一点；博士</em>:一种算法在我们的数据上运行，以发现模式和规则，这些模式和规则存储在数据中并用于创建一个模型，该模型可用于确保预测。</p><p id="6e2c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">因此，我们将涵盖的技术将用于算法和我们的数据，以创建一个准确的模型。</p><h2 id="24ac" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">普通最小二乘法</h2><p id="400e" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">为了找到我们模型的未知参数，我们使用线性最小二乘法。这些方法中最流行的是一种叫做<strong class="jw iu">普通最小二乘法的技术。</strong></p><p id="f77e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">OLS的目标是最小化每个点的残差平方和。它计算残差平方和，因为线下的点有一个负残差，如果我们对它们求和，它会抵消其他正和。平方每个残差确保积极性，避免抵消。</p><blockquote class="jq jr js"><p id="3ec6" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">残差是直线上一个点与其预测值之间的距离。</p></blockquote><p id="634f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">这是一个简洁的可视化:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pd"><img src="../Images/d5abde5bf34861782d0e38c10c9deee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*76A80tjAva9kQKfX.gif"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 6. <a class="ae ly" href="https://mlfromscratch.com/linear-regression-from-scratch/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h2 id="091a" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">r的平方</h2><p id="877a" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">衡量我们模型准确性的最常见方法是通过一种称为<strong class="jw iu"> r、</strong>的技术，也称为<strong class="jw iu">决定系数</strong>。</p><p id="b6bb" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">它通过测量Y的变化百分比告诉我们回归线对实际值的预测有多好，这是由于它在X上的回归<em class="jv">(这就是为什么它的值在0和1之间)</em>。</p><p id="2ded" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">R基本上将回归线各点的残差平方和与均值各点的残差平方和进行比较。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi pe"><img src="../Images/6802d0c377076107f621838e16c66185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KO1y8Y900QRph1-X8g9MIg.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 7. <a class="ae ly" href="https://www.youtube.com/watch?v=p8rIhf6Q1o4" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1f2a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">这个等式可能看起来令人生畏，但它只是简单地比较了均值误差和回归误差。等式中可能会有一些变化，因为SST避免得到负值，所以根据点在线的上方或下方，我们必须加上或减去。</p><p id="b493" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">这里有一个很好的<a class="ae ly" href="https://www.youtube.com/watch?v=2AQKmw14mHM&amp;t=355s" rel="noopener ugc nofollow" target="_blank">视频</a>涵盖了它的基本原理。强烈推荐观看:-)</p><h1 id="45c8" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">第3部分:在我们建造之前</h1><h2 id="e339" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">第3.1节:LR的假设</h2><p id="44ed" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">在进入大楼之前，我们需要考虑LR的几个假设。如果不满足这些假设，那么我们的数据可能会自行崩溃，搞乱我们的模型。</p><ol class=""><li id="5a73" class="mz na it jw b jx jy kb kc ks nb kt nc ku nd kr pf nf ng nh bi translated"><strong class="jw iu">线性</strong> -数据必须是线性的<em class="jv">(见图2.1) </em>。</li><li id="c08a" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">方差</strong> -误差的方差应该是常数<em class="jv">(见下图7.1)</em>。</li><li id="46ff" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">多元正态</strong> -残差必须正态分布<em class="jv">(只有少数异常值，其余点应接近直线)</em>。</li><li id="c5b0" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">误差的独立性</strong> -残差不应相互关联<em class="jv">(见下图7.2)</em>。</li><li id="c582" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">缺乏多重共线性</strong> -特征/自变量之间相关性不高<em class="jv">(一个特征不直接预测另一个特征，如长度和体积)</em></li></ol><div class="mm mn mo mp gt ab cb"><figure class="pg mq ph pi pj pk pl paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><img src="../Images/d79cc4eae78b86830d1e057c06982ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*yj9Myyg1aQFBVuudJdoAAw.png"/></div></figure><figure class="pg mq pm pi pj pk pl paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><img src="../Images/c8b2ac13044d8a1b5886ce56827a520f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*dW2u5WtyhSiuJ-yPe_E_2Q.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk pn di po pp">from left to right — Fig 8.1 and 8.2 <a class="ae ly" href="https://www.youtube.com/watch?v=hVe2F9krrWk&amp;t=179s" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure></div><h2 id="9e92" class="lz kw it bd kx ma mb dn lb mc md dp lf ks me mf lj kt mg mh ln ku mi mj lr mk bi translated">第3.2节:构建MLR模型的方法</h2><p id="1005" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">我说“MLR型号”而不是“LR型号”是有原因的。对于单反模型，我们只有两个变量要实现，所以没有细微差别。但是在处理MLR时，我们可以有三到三百个变量。这就是为什么我们必须采用减少变量的方法。</p><p id="b7c5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">为什么这么做是个好主意，有两个<strong class="jw iu"/>主要原因:<strong class="jw iu"> (1) </strong>我们拥有的变量越多，我们的计算机就越难制作和运行一个有效的模型，<strong class="jw iu"> (2) </strong>我们拥有的变量越多，我们就越难理解变量的影响并从我们的数据中得出有价值的见解。</p><p id="5820" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我们使用一个<strong class="jw iu"> P值分数</strong> <em class="jv">(一个相关变量，衡量一个变量在其他变量中的重要性——通常设置为0.5%或5%) </em>来帮助我们找到我们的特征中实际上对结果有影响的重要变量。这也称为显著性水平</p><p id="8c8e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我们主要有5种方法找到这一点:</p><ol class=""><li id="13d2" class="mz na it jw b jx jy kb kc ks nb kt nc ku nd kr pf nf ng nh bi translated"><strong class="jw iu"> All-in </strong> -这种方法使用我们所有的变量，只有在我们事先知道所有变量都很重要的情况下，我们才会这样做。</li><li id="30b2" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">后院消除</strong> -该方法适合所有变量，并取出具有最高p值的变量。它重新拟合模型，并重复这个过程，直到我们的变量都低于我们的p值。</li><li id="5f46" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">正向消除</strong> -该方法为每个特征创建一个SLR模型，并找出p值最低的一个。然后，它保留该特征，并用该特征和一个额外的预测器拟合所有可能的模型。然后重复这一过程，直到没有要素低于p值。</li><li id="953c" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">双向消除</strong> -该方法利用正向消除的第一步<em class="jv">(为每个特征创建一个SLR模型，并找到p值最低的一个)</em>，然后使用反向消除去掉所有高于我们p值的变量并重复。它也被称为<strong class="jw iu">逐步回归。</strong></li><li id="18bc" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr pf nf ng nh bi translated"><strong class="jw iu">分数比较</strong>——这种方法使用一个特定的优良标准(如赤池标准)，然后用<em class="jv"> n </em>变量<em class="jv"> (2^n -1) </em>构造所有可能的变量，并选择最好的。随着变量的增加，这也是极其不可持续的。</li></ol><p id="f50e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">幸运的是，大多数机器学习库——包括<strong class="jw iu">scikit-learn</strong>——会找到最佳方法，并在我们编码时自动应用。</p><p id="a0c8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">说到编码…</p><h1 id="de26" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">第4部分:构建模型</h1><p id="81b8" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">为了构建这个模型和大多数机器学习模型，我们将使用python库<strong class="jw iu"> skit-learn </strong>。我们使用skit-learn是因为它是最广泛和用户友好的机器学习库之一。加上巨蟒石。</p><blockquote class="nw"><p id="bf05" class="nx ny it bd nz oa pq pr ps pt pu kr dk translated">开始吧！</p></blockquote><figure class="pv pw px py pz mq"><div class="bz fp l di"><div class="qa qb l"/></div></figure><p id="8436" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">出于学习目的，这是一个非常简单的MLR模型。<a class="ae ly" href="https://archive.ics.uci.edu/ml/datasets/Facebook+metrics" rel="noopener ugc nofollow" target="_blank">数据集</a>是从UCI机器学习知识库中检索出来的，包含2014年脸书邮报的指标，来自一家国际化妆品公司的500多篇帖子。</p><p id="f5e1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">它有多个我们可以使用的自变量和因变量。在这种情况下，我们将使用指标来预测终身参与用户作为基线<em class="jv">(尽管可以使用任何其他预测变量)</em>。</p><blockquote class="jq jr js"><p id="bcb3" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">如果你想了解真正的代码，可以去我的<a class="ae ly" href="https://github.com/SabeehHassany/Facebook-Engaged-User-Prediction-MLR" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p></blockquote><p id="72cd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">首先，我们要<strong class="jw iu">导入这三个基本库</strong>，像skit-learn一样，适用于所有的ML。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="37fd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我们可以<strong class="jw iu">通过Pandas dataframe导入数据集</strong>，并使用iloc来分配变量。请记住，数据集的名称必须针对不同的用例进行更新，并且它必须与您的位于同一文件夹中。py文件或上传到Jupyter笔记本或Google Collab。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="80d8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">对于任何数据科学问题，最重要的任务之一是<strong class="jw iu">预处理数据</strong>。因为我们的计算机是非常精确和准时的机器，我们的数据集必须是完美的。</p><p id="a469" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">必须考虑缺失值或NaN(非数字)值，并遵循LR假设。可以通过查看我们的数据或使用单独的函数来检查这些假设，但是，我们必须自己处理缺失和格式不正确的值。</p><blockquote class="jq jr js"><p id="0398" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">有多种方法可以做到这一点，但最有效的方法是使用skit-learn提供的预处理工具。这在不同的情况下会有所不同，但是我们将在这里讨论一些。</p></blockquote><p id="84a2" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">在分配数据集之后，我们可以运行<em class="jv"> print(dataset.info()) </em>来快速可视化我们所有的列，以查看我们需要在哪里修复数据。最好是实际查看我们的数据集或对csv文件使用条件格式，但这也可以。我们得到这样的结果:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi qd"><img src="../Images/60c8840f8e51726ab68ac93b6d3a4a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXWzeeM5WZHHMcAFKekHtw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Fig 9. Null values == bad.</figcaption></figure><p id="4560" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">对于这个数据集，我们总共有3列缺少数据。我们可以使用简单估算函数来帮助我们<strong class="jw iu">估算缺失数据</strong>。</p><p id="cca8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">对于指数8和9(最后一个指数不包括在内，因此为“8:10”)，我们可以使用“中值”来估算，以补偿异常值，对于指数6，我们使用“最频繁”，因为它是一个二进制数据点。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="7f7c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">索引1有<strong class="jw iu">分类数据</strong>，必须使用名为OneHotEncoding的函数进行转换。这必须在输入缺失值后完成，因为OneHotEncoding会自动将编码列作为第一个索引来替换所有其他列。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="e047" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">由于一个简单算法的大数据集和许多变量，我们可以使用90/10 <strong class="jw iu">数据集分割</strong>。出于一致性的考虑，随机状态被调整为3。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="8dd0" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">现在<strong class="jw iu">训练模式</strong> l出奇的容易。在制作了LinearRegression对象的实例之后，我们可以使用它来拟合训练数据和训练模型。LinearRegression()对象的括号是空的，因为我们没有调整模型<em class="jv">(我们将在括号内赋值)</em>的参数，因为与更复杂的模型相比，它们不那么必要/重要。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="8510" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">训练完我们的模型后，我们可以用它来做新的预测。在这里，我们可以分配一个变量，<em class="jv"> y_pred </em>，测试集的预测值。然后，通过使用<strong class="jw iu"> concatenate </strong>函数，我们可以通过<em class="jv"> (len(y_wtv)，1)) </em>将预测值和实际值并排显示在2D数组中，以便于查看。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><p id="5507" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">最后，为了确定我们的模型性能，我们可以计算一个r值。在这个模型中，我们达到了接近0.80的r，这意味着80%的数据可以通过我们的模型来预测。print函数将r值转换成一个字符串，因此可以很容易地打印和读取。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="qc qb l"/></div></figure><h1 id="91e5" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">第五部分:结论</h1><p id="3b23" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">大概就是这样。看，没有看起来那么难！</p><p id="7431" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">现在我们有了这个基本的理解，我们可以努力理解任何机器学习算法——详细的。</p><p id="ac09" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我在这篇文章中提炼了所有你需要知道的东西，但是你可以从不同的角度去了解更多。如果你想深入理解这些概念，我推荐你查看我的图片的所有来源。它们都是惊人的网站、文章或视频，包含大量有用的信息。</p><p id="3727" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">干杯。</p><h1 id="1cc2" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">摘要</h1><ul class=""><li id="39d3" class="mz na it jw b jx lt kb lu ks qe kt qf ku qg kr ne nf ng nh bi translated">线性回归非常容易理解和建立，并预测一个<em class="jv">连续值。</em></li><li id="b894" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated">简单线性回归有<em class="jv">一个</em>自变量，而多元线性回归有<em class="jv">两个或更多自变量。</em></li><li id="04c5" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated">有不同的<em class="jv">数学程序</em>来创建/训练和测量我们模型的准确性</li><li id="f14e" class="mz na it jw b jx ni kb nj ks nk kt nl ku nm kr ne nf ng nh bi translated">在<strong class="jw iu"> <em class="jv"> regressor.fit(X_train，y _ train)</em></strong>-使用我们的模型之前，需要对数据进行预处理以发现缺失点并确认异常值</li></ul><h1 id="59a1" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">希望你喜欢这本书！：</h1><p id="bec8" class="pw-post-body-paragraph jt ju it jw b jx lt jz ka kb lu kd ke ks lv kh ki kt lw kl km ku lx kp kq kr im bi translated">在你离开之前，请允许我自我介绍一下:)</p><p id="c119" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">我是一个好奇的17岁男孩，对机器学习和数据科学及其交叉脑机接口超级感兴趣。我喜欢学习新东西和认识新朋友</p><p id="3416" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated">在<a class="ae ly" href="https://www.linkedin.com/in/sabeeh-hassany-334b6b194/" rel="noopener ugc nofollow" target="_blank"> <em class="jv"> Linkedin </em> </a>，<a class="ae ly" href="https://medium.com/@sabeehhassany" rel="noopener"> <em class="jv">中型</em> </a> <em class="jv"> </em>(哦看！你已经来了)，还是<a class="ae ly" href="https://twitter.com/sabeehhassany" rel="noopener ugc nofollow" target="_blank"> <em class="jv">碎碎念</em> </a> <em class="jv">！</em></p><p id="e56e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke ks kg kh ki kt kk kl km ku ko kp kq kr im bi translated"><strong class="jw iu">访问专家视图— </strong> <a class="ae ly" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>