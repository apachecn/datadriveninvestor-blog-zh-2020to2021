<html>
<head>
<title>Get Started With Q-Learning With Python: How To Automatize A Warehouse Robot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python Q-Learning入门:如何自动化仓库机器人</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/get-started-with-q-learning-with-python-how-to-automatize-a-warehouse-robot-7bfae0180301?source=collection_archive---------5-----------------------#2020-02-26">https://medium.datadriveninvestor.com/get-started-with-q-learning-with-python-how-to-automatize-a-warehouse-robot-7bfae0180301?source=collection_archive---------5-----------------------#2020-02-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="fb15" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">在本教程中，我将向您展示如何使用强化学习来自动化一个自主仓库机器人，以找到不同位置之间的最佳路径。</p></blockquote><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/e2ded2e521a28f147d20f418e80ce026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akTT-8IuqSXJeWy9EJEbkg.jpeg"/></div></div></figure><h1 id="27f7" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">介绍</h1><p id="217e" class="pw-post-body-paragraph jt ju it jw b jx mc jz ka kb md kd ke me mf kh ki mg mh kl km mi mj kp kq kr im bi translated">机器人技术的应用在每个商业领域都在不断扩大。自动化处理重复性任务，旨在消除人工输入，以优化流程和削减成本。</p><div class="mk ml gp gr mm mn"><a href="https://www.datadriveninvestor.com/2019/03/04/patterns-and-robotics-a-complex-reality/" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">模式和机器人:复杂的现实|数据驱动的投资者</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">哈耶克的名著《复杂现象理论》(哈耶克，1964)深入探讨了复杂性的话题，并断言…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb lc mn"/></div></div></a></div><p id="c3d5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">2012年，亚马逊<a class="ae nc" href="https://pitchbook.com/news/articles/ma-flashback-amazon-announces-775m-kiva-systems-acquisition" rel="noopener ugc nofollow" target="_blank">收购了开发仓库机器人和相关技术的公司</a> Kiva Systems，Kiva以7.75亿美元被收购。此外，许多其他公司，如阿里巴巴、大众汽车或<a class="ae nc" href="https://www.scmp.com/tech/start-ups/article/3031314/robotics-start-geekplus-push-expand-chinas-smart-logistics" rel="noopener ugc nofollow" target="_blank"> Geek+ </a>不断实现机器人和相关技术。</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nd ne l"/></div></figure><blockquote class="nf"><p id="c570" class="ng nh it bd ni nj nk nl nm nn no kr dk translated">对于初学者来说，开始这样的话题可能是一场斗争，这就是为什么我认为把事情放到背景中，然后开始深入细节是重要的。</p></blockquote><h1 id="f13d" class="le lf it bd lg lh li lj lk ll lm ln lo lp np lr ls lt nq lv lw lx nr lz ma mb bi translated">让我们开始:</h1><p id="4414" class="pw-post-body-paragraph jt ju it jw b jx mc jz ka kb md kd ke me mf kh ki mg mh kl km mi mj kp kq kr im bi translated">在本指南中，我将模拟自主仓库机器人需要采取的行动，以便以最佳方式收集交付的产品，同时考虑机器人的位置、中间位置和最终位置。</p><p id="5044" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">此模拟中使用的仓库由不同的12个点组成，其形状如下:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b7f76574e0539ee174e8c2778059cb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*LdqTb8aFPEli8LI1EPwfZA.png"/></div></figure><p id="dcd9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">该系统需要实时排列在这12个位置收集产品的优先级。例如，在特定时间<code class="fe nt nu nv nw b">t</code>，系统将返回以下排名:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1e2c8d751f7490655989af7bbb412249.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*sGYc8rIZvtQOwf5gcJ1yJw.png"/></div></figure><p id="e1ff" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">在这个例子中，位置G具有最高优先级，机器人必须以系统计算的最短路线移动到这个位置。</p><p id="b49b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">此外，位置<code class="fe nt nu nv nw b">K</code>和<code class="fe nt nu nv nw b">L</code>位于前3个优先级中，因此系统将通过在到达其最终最高优先级位置之前“步行”到一些中间位置来计算最短路线。</p><p id="4814" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">为了在Python中实现这一逻辑，有必要通过定义以下3个元素来将这一任务置于上下文环境中:</p><ul class=""><li id="a39a" class="ny nz it jw b jx jy kb kc me oa mg ob mi oc kr od oe of og bi translated"><strong class="jw iu">美国</strong></li><li id="29f9" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><strong class="jw iu">动作</strong></li><li id="e431" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><strong class="jw iu">奖励</strong></li></ul><p id="6dc4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><strong class="jw iu">状态</strong>是机器人在每一时刻<code class="fe nt nu nv nw b">t</code>可以处于的位置:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="c24e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><strong class="jw iu">动作</strong>是机器人从一个位置移动到另一个位置时可能做的动作:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="1ac0" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">当然，当机器人在一个特定的位置时，有些动作它不能执行。这在模拟中通过一个<strong class="jw iu">奖励矩阵</strong>来指定，并通过对其不能执行的行为给予奖励0来指定。</p><p id="258c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">奖励矩阵由状态和动作矩阵组成，0代表机器人在该状态下不能执行的动作，1代表机器人可以执行的动作。</p><p id="0507" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">从<code class="fe nt nu nv nw b">A</code>位置开始，根据仓库地图，机器人只能去<code class="fe nt nu nv nw b">A</code>位置，而在<code class="fe nt nu nv nw b">B</code>位置则有可能移动到<code class="fe nt nu nv nw b">A</code>、<code class="fe nt nu nv nw b">C</code>或<code class="fe nt nu nv nw b">F</code>位置。</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi on"><img src="../Images/330a2087a0ae0b761af16cd56ecff35a.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*3N4eL2DxmYVhLgdrtx1eLw.png"/></div></figure><p id="1e16" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">该系统采用奖励矩阵，并向最高优先级位置分配高奖励，返回该位置的最佳路径，该系统基于<strong class="jw iu">马尔可夫决策过程</strong>，该过程可表示为以下元组:</p><p id="c7ac" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">(S, A, T, R)</code></p><p id="9da4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">其中:</p><ul class=""><li id="15f4" class="ny nz it jw b jx jy kb kc me oa mg ob mi oc kr od oe of og bi translated"><code class="fe nt nu nv nw b">S</code>是状态的集合</li><li id="419b" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><code class="fe nt nu nv nw b">A</code>可以进行的一组动作</li><li id="feb6" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><code class="fe nt nu nv nw b">T</code>定义在时间t处于状态s的动作a将导致在时间<code class="fe nt nu nv nw b">t</code> +1处于状态s’的概率的转移规则</li><li id="2ad2" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><code class="fe nt nu nv nw b">R</code>由于动作<code class="fe nt nu nv nw b">A</code>，从状态s转换到状态<code class="fe nt nu nv nw b">s`</code>后接收到的奖励函数</li></ul><p id="eb92" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">系统包含一个策略函数，它给定一个状态<code class="fe nt nu nv nw b">S(t)</code>返回动作<code class="fe nt nu nv nw b">A(t)</code>。</p><p id="33c5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">用π表示所有可能的策略动作的集合产生了最优化问题，其中最优策略π∫最大化累积奖励。</p><p id="a457" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">每一对动作<code class="fe nt nu nv nw b">(s, a)</code>都关联有一个数值，记为<code class="fe nt nu nv nw b">Q-value</code>；在<code class="fe nt nu nv nw b">t=0 </code>处，当在时间<code class="fe nt nu nv nw b">t</code>和状态<code class="fe nt nu nv nw b">s(t)</code>处正在进行随机动作时，所有Q值都被初始化为0，其带来状态<code class="fe nt nu nv nw b">s(t) + 1</code>和奖励<code class="fe nt nu nv nw b">R(s(t), a(t))</code>。</p><p id="bbae" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">这个被称为Q-Learning的整个算法可以总结如下:</p><p id="d80f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">对于所有状态对<code class="fe nt nu nv nw b">s</code>和动作对<code class="fe nt nu nv nw b">a</code>，Q值被初始化为0。</p><p id="24ef" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">初始状态为<code class="fe nt nu nv nw b">s(0)</code>然后，执行随机可能动作，并到达第一状态<code class="fe nt nu nv nw b">s(1)</code>。</p><p id="e045" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">对于每一个<code class="fe nt nu nv nw b">t</code> ≥ <code class="fe nt nu nv nw b">1</code>，直到某一个数字(在本案例研究中为1000次)，重复以下步骤:</p><p id="fa9e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><strong class="jw iu">这可以应用到我们的例子中，让我们看看到底发生了什么:</strong></p><p id="798c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">随机状态<code class="fe nt nu nv nw b">s(t)</code>是从可能的12种状态中选出的</p><ul class=""><li id="58aa" class="ny nz it jw b jx jy kb kc me oa mg ob mi oc kr od oe of og bi translated">播放导致下一个可能状态的随机动作<code class="fe nt nu nv nw b">a(t)</code></li><li id="1b3c" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated">到达下一个状态<code class="fe nt nu nv nw b">s(t) + 1</code>，并且产生奖励<code class="fe nt nu nv nw b">R(s(t), a(t))</code></li><li id="7a71" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated">时间差<code class="fe nt nu nv nw b">TD(t)(s(t); a(t))</code>:计算如下:</li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8033332c2e034c89fceb23264669c9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*VORJ2CbSp0MyrAOcKdZTbQ.png"/></div></figure><ul class=""><li id="9388" class="ny nz it jw b jx jy kb kc me oa mg ob mi oc kr od oe of og bi translated">Q值通过应用<a class="ae nc" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼</a>公式进行更新:</li></ul><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi op"><img src="../Images/39e651899fb6034dcee32768f98f22c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*IifYU8QqfoE8vMkrZjO1OQ.png"/></div></figure><h1 id="958d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">实施:</h1><p id="a481" class="pw-post-body-paragraph jt ju it jw b jx mc jz ka kb md kd ke me mf kh ki mg mh kl km mi mj kp kq kr im bi translated">在这一部分中，正在实施Q-learning过程，以便为位置<code class="fe nt nu nv nw b">G</code>创建奖励矩阵。</p><p id="2768" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">虽然之前已经定义了<strong class="jw iu">动作</strong>和<strong class="jw iu">位置-状态</strong>，但是现在有必要定义奖励矩阵和参数γ和α:</p><p id="21ad" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">gamma = 0.75</code></p><p id="04c5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">alpha = 0.9</code></p><p id="a2da" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">由于位置<code class="fe nt nu nv nw b">G</code>具有最高优先级，可以如下定义奖励矩阵，给予位置<code class="fe nt nu nv nw b">G</code>高奖励:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="2ea8" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">q值由零矩阵初始化:</p><p id="042e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">Q = np.array(np.zeroes([12, 12]))</code></p><p id="887e" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">然后实施Q学习过程，for循环迭代1000次，重复1000次算法步骤:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="d45f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">位置G的Q值由算法计算，并且可以将它们可视化:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi oq"><img src="../Images/69ccc4fc30feac9ab1281349192f2860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJZU9CFji0pUHuyq6miTOA.png"/></div></div></figure><p id="54a6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">从图中可以看出，位置<code class="fe nt nu nv nw b">G</code>的Q值最高，而远离<code class="fe nt nu nv nw b">G</code>的位置Q值较低。</p><p id="f3ab" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">下一步是计算一个函数，该函数能够返回任意位置的最佳路线，而不仅仅是“硬编码”的<code class="fe nt nu nv nw b">G</code>位置。</p><p id="dff2" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">首先，有必要像之前在<code class="fe nt nu nv nw b">location_to_state</code>中所做的那样，将每个状态映射到位置:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="0ec4" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">此时，为了计算返回任意位置最佳路径的函数，有必要重新定义<code class="fe nt nu nv nw b">R</code>矩阵，删除硬编码的奖励:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="5d56" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">现在，让我们将计算理想路线的逻辑封装到一个函数中:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="e481" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">以起始位置和结束位置作为参数调用此函数将返回所需的路径:</p><p id="63f9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">route("E", "G")</code></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/00a36d2c572c9e8192f1a168c9e99457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*ZdSXt8XD-MFvMM7SUvq4qQ.png"/></div></figure><p id="5ac6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated">现在可以创建一个额外的函数<code class="fe nt nu nv nw b">best_route()</code>,将起始、中间和结束位置作为输入，该函数将调用<code class="fe nt nu nv nw b">route()</code>函数两次，第一次在起始和中间位置之间，第二次在中间到结束位置之间:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="om ne l"/></div></figure><p id="6947" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke me kg kh ki mg kk kl km mi ko kp kq kr im bi translated"><code class="fe nt nu nv nw b">best_route("E", "K", "G")</code></p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/760602e53e2117e0ecd8a10bef90d78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*qJweGqapa58Ak449ipba0Q.png"/></div></figure><h1 id="cd55" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><ul class=""><li id="5736" class="ny nz it jw b jx mc kb md me or mg os mi ot kr od oe of og bi translated"><a class="ae nc" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">Q-Learning简介</a></li><li id="8931" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><a class="ae nc" href="https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265" rel="noopener">强化学习</a></li><li id="ef06" class="ny nz it jw b jx oh kb oi me oj mg ok mi ol kr od oe of og bi translated"><a class="ae nc" href="https://medium.com/datadriveninvestor/math-of-q-learning-python-code-5dcbdc49b6f6" rel="noopener">Q-Learning的数学</a></li></ul></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><pre class="kt ku kv kw gt pb nw pc pd aw pe bi"><span id="afc6" class="pf lf it nw b gy pg ph l pi pj"><strong class="nw iu">I have a newsletter 📩.</strong></span><span id="9ae7" class="pf lf it nw b gy pk ph l pi pj">Every week I’ll send you a brief findings of articles, links, tutorials, and cool things that caught my attention. If tis sounds cool to you subscribe.</span><span id="16f2" class="pf lf it nw b gy pk ph l pi pj"><em class="jv">That means </em><strong class="nw iu"><em class="jv">a lot</em></strong><em class="jv"> for me.</em></span></pre><div class="mk ml gp gr mm mn"><a href="https://relentless-creator-2481.ck.page/68d9def351" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">米尔斯形式</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">编辑描述</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">无情-创造者-2481.ck.page</p></div></div></div></a></div></div></div>    
</body>
</html>