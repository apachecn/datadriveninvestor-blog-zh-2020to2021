# 作业调度 DAG 2.0:构建一个更加动态和灵活的分布式计算生态系统

> 原文：<https://medium.datadriveninvestor.com/job-scheduler-dag-2-0-building-a-more-dynamic-and-flexible-distributed-computing-ecosystem-8aa9330695e7?source=collection_archive---------5----------------------->

![](img/7a22848587d3c76b775f54dc352ce7e0.png)

*陈阿里云智能高级技术专家*

# 0.前言

作业调度器和分布式执行系统作为阿里巴巴核心大数据能力的基础，支持阿里巴巴集团和阿里云大数据平台的大部分大数据计算需求。系统上运行的 [MaxCompute](https://www.alibabacloud.com/product/maxcompute) 和 [PAI](https://www.alibabacloud.com/product/machine-learning) 等各种计算引擎每天都在处理用户的海量数据计算需求。在阿里巴巴大数据生态系统中，作业调度系统管理着阿里巴巴集团内外的多个物理集群，**超过 10 万台物理服务器**和**数百万个 CPU 和 GPU 核心**。作业调度器分布式平台每天运行超过**1000 万**个作业并处理**EB**的数据，这使其在行业中处于领先地位。单个作业包含**数十万个计算节点**并管理**数百亿个**边缘连接。这种作业数量和规模推动了作业调度分布式平台在过去 10 年中不断发展。随着更加多样化的作业和进一步的开发需求，作业调度器系统架构需要进一步演进，这是一个巨大的挑战，也是阿里巴巴作业调度器团队的一个巨大机遇。这篇文章描述了阿里巴巴作业调度器团队如何在过去两年中升级核心调度和分布式执行系统，以构建 **DAG 2.0。**

# 1.背景

## 1.1 作业调度程序 DAG 或 AM

纵观全局，运行在物理集群上的分布式系统的架构可以分为资源管理、分布式作业调度和多计算节点运行，如下图所示。有向无环图(DAG)是每个分布式作业的中央管理节点，也就是应用主机(AM。)AM 通常被称为 DAG，因为它协调分布式作业运行。现代分布式系统中的作业运行可以用 DAG 调度和数据混洗来描述[1]。与传统的 Map-Reduce[2]模型相比，DAG 模型可以准确描述分布式作业，也是主流大数据系统的架构设计基础，包括 Hadoop 2.0+、Spark、Flink、TensorFlow 等。DAG 模型与这些主流大数据系统的唯一区别是 DAG 语义是否向用户或计算引擎开发者公开。

![](img/619d8ea7dd3e1bb278a487cc3b7da6d4.png)

在整个分布式系统堆栈中，除了 DAG 执行之外，AM 还有责任。AM 作为分布式作业的中心控制节点，向下游资源管理器申请分布式作业所需的计算资源，与上游计算引擎通信，并向 DAG 执行进程报告收集到的信息。作为唯一能准确理解每个分布式作业运行的组件，AM 还能准确控制和调整 DAG 的执行。在前面的分布式系统堆栈图中，AM 是唯一需要与几乎所有分布式组件进行交互的系统组件，并且在作业运行过程中起着重要的作用。在以前的作业调度系统中，AM 被称为作业主机(JM)。在本文中，我们称之为 DAG 或 AM。

## 1.2 逻辑和物理图

分布式作业的 DAG 使用逻辑或物理图来表示。用户可以看到的 DAG 拓扑通常是一个逻辑图。例如，logview 图是描述作业运行过程的逻辑图，尽管它包含物理信息，即每个逻辑节点的并发性。

![](img/531e40e929da190d4e640f96384e08be.png)

***具体来说:***

*   逻辑图描述了用户想要实现的数据处理过程。就数据库或 SQL 而言，DAG 逻辑图是优化器执行计划的扩展，这对于其他类型的引擎(如 TensorFlow)基本上是相同的。
*   物理图描述了执行计划映射到分布式系统后的特性，例如并发性和数据传输模式。

![](img/495a50a4c7a0fd103dbad788f502e122.png)

将逻辑图转换为物理图有许多等效的方法。DAG 的重要职责之一是选择适当的方法将逻辑图转换为物理图，并灵活地调整该图。如上图所示，将逻辑图映射到物理图是为了回答有关图节点和连接边的物理特征的问题。回答完这些问题后，就可以在分布式系统中执行物理图了。

## 1.3 我们为什么需要升级到 DAG 2.0？

作为作业调度器分布式作业执行框架，DAG 1.0 是在阿里云 Apsara 系统创建时开发的。在过去的 10 年里，DAG 1.0 支持阿里巴巴大数据业务，并在系统规模、可靠性和其他方面发展了优于业内其他框架的优势。尽管 DAG 1.0 在过去 10 年中不断发展，但它继承了 Map-Reduce 执行框架的一些功能，并且在逻辑图和物理图之间没有清晰的分层。因此，DAG 1.0 很难在 DAG 执行或各种计算模式期间支持更多动态功能。目前，在 [MaxCompute](https://www.alibabacloud.com/product/maxcompute) 平台上，两个完全分离的分布式执行框架用于 SQL 作业的离线作业模式和近实时作业模式(smode)。在大多数情况下，性能或资源利用率是优先考虑的。因此，两者之间无法取得良好的平衡。

随着 [MaxCompute](https://www.alibabacloud.com/product/maxcompute) 和 PAI 引擎的更新以及新功能的演进，上层分布式计算能力不断增强。因此，我们需要 AM 在作业管理、DAG 执行和其他方面更加动态和灵活。为了支持未来 10 年计算平台的发展，作业调度程序团队启动了 DAG 2.0 项目，以完全取代 DAG 1.0 的 JobMaster 组件，升级其代码和功能。这将允许它更好地支持上层计算需求，并利用对 shuffle 服务和 FuxiMaster(资源管理器)功能的升级。要提供企业服务，一个好的分布式执行框架需要为阿里巴巴的超大规模、大吞吐量作业以及云上的各种规模和计算模式需求提供支持。除了开发超大规模扩展系统功能，我们还需要让大数据系统更轻松地使用作业调度程序，并利用系统的智能和动态功能来提供适应各种数据规模和处理模式的大数据企业服务。这些是设计 DAG 2.0 体系结构时的重要考虑因素。

![](img/8ae57865cf5bc98c74f099206e21713a.png)

# 2.DAG 2.0 架构和总体设计

在我们调查了行业中各种分布式系统的 DAG 组件后，包括 Spark、Flink、得律阿德斯、Tez 和 TensorFlow，我们在 DAG 2.0 项目中使用了得律阿德斯和 Tez 框架作为参考。通过基础设计，如逻辑和物理图的清晰分层、可扩展的状态机管理、基于插件的系统管理和事件驱动的调度策略，DAG 2.0 可以集中管理计算平台的各种计算模式，并在作业运行期间在不同层提供更好的动态调整能力。

## 2.1 动态作业运行

传统分布式作业的执行计划是在提交作业之前确定的。例如，SQL 语句经过编译器和优化器处理后生成执行图，执行图在作业调度器分布式系统中被转换为执行计划。

![](img/9cd76025a4bf2ee0ad756310df18dff4.png)

这种作业流程在大数据系统中是标准的。但如果 DAG 执行不能适应动态调整，则需要提前确定整个执行计划。因此，很少有机会动态调整正在运行的作业。为了将 DAG 执行逻辑图转换成物理图，分布式系统必须预先理解要处理的数据的作业逻辑和特征，并且能够在作业运行期间准确地回答关于节点和连接边的物理特征的问题。

然而，许多与物理特征相关的问题在作业运行之前是无法察觉的。例如，在分布式作业运行之前，只能获得最初输入的数据特征，如数据量。对于深度 DAG 执行，这意味着只有物理计划(比如根节点的并发选择)才是合适的，下游节点和边的物理特征只能基于某些特定的规则来猜测。当输入数据具有丰富的统计信息时，优化器可以使用执行计划中带有操作符特征的统计信息来推断整个执行过程中每一步生成的中间数据的特征。然而，这种推断必须克服实施过程中的重大挑战，尤其是在阿里巴巴的实际产品环境中。这些挑战包括:

*   **实际输入数据统计缺失:**即使是 SQL 作业处理的结构化数据，也可能无法正确收集到源表的数据特征统计。由于各种数据存储方法和缺乏精确的统计方法，大多数源表没有完整的数据统计。收集需要在集群内部和外部处理的非结构化数据特征的统计数据甚至更加困难。
*   **分布式作业中的大量用户逻辑黑盒:**一个通用的大数据处理系统，要支持用户逻辑的运行。计算引擎和分布式系统无法理解使用 Java 或 Python 实现的用户逻辑，例如 SQL 作业中常用的 UDF、UDTF、UDJ、提取器和输出器。在整个作业过程中，用户逻辑是一个黑匣子。例如，MaxCompute 平台上超过 20%的在线 SQL 作业，尤其是重要的基线作业，都包含用户代码。在大多数情况下，大量的用户代码会导致优化器无法预测中间数据的特征。
*   **不正确的优化器预测代价高昂:**当优化器选择一个执行计划时，它会选择一个在数据具有特定特性时能够优化性能的计划。但是，如果由于不正确的假设而选择了不正确的计划，则性能可能会下降，或者作业可能会失败。通常过多的基于静态信息的预测无法达到理想的效果。

关于作业运行的不确定性可能由各种原因引起，并且需要良好的分布式作业运行系统来基于中间数据特征动态地调整作业运行。但是，只有在执行过程中生成中间数据后，才能准确获得中间数据特征。因此，缺乏动态功能可能会导致在线操作出现问题，包括以下问题:

*   **物理资源的浪费:**比如某个计算节点事先选择的资源类型可能不合适，或者可能会消耗大量的计算资源来处理稍后会被丢弃的无效数据。
*   **严重的长尾问题:**例如，由于中间数据失真或编排不当，某个阶段的计算节点可能需要处理非常大或非常小的数据量。
*   **不稳定的作业:**优化器错误的静态计划选择可能会导致执行计划无法完成。

DAG 或 AM 作为分布式作业的唯一中心节点和调度控制节点，是唯一能够收集和聚集相关数据并基于数据特征动态调整作业运行的分布式系统组件。该组件可以调整简单的物理执行图(如动态并发)以及复杂的混排方法和数据编排方法。逻辑执行图可能还需要根据数据特征进行调整。**逻辑图的动态调整是我们在 DAG 2.0 中探索的新解决方案。**

节点、边和图形的清晰物理和逻辑分层、基于事件的数据收集和调度管理，以及基于插件的功能实现，使 DAG 2.0 能够在作业运行期间收集数据，并在将逻辑图形转换为物理图形时使用它来系统地回答基本问题。必要时，物理图和逻辑图都可以是动态的，并且可以适当地调整执行计划。在下面的实现场景中，我们将进一步说明 DAG 2.0 强大的动态功能如何帮助确保更具适应性和效率的分布式作业运行。

[](https://www.datadriveninvestor.com/2020/04/29/utilizing-computing-in-the-fight-against-sars-cov2/) [## 利用计算抗击 SARS-CoV2 |数据驱动型投资者

### 全世界所有的学者、研究人员、科学家和组织都在尽一切努力对抗…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/04/29/utilizing-computing-in-the-fight-against-sars-cov2/) 

## 2.2 统一的 AM 或 DAG 执行框架

DAG 2.0 可以描述抽象和分层节点、边和图架构中节点和边的不同物理特征，以支持不同的计算模式。各种分布式数据处理引擎的分布式执行框架，包括 Spark、Flink、Hive、Scope、TensorFlow 等，都是从得律阿德斯提出的 DAG 模型中衍生出来的[1]。我们认为图的抽象和分层描述可以更好地描述 DAG 系统中的各种模型，如离线、实时、流式和渐进式计算。DAG 2.0 刚实现的时候，它的首要目标是使用同一套代码和架构系统来统一运行在作业调度器平台上的几种计算模式，包括 MaxCompute 离线作业和近实时作业、PAI TensorFlow 作业以及其他非 SQL 作业。未来，我们计划逐步探索更新颖的计算模式。

**2.2.1 离线和近实时作业的统一执行框架**

MaxCompute 平台上的大多数作业都是 SQL 离线作业(批处理)和接近实时的作业(smode。)由于历史原因，离线作业和近实时作业模式下的 MaxCompute SQL 作业的资源管理和运行是基于两套完全分离的代码实现的。这两组代码和这两种模式的特性不能被重用。因此，我们无法在资源利用率和执行性能之间取得平衡。在 DAG 2.0 模型中，这两种模式是集成统一的，如下图所示。

![](img/307cefd48a75b3ea460517610bb34633.png)

在将不同的物理特征映射到逻辑节点和边之后，可以准确地描述离线作业和接近实时的作业。

*   **离线作业:**各节点根据自身需求申请资源。一个逻辑节点代表一个调度单元。通过节点之间的连接边缘传输的数据被存储到磁盘以确保可靠性。
*   **近实时作业:**在一个调度单元内对一个作业的所有节点进行联动调度。节点间连接边上的数据通过网络或内存传输，数据管道用于确保最佳性能。

由于离线作业模式的特点，如按需资源应用和中间数据存储到磁盘，在线作业在资源利用率、规模和稳定性方面具有明显的优势。在接近实时的作业模式下，常驻计算资源池和 gang 调度(贪婪资源应用)减少了作业运行期间的开销，确保了流水线数据传输，并加速了作业运行。但是，由于其资源使用特性，这种模式无法支持大范围的大规模作业。DAG 2.0 在同一架构上统一了这两种计算模式。这种统一的描述方法使得平衡离线作业的高资源利用率和近实时作业的高性能成为可能。当调度单元可以自由调整时，一种新的混合计算模式就成为可能。我们称之为冒泡执行模式。

![](img/00a8d6986c0b007b8bfac3bb1dd90273.png)

这种冒泡模式使 DAG 用户(MaxCompute optimizer 等上层计算引擎的开发人员)能够根据执行计划的功能和引擎用户对资源使用和性能的敏感度，灵活地在执行计划中剪切出冒泡子图。在气泡中，使用了直接网络连接和计算节点预取等方法来提高性能。不使用冒泡执行模式的节点仍然以传统的离线作业模式运行。离线作业模式和接近实时的作业模式可以被描述为气泡模式的两个极端情况。在新的统一模型中，计算引擎和执行框架可以根据实际需求选择不同的资源利用率和性能组合。典型的应用场景包括:

*   **贪婪气泡:**当可用资源有限(如集群规模或配额)时，无法对大规模作业进行群组调度，用户对资源利用率不敏感，只想快速运行大规模作业，可以实施贪婪气泡切割，根据可用计算节点的数量切割大气泡。
*   **高效气泡:**在作业运行过程中，节点之间的操作可能会面临天然的障碍，比如排序操作和哈希表的建立。当通过障碍边连接的两个节点被切割成气泡时，由于调度开销的减少，作业的 E2E 性能将会提高。然而，数据不能完全流水线化，因此资源利用率不能最大化。当用户对资源利用率敏感时，我们需要防止泡沫中的障碍边缘。计算引擎可以基于执行计划来确定期望的资源利用率。

这里我们只列出两个简单的策略。还有许多其他完善的和有针对性的优化策略。DAG 层提供的灵活的基于气泡的计算允许上层计算引擎在不同的场景中选择适当的策略，以更好地支持各种计算需求。

**2.2.2 支持新的计算模式**

DAG 1.0 执行框架的底层设计深受 Map-Reduce 模式的影响。节点之间的边缘连接混合了各种语义，包括调度序列、运行序列和数据混排。在由边连接的两个节点中，只有在上游节点运行、退出并生成数据后，才能调度下游节点。对于一些新的计算模式来说，情况并非如此。例如，在参数服务器计算模式下，参数服务器(PS)和 worker 在运行过程中具有以下特性:

*   PS 作为参数服务实体，可以独立运行。
*   worker 作为参数的消费者和更新者，可以在 PS 运行后运行，需要在运行过程中与 PS 交换数据。

在这种运行模式下，PS 和 worker 具有调度依赖关系。但是，PS 和 worker 必须同时运行，所以没有逻辑说只能在 PS 退出后调度 worker。所以在 DAG 1.0 框架中，PS 和 worker 只能看作是两个独立的阶段进行调度和运行。此外，所有 PSs 和工作人员只能通过计算节点之间的直接连接，并在外部实体的帮助下，如 ZooKeeper 或 Apsara 名称服务和分布式锁同步系统，相互通信和协调。因此，作为中心管理节点的 AM 或 DAG 不起作用，作业由计算引擎管理，并通过计算节点之间的协调来完成。这种分散式管理方法无法处理复杂的场景，例如故障转移。

在 DAG 2.0 框架中，为了准确描述节点间的调度和运行关系，我们引入并实现了并发边特性。使用并发边缘连接的上游和下游节点可以按顺序调度并同时运行。调度时间也可以灵活配置。比如可以同步调度上下游节点，也可以在上游节点运行一定时间后，通过触发事件来调度下游节点。通过这种灵活的描述功能，可以使用以下 DAG 来描述 PS 作业，从而确保更准确地描述作业节点之间的关系。这意味着 AM 可以理解作业拓扑，从而有效地管理作业。例如，它可以对不同计算节点的故障转移使用不同的处理策略。

![](img/17824a5be01c02f8692a3ed4f09d75ea.png)

此外，DAG 2.0 的新描述模型允许对 PAI 平台上的 TensorFlow 或 PS 作业以及新的创新工作进行更多动态优化。在前面的动态 PS DAG 中，引入了一个控制节点。该节点可以在 PS 工作负载运行前后动态调整作业的资源应用和并发性，以优化作业运行。

事实上，并发边描述了上下游节点运行或调度时间的物理特征。它也是对架构的重要扩展，具有清晰的逻辑和物理图形分层。除了 PS 作业模式之外，并发边缘对于 bubble 执行模式也很重要，它统一了离线作业和接近实时的作业模式。

# 3.DAG 2.0 和上层计算引擎之间的集成

作为计算平台分布式运行的基础，DAG 2.0 为各种上层计算引擎提供了更加灵活高效的执行能力。这些功能是基于详细的计算场景实现的。接下来，我们将使用 DAG 2.0 与各种上层计算引擎(如 MaxCompute 和 PAI)之间的互通来描述 DAG 2.0 调度和执行框架如何为上层计算和应用提供支持。

## 3.1 DAG 运行过程中的动态调整

由于它必须在不同的平台上运行计算作业，MaxCompute 有各种计算场景，特别是用于离线作业的复杂逻辑，这为实现动态图形功能提供了多样化的场景。我们将通过查看动态物理和逻辑图来给出一些例子。

**3.1.1 动态并发调整**

基于作业运行期间的中间数据量的动态并发调整是基本的 DAG 动态调整能力。在传统的静态 MR 作业中，可以根据读取的数据量准确确定映射器并发。但是，reducer 并发只能推导出来。如下图所示，当提交一个 1 TB 的 MR 作业进行处理时，reducer 并发数 500 是根据 mapper 并发数 1000 推导出来的。然而，如果映射器过滤了大量数据，并最终生成了 10 MB 的中间数据，那么 500 个并发的 reducers 显然是对资源的浪费。DAG 必须根据映射器生成的实际数据量动态调整缩减器并发性(500 比 1)。

![](img/186f6349f396617137e58eecefe72baa.png)

在实际实现中，上游节点的输出分区数据简单地按照并发调整比例进行聚合。如下图所示，当并发从 10 调整到 5 时，可能会出现不必要的数据倾斜。

![](img/86dca4ee32e2b1a4a7ace76ab496eaaf.png)

基于中间数据的 DAG 2.0 动态并发调整综合考虑了数据分区偏斜的可能性，优化了动态调整策略，保证调整后数据分布均匀。这有效地防止了由动态调整引起的数据偏斜。

![](img/6c162092a379d4dec1a81a4a713d65c5.png)

这种最常见的下游节点并发调整是 DAG 2.0 动态物理图的直观呈现。在 DAG 2.0 项目中，我们还基于计算引擎的数据处理特性探索了基于源数据的动态并发调整。例如，当两个源表的数据以表示已处理数据量所需的节点大小进行联接(M1 在 J 处联接 M2)时，如下图所示，M1 处理一个中间数据表(例如，M1 并发数为 10)，而 M2 处理一个大型数据表(M2 并发数为 1000)。)最低效的调度是基于 10 + 1000 并发。此外，所有 M2 输出都需要被混洗到 J 中，因此，J 并发也很大(大约 1000。)

![](img/e8841a884ff93f89d79470cb88d7c821.png)

在这种计算模式中，M2 只需要读取和处理可以与 M1 输出连接的数据。这意味着，如果在考虑总体执行成本后，M1 输出数据远小于 M2 的预期，我们可以首先安排 M1 进行计算，在 AM 或 DAG 聚合 M1 输出数据统计，并只选择对 M2 有效的数据。对 M2 有效的数据的选择本质上是一个谓词下推过程，它可以基于计算引擎和运行时的优化器来共同确定。这意味着该场景中的 M2 并发调整与上层计算密切相关。

例如，如果 M2 处理一个有 1000 个分区的分区表，并且下游联接器在分区键上进行联接，那么理论上只需要读取可以与 M1 的输出进行联接的分区中的数据。如果 M1 输出只包含 M2 源表的三个分区键，M2 只需要调度三个计算节点来处理来自这三个分区的数据。因此，M2 并发从默认的 1000 减少到 3。这极大地减少了计算资源消耗，并将作业运行速度提高了数倍，同时保持了相同的逻辑计算等效性和正确性。这种情况下的优化包括:

*   M2 并发性和要处理的数据量大大减少。
*   需要混洗到 J 的 M2 数据量和混洗所需的计算资源大大减少。
*   并发性和要处理的数据量大大减少。

如上图所示，为了确保 M1 到 M2 的调度顺序，DAG 在 M1 和 M2 之间引入了依赖边。这种依赖性边缘仅呈现执行序列，并且不具有数据改组。它不同于传统 MR 或 DAG 执行框架中的紧密绑定边缘连接和数据混洗的假设。它是 DAG 2.0 中 edge 的扩展之一。

DAG 执行引擎作为底层分布式调度执行框架，直接与上层计算引擎的开发团队交互工作。除了性能提升，用户无法直接感知 DAG 执行引擎的升级。下面这个例子是用户可以感知到的。这向用户展示了 DAG 动态执行功能和优势。这个例子是基于 DAG 动态能力的极限优化。

对于 SQL 用户来说，LIMIT 是一种常见的操作，用于理解对数据的一些基本即席操作的数据表特性。下面显示了一个极限示例。

`SELECT * FROM tpch_lineitem WHERE l_orderkey > 0 LIMIT 5;`

在分布式执行框架中，该操作的执行计划拆分源表，调度所需数量的映射器以读取所有数据，并将映射器输出聚合到 reducer 以进行最终的限制操作。如果源表(前面例子中的`tpch_lineitem`)很大，需要 1000 个映射器来读取，那么整个分布式执行过程的调度成本是 1000 个映射器和一个缩减器。在这个过程中，有一些方面是上层计算引擎可以优化的。例如，每个映射器可以在生成 LIMIT(上例中的 LIMIT 5)所需的记录数后退出，因此它不需要处理分配给它的所有数据碎片。然而，在静态执行框架中，必须对 1001 个计算节点进行调度，以获得这一简单信息。这导致执行即席查询的巨大开销，尤其是当集群资源紧张时。

为了促进这种限制场景，DAG 2.0 基于新执行框架的动态功能进行了一些优化，包括:

*   **上游节点的指数级启动:**如果不是所有上游映射器节点都有很高的运行概率，DAG 框架会按指数级批量调度映射器节点，即 1、10…和满。
*   **下游节点提前调度:**上游节点生成的记录将作为执行过程中的统计数据上报给 am。当 AM 确定上游节点已经生成了足够多的记录时，它可以提前调度下游 reducer 节点来消费上游数据。
*   **上游节点提前终止:**当下游减速器确定输出限制数满足要求时，立即退出。AM 可以触发上游映射器节点的提前终止。在这种情况下，大多数映射器节点可能不会被调度。整个工作也可以提前完成。

在执行过程中，计算引擎和 DAG 之间的灵活和动态交互可以大大减少资源消耗并加速作业运行。根据离线测试结果和实际在线性能，执行一个 mapper 节点后，大部分作业可以提前退出，并不是所有 mapper 节点都需要调度。

下图显示了在离线测试中，当映射器并发数为 4，000 时，优化上述查询前后的差异。

![](img/877abe1ec71a630d083effa78e999103.png)

优化后，执行时间减少了 5 倍以上，计算资源消耗减少了数百倍。

作为一个典型的例子，这个离线测试结果是一个理想的情况。为了评估实际效果，我们选择了在 DAG 2.0 发布后进行限制优化的在线作业。一周后的统计结果是:

优化节省了相当于 254.5 个核心 x 最小 CPU + 207.3 GB x 最小的计算资源，并为每个作业减少了 4，349 个计算节点。

作为针对特殊场景的优化，限制执行优化涉及 DAG 执行过程中不同策略的调整。这种分段优化直观地展示了 DAG 2.0 架构的优势。灵活的架构确保了在 DAG 执行期间更大的动态调整能力和涉及计算引擎的更专用的优化。

不同场景下的动态并发调整和调度执行策略的动态调整只是图物理特征动态调整的两个例子。物理特征运行时调整在 DAG 2.0 框架中有各种应用，例如运行期间针对数据偏斜的动态数据编排和洗牌。接下来介绍一下 DAG 2.0 中逻辑图动态调整的一些思路。

**3.1.2 动态逻辑图调整**

在分布式 SQL 作业中，映射连接是一种常见的优化方法。如果要连接的两个表中的一个很小，并且其数据可以存储在单个计算节点的内存中，则不需要对其执行混洗。它的所有数据都被广播到处理大型表的每个分布式计算节点。在内存中建立哈希表来完成连接。Map join 可以显著减少大型表的洗牌和排序，并提高作业运行性能。但是，如果小表的数据不能存储在单个计算节点的内存中，那么当在内存中建立散列表时，将会出现 OOM。因此，整个分布式作业将失败，需要再次运行。即使正确使用映射连接可以极大地提高性能，但在大多数情况下，优化器需要来自用户的显式映射连接提示来生成映射连接计划。此外，用户和优化器都不能准确地确定非源表的输入，因为只有在作业运行期间才能准确地获得中间数据量。

![](img/4c8e8f46e201d4a77ae958a56a56d969.png)

映射连接和默认排序-合并连接是两种不同的优化器执行计划。在 DAG 层，它们对应于两个不同的逻辑图。为了在作业运行期间支持基于中间数据特征的动态优化，DAG 框架需要具有动态逻辑图执行能力。这正是 DAG 2.0 中开发的条件连接特性所提供的。如下图所示，当不能预先确定用于 join 操作的算法时，分布式调度执行框架允许优化器提交有条件的 DAG。这个 DAG 包含两个 join 方法的执行计划分支。在执行期间，AM 根据上游数据量动态选择要执行的分支(计划 A 或计划 B)。这种动态逻辑图执行过程确保在作业运行时基于生成的中间数据特征选择最佳执行计划。

![](img/c319ab093455daa8db2dbee28972f295.png)

条件连接是动态逻辑图的第一个实现场景。当选择一批适用的在线作业进行测试时，与静态执行计划相比，动态条件连接将性能提高了大约 300%。

![](img/166b7e31439ce25b6cf0406d494d4f01.png)

## 3.2 气泡模式

冒泡模式是我们在 DAG 2.0 体系结构中探索的一种新的作业运行模式。我们可以调整气泡的大小和位置，以获得性能和资源利用率之间的不同平衡。我们将给出一些直观的例子来帮助你理解冒泡模式在分布式作业中是如何应用的。

![](img/ce4b739ad747e1b8bac7795a4d77af67.png)

如上图所示，TPC-H Q21 工单被分成三个气泡。数据可以在节点之间高效地流水线化，调度可以通过热节点加速。资源消耗(CPU x 时间)是近实时作业的 35%，性能是一体化调度模式下近实时作业的 96%，比离线作业高 70%左右。

在标准的 TPC-H 1 TB 测试中，与离线作业模式和接近实时的作业一体化模式(联合调度)相比，气泡模式可确保性能和资源利用率之间更好的平衡。)当选择贪婪冒泡策略时(冒泡大小上限为 500)，与离线批处理模式相比，冒泡模式将作业性能提高了 200%，而资源消耗仅增加了 17%。冒泡模式可以获得接近实时作业一体化模式的 85%的性能，而资源消耗不到 40%(CPU x 时间。)冒泡模式确保性能和资源利用之间的良好平衡，从而确保高效的系统资源利用。泡泡模式已经应用于阿里巴巴集团的所有在线工作。实际的在线工作表现与 TPC-H 测试中观察到的相似。

![](img/1f665e09ad019b4a4cd08a86f8e716a0.png)![](img/75928e95ded8249724132b1f6ef531ba.png)

如前所述，气泡模式支持不同的切割策略，但这里只讨论切割策略。这种 DAG 分布式调度气泡执行功能与上层计算引擎(如 MaxCompute optimizer)紧密结合，使我们能够根据作业的可用资源和计算特性，在性能和资源利用率之间取得最佳平衡。

# 4.动态资源配置和管理

在传统的分布式作业中，每个计算节点所需的资源(CPU、GPU 和内存)的类型和数量是预先确定的。但是，在分布式作业运行之前，很难选择合适的计算节点类型和大小。甚至计算引擎开发人员也需要一些复杂的规则来估计正确的配置。在需要将配置发送给用户的计算模式中，用户更难选择合适的配置。

在这里，我们将以 PAI 平台上的 TensorFlow (TF)作业为例，描述 DAG 2.0 中的动态资源配置功能如何帮助 TF 作业选择合适的 GPU 类型，提高 GPU 利用率。与 CPU 相比，GPU 是一种新型的计算资源，具有更快的硬件更新速度。大多数普通用户并不熟悉 GPU 的计算特性。因此，用户经常会选择不合适的 GPU 类型。GPU 是线上运营的稀缺资源。在线申请的 GPU 数量总是超过一个集群中的 GPU 总数。因此，用户必须长时间排队才能获得资源。然而，集群中的实际 GPU 利用率平均约为 20%，较低。应用数量与实际使用之间的差距，通常是由于作业配置时指定的 GPU 资源配置不当造成的。

在 DAG 2.0 框架中，为 PAI TF 作业的 GPU 资源提供了一个额外的计算控制节点。有关更多信息，请参见第 2.2.2 节“对新计算模式的支持”中有关动态 PS DAG 的信息该节点可以运行 PAI 平台的资源预测算法来确定当前作业所需的 GPU 类型，然后向 AM 发送动态事件，以在必要时修改下游工作器申请的 GPU 类型。可以通过对样本数据或基于历史的优化(HBO)进行模拟来预测准确的资源使用情况，这可以预测适当的资源使用情况、给定的数据特征、训练算法类型以及从历史作业中收集的信息。

在这种情况下，控制节点和工作节点通过并发边缘连接。当控制节点选择资源类型并发送事件时，调度在边缘被触发。作业运行期间的这种动态资源配置提高了在线功能测试中超过 40%集群的 GPU 利用率。

作为物理特性的一个重要维度，作业运行时计算节点的动态资源特性调整在 PAI 和 MaxCompute 平台上得到了广泛的应用。在 MaxCompute SQL 作业中，可以根据上游数据特征有效预测上游和下游节点的 CPU 或内存大小。当发生 OOM 时，重试计算节点所申请的内存可以被提高以防止作业失败。这些都是 DAG 2.0 架构中实现的新特性，这里就不细说了。

# 5.工程和发布

Dag 的动态能力和灵活性是分布式系统的基础。当与上层计算引擎结合时，这可以确保更高效和准确的上层计算，并显著提高特定场景下的性能和资源利用率。这些是在 DAG 2.0 项目中开发的新功能和计算模式。除了与计算引擎互通以实现更高效的执行计划之外，敏捷调度是 AM 或 DAG 执行性能的一个基本特征。DAG 2.0 调度策略基于事件驱动框架和灵活的状态机设计来实现。下图显示了 DAG 2.0 在基础工程素养和性能方面的成就。

![](img/c015dcf0f683854fdeb2fdac1e45702c.png)

以最简单的 Map-Reduce (MR)作业为例。对于这种类型的作业，很少有机会优化调度执行。调度执行取决于调度系统在整个处理过程中的灵活性和全面的去阻塞能力。该示例还强调了 DAG 2.0 的调度性能优势。工作规模大的时候优势明显。在更好地代表生产工作负载特征的 TPCDS 基准测试中，即使禁用了动态 DAG 等功能以允许基于相同的执行计划和运行时进行公平比较，DAG 2.0 仍能带来显著的性能提升，这要归功于其高效的调度。

![](img/3e34a689829a1d3b98a24705deb71540.png)

如何才能用一个下一代框架与线上场景无缝互通，保证大规模的线上应用？这是一个重要的主题，可以说明实际生产系统升级和新系统的小规模概念验证之间的区别。作业调度系统现在支持阿里巴巴集团内外大数据计算平台上的数千万个分布式作业。要完全取代 10 年来支持我们大数据业务的在线分布式生产系统 DAG 或 AM 核心分布式调度执行组件，同时防止现有场景中的故障，我们需要的不仅仅是高级架构和设计。在运行过程中改变引擎以确保高质量的系统升级比设计新的系统架构更具挑战性。要执行这样的升级，稳定的工程基础和测试或发布框架是必不可少的。没有这样的基础，上层的动态特性和新的计算模式是不可能的。

![](img/721664faaa0f8cdfe54165b5e1b0d020.png)

目前，DAG 2.0 涵盖了 MaxCompute 平台上的所有 SQL 离线作业和近实时作业，以及 PAI 平台上的 TensorFlow 作业(CPU 和 GPU)和 PyTorch 作业。它每天支持数千万个分布式工作，并满足了 2019 年双 11 和双 12 的要求。在双 11 和双 12 期间数据峰值(比 2018 年增长超过 50%)的压力下，DAG 2.0 确保了阿里巴巴在双 11 和双 12 的重要基线按时输出。更多类型的作业(如跨群集拷贝作业)正在迁移到 DAG 2.0 体系结构，并且计算作业功能已经基于新的体系结构进行了升级。DAG 2.0 框架的发布为各种计算场景下新功能的开发奠定了坚实的基础。

# 6.观点

作业调度程序 DAG 2.0 的核心架构旨在巩固阿里巴巴计算平台长期发展的基础，支持上层计算引擎与分布式调度的结合，以实现各种创新，创建新的计算生态系统。这种架构升级是这一过程中重要的第一步。为了支持不同规模和模式的企业级和全频谱计算平台，我们需要将新架构的功能与上层计算引擎和作业调度系统的其他组件深度集成。在阿里巴巴应用场景中，DAG 2.0 在保持作业规模领先的同时，实现了架构和功能的多项创新，包括:

*   在分布式执行框架中，它确保作业运行期间逻辑和物理图的动态调整。
*   通过气泡机制，它实现了一种混合计算模式，以在资源利用率和作业性能之间找到最佳平衡。

此外，清晰的 DAG 2.0 系统架构确保了新功能的快速开发，促进了平台和引擎的创新。由于空间限制，我们只能引入某些新的功能和计算模式。我们还实施或正在开发许多其他创新功能，例如:

*   **近实时作业系统架构升级:**资源管理与多作业管理解耦，近实时作业支持动态图。
*   针对常驻单容器和多插槽作业的缓存感知查询加速服务(MaxCompute 短查询)
*   基于状态机的作业节点管理和故障时智能重新运行
*   动态可定义的洗牌法:使用递归洗牌或其他方法，动态解决大规模在线作业的内投问题。
*   针对分布式作业中的各种数据不对称问题，对中间数据进行自适应动态拆分和聚合
*   PAI 平台上 TF GPU 作业的多个执行计划
*   在 DAG 执行期间，通过与优化器的交互进行渐进式交互式动态优化
*   支持命令式语言特性以及与语义的交互，如 IF、ELSE 和循环通过能力，如 DAG 动态增长

改进的核心调度能力可以为各种分布式上层计算引擎提供企业级服务能力。这些改进的计算调度能力的好处将通过阿里云计算引擎传递给客户企业，如 [MaxCompute](https://www.alibabacloud.com/product/maxcompute) 。在过去的 10 年里，阿里巴巴的企业被推动建立了业内最大的云分布式平台。为了更好地服务阿里巴巴集团和我们的企业用户，我们希望平台的企业级服务能力能够在性能&规模和智能适配方面不断提升。这将更容易使用分布式计算服务，并确保包容性大数据。

更多信息请访问 [MaxCompute 官网](https://www.alibabacloud.com/product/maxcompute)

# 原始来源:

[](https://www.alibabacloud.com/blog/job-scheduler-dag-2-0-building-a-more-dynamic-and-flexible-distributed-computing-ecosystem_596558) [## 作业调度 DAG 2.0:构建一个更加动态和灵活的分布式计算生态系统

### 阿里云 MaxCompute 年 8 月 31 日 108 陈阿里云智能高级技术专家

www.alibabacloud.com](https://www.alibabacloud.com/blog/job-scheduler-dag-2-0-building-a-more-dynamic-and-flexible-distributed-computing-ecosystem_596558) 

**访问专家视图—** [**订阅 DDI 英特尔**](https://datadriveninvestor.com/ddi-intel)