<html>
<head>
<title>Deep Learning LSTM for Sentiment Analysis in Tensorflow with Keras API.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习LSTM在Tensorflow中的情感分析。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/deep-learning-lstm-for-sentiment-analysis-in-tensorflow-with-keras-api-92e62cde7626?source=collection_archive---------0-----------------------#2020-02-13">https://medium.datadriveninvestor.com/deep-learning-lstm-for-sentiment-analysis-in-tensorflow-with-keras-api-92e62cde7626?source=collection_archive---------0-----------------------#2020-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/081b72ae0266175a5f0a0d2e6f0c8461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gn6dE4Om0Ai-RNTP7YGmQg.jpeg"/></div></div></figure><h2 id="73a9" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">简介</strong></h2><p id="141f" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">情感分析是确定语言是否反映积极、消极或中性情感的过程。<br/>分析顾客的情绪对企业有很多好处。例如。</p><ul class=""><li id="75f3" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">一家公司可以根据客户的意见过滤客户的反馈，以确定他们在服务方面需要改进的地方。</li><li id="56ef" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated">一家公司可以通过监控顾客对其产品评论的情绪来轻松管理他们的在线声誉</li></ul><p id="d622" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">在本教程中，我们将建立一个<a class="ae mi" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>模型，将文本分类为负面或正面。</p><h2 id="33f0" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">要求</strong></h2><ul class=""><li id="e347" class="lp lq iq kw b kx ky lb lc kh mj kl mk kp ml lo lw lx ly lz bi translated"><em class="mm">数据:</em> <strong class="kw ir"> </strong>所使用的数据是在<a class="ae mi" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上可获得的关于一家美国主要航空公司的推文集合。</li><li id="dd4a" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated"><a class="ae mi" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"><em class="mm">tensor flow</em></a><em class="mm"/>1 . 15 . 0版或更高版本，带<a class="ae mi" href="https://www.tensorflow.org/api_docs/python/tf/keras" rel="noopener ugc nofollow" target="_blank"> Keras </a> API</li><li id="80c5" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated"><a class="ae mi" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mm">熊猫</em> </a></li><li id="3c33" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated"><a class="ae mi" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mm"> Numpy </em> </a></li></ul><h2 id="c9a6" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">数据准备</h2><p id="6f9f" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">让我们看看数据是什么样子的:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2ebc" class="jy jz iq ms b gy mw mx l my mz">import pandas as pd</span><span id="6b84" class="jy jz iq ms b gy na mx l my mz">df= pd.read_csv('Tweets.csv', sep=',')<br/>df.head(10)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/901e09bd8252a4d805da84d852c7d7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-66q6ix-gByre1QZ2U-fg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Data preview</figcaption></figure><p id="432c" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated"><strong class="kw ir"> <em class="mm">准备数据的步骤:</em> </strong></p><ul class=""><li id="8c17" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">选择相关列:<br/>本项目需要的数据列是<strong class="kw ir"><em class="mm">airline _ sensation</em></strong>和<strong class="kw ir"> <em class="mm"> text </em> </strong>列。我们正在解决一个分类问题，所以<strong class="kw ir">文本</strong>将是我们的特征，而<strong class="kw ir"> <em class="mm">航空公司_情绪</em> </strong>将是标签。</li></ul><p id="0d84" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">当输入是数字时，机器学习模型工作得最好。我们将把所有选择的列转换成它们需要的数字格式。</p><ul class=""><li id="e0ba" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">将<strong class="kw ir"> <em class="mm">航空公司_人气</em> </strong>列转换为数值类别:</li><li id="3987" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated">将<strong class="kw ir"> <em class="mm">文本</em> </strong>列转换为数字向量。(<em class="mm">稍后再详细介绍</em>)</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="8d7b" class="jy jz iq ms b gy mw mx l my mz">#select relavant columns<br/>tweet_df = df[['text','airline_sentiment']]</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/28a27877a24594f0a15a783b65eeda2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*9Z0sgkVK_OoZI1Waqxb7lA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Selected relevant columns</figcaption></figure><p id="6898" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">我们需要将推文分为负面或正面，因此我们将过滤掉带有中性情绪的行。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6d8a" class="jy jz iq ms b gy mw mx l my mz"><br/>tweet_df = tweet_df[tweet_df['airline_sentiment'] != 'neutral']</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/71b09cfdce6b0cf9437bdb433aa4ad74.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*x2GyENOsZ6UrE2ENSt5mHA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Data without neutral sentiment</figcaption></figure><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="fa80" class="jy jz iq ms b gy mw mx l my mz"># convert airline_seentiment to numeric<br/>sentiment_label = tweet_df.airline_sentiment.factorize()</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9cd6fcab6d861fd55483305ebe4cad83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*r7MDtV8mxqDAaAGlClT0JQ.png"/></div></figure><p id="8d16" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">调用<strong class="kw ir">因式分解</strong>方法会返回数字类别数组和类别索引。在这种情况下，指数0是积极的，指数1是消极的情绪。</p><div class="nj nk gp gr nl nm"><a href="https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">深度学习用7个步骤解释-更新|数据驱动的投资者</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa jw nm"/></div></div></a></div><h2 id="b1eb" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">为NLP准备文本:</h2><p id="9098" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">正如我前面所说的，机器学习模型的输入需要采用数字格式。这可以通过以下方式实现:</p><ul class=""><li id="16b4" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">给句子中的每个单词指定一个数字，并用它们各自指定的数字替换每个单词。</li><li id="4c09" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated">使用单词<a class="ae mi" href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa" rel="noopener" target="_blank">嵌入</a>。这能够捕捉句子或文档中单词的上下文。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="66bf" class="jy jz iq ms b gy mw mx l my mz">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="01a5" class="jy jz iq ms b gy na mx l my mz">tweet = tweet_df.text.values<br/>tokenizer = Tokenizer(num_words=5000)<br/>tokenizer.fit_on_texts(tweet)</span><span id="267d" class="jy jz iq ms b gy na mx l my mz">vocab_size = len(tokenizer.word_index) + 1</span><span id="9c3f" class="jy jz iq ms b gy na mx l my mz">encoded_docs = tokenizer.texts_to_sequences(tweet)</span><span id="53ba" class="jy jz iq ms b gy na mx l my mz">padded_sequence = pad_sequences(encoded_docs, maxlen=200)</span></pre><p id="6973" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">从上面的代码可以看出:</p><ul class=""><li id="a2db" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">我们从数据框中获取实际的文本</li><li id="419e" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated">用5000字的限制初始化记号赋予器。这是我们想要编码的字数。</li><li id="cae8" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated">我们调用<strong class="kw ir"> fit_on_texts </strong>来创建单词和数字的关联，如下图所示。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="61d0" class="jy jz iq ms b gy mw mx l my mz">print(tokenizer.word_index)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/54c4658ebe50799b73fa100201047e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFVz2GXT-HNF7KUzCDjUlw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">word index</figcaption></figure><ul class=""><li id="ec22" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated">调用<strong class="kw ir"> text_to_sequence </strong>将句子中的单词替换为它们各自的关联数字。这将每个句子转换成数字序列。</li></ul><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="3de9" class="jy jz iq ms b gy mw mx l my mz">print(tweet[0])<br/>print(encoded_docs[0])</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b189e50ae0601eed3dc26f88b9aec49c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*Ro12i0MMmfv7iEr6zGiFTQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">A tweet and it’s encoded version</figcaption></figure><p id="e4be" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">从上面的结果中，你可以看到tweet被编码成一个数字序列。例如<strong class="kw ir"> <em class="mm">到</em> </strong>和<strong class="kw ir"><em class="mm"/></strong>分别转换为<strong class="kw ir"> 1 </strong>和<strong class="kw ir"> 2 </strong>。<br/>查看上面的单词索引进行验证。</p><p id="7614" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">句子或推文有不同的字数，因此，数字序列的长度会有所不同。<br/>我们的模型要求输入具有相等的长度，所以我们将不得不填充序列以具有选择的输入长度。这是通过调用长度为200的<strong class="kw ir"> pad_sequence </strong>方法来完成的。<br/>所有输入序列的长度将为200。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="eb4b" class="jy jz iq ms b gy mw mx l my mz">print(padded_sequence[0])</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/637b19c1b0ff72f3c7e36d495a27a093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*ZW2I34dsu5CT-bABfElEWQ.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Padded Sequence.</figcaption></figure><h1 id="c568" class="oe jz iq bd ka of og oh kd oi oj ok kg ol om on kk oo op oq ko or os ot ks ou bi translated">建立模型</h1><p id="2084" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">既然我们已经处理了输入。是时候建立模型了。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="1a90" class="jy jz iq ms b gy mw mx l my mz"># Build the model</span><span id="4291" class="jy jz iq ms b gy na mx l my mz">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import LSTM,Dense, Dropout,<br/>from tensorflow.keras.layers import SpatialDropout1D<br/>from tensorflow.keras.layers import Embedding</span><span id="5563" class="jy jz iq ms b gy na mx l my mz">embedding_vector_length = 32</span><span id="d4eb" class="jy jz iq ms b gy na mx l my mz">model = Sequential()</span><span id="a22e" class="jy jz iq ms b gy na mx l my mz">model.add(Embedding(vocab_size, embedding_vector_length,     <br/>                                     input_length=200) )</span><span id="9eb6" class="jy jz iq ms b gy na mx l my mz">model.add(SpatialDropout1D(0.25))</span><span id="d33b" class="jy jz iq ms b gy na mx l my mz">model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))</span><span id="e3cb" class="jy jz iq ms b gy na mx l my mz">model.add(Dropout(0.2))</span><span id="7886" class="jy jz iq ms b gy na mx l my mz">model.add(Dense(1, activation='sigmoid'))</span><span id="457f" class="jy jz iq ms b gy na mx l my mz">model.compile(loss='binary_crossentropy',optimizer='adam', <br/>                           metrics=['accuracy'])</span><span id="9800" class="jy jz iq ms b gy na mx l my mz">print(model.summary())</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/4a29ba8e8a583830ed6779e618da9092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*m4owvpCTfcXdqUF8NoM8qg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Model Summary</figcaption></figure><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/28353205690125256398dbfd3aa658d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*cWfWK1xZQB9EtDvukRGHRA.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Model Structure</figcaption></figure><p id="8b08" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">这就是我们使用LSTM层的地方。该模型由嵌入层、<strong class="kw ir"> LSTM </strong>层和稠密层组成，稠密层是一个以sigmoid为激活函数的全连接神经网络。</p><p id="6afc" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">为了避免过度拟合，在层间和<strong class="kw ir"> LSTM </strong>层添加了漏点。</p><h1 id="16eb" class="oe jz iq bd ka of og oh kd oi oj ok kg ol om on kk oo op oq ko or os ot ks ou bi translated">LSTM</h1><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ox"><img src="../Images/96b5abd81c354b97cd70d3af33824147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CXFcIW6nFgIlP2j4.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">source: <a class="ae mi" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><blockquote class="oy oz pa"><p id="2228" class="ku kv mm kw b kx lr kz la lb ls ld le pb mf lg lh pc mg lj lk pd mh lm ln lo ij bi translated">长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖性。它们是由<a class="ae mi" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">hoch Reiter&amp;schmid Huber(1997)</a>提出的，并在随后的工作中被许多人提炼和推广。它们在各种各样的问题上表现得非常好，现在被广泛使用。</p><p id="5743" class="ku kv mm kw b kx lr kz la lb ls ld le pb mf lg lh pc mg lj lk pd mh lm ln lo ij bi translated">LSTMs的明确设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西！</p><p id="1d9d" class="ku kv mm kw b kx lr kz la lb ls ld le pb mf lg lh pc mg lj lk pd mh lm ln lo ij bi translated">【http://colah.github.io/posts/2015-08-Understanding-LSTMs/】来源 : <a class="ae mi" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><h1 id="5db7" class="oe jz iq bd ka of og oh kd oi oj ok kg ol om on kk oo op oq ko or os ot ks ou bi translated">火车模型</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="d621" class="jy jz iq ms b gy mw mx l my mz">history = model.fit(padded_sequence,sentiment_label[0],<br/>                  validation_split=0.2, epochs=5, batch_size=32)</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pe"><img src="../Images/fb40d5f09bc7c0b24be7dc1a240ce399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2JQYyPccGG25fahc_daffA.png"/></div></div></figure><p id="1ef8" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">该模型经过5个时期的训练，验证准确率达到92%左右。</p><p id="983e" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated"><strong class="kw ir">注意:</strong> <em class="mm">由于模型的随机性质，您的结果可能会略有不同，尝试运行几次，您将获得大致相同的验证准确度。</em></p><h1 id="77b4" class="oe jz iq bd ka of og oh kd oi oj ok kg ol om on kk oo op oq ko or os ot ks ou bi translated">测试模型</h1><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="a7c1" class="jy jz iq ms b gy mw mx l my mz">test_word ="This is soo sad"</span><span id="21ce" class="jy jz iq ms b gy na mx l my mz">tw = tokenizer.texts_to_sequences([test_word])<br/>tw = pad_sequences(tw,maxlen=200)<br/>prediction = int(model.predict(tw).round().item())</span><span id="62ef" class="jy jz iq ms b gy na mx l my mz">sentiment_label[1][prediction]</span></pre><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/eab1a37cce46c62a930419fbdab8673f.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*Vb7lXWn4-C5fak4okU7KYg.png"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Prediction result</figcaption></figure><p id="7cb6" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">该模型用一个样本文本进行了测试，以了解它如何预测情感，我们可以看到它预测了句子的正确情感。</p><p id="7c82" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">你可以在Google Colab <a class="ae mi" href="https://colab.research.google.com/github/paulkarikari/LSTM-sentiment-analysis-with-tensorflow-keras-api/blob/master/Tutorial_sentiment_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>运行整个笔记本，或者在<a class="ae mi" href="https://github.com/paulkarikari/LSTM-sentiment-analysis-with-tensorflow-keras-api/blob/master/Tutorial_sentiment_analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>查看整个笔记本</p><p id="297b" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated"><strong class="kw ir">资源</strong></p><ul class=""><li id="2d2b" class="lp lq iq kw b kx lr lb ls kh lt kl lu kp lv lo lw lx ly lz bi translated"><a class="ae mi" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li id="9b36" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated"><a class="ae mi" href="https://keras.io/examples/imdb_lstm/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/imdb_lstm/</a></li><li id="265c" class="lp lq iq kw b kx ma lb mb kh mc kl md kp me lo lw lx ly lz bi translated"><a class="ae mi" href="https://keras.io/layers/recurrent/" rel="noopener ugc nofollow" target="_blank">https://keras.io/layers/recurrent/</a></li></ul><p id="d7a7" class="pw-post-body-paragraph ku kv iq kw b kx lr kz la lb ls ld le kh mf lg lh kl mg lj lk kp mh lm ln lo ij bi translated">在本教程中，您学习了如何通过Keras API在Tensorflow中使用深度学习LSTM进行情感分析。</p><figure class="mn mo mp mq gt jr"><div class="bz fp l di"><div class="pg ph l"/></div></figure></div></div>    
</body>
</html>