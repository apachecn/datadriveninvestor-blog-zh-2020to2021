<html>
<head>
<title>Approaching The Quora Insincere Question Classification Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora虚假问题分类问题探讨</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/approaching-the-quora-insincere-question-classification-problem-eb27b0ad3100?source=collection_archive---------9-----------------------#2020-08-09">https://medium.datadriveninvestor.com/approaching-the-quora-insincere-question-classification-problem-eb27b0ad3100?source=collection_archive---------9-----------------------#2020-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="gh gi jx"><img src="../Images/3c3901ed0c412e5d3dbde45cd463cee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*att4pqUbOEp4DY17"/></div></div></figure><p id="6aee" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">Quora无诚意问题分类是kaggle在自然语言处理领域组织的一次挑战。挑战的主要目的是找出有毒和分裂的内容。这是一个二元分类问题，其中0类代表不真诚的问题，1类代表不真诚的问题。这个博客将专门讨论数据建模部分。</p><h1 id="d978" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">预处理:</h1><p id="7e0e" class="pw-post-body-paragraph kj kk it kl b km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc mj le lf lg im bi translated">第一步，我们将使用pandas读取数据。这段代码片段会将文件读入熊猫数据帧。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="d1b1" class="mp li it ml b gy mq mr l ms mt">train=pd.read_csv(‘/kaggle/input/quora-insincere-questions-classification/train.csv’)<br/>test_df=pd.read_csv(‘/kaggle/input/quora-insincere-questions-classification/test.csv’)<br/>sub=pd.read_csv(‘/kaggle/input/quora-insincere-questions-classification/sample_submission.csv’)</span></pre><p id="dcc8" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">我们可以用shape方法知道数据的形状。</p><p id="3599" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">最初，我们会尝试将训练数据集分为训练和验证两部分。为此，我们可以借助sklearn。下面的代码片段将帮助我们实现它。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="c5a2" class="mp li it ml b gy mq mr l ms mt">train_df,val_df=train_test_split(train,test_size=0.1)</span></pre><p id="5569" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">在第一步中，我们将尝试填充包含空值的问题。为此我们可以使用<strong class="kl iu"> <em class="mu"> fillna </em> </strong>方法。下面的代码片段会做同样的事情。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="907d" class="mp li it ml b gy mq mr l ms mt">train_x=train_df['question_text'].fillna('__na__').values<br/>val_x=val_df['question_text'].fillna('__na__').values<br/>test_x=test_df['question_text'].fillna('__na__').values</span></pre><p id="84dc" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">现在是选择一些重要参数的时候了。这些是<em class="mu">嵌入尺寸</em>、<em class="mu">最大特征</em>和<em class="mu">最大长度</em>。这里，embedd_size表示我们要表示的每个单词的单词向量大小，而max_features表示我们应该考虑的最高频率单词的数量。例如，如果我们考虑max_feature为50000，这将意味着我们在将50000个最常出现的单词转换成向量时，应该将它们考虑在内。类似地<em class="mu"> max_len </em>将暗示我们将从头开始多少单词。例如<em class="mu"> max_len </em> 100意味着我们只考虑前100个单词。以下是为此选择的参数。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="385c" class="mp li it ml b gy mq mr l ms mt">embedd_size=300<br/>max_features=50000<br/>max_len=100</span></pre><p id="dd62" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">现在考虑下面的代码片段。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="bcd8" class="mp li it ml b gy mq mr l ms mt">tokenizer=Tokenizer(num_words=max_features)<br/>tokenizer.fit_on_texts(list(train_df))<br/>train_x=tokenizer.texts_to_sequences(train_x)<br/>val_x=tokenizer.texts_to_sequences(val_x)<br/>test_x=tokenizer.texts_to_sequences(test_x)</span></pre><p id="434f" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">在这里，我们将在第一行中考虑最常见的50000个单词。第二行将根据每个单词在句子中出现的位置将其转换成一个唯一的数字。第三、第四和第五行中的text_to_sequence方法会将每个句子转换成数字。例如，要转换"<em class="mu">India wind the match</em>"我们将在前面的方法<strong class="kl iu"> <em class="mu"> fit_on_texts </em> </strong>中查找分配给每个单词的整数，并相应地更改句子。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="f6e0" class="mp li it ml b gy mq mr l ms mt">train_x=pad_sequences(train_x,maxlen=max_len)<br/>val_x=pad_sequences(val_x,maxlen=max_len)<br/>test_x=pad_sequences(test_x,maxlen=max_len)</span></pre><p id="2eaa" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">上面的代码片段将确保每个句子都被转换成特定的长度。这样做是为了在成批给出这些序列时，它们将适合特定的长度。因此，需要填充或截短一些序列。</p><div class="mv mw gp gr mx my"><a href="https://www.datadriveninvestor.com/2020/07/31/using-machine-learning-in-brain-computer-interfaces/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">在脑机接口中使用机器学习|数据驱动的投资者</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">神经技术是一个刚刚开始大步前进的前沿领域。有了所有的技术…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm kh my"/></div></div></a></div><h1 id="48d8" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">建模:</h1><p id="b695" class="pw-post-body-paragraph kj kk it kl b km mf ko kp kq mg ks kt ku mh kw kx ky mi la lb lc mj le lf lg im bi translated">为了建立这个分类模型，我们将采用双向lstm。在此之前，我们必须将文本转换成数字。我们在最后一段做了第一步，每个单词都被转换成唯一的整数。下一步，我们将使用keras嵌入层将单词转换成矢量。我们也可以选择使用任何预训练的单词嵌入，但这里我们选择了嵌入层来在训练时学习单词嵌入。嵌入层最初给单词分配随机向量，但是随着模型训练的进行，学习每个单词的嵌入。下面的片段告诉我们整个建模策略。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="b69d" class="mp li it ml b gy mq mr l ms mt">inp=Input(shape=(max_len))<br/>x=Embedding(max_features,embedd_size)(inp)<br/>x=Bidirectional(LSTM(128,return_sequences=<strong class="ml iu">True</strong>))(x)<br/>x=GlobalMaxPool1D()(x)<br/>x=Dense(16,activation='relu')(x)<br/>x=Dropout(0.2)(x)<br/>x=Dense(1,activation='sigmoid')(x)<br/>model=Model(inputs=inp,outputs=x)<br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</span></pre><p id="63f2" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">在嵌入之后，我们将使用双向lstm。这里的参数<strong class="kl iu"><em class="mu">return sequence = True</em></strong>暗示我们希望得到每个隐藏状态的输出。最差的一个是假的，这意味着我们只想要最终状态的输出。类似地，<strong class="kl iu"> <em class="mu"> GlobalMaxPool1D </em> </strong>意味着对于每个句子向量，我们将只取最高值。添加脱落层以处理过拟合。具有sigmoid激活函数的密集层将输出0和1之间的值。compile方法在尚未执行任何训练的情况下构建模型。</p><p id="8285" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">在编译方法中，我们只定义了架构并初始化了它。现在我们必须调整参数，以获得最佳模型。为此，我们通过模型传递训练数据。<em class="mu">拟合</em>方法通过模型传递数据并计算损失。同样基于损耗，它进行反向传播并调整参数。下面的代码片段做了同样的事情。</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="f59e" class="mp li it ml b gy mq mr l ms mt">model.fit(train_x,train_out,batch_size=256,epochs=2,validation_data=(val_x,val_out))</span></pre><p id="2a68" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">经过2个时期的训练，该模型达到了93%的准确率。我们可以改变参数并进行超参数调整以获得更好的模型。</p><p id="a381" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">本次比赛的绩效指标是F1分数。因为数据集是不平衡的。</p><p id="fafd" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">来预测我们可以写的模型</p><pre class="jy jz ka kb gt mk ml mm mn aw mo bi"><span id="f1bf" class="mp li it ml b gy mq mr l ms mt">pred_y=model.predict([test_x],batch_size=256)</span></pre><p id="9e7c" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">完整的代码可以在<a class="ae nn" href="https://github.com/mohantyaditya/quora-insincere-classification/blob/master/quora%20insincere.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/mohantyaditya/quora-ulgency-classification/blob/master/quora % 20 ulgency . ipynb</a>找到</p><p id="4ff5" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated">这是解决问题的一个非常基本的方法。我们可以尝试使用预先训练的嵌入向量来获得更好的结果。</p><p id="550c" class="pw-post-body-paragraph kj kk it kl b km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg im bi translated"><strong class="kl iu">访问专家视图— </strong> <a class="ae nn" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>