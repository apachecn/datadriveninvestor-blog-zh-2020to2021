<html>
<head>
<title>How to Train an Artificial Brain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练一个人工大脑</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-to-train-an-artificial-brain-7704805aeef6?source=collection_archive---------10-----------------------#2020-09-15">https://medium.datadriveninvestor.com/how-to-train-an-artificial-brain-7704805aeef6?source=collection_archive---------10-----------------------#2020-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8bf5ff167048021e6c3baa09fa181044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*us-FkiWvBFs4-AOTNYR8gA.jpeg"/></div></div></figure><p id="8752" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个宇宙中最灵活的东西是什么？苗条身材。杂技演员？拉伸阿姆斯特朗？不对！它实际上就是你现在正在使用的东西——🧠！</p><p id="9935" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想想看，我们人类能够做出疯狂的事情！我们可以计算天体的运动，诊断疾病，发明新技术，这一切都是靠人类的思维。事实上，你头骨里的<strong class="kd iu">硬件</strong> <strong class="kd iu">可以做任何其他人类在</strong>之前做过的事情！</p><p id="96da" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以让我们把同样的力量转移到一个人造系统中。这就是神经网络(NN)的用武之地！他们拥有与我们不相上下的<strong class="kd iu">动力和灵活性</strong>！NNs可以<a class="ae kz" href="https://www.bbc.com/news/health-50857759#:~:text=Artificial%20intelligence%20is%20more%20accurate,images%20from%20nearly%2029%2C000%20women." rel="noopener ugc nofollow" target="_blank">诊断癌症</a>，<a class="ae kz" href="https://www.defenseone.com/technology/2020/08/ai-just-beat-human-f-16-pilot-dogfight-again/167872/#:~:text=An%20AI%20algorithm%20has%20again,adversary%20aircraft%20in%20a%20dogfight." rel="noopener ugc nofollow" target="_blank">在斗狗中击败战斗机飞行员</a>，<a class="ae kz" href="https://openai.com/blog/musenet/" rel="noopener ugc nofollow" target="_blank">制造优美的音乐</a>！</p><p id="3a54" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是大脑和神经网络是如何达到能够掌握如此专门任务的地步的呢？当然是通过训练！💪</p><p id="fb33" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注意</strong>:跟随这篇文章，它是🔑对NNs如何工作有一个扎实的理解<strong class="kd iu">！<a class="ae kz" href="https://medium.com/datadriveninvestor/the-easiest-and-most-intuitive-explanation-of-neural-networks-53162e5ce104?source=friends_link&amp;sk=0739bb1862ff1b875d7a6958f2f730ba" rel="noopener">点击这里阅读我关于神经网络如何工作的文章</a>！</strong></p><h2 id="8379" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">等等，什么是真正的训练？</h2><p id="8720" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当我们训练我们的大脑时，我们学习如何做微积分或如何翻转煎饼。从外面看，我们长得一样！但是在内部，我们的大脑已经重新连接了神经元T21，所以我们可以做这些事情。</p><p id="3a31" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">同样，NNs也通过<strong class="kd iu">增加</strong>或<strong class="kd iu">减少</strong>其值来重新布线其参数。</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ly"><img src="../Images/d3133e72f03399d788d33ed8be956c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ck4O2AhDODFwIb9jd9JVDg.jpeg"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">A neuron has thousands of connections that it has to adjust</figcaption></figure><h1 id="4b32" class="mh lb it bd lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu mv mw lr mx bi translated">训练参数的过程:</h1><p id="81ff" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当训练我们的参数时，我们希望一切都是自动的。我们不能手动调整参数(那会花很长时间)。相反，我们让计算机为我们做这件事，用一个<strong class="kd iu">损失函数</strong>和<strong class="kd iu">随机梯度下降(SGD) </strong>。</p><h2 id="ca80" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">损失函数:</h2><p id="c1be" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">任何培训的一个重要部分就是反馈！你需要教练告诉你什么是对的，什么是错的。对于<strong class="kd iu"> NNs </strong>来说<strong class="kd iu">教练</strong>就是<strong class="kd iu">损失函数</strong>！</p><p id="83c7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">损失函数评估神经网络的性能。它接收神经网络和目标的预测，然后计算一个<strong class="kd iu">损失。</strong>损失有很多种，但是对于所有的损失函数:<strong class="kd iu">损失越低越好</strong>！</p><div class="my mz gp gr na nb"><a href="https://www.datadriveninvestor.com/2020/09/15/a-college-student-used-a-language-generating-ai-tool-to-create-a-viral-blog-post/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">一名大学生使用语言生成人工智能工具创建了一个病毒式博客帖子|数据驱动…</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">作为作家，我们喜欢告诉自己，我们处在一个无法自动化的职业中，至少短期内不会。但是…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np jz nb"/></div></div></a></div><h2 id="e4f2" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">SGD背后的直觉:</h2><p id="4cc9" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">想象你在玩一个游戏:</p><ul class=""><li id="af16" class="nq nr it kd b ke kf ki kj km ns kq nt ku nu ky nv nw nx ny bi translated">你在一个美丽的丘陵地带🌄</li><li id="4a0c" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">你戴着眼罩</li><li id="8a85" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">你被安排在一个随机的位置</li><li id="cced" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">目标:到达地形的最低点</li></ul><p id="9145" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你要去哪个方向？你想尽可能快地到达底部。那会是山的<strong class="kd iu">坡</strong>的方向！朝那个方向迈一步，重新找到斜坡，然后<strong class="kd iu">重复</strong>！继续前进，你最终会找到底部！🙌</p><p id="b0a8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将<strong class="kd iu">损失函数</strong>替换为<strong class="kd iu">草地、</strong>，将<strong class="kd iu">参数</strong>替换为<strong class="kd iu">你、</strong>和<strong class="kd iu">你已经获得了SGD </strong>！参数从损失场的某个随机位置开始。然后他们四处摸索，找到最陡的地方，朝那个方向走。他们继续前进，直到他们找到亏损领域的底部！</p><p id="0486" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">训练=获取参数到底</strong>！当你的参数接近底部时，训练就结束了。</p><p id="9f15" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">🔑不要被这些大牌吓倒。99%的时间他们可以简化下来！(SGD →寻山底之道！)</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/a2851d63069cb2875af8b5407c1ff4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-sk8pkd1yKg9CcpEetRYQ.jpeg"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The Loss Field</figcaption></figure><h2 id="63d4" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">SGD背后的数学原理:</h2><p id="a60a" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">那么我们如何在数学上表示SGD呢？<strong class="kd iu">陡度=导数</strong>！为了找到损失场的<strong class="kd iu">最陡部分</strong>，你对损失函数相对于你的参数求<strong class="kd iu">偏导数。然后你<strong class="kd iu">将</strong>的导数乘以<strong class="kd iu">的学习率</strong>(将在后面解释)来创建<strong class="kd iu">步骤</strong>。然后<strong class="kd iu">从参数中减去</strong>步长，得到更新后的参数。</strong></p><p id="26d2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你必须一遍又一遍地为每个参数重复这个过程。但是不用担心！<strong class="kd iu">计算机很快</strong>，它们可以<strong class="kd iu">自动为我们计算导数</strong>！</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/8d230de57f40e306e315c4dd6cc1dd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*_AZcnr47VdBVMoHGKrq9Yw.png"/></div></div><figcaption class="md me gj gh gi mf mg bd b be z dk">Formula for SGD</figcaption></figure><ul class=""><li id="5f30" class="nq nr it kd b ke kf ki kj km ns kq nt ku nu ky nv nw nx ny bi translated">θⱼ =任何参数</li><li id="00b8" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">α =学习率</li><li id="83b6" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">J(θ) =损失函数</li></ul><h1 id="52ac" class="mh lb it bd lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu mv mw lr mx bi translated">学习率:</h1><p id="e20a" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">学习率是在5个时期和100个时期训练你的模型之间的差别！1 epoch是你训练数据的一个完整循环。选择的时候要用心！👀</p><p id="aab5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">坚持用<strong class="kd iu">类比</strong>，学习率就是你的步伐有多大<strong class="kd iu"/>。它们可以是巨大的进步(高学习率)或微小的步伐(小学习率)。在<strong class="kd iu">数学</strong>方面，学习率是<strong class="kd iu">你用</strong>乘以导数来得到你的步长。👟</p><p id="dca1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">选择学习率是棘手的部分。但是说到底，就<strong class="kd iu">实验</strong>，看看哪个学习率效果最好！</p><p id="9f45" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我自己制作的一些漂亮的gif，向你展示如果你选择了完美、太低、太高和太高的学习率会发生什么！</p><p id="9a3f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">gif上的注释</strong>:绿色抛物线代表希尔/损失场。这些小跳跃代表了我们正在采取的步骤。该标志表示损失函数的底部。</p><h2 id="a23f" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">完美学习率:</h2><p id="cfce" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">这个<strong class="kd iu">很快收敛</strong>到最小值！<strong class="kd iu">步长自然减小</strong>，因为每个点的导数越来越接近0。👍</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bb49edc121f25461f9062e26dbd3bf83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*gLqxhxse9WdZOGn-EPtEpw.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The perfect learning rate</figcaption></figure><h2 id="1751" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">学习率太低:</h2><p id="33c7" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当学习率太低时,<strong class="kd iu">将永远</strong>到达底部。这是因为我们只迈出很小很小的一步，几乎不会让我们有任何改变。结果，你必须训练更多的纪元，这<strong class="kd iu">增加了过度适应</strong>的风险。👎</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4134d17e3deee1e7b9f79e2a9538929c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*1jmwVtW_sEWyIJN3vtvAyw.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The Learning rate is too low</figcaption></figure><h2 id="a00f" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">学习率太高:</h2><p id="017e" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当学习率太高时，T21也要花很长时间才能到达底部。这是因为步长经常超过最小值。这需要太多的历元，这有使<strong class="kd iu">过拟合</strong>的风险。👎</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/75e1c90e9f6b005406e1c5f36dd93afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*kIUHQ6l0-7k2fJ_TqxbXdQ.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The Learning Rate is too high</figcaption></figure><h2 id="7b35" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">学习率太高:</h2><p id="0efd" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当我们的学习率太高时，我们就会偏离方向，永远得不到答案。当你的损失呈指数增长时，你就知道这是在发生了！👎 👎 👎</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bec072f77377a25451717bf794ad96f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*sJeUys9SPHbw_IPEHH5Rrw.gif"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">The Learning Rate is way too high</figcaption></figure></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><h1 id="ff70" class="mh lb it bd lc mi oo mk lf ml op mn li mo oq mq ll mr or mt lo mu os mw lr mx bi translated">度量与损失函数:</h1><p id="a87b" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">当你用PyTorch或TensorFlow训练神经网络时，这两列总是出现。那么它们是什么呢？它们在提供关于神经网络如何工作的反馈方面是相似的，但是它们有两个不同的目的:</p><h2 id="f87f" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">度量:</h2><ul class=""><li id="656f" class="nq nr it kd b ke lt ki lu km ot kq ou ku ov ky nv nw nx ny bi translated"><strong class="kd iu">人为查看</strong>并检查其性能的任意测量</li><li id="6726" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated"><strong class="kd iu">凭直觉挑选</strong></li><li id="7032" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">根据度量的类型，<strong class="kd iu">可以增加或减少</strong></li></ul><h2 id="fdf4" class="la lb it bd lc ld le dn lf lg lh dp li km lj lk ll kq lm ln lo ku lp lq lr ls bi translated">损失函数:</h2><ul class=""><li id="1e76" class="nq nr it kd b ke lt ki lu km ot kq ou ku ov ky nv nw nx ny bi translated">计算机 <strong class="kd iu">对SGD </strong>使用的一种度量<strong class="kd iu"/></li><li id="507d" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">被选中是因为它的<strong class="kd iu">与我们的目标</strong>相似，并且<strong class="kd iu">可以被微分</strong></li><li id="d39f" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated"><strong class="kd iu">应始终减少</strong></li></ul><p id="353a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">等等，为什么它们不能是一样的东西？嗯，他们可以，但通常情况下，他们不是。一个例子将有助于澄清为什么。假设我们的任务是将数字图像分类成数字。度量将是我们的神经网络的<strong class="kd iu">准确度</strong>(#正确/总)。</p><p id="9055" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们用这个作为我们的损失函数，我们会有一些严重的问题。首先，它是<strong class="kd iu">甚至不可微</strong>。就算是，也还是很恐怖。这是因为如果我们将所有参数改变0.001%，我们的<strong class="kd iu">输出将是相同的</strong>。当它们相同时，我们就有大问题了！我们的<strong class="kd iu">导数是0 </strong>。</p><p id="d7f8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这意味着我们被困住了。<strong class="kd iu">步长</strong>将始终为<strong class="kd iu">等于0 </strong>。我们不能动，也没有出路。即使重启，提高学习率，还是有巨大的几率卡死！😖</p><p id="5079" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以我们不用精度来表示损失函数。相反，我们将使用类似<a class="ae kz" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">的二进制交叉熵损失</strong> </a>。<strong class="kd iu">可微</strong>和<strong class="kd iu">参数<strong class="kd iu">的微小变化</strong>将改变损失</strong>。😄</p><figure class="lz ma mb mc gt ju gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/93688f1c54a79b8b1ae75a5b89167894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*Fh6vNJ3R7EGDFekUHHyWuA.png"/></div><figcaption class="md me gj gh gi mf mg bd b be z dk">MNIST Dataset, The Hello World Of AI</figcaption></figure><h1 id="e85d" class="mh lb it bd lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu mv mw lr mx bi translated">批处理:</h1><p id="2f46" class="pw-post-body-paragraph kb kc it kd b ke lt kg kh ki lu kk kl km lv ko kp kq lw ks kt ku lx kw kx ky im bi translated">好了，最后一个话题！到目前为止，每次我们运行神经网络并将预测与目标进行比较时，我们都会<strong class="kd iu">更新参数。但是这是做这件事的最佳方式吗？其实不是！这在计算上<strong class="kd iu">很昂贵</strong>并且给<strong class="kd iu">不好的结果</strong>！👎</strong></p><p id="c806" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，如果你有7000个训练样本，程序必须在一个时期内运行7000次！每次都有成千上万的参数更新，每一个参数都需要几十次运算才能找到导数！</p><p id="5f37" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">电脑很快，但那要花很长时间。再加上一次只训练一个样本导致<strong class="kd iu">颠簸损失场</strong>。这是因为当它训练时，它总是100%正确或0%正确。这导致神经网络<strong class="kd iu">训练不太准确</strong>并导致不良结果。😖</p><p id="6040" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么如何才能修复呢？我们可以<strong class="kd iu">将多个样本</strong>一起批处理，并在更新参数之前对整批样本训练神经网络。这个<strong class="kd iu">解决了速度问题</strong>因为计算机可以并行贯穿整批！此外,<strong class="kd iu">损失字段不那么颠簸,</strong>因为损失都是平均的(不再是100%正确或始终0%正确)。😄</p><h1 id="0518" class="mh lb it bd lc mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu mv mw lr mx bi translated">🔑外卖:</h1><ul class=""><li id="cedd" class="nq nr it kd b ke lt ki lu km ot kq ou ku ov ky nv nw nx ny bi translated"><strong class="kd iu"> NNs通过调整<strong class="kd iu">参数</strong>来训练</strong>🔼和🔽</li><li id="ef1b" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated"><strong class="kd iu">由于<strong class="kd iu">损失函数和SGD </strong>，训练是自动进行的</strong></li><li id="fbe9" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated"><strong class="kd iu">损失函数</strong>就像<strong class="kd iu">丘陵地</strong>，其中<strong class="kd iu"> SGD </strong>是到达底部的<strong class="kd iu">过程</strong></li><li id="c4f4" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated">选择<strong class="kd iu">正确的学习率很重要</strong></li><li id="29bc" class="nq nr it kd b ke nz ki oa km ob kq oc ku od ky nv nw nx ny bi translated"><strong class="kd iu">配料加速</strong>加速<strong class="kd iu">训练</strong>并增加<strong class="kd iu">精度</strong></li></ul></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="5dfa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢阅读！我是Dickson，17岁的技术爱好者，很高兴能加速自己的步伐，影响数十亿人🌎</p><p id="e2f3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你想跟随我的旅程，你可以加入<a class="ae kz" href="https://bit.ly/DicksonNewsletter" rel="noopener ugc nofollow" target="_blank">我的每月简讯</a>，查看<a class="ae kz" href="https://bit.ly/DicksonWebsite" rel="noopener ugc nofollow" target="_blank">我的网站</a>，连接<a class="ae kz" href="https://bit.ly/DicksonLinkedin" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae kz" href="https://bit.ly/DicksonTwitter" rel="noopener ugc nofollow" target="_blank"> Twitter </a>😃</p><p id="af75" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">访问专家视图— </strong> <a class="ae kz" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>