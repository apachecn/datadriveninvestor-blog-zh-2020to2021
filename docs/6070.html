<html>
<head>
<title>ALBERT: A Lite BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艾伯特:小伯特</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/albert-a-lite-bert-d47ac9322d05?source=collection_archive---------3-----------------------#2020-10-11">https://medium.datadriveninvestor.com/albert-a-lite-bert-d47ac9322d05?source=collection_archive---------3-----------------------#2020-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c02c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f9e8415dc87448d4f87423a334b2a747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mGd-Q7CPyXYwNLIl"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@prateekkatyal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Prateek Katyal</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ddde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT  pretraining是语言建模的先驱。自那以后，NLP的技术水平一直在发展。然而，该公约说，较大的模型表现更好。但是，大型模型阻碍了扩展。训练他们既困难又昂贵。而且，训练速度随着模型规模的增大而降低。</p><p id="2336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论谷歌人工智能在论文中提出的艾伯特模型，“<a class="ae ky" href="https://arxiv.org/pdf/1909.11942.pdf" rel="noopener ugc nofollow" target="_blank">艾伯特:一个用于语言表示的自我监督学习的Lite BERT。</a>“本文主要提出了两种针对原始BERT架构的参数缩减技术(以克服上述问题):</p><ol class=""><li id="3516" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">因子分解嵌入参数化</li><li id="8a3f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">跨层参数共享</li></ol><p id="0124" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时保持性能</p><p id="f12b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，为了提高性能，提出了一个自监督的句子顺序预测目标。这解决了伯特的NSP任务的无效性。</p><h1 id="9c45" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">因子分解嵌入参数化</h1><p id="618e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在BERT预训练中，我们本质上取一个大小为<strong class="lb iu">T5 Vx<em class="ng">E</em>T9】的嵌入矩阵，其中<strong class="lb iu">T11】VT13】为<strong class="lb iu">vocab _ size，T17<strong class="lb iu">E</strong>为<strong class="lb iu"><em class="ng">embedding _ dim</em></strong>。而这里，<strong class="lb iu"> <em class="ng"> H = E，</em> </strong>其中<strong class="lb iu"> <em class="ng"> H </em> </strong>就是<strong class="lb iu"> <em class="ng"> hidden_dim </em> </strong>。</strong></strong></strong></p><p id="a051" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，单词(或单词块)嵌入的主要工作是学习标记的<strong class="lb iu">上下文无关的</strong>表示。另一方面，隐藏层嵌入的工作是学习记号的<strong class="lb iu">上下文相关的</strong>表示。它模糊地意味着单词嵌入学习捕获标记之间的对应关系，而不管数据的分布如何，并且隐藏层嵌入学习捕获标记之间的模式，用于它正在被训练的特定分布。</p><p id="048b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，对于更大的<strong class="lb iu"> <em class="ng">、隐藏_尺寸</em> </strong>，该模型将捕获更高的上下文信息。然而，对于标准的参数化，这将是嵌入矩阵的巨大开销。因此，该论文的作者提出了一种参数化技术来分离单词和隐藏层嵌入。想法是将<strong class="lb iu"> <em class="ng"> vocab_size </em> </strong>长度的单热向量投影到一个小得多的<strong class="lb iu"> <em class="ng"> embedding_dim </em> </strong>大小的向量(比如128)。然后，将这个矢量投影到一个大得多的<strong class="lb iu"> <em class="ng"> hidden_dim </em> </strong>大小的矢量(比如768)。算算吧！</p><blockquote class="nh ni nj"><p id="9982" class="kz la ng lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">因此，我们正在优化参数的数量，从<strong class="lb iu"> <em class="it"> O(V x H) </em> </strong>到<strong class="lb iu"> <em class="it"> O(V x E + E x H) </em> </strong>。如果<strong class="lb iu"><em class="it"/></strong>比<strong class="lb iu"> <em class="it"> E </em> </strong>大得多，这就产生了巨大的差异。</p></blockquote><div class="nn no gp gr np nq"><a href="https://www.datadriveninvestor.com/2020/09/02/artificial-intelligence-helps-you-be-smart-with-money/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">人工智能帮助你聪明理财|数据驱动的投资者</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">自动化和人工智能为创新平台提供动力，简化双方的财务流程…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ks nq"/></div></div></a></div><h1 id="63b2" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">跨层参数共享</h1><p id="1216" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">有几种方法可以跨层共享权重:可以对所有前馈层共享权重，也可以对注意力层共享权重。</p><blockquote class="nh ni nj"><p id="a720" class="kz la ng lb b lc ld ju le lf lg jx lh nk lj lk ll nl ln lo lp nm lr ls lt lu im bi translated">ALBERT跨层共享其所有参数。</p></blockquote><p id="e438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下比较表明，权重共享不仅减少了模型中的参数，而且有助于稳定网络参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/87ccca0988c24940f31d30092575ad70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ardzqsoJbkfBIAN2MGn8pw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">L2 Distances via <a class="ae ky" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">ALBERT Paper</a></figcaption></figure><p id="9dd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左图比较了每层的输入和输出嵌入<strong class="lb iu">之间的L2距离。右边的图测量余弦相似性/距离。尽管它们甚至在24层之后也不收敛到0，但是在ALBERT的情况下的平滑曲线表明参数稳定。</strong></p><h1 id="079a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">句子顺序预测(SOP)目标</h1><p id="e329" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">伯特建议，除了掩蔽语言建模(MLM)目标之外，还要进行下一个句子预测(NSP)任务的预训练。然而，最近的研究表明这一目标的无效性。艾伯特的作者推测，这可能是因为它作为一项任务缺乏难度。</p><p id="4ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NSP将话题预测和连贯预测结合在一项任务中，因此简单的话题预测目标掩盖了连贯。为了克服这一点，ALBERT接受了一项名为句子顺序预测(SOP)的纯连贯性任务的训练。在SOP中，就像BERT一样，附加两个连续的句子。但是在这里，正样本是正确顺序的句子，负样本是相反顺序的句子。因此，目标是预测两个连续句子在语料库中的出现顺序是否正确。</p><p id="f0eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实验表明，NSP根本不能解决SOP目标。鉴于SOP在NSP目标上产生了合理的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/ac9adb2220f04dcf02a88bd5e0ffd3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3URrdL25MMr7WwwEx4hUAw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">NSP on SOP and vice-versa via <a class="ae ky" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">ALBERT Paper</a></figcaption></figure><h1 id="739d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结果</h1><p id="b5a9" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">评估该模型的语言理解能力的最好方法之一是在阅读和理解测试中测试它。为此，使用了<a class="ae ky" href="https://www.aclweb.org/anthology/D17-1082/" rel="noopener ugc nofollow" target="_blank">比赛数据集</a>。</p><p id="3bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当在BERT数据集(维基百科和书籍)上训练时，ALBERT-xxlarge(比BERT-large更少的参数)产生与BERT相同范围内的分数(82.3)。然而，当它在用于<a class="ae ky" href="https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710" rel="noopener" target="_blank"> XLNET </a>和<a class="ae ky" href="https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946" rel="noopener"> RoBERTa </a>的更大数据集上训练时，性能大大提高，并且ALBERT达到了新的艺术水平分数(89.4):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c87df7e2df88f6f2a2f08fb499c10d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*OCkXh124_tZXLEru.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Performance on RACE (Random Guess Baseline Score is 25 and the Maximum Score is 95) via <a class="ae ky" href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener ugc nofollow" target="_blank">Google AI Blog</a></figcaption></figure><h1 id="182a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="cef0" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在本文中，我们看到了原始BERT预训练目标的另一个修改，以及它如何能够击败所有竞争对手，同时确保计算效率。</p><h1 id="3f82" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><div class="nn no gp gr np nq"><a href="https://arxiv.org/abs/1909.11942" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">ALBERT:一个用于语言表达自我监督学习的Lite BERT</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">在预训练自然语言表示时，增加模型的大小通常会导致在…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="nn no gp gr np nq"><a href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd iu gy z fp nv fr fs nw fu fw is bi translated">ALBERT:一个用于语言表达自我监督学习的Lite BERT</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">自从一年前BERT问世以来，自然语言研究已经采用了一种新的范式，利用了大量的…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">ai.googleblog.com</p></div></div><div class="nz l"><div class="oi l ob oc od nz oe ks nq"/></div></div></a></div><h2 id="96d0" class="oj mk it bd ml ok ol dn mp om on dp mt li oo op mv lm oq or mx lq os ot mz ou bi translated">访问专家视图— <a class="ae ky" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank">订阅DDI英特尔</a></h2></div></div>    
</body>
</html>