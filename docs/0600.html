<html>
<head>
<title>Firth’s Logistic Regression: Classification with Datasets that are Small, Imbalanced or Separated</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">弗斯的逻辑回归:用小的、不平衡的或分离的数据集分类</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/firths-logistic-regression-classification-with-datasets-that-are-small-imbalanced-or-separated-49d7782a13f1?source=collection_archive---------0-----------------------#2020-02-07">https://medium.datadriveninvestor.com/firths-logistic-regression-classification-with-datasets-that-are-small-imbalanced-or-separated-49d7782a13f1?source=collection_archive---------0-----------------------#2020-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9cbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据科学家有一系列巧妙编程的分类算法，当数据集相对较大且运行良好时，这些算法工作得非常好。不幸的是，大型、无问题的数据集是例外，而不是医学研究、经济分析、政治预测和许多其他领域的标准，在这些领域，定量见解特别有价值。</p><p id="0a90" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，这并不意味着为处理不太理想的数据集而设计的算法不存在，只是它们不为人所知。这是一系列文章中的第一篇，这些文章将阐明一系列基于惩罚最大似然估计的方法。</p><div class="ko kp gp gr kq kr"><a href="https://www.datadriveninvestor.com/2019/01/23/which-is-more-promising-data-science-or-software-engineering/" rel="noopener  ugc nofollow" target="_blank"><div class="ks ab fo"><div class="kt ab ku cl cj kv"><h2 class="bd iu gy z fp kw fr fs kx fu fw is bi translated">数据科学和软件工程哪个更有前途？数据驱动的投资者</h2><div class="ky l"><h3 class="bd b gy z fp kw fr fs kx fu fw dk translated">大约一个月前，当我坐在咖啡馆里为一个客户开发网站时，我发现了这个女人…</h3></div><div class="kz l"><p class="bd b dl z fp kw fr fs kx fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="la l"><div class="lb l lc ld le la lf lg kr"/></div></div></a></div><p id="645f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们今天的话题是<strong class="js iu">弗斯的逻辑。</strong>弗斯的logit于1993年由华威大学教授大卫·费斯创建，旨在解决标准最大似然估计可能出现的问题，但已发展成为一种通用工具，用于减少分类模型中的偏差。它最常用于在处理三种有问题的数据集时获得更好的结果:</p><ol class=""><li id="640e" class="lh li it js b jt ju jx jy kb lj kf lk kj ll kn lm ln lo lp bi translated"><strong class="js iu">小型数据集</strong></li><li id="c556" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn lm ln lo lp bi translated"><strong class="js iu">不平衡数据集</strong></li><li id="d25c" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn lm ln lo lp bi translated"><strong class="js iu">分离的数据集</strong></li></ol><p id="961b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在接下来的部分中，我将介绍这些对分类提出的挑战的性质，深入研究Firth的Logit背后的理论，并解释如何用Python和r。</p><p id="af58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(<a class="ae lv" href="https://medium.com/@remycanario17/log-f-m-m-logit-the-best-classification-algorithm-for-small-datasets-fc92fd95bc58" rel="noopener">关于最佳小数据集分类算法的讨论，请参阅本系列的第二部分Log-F(m，m)逻辑回归</a>)</p><h1 id="0f23" class="lw lx it bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">问题是</h1><h2 id="b1fa" class="mu lx it bd ly mv mw dn mc mx my dp mg kb mz na mk kf nb nc mo kj nd ne ms nf bi translated">小样本量</h2><p id="19ef" class="pw-post-body-paragraph jq jr it js b jt ng jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj nk kl km kn im bi translated">小样本量存在两个主要问题。首先，小样本不太可能准确反映人口分布，如果不这样，它们产生的模型就不能很好地概括。</p><p id="0927" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其次，支撑许多主要机器学习分类算法的最大似然估计(MLE)技术是一种渐近一致的估计器，这意味着它只有在应用于大型数据集时才是无偏的。</p><p id="0d56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个问题都会影响模型预测的质量。例如，看看Kaggle的信用卡欺诈数据集在不同样本量下的逻辑回归模型的结果。不仅准确性分数大幅下降，而且与几个小样本的平均值相比，差异也很大，这表明单个小样本的输出是多么不可靠。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nl"><img src="../Images/c7091cfa13e4559f09af1a3ef2c4ed15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oq0uwGyv78uIS1tf6wVJDw.png"/></div></div></figure><p id="2b41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你关心一个事件的概率，问题会变得更糟，比如在计算信用风险时。正如你所看到的，随着样本量减少到300以下，我们刚刚看到的相同模型的二元交叉熵增加了10到70倍。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nw"><img src="../Images/c618b824bfffc5d8dacbf6c772d94a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tymJbahheeFZ_m2dnMEV_w.png"/></div></div></figure><p id="5429" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，小样本量对模型最有害的影响与推理有关。小样本会产生系数绝对值过大的模型。看一下这个表，它显示了与大样本量基线相比，逻辑回归系数大小总和的百分比变化。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nx"><img src="../Images/0e72abfb3e15a653ab6f71f5e3f434ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lrUGok6Cpt2JpHbpalS9rA.png"/></div></div></figure><p id="c5f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型的边际效应表现出相似的行为，这意味着我们不能相信小样本能给我们一个X变量对预测的真实影响的准确估计。</p><p id="1460" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我使用逻辑回归获得了这些结果，但根据经验研究，在处理小数据集时，其他标准机器学习工具(特别是神经网络、决策树、k-最近邻、支持向量机、随机森林)都没有逻辑回归提供一致的优势。</p><h2 id="5c03" class="mu lx it bd ly mv mw dn mc mx my dp mg kb mz na mk kf nb nc mo kj nd ne ms nf bi translated">不平衡数据集</h2><p id="99cf" class="pw-post-body-paragraph jq jr it js b jt ng jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj nk kl km kn im bi translated">即使有大量的观察值，如果数据集的某一种结果比另一种结果多得多，您也不太可能从标准分类算法中获得有用的结果。这是因为当数据中有0.0017%的少数类时，通过预测所有输入都属于多数类，正常ML算法使用的损失函数可能会最小化。</p><p id="a60c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有几种处理不平衡数据集的更广为人知的技术，包括欠采样、使用合成少数过采样技术(SMOTE)进行数据扩充，以及按类对样本进行加权。这些可能是有效的，但是根据我的经验，Firth的logit产生的好结果更稳定(尽管它通常比我将在下一篇文章中介绍的Firth的logit的修改要好)。</p><h2 id="d749" class="mu lx it bd ly mv mw dn mc mx my dp mg kb mz na mk kf nb nc mo kj nd ne ms nf bi translated">完全或准完全分离</h2><p id="091f" class="pw-post-body-paragraph jq jr it js b jt ng jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj nk kl km kn im bi translated">完全分离和准完全分离是指您所拥有的数据可以完美预测样本中每个响应的情况。</p><p id="b631" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(数学上，当你有一个预测因子β，使得所有事件的x`β &gt;为0，所有非事件的x`β &lt;为0时，完全分离发生。准完全是≥/≤等价。)</p><p id="b186" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可以将完全分离视为一种特别严重的过度拟合情况:模型不会告诉您任何关于样本之外的数据的有用信息，因为它的系数趋向于(负)无穷大，并且每个预测要么是1，要么是0。那是如果你得到结果的话:当数据完全或准完全分离时，依赖迭代过程的算法通常根本不起作用(即不收敛)。</p><h1 id="c296" class="lw lx it bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">解决方案</h1><p id="3ab3" class="pw-post-body-paragraph jq jr it js b jt ng jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj nk kl km kn im bi translated">这是逻辑回归的悠久历史使其优于其他分类器的一个例子:这些问题对logit的影响已经得到了广泛的研究，并且存在许多修正它们的修正。</p><p id="500a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于逻辑回归，使用:</p><ul class=""><li id="645a" class="lh li it js b jt ju jx jy kb lj kf lk kj ll kn nz ln lo lp bi translated"><strong class="js iu">小样本量:</strong>系数太大，预测过于自信</li><li id="4412" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn nz ln lo lp bi translated"><strong class="js iu">完全分离</strong>:模型不收敛</li><li id="dc46" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn nz ln lo lp bi translated"><strong class="js iu">罕见事件</strong>:预测偏向于零</li></ul><p id="ee25" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那我们该怎么办？</p><p id="1d81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一种解决方案是完全避免MLE，并使用马尔可夫链蒙特卡罗来估计模型。这种被称为<a class="ae lv" href="http://resource.heartonline.cn/20150528/1_3kOQSTg.pdf" rel="noopener ugc nofollow" target="_blank">精确逻辑</a>的方法非常精确，但是对于超过50个观察值和/或超过少数几个特征的数据集来说，它的计算强度达到了不可行的程度。</p><p id="47f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一种方法，惩罚最大似然估计(PMLE)，通过引入抵消偏差的惩罚来以毒攻毒。</p><p id="c434" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">标准逻辑回归通过最大化以下对数似然函数进行操作:</p><blockquote class="oa"><p id="64fe" class="ob oc it bd od oe of og oh oi oj kn dk translated">ℓ(β)=σ[yᵢlog(πᵢ)+(1yᵢ)对数(1πᵢ)]</p></blockquote><p id="b97d" class="pw-post-body-paragraph jq jr it js b jt ok jv jw jx ol jz ka kb om kd ke kf on kh ki kj oo kl km kn im bi translated">顾名思义，惩罚最大似然估计给该函数增加了一个惩罚:</p><blockquote class="oa"><p id="d4aa" class="ob oc it bd od oe of og oh oi oj kn dk translated">ℓ(β)=σ[yᵢlog(πᵢ)+(1yᵢ)对数(1πᵢ)]+罚分</p></blockquote><p id="9b56" class="pw-post-body-paragraph jq jr it js b jt ok jv jw jx ol jz ka kb om kd ke kf on kh ki kj oo kl km kn im bi translated">眼熟吗？它应该是因为这是我们用来实现L2(山脊)和L1(拉索)正则化的相同的基本公式，事实上，L2经常可以在处理我们正在谈论的偏见方面做得很好，特别是如果我们对准确性分数比对交叉熵更感兴趣的话。</p><p id="3959" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也就是说，正则化只是我们问题的部分解决方案，因为除了缩小系数之外，我们还关心如何使预测更加保守，这就是弗斯的逻辑发挥作用的地方。它使用费希尔信息矩阵的行列式的平方根作为罚函数，当βs = 0且预测值= 0.5(最大不确定性)时，罚函数最大。</p><blockquote class="oa"><p id="ecba" class="ob oc it bd od oe of og oh oi oj kn dk translated">弗斯(β) =ℓ(β) + 0.5log[det I(β)]</p></blockquote><p id="98c7" class="pw-post-body-paragraph jq jr it js b jt ok jv jw jx ol jz ka kb om kd ke kf on kh ki kj oo kl km kn im bi translated">对这种选择惩罚(它从系数中去除了O(n^−1偏差)的频率主义论证是相当技术性的，但是贝叶斯解释是直观的:</p><p id="11dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">0.5log[det I(β)]相当于Jeffrey的不变先验，可以认为是数据包含的信息量的倒数，所以把它加到对数似然函数中就意味着系数会随着我们的无知程度成比例缩小。</p><figure class="nm nn no np gt nq gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi op"><img src="../Images/e6d23229e12ce1ef05e48ad4a646c044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mf-DLg1hfLvzWGDdZtI2dQ.png"/></div></div><figcaption class="oq or gj gh gi os ot bd b be z dk">If the coefficients are too big, let’s make them smaller by using U* instead of U (figure from <a class="ae lv" href="https://www.stat.sfu.ca/content/dam/sfu/stat/alumnitheses/2016/Wen%2C%20Jiying%20finalversion.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ou">Penalized Logistic Regression in Case-Control Studies</em></a><em class="ou"> by Wen Jiying)</em></figcaption></figure><p id="ea70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了将此付诸实践，我们替换正常的得分函数:</p><blockquote class="oa"><p id="4171" class="ob oc it bd od oe of og oh oi oj kn dk translated">u(β)=(yπ)x</p></blockquote><p id="40d1" class="pw-post-body-paragraph jq jr it js b jt ok jv jw jx ol jz ka kb om kd ke kf on kh ki kj oo kl km kn im bi translated">使用:</p><blockquote class="oa"><p id="4d9f" class="ob oc it bd od oe of og oh oi oj kn dk translated">u *(β)=[yπ+h(0.5π)]x</p></blockquote><p id="016a" class="pw-post-body-paragraph jq jr it js b jt ok jv jw jx ol jz ka kb om kd ke kf on kh ki kj oo kl km kn im bi translated">其中:</p><ul class=""><li id="1c28" class="lh li it js b jt ju jx jy kb lj kf lk kj ll kn nz ln lo lp bi translated">π =模型的预测</li><li id="cc89" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn nz ln lo lp bi translated">h =帽子矩阵的对角线= √W X inv(I) X.t √W</li><li id="5b5d" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn nz ln lo lp bi translated">W = diag[ π(1-π)]</li><li id="94e1" class="lh li it js b jt lq jx lr kb ls kf lt kj lu kn nz ln lo lp bi translated">I =费希尔信息矩阵= X.tWX</li></ul><p id="bcc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">帽子矩阵的对角线代表每行观察对模型的影响(也称为其<em class="ny">杠杆)</em>。因此，除了最小化残差之外，我们还考虑每行对模型贡献的信息量，以及预测与0.5的距离。</p><p id="b47b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，弗斯的逻辑并非没有问题。其中最主要的是它缩小了截距，这是不可取的，因为截距是其它系数值的直接函数，而且真实截距值很少接近零。此外，弗斯惩罚对优势比的影响是不透明的，这使得模型更难解释。</p><p id="7f55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，毫无疑问，它成功地实现了为问题数据集生成更小的系数和更保守的预测的目标。我将在另一篇文章中对各种分类器的性能进行综合比较，但使用我们之前看到的Kaggle数据进行的快速后台比较显示，在二进制交叉熵和准确度分数开始衰减到200之前，弗斯的logit减少了所需的观察次数，之后它们下降得相当慢。与此同时，弗斯系数的大小以平均比标准逻辑回归低六倍的速度增长。</p><h1 id="d9fb" class="lw lx it bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">履行</h1><p id="6222" class="pw-post-body-paragraph jq jr it js b jt ng jv jw jx nh jz ka kb ni kd ke kf nj kh ki kj nk kl km kn im bi translated">r有几个包可以为你实现Firth的logit，包括brglm和logistf。要使用brglm包实现这一点，只需在指定模型时将pl参数设置为true。</p><pre class="nm nn no np gt ov ow ox oy aw oz bi"><span id="0c42" class="mu lx it ow b gy pa pb l pc pd">brglm(formula, data = df, family=’binomial’, pl=True)</span></pre><p id="541f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我还没有找到用Python实现Firth的logit的包，但是从头开始编码并不是特别难。下面是计算第一次预测的基本函数:</p><pre class="nm nn no np gt ov ow ox oy aw oz bi"><span id="d47f" class="mu lx it ow b gy pa pb l pc pd">import pandas as pd<br/>import numpy as np</span><span id="03cb" class="mu lx it ow b gy pe pb l pc pd">def firth_logit(X,y,num_iter=5000,learning_rate=0.01):</span><span id="5407" class="mu lx it ow b gy pe pb l pc pd">    #I<strong class="ow iu">nitialize weights</strong><br/>    weights = np.ones(X.shape[1])<br/>    <br/>    #<strong class="ow iu">Define get_predictions function<br/>    </strong>def get_predictions(X,weights):<br/>        z = np.dot(X,weights)<br/>        y_pred =  1/(1 + np.exp(-z))</span><span id="0afe" class="mu lx it ow b gy pe pb l pc pd">    #<strong class="ow iu">Perform gradient descent</strong><br/>    for i in range(num_iter):<br/>        <br/>        y_pred = get_predictions(X,weights)<br/>        <br/>        #<strong class="ow iu">Calculate Fisher information matrix</strong><br/>        Xt = X.transpose()<br/>        W = np.diag(y_pred*(1-y_pred))<br/>        I = np.linalg.multi_dot([Xt,W,X])<br/>        <br/>        #<strong class="ow iu">Find diagonal of Hat Matrix</strong><br/>        sqrtW = W**0.5<br/>        H = np.linalg.multi_dot([sqrtW,X,np.linalg.inv(I),Xt,sqrtW])<br/>        hat_diag = np.diag(H)<br/>        <br/>        #<strong class="ow iu">Calculate U_star</strong><br/>        U_star = np.matmul((y -y_pred + hat_diag*(0.5 - y_pred)),X)<br/>        <br/>        #<strong class="ow iu">Update weights</strong><br/>        weights += np.matmul(np.linalg.inv(I),U_star)*learning_rate<br/>    <br/>    #<strong class="ow iu">Get final predictions</strong><br/>    y_pred =  get_predictions(X,weights)</span><span id="120e" class="mu lx it ow b gy pe pb l pc pd">    return y_pred</span></pre><p id="003e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">瞧啊。如果你想要一个截距，你需要在你的数据集中添加一列1，否则你就一切就绪了。</p><p id="189f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用这段代码的一个重要注意事项是:该算法涉及的线性代数是计算密集型的，因此如果您计划将Firth的logit应用于一个大型不平衡/分离的数据集，我建议将您的数据分割成块，分别建模，并对输出进行平均。这些块应该有多大取决于你的电脑，但15，000行似乎是我的16g内存的极限。</p><p id="e80e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在你知道了。</p><p id="447c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lv" rel="noopener ugc nofollow" target="_blank" href="/(For a review of the best small dataset classification algorithm, head to pt. 2 of this series on the Log-F(m,m) Logistic Regression)">本系列的下一篇文章将讨论Log-F(m，m)逻辑回归，</a>小数据集的最佳分类算法，之后我将介绍弗斯逻辑回归的三个衍生版本，它们旨在更好地处理不平衡数据集/罕见事件。</p><p id="d54c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi">_______</p><p id="57a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来源:</p><p id="87bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">弗斯，大卫。"最大似然估计的偏差减少."<em class="ny">生物信息学</em>，第80卷，第1期，1993年，第27–38页。</p><p id="acbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">龙j .斯科特。"分类和有限因变量的回归模型."Sage出版公司，1997年。</p><p id="92b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">何晓乐，卡莱尔和凯利·麦卡斯基。"用小样本估计Logit模型."2017.</p><figure class="nm nn no np gt nq"><div class="bz fp l di"><div class="pf pg l"/></div></figure></div></div>    
</body>
</html>