<html>
<head>
<title>Things Behind the Decision Tree Method- Algorithm, Model Building and Issues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树方法背后的东西——算法、模型建立和问题</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/things-behind-the-decision-tree-method-algorithm-model-building-and-issues-199670478a6f?source=collection_archive---------2-----------------------#2020-04-27">https://medium.datadriveninvestor.com/things-behind-the-decision-tree-method-algorithm-model-building-and-issues-199670478a6f?source=collection_archive---------2-----------------------#2020-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/bc75733f0233ec1baf6e70fcbd2d62d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xg700IhqpmAhtFadwS7fvw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@nahakiole?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Robin Glauser</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="53a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">什么是决策树？</strong>使用树状模型进行分类和回归的监督学习方法。它有助于找到大量候选输入变量和目标变量之间的关系。然而，这是一种贪婪的算法，不能产生使误差最小化的最优决策树。</p><p id="2f49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树使用带有if-then-else决策规则的变量将大量记录划分为连续的较小记录集。目标是探索训练数据，并通过将大量异质群体清晰地分成更小且更同质的群体(即，杂质的最大减少)来构建模型。然后将相同的模型应用于测试数据，以预测特定的目标变量。</p><h1 id="0692" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">通过简单的例子理解决策树</h1><p id="c9f0" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了解释决策树算法的思想，我将从一个简单的二进制分类问题开始。我构建了一个包含4列和10条记录的训练数据集。“以前被炒过”、“学位水平”、“工作经验”是自变量，“拿到Offer”是我们的目标变量。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/d1588280d90b1e15bdb02013a66c3152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vx5LNdgO8vamPBI05iD3Rw.jpeg"/></div></div></figure><p id="3d5e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们构建决策树来分割数据，直到没有剩余的属性用于进一步划分，这意味着，通常，其中一个类别达到0。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/e021e952dd41b27aa76b19b488f462be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-lAneVYfTqH9me7E-QqBQ.jpeg"/></div></div></figure><p id="c06e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个图表中，树中的每条路径都是一条规则(从根到叶)，所以在这个树中有5条规则。然后我们把这些规则放在我们想要预测的数据上。</p><ul class=""><li id="2f4b" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">R1(否):已被解雇+工作经验&lt; 3 years</li><li id="aba5" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">R2 (Yes): has been fired + working experience &gt; 3年</li><li id="f13e" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">R3(是):从未被解雇+硕士或博士学位</li><li id="1e8a" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">R4(否):从未被解雇+学士学位+3年工作经验</li><li id="eeee" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">应用上述规则，我们可以得出这样的结论:一个人是否获得offer的结果从第11行到第15行:“是”、“是”、“否”、“是”、“是”。这只是解决方案之一，可能不是最佳方案，但它解释了算法背后的一般思想。</li></ul><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/3fcc66a181333d03c9e9d39d44658291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMamCPxbb5O2DmZ8EFgAkA.jpeg"/></div></div></figure><p id="6657" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用Python构建决策树模型</p><h1 id="857e" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">在手工构建决策树之后，我现在使用相同的数据，并用Python来做。</h1><p id="e8c1" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">由于<a class="ae kc" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" rel="noopener ugc nofollow" target="_blank"> DecisionTreeClassifier() </a>通过考虑和计算准则、分裂器、树的深度、样本叶数等来确定分裂变量的顺序。，我们得到了与手动做不同的预测:“不”，“是”，“不”，“是”，“不”。也可以改变参数或参数组合以期望更好的性能。</p><pre class="mf mg mh mi gt my mz na nb aw nc bi"><span id="e581" class="nd lc iq mz b gy ne nf l ng nh">import pandas as pd<br/>import graphviz<br/>from sklearn import tree<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.tree import export_graphviz</span><span id="512f" class="nd lc iq mz b gy ni nf l ng nh">#create train and test data<br/>train = pd.DataFrame({'Fired': list('YNNNYNNYNN'), 'Degree': list('BMBMMBBBDB'), 'Experience': [1,0,6,6,2,8,10,4,15,1], 'Offer': list('NYYYNYYYYN')})<br/>test = pd.DataFrame({'Fired': list('NYNNN'), 'Degree': list('DBBMM'), 'Experience': [2,5,2,7,1]})</span><span id="d5f7" class="nd lc iq mz b gy ni nf l ng nh">#one hot encoding for categorical variables<br/>train_copy = train.iloc[:, :-1]<br/>categoricalFeatures = ['Fired', 'Degree']<br/>combine = pd.concat([train_copy, test], keys=[0,1])<br/>combine = pd.get_dummies(combine, columns=categoricalFeatures)</span><span id="bf15" class="nd lc iq mz b gy ni nf l ng nh">#seperate features and target variable<br/>xtrain = combine.xs(0)<br/>ytrain = train['Offer']<br/>xtest = combine.xs(1)</span><span id="8b29" class="nd lc iq mz b gy ni nf l ng nh">#bulid default model<br/>clf = DecisionTreeClassifier()<br/>clf.fit(xtrain, ytrain)<br/>clf_pred = clf.predict(xtest)</span></pre><p id="b413" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是模型中的树形图。注意，我们可以在树生长时再次使用同一个分裂变量(本例中为“Experience”)。此外，为了确定最佳分割，我们不仅试图使标准值(本例中的Gini)变得更低，显示出低程度的不纯，而且还希望我们的节点尽可能均匀分布，以便结果大部分聚集在同一类中。</p><pre class="mf mg mh mi gt my mz na nb aw nc bi"><span id="0a22" class="nd lc iq mz b gy ne nf l ng nh">#visualize decision tree<br/>graph_tree = export_graphviz(clf, max_depth=3, feature_names=xtrain.columns, filled=True, rounded=True)<br/>graph = graphviz.Source(graph_tree)</span></pre><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/c8d161324c720967233c3c6e6abd26f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOBZ2g2sUsetCHEiMRJ8EA.jpeg"/></div></div></figure><p id="3aad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">处理过拟合问题的方法</p><h1 id="6caa" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">在使用决策树方法进行分类时，我们可能会面临两个问题:欠拟合和过拟合，尤其是后者。当开发的模型过于简单时，会出现欠拟合问题，导致训练误差和测试误差都很大。相比之下，当模型过于复杂时，会出现过拟合问题，但实际上并不应该如此，这使得训练误差较小，而测试误差较大。</h1><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nk"><img src="../Images/0dadb511280a8c0ce1cfa8267fe410eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LclkHXMxXz4DWJ8uzaChA.png"/></div></div></figure><p id="6297" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">公平算法|数据驱动的投资者</p><div class="nl nm gp gr nn no"><a href="https://www.datadriveninvestor.com/2020/02/22/algorithms-for-fairness/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">许多人都有算法偏见。软件工程师关心算法偏差，因为我们关心…</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">www.datadriveninvestor.com</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">决策树容易过度拟合的原因是我们没有限制最大深度。因为它具有无限的灵活性，所以它可以不断增长，直到对于每一次观察都有一个叶子节点。为了解决过度拟合问题，除了改变模型的参数之外，我们还可以通过仅选择有效变量来进行修剪过程。</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc jw no"/></div></div></a></div><p id="c59a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">预修剪</strong>是在决策树变成完全成长的树之前停止的修剪过程。也就是在划分之前，对每个节点进行估算。如果当前节点的划分不能提高树的泛化性能(熵或基尼指数)，则停止划分，并将当前节点标记为结束节点(叶)。</p><p id="e2fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">后期修剪</strong>是在决策树成长为整体后进行修剪。从树枝(树叶)末端到树干(屋顶)方向对树进行调查。如果在修剪后泛化误差得到改善，那么这种修剪用一个结束节点(叶子)代替子树。</p><p id="5592" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策树的影响</p><h1 id="0135" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">优势:</h1><p id="f135" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">易于理解和解释</p><ul class=""><li id="df08" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">应用于实际问题，通常有助于绘制业务规则</li><li id="beab" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">不需要准备数据，对缺失值不敏感</li><li id="acb0" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">能够处理数值和分类数据，而其他方法需要数据类型的一致性</li><li id="fb75" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">缺点:</li></ul><p id="9c11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关联的结果会使计算变得非常复杂</p><ul class=""><li id="b492" class="mk ml iq kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">需要对时间顺序数据做更多的预处理</li><li id="5ce6" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">数据的微小变化会导致最佳决策树结构的巨大变化</li><li id="9041" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">对于样本大小不一致的数据，结果可能偏向于具有更多值的那些要素</li><li id="67d8" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">如果类别太多，会给出相对不准确的结果，而其他方法在处理类似数据时会表现得更好</li><li id="d455" class="mk ml iq kf b kg mt kk mu ko mv ks mw kw mx la mp mq mr ms bi translated">关于我</li></ul></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><h1 id="54d4" class="lb lc iq bd ld le ok lg lh li ol lk ll lm om lo lp lq on ls lt lu oo lw lx ly bi translated">嘿，我是凯莉。我喜欢探索数据，寻找生活中有趣的事情。如果你觉得我的文章有帮助，请为我鼓掌分享。我也欢迎任何反馈，评论和建设性的批评，以使我的文章更好。你可以打<a class="ae kc" href="mailto:kelly.szutu@gmail.com" rel="noopener ugc nofollow" target="_blank">k<em class="op">elly.szutu@gmail.com</em>T3】找到我</a></h1><p id="9f76" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi">Hey, I’m Kelly. I like to explore data and find interesting things in life. If you think my article is helpful, please clap for me and share it. I also welcome any feedback, comments, and constructive criticism to make my articles better. You can reach me at <a class="ae kc" href="mailto:kelly.szutu@gmail.com" rel="noopener ugc nofollow" target="_blank">k<em class="op">elly.szutu@gmail.com</em></a></p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="oq or l"/></div></figure></div></div>    
</body>
</html>