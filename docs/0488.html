<html>
<head>
<title>Smart Compose Using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的智能作曲</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/smart-compose-in-email-using-deep-learning-c9cc29d9d60a?source=collection_archive---------5-----------------------#2020-02-01">https://medium.datadriveninvestor.com/smart-compose-in-email-using-deep-learning-c9cc29d9d60a?source=collection_archive---------5-----------------------#2020-02-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/765bc930344371f9c568dea5745f6c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3GsFQTW8fDmjK9xzjr3-w.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">PicCredit:Unsplash</figcaption></figure><p id="9c6a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">深度学习的进步将自然语言处理带到了一个新的高度。无论是情感分析还是文本摘要，一切都变得非常简单。此外，像LSTM这样的语言模型的出现使得单词预测变得更准确、更好。在这篇文章中，我们将看到如何使用LSTM构建一个像gmail一样的智能编辑器。</p><figure class="le lf lg lh gt ju gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/3a71c748d4ece338e2b487ce0010f156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/0*QAHZ9Ej5UoWNB8bI"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">(Gmail’s smart compose feature)</figcaption></figure><h1 id="3240" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">数据收集:</h1><p id="2508" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">为了让任何人工智能模型工作，我们需要一个数据集来训练我们的模型。这里我们将使用来自kaggle的数据集，称为<em class="ml"> kaggle-spam-dataset。</em>我们可以尝试其他更大的数据集。因为当有足够的数据时，深度学习模型表现良好。</p><div class="mm mn gp gr mo mp"><a href="https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">深度学习用7个步骤解释-更新|数据驱动的投资者</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd jz mp"/></div></div></a></div><h1 id="b00c" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">预处理:</h1><p id="294b" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">作为预处理阶段，我们将获取数据集并应用基本的数据清洗技术(<em class="ml">我们可以在任何标准的nlp教程</em>中了解它们)。清理数据集后，我们将对其进行标记化。我们还将找到数据集中不同单词的数量，并根据它们出现的频率对它们进行排序。我们还将把单词转换成索引，这样每个单词都有一个唯一的索引。我们还会有一个字典来做相反的事情，即把索引转换回单词。</p><h1 id="5c75" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">数据生成:</h1><p id="f174" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">在预处理步骤之后，我们将调整数据集，使其能够服务于我们的单词预测目的。因此，我们应该这样划分数据，既有输入数据，也有输出数据。输出数据是模型的预期结果，即要预测的单词。输入作为给定单词的索引序列进入模型，输出是要预测的单词的索引。</p><p id="5d6e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们取任意长度20，其中前19个字作为输入，最后一个字作为输出。我们期望我们的模型最大化输出单词的概率。类似地，我们将从第1个索引到第20个索引的单词作为输入，第21个单词作为输出。对于数据集的其余部分，我们将遵循这个过程，可以很容易地用python进行编码。最后，我们将把输出标签转换成一次性编码的表示。</p><h1 id="3d44" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">型号:</h1><p id="de95" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">我们会用LSTM作为基本模型。选择LSTM的原因是因为他们具有长期依赖性的能力，因为我们采用19个单词作为输入，并期望第20个单词由LSTM预测。虽然最近在本帖中已经发明了更强大的模型，但我们仍将讨论基本的lstm。</p><figure class="le lf lg lh gt ju gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/41f426067cd6a55f6b0e3695edfb1fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/0*s-8NjHEMfqODzWdX"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">(Language modelling using RNN,Source:<a class="ae nf" href="http://torch.ch/blog/2016/07/25/nce.html" rel="noopener ugc nofollow" target="_blank">http://torch.ch/blog/2016/07/25/nce.html</a>)</figcaption></figure><p id="ff93" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们将使用具有256个单元的一层LSTM，以便每个lstm找到不同的表示。我们还可以使用各种其他架构，比如使用更多或更少的lstms。最后，我们将得到密集图层，其形状与输出数据集中的单词数相同。</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="03dc" class="nl lj it nh b gy nm nn l no np">model=Sequential()</span><span id="9b2b" class="nl lj it nh b gy nq nn l no np">model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2])))</span><span id="bb89" class="nl lj it nh b gy nq nn l no np">model.add(Dropout(0.2))</span><span id="2d0f" class="nl lj it nh b gy nq nn l no np">model.add(Dense(Y.shape[1],activation=’softmax’))</span><span id="6b6d" class="nl lj it nh b gy nq nn l no np">model.compile(loss=categorical_crossentropy,optimizer=’adam’)</span><span id="11be" class="nl lj it nh b gy nq nn l no np">model.fit(X,Y,epochs=10,batch_size=128)</span></pre><p id="d091" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">keras中的上述代码片段显示了粗略的架构，可以根据可用的硬件进行调整。我们也可以借助试听来进行更好的训练。</p><h1 id="7af6" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">预测:</h1><p id="2c1a" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">训练完模型后，就轮到我们做预测了。因此，基本的方法是给模型一个单词序列作为输入，并期望模型输出概率最高的单词作为输出。在获得所需的输出后，我们不应该忘记将结果转换成所需的单词，因为结果将是一个索引。</p><h1 id="554c" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论:</h1><p id="802d" class="pw-post-body-paragraph kf kg it kh b ki mg kk kl km mh ko kp kq mi ks kt ku mj kw kx ky mk la lb lc im bi translated">这篇文章讲述了使用基本语言建模技术进行智能预测的一般思想。当提供更大的数据时，该模型将执行得更好。转换器也可以用于同样的目的，这被认为是语言建模问题中的最新技术。</p><figure class="le lf lg lh gt ju"><div class="bz fp l di"><div class="nr ns l"/></div></figure></div></div>    
</body>
</html>