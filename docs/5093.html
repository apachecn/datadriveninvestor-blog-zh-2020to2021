<html>
<head>
<title>Elements of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的要素</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/elements-of-neural-networks-6fa6edeb077c?source=collection_archive---------11-----------------------#2020-09-05">https://medium.datadriveninvestor.com/elements-of-neural-networks-6fa6edeb077c?source=collection_archive---------11-----------------------#2020-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/4d4be5e6ec3d2c08ef08bbb17ed5cb07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VOZiOP9bS0yqZnwM"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@joshriemer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Josh Riemer</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="b0e1" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">激活功能:</h1><h1 id="f909" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">简介:</h1><p id="f720" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如今，互联网提供了获取大量信息的途径。无论我们需要什么，只需谷歌(搜索)即可。然而，当我们拥有如此多的信息时，我们面临的挑战是分离相关和不相关的信息。</p><p id="a390" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当我们的大脑同时接收到大量信息时，它会努力理解这些信息，并将其分为“有用”和“不太有用”的信息。在神经网络的情况下，我们需要一种类似的机制来将输入信息分类为“有用”或“不太有用”。</p><p id="76e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这对网络的学习方式很重要，因为并非所有的信息都同样有用。有些只是噪音。这就是激活功能发挥作用的地方。激活函数帮助网络使用重要的信息并抑制不相关的数据点。</p><p id="1dd7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">让我们浏览这些激活函数，了解它们是如何工作的，并找出哪些激活函数适合哪种问题陈述。</p><h1 id="f324" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">神经网络概述:</h1><p id="06c2" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在我深入到激活函数的细节之前，让我们快速浏览一下神经网络的概念和它们是如何工作的。神经网络是一种非常强大的机器学习机制，它基本上模仿了人脑的学习方式。</p><p id="da6b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">大脑从外界接收刺激，对输入进行处理，然后产生输出。随着任务变得复杂，多个神经元形成一个复杂的网络，在它们之间传递信息。</p><p id="3d1a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">人工神经网络试图模仿类似的行为。你下面看到的网络是由相互连接的神经元组成的神经网络。每个神经元都以其权重、偏置和激活功能为特征。</p><p id="576b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">输入被馈送到输入层，神经元使用权重和偏差对该输入执行线性变换。</p><p id="0fd9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh"> x =(重量*输入)+偏差</em></p><p id="6bf0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">之后，激活函数被应用到上面的结果。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/e7c46f668290829f836e41e7d945d114.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/0*qtwUS8cEal-WVE2Z"/></div></figure><p id="fd13" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">最后，激活函数的输出移动到下一个隐藏层，并重复相同的过程。信息的这种向前移动被称为向前传播。</p><p id="149d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如果生成的输出与实际值相差甚远怎么办？使用来自正向传播的输出，计算误差。基于这个误差值，神经元的权重和偏差被更新。这个过程被称为反向传播。</p><div class="mn mo gp gr mp mq"><a href="https://www.datadriveninvestor.com/2019/04/04/neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">冗长但有价值的神经网络指南|数据驱动的投资者</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">如今可用于机器学习的神经网络类型如此之多，以至于它值得一个冗长的指南…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne jz mq"/></div></div></a></div><h1 id="a0ca" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">没有激活功能可以吗？</h1><p id="0eeb" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们知道，使用激活函数会在正向传播期间在每一层引入一个额外的步骤。现在的问题是——如果激活函数增加了如此多的复杂性，我们能没有激活函数吗？</p><p id="871a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">想象一个没有激活功能的神经网络。在这种情况下，每个神经元将只使用权重和偏差对输入执行线性变换。虽然线性变换使神经网络更简单，但该网络的功能会更弱，并且不能从数据中学习复杂的模式。</p><p id="7cc0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">没有激活函数的神经网络本质上只是一个线性回归模型。</p><p id="1c1d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，我们对神经元的输入使用非线性变换，并且网络中的这种非线性是由激活函数引入的。</p><p id="59a9" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在下一节中，我们将看看不同类型的激活函数。</p><p id="78df" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">二元阶跃函数:</strong></p><p id="e2b2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">二元阶跃函数是基于阈值的激活函数。如果输入值高于或低于某个阈值，神经元就会被激活，并向下一层发送完全相同的信号。</p><p id="10eb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">阶跃函数的问题在于它不允许多值输出，例如，它不支持将输入分类到几个类别中的一个。</p><p id="9932" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">数学上，</p><p id="d179" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">f(x) = 1，x &gt;= 0</p><p id="79e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">= 0，x&lt; 0</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/52196abc0bb2bc0a48ef876ff22a403d.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/0*z3Q2hvEtyd3wQqaR"/></div></figure><p id="e867" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">s形</strong></p><p id="b113" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们要看的下一个激活函数是Sigmoid函数。它是应用最广泛的非线性激活函数之一。Sigmoid转换范围0和1之间的值。</p><p id="5b6f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这是sigmoid- f(x) = 1/(1+e^-x的数学表达式</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7ddebff486db0e6b482f06bfb9a8ff18.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/0*Nbbphi5mGrR6MYi6"/></div></figure><p id="7fee" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">优势:</p><ul class=""><li id="f094" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">平滑渐变，防止输出值“跳跃”。</li><li id="3a6c" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">输出值介于0和1之间，使每个神经元的输出正常化。</li><li id="98e1" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">清晰预测-对于大于2或小于-2的X，倾向于将Y值(预测)带到曲线边缘，非常接近1或0。这可以实现清晰的预测。</li></ul><p id="0568" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">谭</strong></p><p id="11dc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">双曲正切函数非常类似于sigmoid函数。唯一不同的是，它是关于原点对称的。在这种情况下，值的范围是从-1到1。因此，下一层的输入不会总是相同的符号。</p><p id="3255" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">双曲正切函数定义为-双曲正切(x)= 2正弦(2x)-1</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5e685f386127a80bd5cc93a4a5b5ee6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*YuhRBgxEwbnpNv7Q"/></div></figure><p id="29fe" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">优势</p><ul class=""><li id="c422" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">以零为中心-更容易对具有强负值、中性值和强正值的输入进行建模。</li><li id="7bbe" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">否则就像Sigmoid函数。</li></ul><p id="b15f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> ReLU </strong></p><p id="4817" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">ReLU函数是另一个非线性激活函数，它在深度学习领域已经获得了普及。ReLU代表整流线性单元。只有当线性变换的输出小于0时，神经元才会被去激活。</p><p id="185f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">下面的图将帮助您更好地理解这一点- f(x) = max(0，x)</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ea64328e3bfe367400144ea459a22f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/0*ga0422uJ9j2oVAx4"/></div></figure><p id="7da0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">优势</p><ul class=""><li id="dd92" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">计算效率—允许网络快速收敛</li><li id="785f" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">非线性——虽然看起来像线性函数，但ReLU有一个导数函数，并允许反向传播</li></ul><p id="bc4a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">不足之处</p><ul class=""><li id="1843" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">垂死的ReLU问题——当输入接近零或为负时，函数的梯度变为零，网络无法执行反向传播，也无法学习。</li></ul><p id="94fb" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">泄漏的ReLU </strong></p><p id="d57c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Leaky ReLU函数只不过是ReLU函数的改进版本。正如我们看到的，对于ReLU函数，当x &lt;0, which would deactivate the neurons in that region.</p><p id="0e4a" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for negative values of x, we define it as an extremely small linear component of x. Here is the mathematical expression-</p><p id="ea48" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">f(x) = 0.01x , x &lt; 0</p><p id="036b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">= x , x &gt; = 0时，梯度为0</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6f248409d31855060ba078d423df68d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/0*gjcSKkm2FI2qWg64"/></div></figure><p id="ed68" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">优势</p><ul class=""><li id="8aca" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">防止死亡ReLU问题-ReLU的这种变化在负区域有一个小的正斜率，因此它确实支持反向传播，即使对于负输入值也是如此</li><li id="e592" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">否则像ReLU</li></ul><p id="1833" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">唰</strong></p><p id="1ccd" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">Swish是一个鲜为人知的激活功能，是由谷歌的研究人员发现的。Swish的计算效率与ReLU一样高，并且在更深层次的模型上表现出比ReLU更好的性能。swish的值范围从负无穷大到无穷大。该功能定义为–</p><p id="23bf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">f(x) = x*sigmoid(x)</p><p id="57d4" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">f(x) = x/(1-e^-x)</p><p id="2383" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu"> Softmax: </strong></p><p id="2bbc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">与二进制分类(0或1)不同，我们需要在神经网络的输出层有多个概率。换句话说，输出层中隐藏单元的数量等于类的数量。例如，我们想从给定的数据集中识别猫、狗和母鸡。我们可以把猫归为第1类，狗归为第2类，母鸡归为第3类，以上都不属于第0类。在这种情况下，输出层将具有四个激活单元(4 x 1矩阵)。每个激活单元计算其各自类别的概率，所有元素的总和应为1。</p><p id="21e0" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">softmax函数或归一化指数函数可用于表示分类分布，即“K”个不同可能结果的概率分布。简单地说，给定的图片是猫、狗、母鸡或者都不是的概率是多少？</p><p id="d917" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">优势</p><ul class=""><li id="64f5" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">能够处理多个类在其他激活函数中只能处理一个类-在0和1之间对每个类的输出进行归一化，并除以它们的总和，给出输入值在特定类中的概率。</li><li id="6e44" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">对输出神经元有用-通常Softmax仅用于输出层，用于需要将输入分类为多个类别的神经网络。</li></ul><h1 id="fd55" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">学习率:</h1><p id="549f" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果我们希望了解什么是学习率以及为什么会有学习率，我们必须首先了解监督学习场景的高级机器学习过程:</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/3164424baaa9011073eb61b6187dddef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*izKz6fGtgPCRsWvi"/></div></div></figure><p id="4718" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">前馈数据并计算损耗</strong></p><p id="588f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">如你所见，神经网络不断改进。这是通过前馈训练数据来实现的，为馈入模型的每个样本生成一个预测。当通过损失函数将预测与实际(已知)目标进行比较时，可以确定模型的表现有多好(或者，严格地说，有多差)。</p><p id="0124" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">学习率</strong>是一个超参数，它控制我们根据损耗梯度调整网络权重的程度。该值越低，我们沿下坡行驶的速度越慢。虽然这可能是一个好主意(使用低学习率),以确保我们不会错过任何局部最小值，但这也可能意味着我们将需要很长时间才能收敛</p><p id="a286" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">以下公式显示了这种关系:</p><p id="8393" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><em class="mh">new _ weight = existing _ weight—learning _ rate * gradient</em></p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/cdb629a5c0610c51596f6d8384310428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KJInMhGMbwTub7Qe"/></div></div></figure><h1 id="e715" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">辍学:</h1><p id="0d4e" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">拥有大量参数的深度神经网络是非常强大的机器学习系统。然而，过拟合在这种网络中是一个严重的问题。大型网络使用起来也很慢，很难通过在测试时组合许多不同的大型神经网络的预测来处理过拟合。辍学是解决这个问题的一种方法。关键思想是在训练期间从神经网络中随机丢弃单元(连同它们的连接)。这可以防止单位之间过度的相互适应。</p><p id="5c4f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">点击这里阅读更多:<a class="ae kf" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">辍学:防止神经网络过度拟合的简单方法</a></p><p id="8639" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">问题:</p><p id="7ce1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当全连接层有大量神经元时，更有可能发生协同适应。协同适应指的是一层中的多个神经元从输入数据中提取相同或非常相似的隐藏特征。当两个不同神经元的连接权重几乎相同时，会发生这种情况。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/692032a55bed18f58e5f528b81eb7973.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/0*_y7oBPNzH4JkwXa_"/></div></figure><p id="9dce" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这给我们的模型带来了两个不同的问题:</p><ul class=""><li id="c371" class="nh ni it lg b lh mc ll md lp nj lt nk lx nl mb nm nn no np bi translated">计算相同产量时机器资源的浪费。</li><li id="f7fe" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb nm nn no np bi translated">如果许多神经元正在提取相同的特征，它为我们的模型增加了这些特征的重要性。如果重复提取的特征仅特定于训练集，这将导致过度拟合。</li></ul><h1 id="7cde" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">批量标准化:</h1><p id="1c53" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">训练深度神经网络是复杂的，因为每层输入的分布在训练期间随着前几层参数的改变而改变。</p><p id="af3d" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">由于需要较低的学习率和仔细的参数初始化，这减慢了训练，并且使得训练具有饱和非线性的模型变得非常困难。</p><p id="bfaf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们将这种现象称为内部协变量移位，并通过标准化层输入来解决问题。</p><p id="628f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">我们的方法从使标准化成为模型架构的一部分和为每个训练小批量执行标准化中汲取力量。</p><p id="3f53" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">批处理规范化允许我们使用更高的学习速率，并且不需要太在意初始化。</p><p id="5131" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">它还起到了规范作用，在某些情况下消除了辍学的必要性。</p><p id="fb20" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">应用于最先进的图像分类模型，批量归一化以少14倍的训练步骤实现了相同的准确性，并以显著的优势击败了原始模型。</p><p id="cca1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这里阅读更多:<a class="ae kf" href="https://kharshit.github.io/blog/2018/12/28/why-batch-normalization" rel="noopener ugc nofollow" target="_blank">为什么要批量归一化？</a></p><h1 id="1769" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">一些互动资源:</h1><ol class=""><li id="7346" class="nh ni it lg b lh li ll lm lp ob lt oc lx od mb oe nn no np bi translated"><a class="ae kf" href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">可视化神经网络中的激活功能</a></li><li id="d268" class="nh ni it lg b lh nq ll nr lp ns lt nt lx nu mb oe nn no np bi translated"><a class="ae kf" href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.59663&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" rel="noopener ugc nofollow" target="_blank">在你的浏览器中修补神经网络。</a></li></ol><p id="b261" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><strong class="lg iu">访问专家视图— </strong> <a class="ae kf" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="lg iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>