<html>
<head>
<title>From Word Embeddings to Sentence Embeddings — Part 2/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从单词嵌入到句子嵌入——第2/3部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1?source=collection_archive---------1-----------------------#2020-04-01">https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1?source=collection_archive---------1-----------------------#2020-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/9c3e941730ac315e06f0ab525986ef24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rzZmTyzUW2wQOHfs-YDQLw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Designed by <a class="ae kc" href="https://br.freepik.com/fotos-gratis/letras-formando-a-palavra-pratica_1330193.htm" rel="noopener ugc nofollow" target="_blank">Freepik</a></figcaption></figure><p id="5709" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[关于这篇和更多的帖子，请查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="0e3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你好。这篇文章是关于<strong class="kf ir">句子嵌入</strong>的三部分系列文章的第二部分。如果你没有读过第一部分，你可以在这里找到它<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917" rel="noopener"/>。</p><p id="8b41" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将解释两种创建句子嵌入的方法:Doc2vec和InferSent。</p><p id="5252" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了从<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917" rel="noopener"> TF-IDF表示</a>改进句子表示，我们必须考虑每个单词的语义和词序。句子嵌入试图对所有这些进行编码。</p><p id="5995" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">句子嵌入类似于单词嵌入。每个嵌入都是一个低维向量，以密集的格式表示一个句子。有不同的算法来创建句子嵌入，相同的目标是为相似的句子创建相似的嵌入。</p><h1 id="4336" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">Doc2vec</h1><p id="ae7a" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> Doc2vec </strong>算法(或段落向量)是由当时谷歌的两位研究科学家Quoc Le和Tomas Mikolov [1]在2014年提出的。它基于Word2vec算法，该算法创建单词的嵌入。该算法遵循一个假设，即一个单词的含义是由出现在附近的单词给出的。</p><blockquote class="me mf mg"><p id="90f3" class="kd ke mh kf b kg kh ki kj kk kl km kn mi kp kq kr mj kt ku kv mk kx ky kz la ij bi translated">从一个人交的朋友身上，你就可以知道他说了什么</p></blockquote><p id="82ef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者提出了该算法的两种变体:分布式内存模型(DM)和分布式单词包(DBOW)。</p><h2 id="0a18" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">分布式存储模型</h2><figure class="my mz na nb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/553260ba9f0ac8d9185a8c5b516f4777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UFujbSYCa7kJMm1t.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1 — Neural Network architecture of the DM model. (Source: [1])</figcaption></figure><p id="40f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图1中，描述了DM模型的神经网络体系结构。让我们从分析训练阶段开始，然后我们将看到模型如何为一个句子创建嵌入。</p><p id="f024" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练语料库中的每个句子和每个单词都被转换成独一无二的表示。两者都有嵌入，分别存储在矩阵<em class="mh"> D </em>和<em class="mh"> W </em>中。训练是通过在句子上传递一个滑动窗口来完成的，试图根据上下文中的前一个单词和句子向量(或图1中的段落矩阵)来预测下一个单词。下一个单词的分类是通过将句子和单词向量的连接传递到softmax层来完成的。对于不同的句子，词向量是相同的，而句子向量是不同的。两者都在培训阶段的每一步进行更新。</p><div class="nc nd gp gr ne nf"><a href="https://www.datadriveninvestor.com/2020/03/24/encoder-decoder-sequences-how-long-is-too-long/" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd ir gy z fp nk fr fs nl fu fw ip bi translated">编码器解码器序列:多长是太长？数据驱动的投资者</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">在机器学习中，很多时候我们处理的输入是序列，输出也是序列。我们称这样的一个…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt jw nf"/></div></div></a></div><p id="c3bb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预测阶段也是通过在句子上传递一个滑动窗口来完成的，试图在给定前一个单词的情况下预测下一个单词。除了句子向量的权重之外，模型的所有权重都是固定的，句子向量的权重在每一步都被更新。在为一个句子计算了下一个单词的所有预测之后，句子嵌入是结果句子向量。</p><h2 id="8884" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">分布式词袋模型</h2><figure class="my mz na nb gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/cad6b0b605bd604efd81c9021cede185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*woR_b-oS5WLOybxX.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2 — Neural Network architecture of the DBOW model. (Source: [1])</figcaption></figure><p id="2223" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图2显示了DBOW模型的神经网络架构。该模型忽略了词序，具有更简单的架构，需要学习的权重更少。</p><p id="b94b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练语料库中的每个句子也被转换成独热表示。在每次迭代中，通过从语料库中选择随机的句子，并从该句子中选择随机数量的单词来完成训练。该模型将尝试仅基于句子ID来预测那些单词，并且句子向量将被更新(在图2中，分别是段落ID和段落矩阵)。</p><p id="aa35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在预测阶段，使用来自句子的随机单词样本训练新的句子ID，但是softmax层的权重是固定的。句子向量在每一步都被更新，得到的句子向量是该句子的嵌入。</p><h2 id="dd9a" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr lp ks ms mt lt kw mu mv lx mw bi translated">比较</h2><p id="2c43" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">比较这两种方法，<strong class="kf ir"> DM模型比DBOW模型</strong>有一些技术优势:</p><ul class=""><li id="72b4" class="nv nw iq kf b kg kh kk kl ko nx ks ny kw nz la oa ob oc od bi translated">DM模型考虑了词序，而DBOW模型没有。</li><li id="73a0" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">DBOW模型不使用单词向量，这意味着单词的语义没有被保留，并且更难检测单词之间的相似性。</li><li id="9e00" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">由于DBOW模型的架构更简单，因此需要更多的步骤来训练以获得精确的向量。</li></ul><p id="95fd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DM模型的主要缺点是生成嵌入所需的时间和资源，这高于DBOW模型。</p><p id="e62a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">什么方法能产生更好的句子嵌入？在最初的论文中，作者说DM“持续优于”DBOW。然而，<strong class="kf ir">最近的研究报告称，DBOW方法更适合大多数任务</strong> [2]。Doc2Vec [3]在Gensim中的实现将DBOW方法作为默认算法，因为发现它比DM方法具有更好的结果。</p><h1 id="e8f6" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">InferSent</h1><p id="6678" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">InferSent是另一种句子嵌入方法，由脸书AI Research在2018年提出[4]，Github中提供了实现和训练好的模型[5]。</p><p id="529f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它在训练过程中与之前的算法有一些不同:它不是无监督学习来训练一个语言模型(LM)(一个预测下一个单词的模型)，而是<strong class="kf ir">使用监督学习来执行自然语言推理(NLI) </strong>(一个预测假设与前提相比是真(蕴涵)、假(矛盾)还是未确定(中性)的模型)。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/64103de3b30bd6cea927229ce9c8052e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*Z-AUzK9UOAnR8JwF.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3 — Generic architecture for training embeddings using NLI. (Source: [4])</figcaption></figure><p id="0473" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图3展示了这种方法的通用培训架构。<em class="mh"> u </em>和<em class="mh"> v </em>具有共享的权重，是我们最终会得到的句子嵌入。</p><p id="b87f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练阶段，前提和假设的句子嵌入连同其元素方面的乘积和元素方面的差异被连接。生成的矢量被输入到多个完全连接的层中，最终得到一个3级softmax(这些级是蕴涵、矛盾或中性)。</p><p id="9ea3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">创建句子嵌入的架构应该是什么？作者尝试了不同的架构，但在这里我将只描述一个具有最好结果的架构，即在INF sent:a<strong class="kf ir">BiLSTM中实现的具有最大池</strong>的架构。</p><figure class="my mz na nb gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e9b6737e8161b0d87b1733991ffb039a.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/0*_YsKVOU8evY3L0IC.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4 — Bi-LSTM with max pooling architecture used in InferSent to generate embeddings. (Source: [4])</figcaption></figure><p id="a906" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图4描述了具有最大池的双LSTM的架构。在这篇文章中，我不会详细解释什么是LSTM(长短期网络)。</p><p id="73fa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，LSTM是一种神经网络，能够记住以前的输入，并在计算下一个输出时使用它们(递归神经网络)。这是通过一个隐藏向量来完成的(图8中的<em class="mh"> h </em>，它表示输入的当前状态的记忆。这种架构包含一个双向LSTM网络，这意味着应用了两个LSTM网络:一个负责前面的输入以预测下一步的输出(向前LSTM)，另一个LSTM则相反:它从头到尾查看输入，并尝试按此顺序进行预测(向后LSTM)。</p><p id="b0ff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后连接两个LSTM网络的输出向量，最后的嵌入是隐藏单元维度上的最大值，如图4所示。</p><p id="4e2d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以使用监督数据进行训练和复杂的递归神经网络(RNN)架构为代价，这种方法创建了很好的句子嵌入。</p><p id="bff9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着变形金刚和伯特的出现，另一个建筑成为了2019年的SOTA——<strong class="kf ir">句子——伯特</strong>。阅读<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-3-3-e67cc4c217d7" rel="noopener"> part 3 </a>了解更多，查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a>类似帖子。</p><p id="a5aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你的坚持！</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="95b1" class="lb lc iq bd ld le os lg lh li ot lk ll lm ou lo lp lq ov ls lt lu ow lw lx ly bi translated">参考</h1><ul class=""><li id="e5e7" class="nv nw iq kf b kg lz kk ma ko ox ks oy kw oz la oa ob oc od bi translated">[1]: Quoc Le和Tomas Mikolov:《句子和文档的分布式表示》，2014；<a class="ae kc" href="https://arxiv.org/abs/1405.4053" rel="noopener ugc nofollow" target="_blank"> arXiv:1405.4053 </a>。</li><li id="1567" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">[2]: Jey Han Lau和Timothy Baldwin:《对doc2vec的实证评估与对文档嵌入生成的实践洞察》，2016年，第一届NLP表征学习研讨会论文集，德国柏林，第78–86页；<a class="ae kc" href="https://arxiv.org/abs/1607.05368" rel="noopener ugc nofollow" target="_blank"> arXiv:1607.05368 </a>。</li><li id="3e70" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">[3]: Gensim — <a class="ae kc" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> Doc2vec段落嵌入</a>。</li><li id="d846" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">[4]: Alexis Conneau，Douwe Kiela，Holger Schwenk，Loic Barrault:《从自然语言推理数据中监督学习通用句子表征》，2017；<a class="ae kc" href="https://arxiv.org/abs/1705.02364" rel="noopener ugc nofollow" target="_blank"> arXiv:1705.02364 </a>。</li><li id="6c1f" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">[5]: <a class="ae kc" href="https://github.com/facebookresearch/InferSent" rel="noopener ugc nofollow" target="_blank">推断实现</a>。</li><li id="875c" class="nv nw iq kf b kg oe kk of ko og ks oh kw oi la oa ob oc od bi translated">[6]:克里斯多夫·奥拉赫，<a class="ae kc" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解LSTM网络</a>。</li></ul><figure class="my mz na nb gt jr"><div class="bz fp l di"><div class="pa pb l"/></div></figure></div></div>    
</body>
</html>