<html>
<head>
<title>Review On YOLOv2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLOv2综述</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/review-on-yolov2-11e93c5ea3f1?source=collection_archive---------0-----------------------#2020-06-02">https://medium.datadriveninvestor.com/review-on-yolov2-11e93c5ea3f1?source=collection_archive---------0-----------------------#2020-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="64c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将对约瑟夫·雷德蒙和阿里·法尔哈迪发表的YOLO9000论文写一篇简短的评论。他们推出了YOLOv2和YOLO9000实时检测系统。YOLO9000可以检测9000多种物体类别。在67 FPS时，YOLOv2在VOC 2007上获得了76.8的mAP。在40 FPS的情况下，YOLOv2获得了78.6mAP，超过了更快的R-CNN和ResNet以及SSD等最先进的方法，同时运行速度也明显更快。我建议在阅读这篇文章之前先阅读我在YOLOv1 上的评论。</p><p id="93ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我将文章分为三个部分</p><ol class=""><li id="258b" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><strong class="jp ir">优于YOLOv1的改进</strong></li><li id="c8d4" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">暗网-19 </strong></li><li id="9baa" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir"> YOLO9000 </strong></li></ol></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="cc63" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak"> 1。对YOLO v1的改进</strong></h1><h2 id="33d5" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 1.1。批量标准化:</strong></h2><ul class=""><li id="5aff" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">YOLOv2中的所有卷积层都使用了批标准化。</li><li id="55aa" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">这使地图提高了2%。</li></ul><h2 id="6021" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 1.2。高分辨率分类器:</strong></h2><p id="a87c" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">原始YOLO训练如下。</p><p id="be00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在224 x 224上训练分类器。然后他们将分辨率提高到448用于<strong class="jp ir">检测。</strong></p><p id="8f41" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">而对于YOLOv2，</p><p id="9994" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们最初在224×224的图像上训练模型，然后在为<strong class="jp ir">检测</strong>进行训练之前，他们在图像网上以全448×448分辨率对分类网络进行了10个时期的微调。因此，网络有时间调整滤波器，以便更好地处理更高分辨率的输入。这提高了4%的地图。</p><h2 id="1aea" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 1.3。与锚定框卷积:</strong></h2><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ff465b19ed0046c09b4d096c14311d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*RsYY7pQtF4_xG9ymeN6jCA.png"/></div></figure><p id="df67" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">YOLO v1试图将对象分配到包含对象中间的网格单元。在上面的图像中，我们可以看到黄色网格单元包含汽车和女孩的中点。但是由于网格单元只能检测一个物体，问题就出现了。为了解决这个问题，作者尝试使用k边界框(在YOLO v2中)允许网格单元检测多个对象。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9046113b02a584590f6d5046a1e820f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*PvRVf3Js0RLdCL1GdRADNg.png"/></div></figure><p id="8e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了预测k边界框，YOLO v2使用了<strong class="jp ir">锚框的思想。</strong></p><ul class=""><li id="755f" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">YOLO使用卷积特征提取器上的全连接层直接预测边界框的坐标。在YOLO v2中，所有完全连接的层被移除，并且使用锚定框来预测边界框。</li><li id="9b63" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">移除一个池层以提高输出的分辨率</li><li id="a879" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">使用416×416的图像来训练检测网络，并且获得13×13的特征图，即，它们被下采样32倍。</li><li id="7b33" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">因此，我们对每个锚盒进行坐标和置信度得分(客观性预测)预测。在YOLO之后，对象性预测仍然预测地面真实的IOU，并且假设存在对象，所提出的框和类预测预测该类的条件概率</li><li id="e4af" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">使用锚箱时，精确度会略有下降。在没有锚盒的情况下，我们的中间模型得到69.5个mAP，召回率为81%。使用锚盒，我们的模型得到了69.2个mAP，召回率为88%。</li></ul><h2 id="03b3" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 1.4。如何选择锚箱数量？(</strong>维星团<strong class="ak"> ) </strong></h2><p id="86d4" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">他们针对不同的k值在所有边界框上运行k均值聚类，并绘制出质心最近的平均IOU。重要的是，他们没有使用欧几里德距离，而是在包围盒和质心之间使用iou。使用基于标准欧几里德距离的k-means聚类不够好，因为较大的盒子比较小的盒子产生更多的误差。</p><p id="fd92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们在k=5时得到最好的结果。他们使用以下公式计算边界框和质心之间的距离:</p><p id="aed6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nj"> d(方框，形心)= 1iou(方框，形心)</em> </strong></p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0a04bd731d8c276b1ed39b1053a616df.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*uz3tsR_C3fE5m2SCJaFYWg.png"/></div></figure><p id="5132" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">左图显示平均iou，右图显示VOC和COCO的相对坐标。</p><h2 id="490b" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 1.5。位置预测</strong></h2><ul class=""><li id="e7d6" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">YOLO v1对位置预测没有限制。这使得模型在早期迭代中不稳定。</li><li id="a14a" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">YOLO v2使用逻辑激活σ来限定位置，这使得该值落在0到1之间。</li><li id="07ad" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">网络为每个单元预测5个边界框。它为每个边界框<strong class="jp ir"> tx，ty，tw，th </strong>和<strong class="jp ir"> to预测5个坐标。</strong>如果单元格从图像的左上角偏移<strong class="jp ir"> (cx，cy) </strong>并且锚定框具有宽度和高度<strong class="jp ir"> pw，ph，</strong>则预测对应于:</li></ul><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fd479fd7d59fc0edf7f0a3ad7079bea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*zfU7iJqaMcpYupLoBRvelg.png"/></div></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4baec48a53171a2c87c272b1c3a2087e.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*Wt-5Dn1TsS3e6kM72yhoxg.png"/></div></figure><p id="976b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，如果我们在一个特定的网格单元上使用两个定位框，它将输出两个框(假设一个是蓝色的，一个是红色的)。现在以蓝盒子为例。我们不仅将这个框分配给网格单元，还分配给具有最大iou的锚框(上图中的虚线框)。</p><h2 id="1e05" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">1.6.细粒度特征</h2><ul class=""><li id="2d90" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">13×13特征图足以检测较大的物体。</li><li id="bdec" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">但是为了检测更小的目标，来自前一层的26×26×512特征图被映射成13×13×2048特征图，然后与原始的13×13特征图连接用于检测。</li><li id="af1d" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">这提高了1%的地图。</li></ul><h2 id="e8e4" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">1.7多尺度训练</h2><ul class=""><li id="e4ad" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">YOLOv1使用448x448的输入分辨率进行检测训练。</li><li id="bdae" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">然而，在YOLOv2中，由于我们的模型只使用卷积层和池层，因此可以动态调整大小。</li><li id="d064" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">对于每10个批次，随机选择新的图像尺寸。图像尺寸从{320，352，…，608}中选择。</li></ul><h2 id="4adb" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">1.8.增量改进总结</h2><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/70fff153feac9878324a7141ada92489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*TVHW23NLnqWXKfDu9QNbcw.png"/></div></figure></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="3423" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak"> 2。暗网19 </strong></h1><p id="4c28" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">为了克服复杂性和准确性的问题，作者提出了一种称为<strong class="jp ir"> Darknet-19 </strong>的新分类模型，用作YOLOv2的主干。</p><div class="no np gp gr nq nr"><a href="https://www.datadriveninvestor.com/2020/03/04/on-artificial-intelligence-and-surveillance-capitalism/" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd ir gy z fp nw fr fs nx fu fw ip bi translated">人工智能和监督资本主义|数据驱动的投资者</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">大科技，总是现在:人工智能推动的大科技，已经使购物，搜索，在你的…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ng nr"/></div></div></a></div><p id="9a64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Darknet-19有19个卷积层和5个最大池层。在ImageNet上取得了91.2%的top-5准确率，优于VGG (90%)和YOLO网络(88%)。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6cfd95cc617e8fac477f49346f4a431a.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*7hn2mcJPVSmInq4W98l9kQ.png"/></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">Darknet 19</figcaption></figure><p id="532a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">YOLOv2输出形状为13x13x(kx(1+4+20))其中k是锚盒的数量，20是类的数量。我们有k=5。所以输出形状会是<strong class="jp ir"> 13x13x125。</strong></p><h2 id="f1ee" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">2.1培训</h2><p id="1751" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">该模型首先被训练用于分类，然后用于检测。</p><p id="d538" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">分类训练:</strong></p><ul class=""><li id="2e30" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">首先，他们使用随机梯度下降和权重衰减在ImageNet 1000类数据集中训练Darknet-19网络160个时期。</li><li id="029e" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">在训练过程中，作者使用了标准的数据增强技巧，包括随机裁剪、旋转、色调、饱和度和曝光偏移。</li><li id="a71b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">在我们对224×224的图像进行初步训练后，他们使用更大尺寸的图像448×448对darkenet网络进行了10个时期的微调</li></ul><p id="7224" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">检测训练:</strong></p><ul class=""><li id="0377" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">在用于分类的训练之后，darknet-19的最后一层被移除，并且被替换为具有1024个滤波器的3×3卷积，随后是具有我们检测所需的输出数量的1×1卷积(13×13×125)。还添加了一个<strong class="jp ir">通过</strong>层，这样我们的模型就可以使用之前层的精细纹理特征。</li><li id="72ff" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">然后，他们在检测数据集(VOC和COCO数据集)上训练网络160个时期</li></ul><h2 id="b647" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated"><strong class="ak"> 2.2与其他车型的对比</strong></h2><p id="3140" class="pw-post-body-paragraph jn jo iq jp b jq mr js jt ju ms jw jx jy mx ka kb kc my ke kf kg mz ki kj kk ij bi translated">YOLOv2比其他对象检测算法更快。此外，它可以运行在不同大小的图像上，以提供速度和准确性之间的平滑权衡。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ee5a94967d7e228b00e76625ec09cccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*7nPaZnIswxiWpV6FcY2VJg.png"/></div></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6275f9c6865387ee9154dc2fe0fa8ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*prqUJhNhfudgFXB0W4TxIA.png"/></div></figure></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="b9f7" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">3.YOLO9000使用单词树(更强)</h1><ul class=""><li id="0ca0" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">提出了一种分类和检测数据联合训练的机制。也就是说，在训练过程中，他们混合了来自检测和分类数据集的图像。当网络看到用于检测的图像时，全部YOLO2损失被反向传播，而当网络看到分类图像时，只有分类部分损失被反向传播。</li></ul><h2 id="2a4d" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">挑战:</h2><ul class=""><li id="2ccd" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">我们知道，检测数据集通常有狗、猫、船等类别，而分类数据集中有“诺福克梗”、“约克夏梗”等类别。如果我们需要训练两个数据集，我们需要找到一种方法。</li><li id="dc4b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">通常我们在所有的类中直接使用softmax。但在这里我们不能这样做，因为softmax采取互斥事件，但在这里“狗”和“诺福克梗”并不互斥。所以我们不能直接使用softmax。</li><li id="9527" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">解决方法是使用<strong class="jp ir">分级分类</strong>进行标注。</li></ul><h2 id="c771" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">层次分类</h2><ul class=""><li id="b69d" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated">这里的图像网络标签是从WordNet中提取的。在WordNet中，“诺福克梗”和“约克夏梗”都是“梗”的下位词，梗是“猎狗”的一种，梗是“狗”的一种，梗是“犬科动物”，等等</li><li id="000b" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">他们使用Imagenet标签和WordNet建立一个单词树。</li></ul><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="oo op di oq bf or"><div class="gh gi on"><img src="../Images/a56dd18fdfc29c4f25042221c4ddf607.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*m0defqDGxQoYiRaAnP9tqw.png"/></div></div></figure><p id="177c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">他们结合以下数据集进行训练:</strong></p><p id="93b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> i)微软COCO </strong>:包含100k图片，80个类，检测标签，类比较笼统像“狗”或者“船”。</p><p id="70c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">ii)ImageNet</strong>:1300万张图像，22k个类别，分类标签，类别更具体，如“诺福克梗”、“约克夏梗”，或“贝德灵顿梗”。</p><ul class=""><li id="a3da" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">他们使用COCO数据集和来自Image net的top 9000类创建了一个组合数据集。相应的单词树由<strong class="jp ir"> 9418 </strong>类组成。额外的类是通过用树中的synsets映射原始类而形成的。</li><li id="6403" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">在训练期间，地面真相标签被传播。也就是说，如果一张图片被标记为“诺福克梗”,它也会被标记为“狗”。利用这个数据集，他们训练了YOLO9000。他们使用了3个锚箱，而不是5个锚箱。</li></ul><p id="dc2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了计算条件概率，我们的模型预测了9418个值的向量，并且我们计算了作为相同概念的下位词的所有子集的softmax。输出条件概率如下:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a244d58c92190f8d6faadbcdbfde4aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*8vUgsFAPFE3FhRJS-Qtang.png"/></div></figure><p id="2b22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们需要找到绝对概率，我们找到它如下:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0f2a1f8998dd2145001e1ed0aa2eaa66.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*D07z66lBduyIQa0V3SeTKA.png"/></div></figure><p id="c152" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了分类的目的，我们假设图像包含一个对象:Pr(物理对象)= 1</p><ul class=""><li id="994f" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">当它看到分类图像时，我们只反向传播分类损失。要做到这一点，我们只需找到预测该类最高概率的边界框，并计算其预测树上的损失。</li><li id="d6dd" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated">当它看到检测图像时，整个yolo2损失被反向传播。我们还假设预测的框与地面真实标签重叠至少0.3 IOU，并且我们基于该假设反向传播物体损失。</li></ul><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d0d33654c4fb79a5fe4983d9f02c8755.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*Hjo-NHvkLRqrY0puvOvVvQ.png"/></div><figcaption class="oh oi gj gh gi oj ok bd b be z dk">experiment prediction using 1369 classes(1000 + 369)</figcaption></figure><ul class=""><li id="9643" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk mw ks kt ku bi translated">因此，使用这种联合训练，YOLO9000学习使用COCO中的检测数据在图像中找到对象，并学习使用ImageNet中的数据对各种各样的对象进行分类。</li></ul><h2 id="a57a" class="mf li iq bd lj mg mh dn ln mi mj dp lr jy mk ml lv kc mm mn lz kg mo mp md mq bi translated">3.1 YOLO 9000结果</h2><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f3c925621a0072fad456cdde54115216.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*0_DJ5CLyMh9C7TcywHe5wg.png"/></div></figure></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="ea9f" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">参考</h1><ul class=""><li id="6514" class="km kn iq jp b jq mr ju ms jy mt kc mu kg mv kk mw ks kt ku bi translated"><a class="ae kl" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></li><li id="24fd" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated"><a class="ae kl" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65" rel="noopener" target="_blank">https://towards data science . com/review-yolo v2-yolo 9000-you-only-look-once-object-detection-7883 D2 b02a 65</a></li><li id="e2ad" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk mw ks kt ku bi translated"><a class="ae kl" href="https://medium.com/@amrokamal_47691/yolo-yolov2-and-yolov3-all-you-want-to-know-7e3e92dc4899" rel="noopener">https://medium . com/@ amrokamal _ 47691/yolo-yolov 2-and-yolov 3-all-you-want-to-know-7 E3 e 92 DC 4899</a></li></ul><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ow ox l"/></div></figure></div></div>    
</body>
</html>