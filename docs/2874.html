<html>
<head>
<title>TD Learning — Solving the evaluation problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TD学习——解决评估问题</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/td-learning-solving-the-evaluation-problem-7f63b658414c?source=collection_archive---------6-----------------------#2020-05-20">https://medium.datadriveninvestor.com/td-learning-solving-the-evaluation-problem-7f63b658414c?source=collection_archive---------6-----------------------#2020-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="01b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/datadriveninvestor/reinforcement-learning-monte-carlo-for-policy-evaluation-312fd2e8331d" rel="noopener">上一篇博文</a>中，我们已经讨论了蒙特卡罗如何解决无模型环境的评估问题。</p><p id="2eeb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是蒙特卡洛也有它的缺点。当考虑连续序列时，它并不理想。例如，核反应堆的冷却系统怎么样。也许只是我…但是如果这能一周七天没日没夜地工作，那就太好了。没有吗？</p><p id="3456" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们想在实际播出许多不同的剧集之前，对<em class="km">的状况有一个估计。这对于我们的核反应堆例子来说很有意义，在这里，我们不想等到达到事件的结束状态“因爆炸而灭绝”之后，才考虑改变坏状态的值。</em></p><p id="7763" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是时间差异学习或TD学习算法发挥作用的地方。同样，我只展示如何解决评估/预测问题。为了简单起见，我将集中讨论v_π(s)。</p><h1 id="8f48" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">TD(0)学习</h1><p id="9a3d" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">先说TD(0)。</p><p id="0bee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像上一篇关于蒙特卡洛的文章一样，我们正面临着一个无模型的环境，而且一开始还不知道转移概率。代理仍然需要首先通过经验了解底层环境。</p><p id="1e1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，为了避免等到一集结束，我们需要唤醒我们的老朋友<strong class="jp ir">贝尔曼方程式</strong> …耶！</p><p id="a62c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用我们所知道的贝尔曼方程来更新一个状态的值，但是这次我们没有考虑所有未来可能的结果。我们遵循策略π一步，并通过从状态s_t采取动作<em class="km"> a </em>而结束于状态s_t+1。</p><p id="5a5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，对于增量蒙特卡罗，我们在本集结束时做了以下工作来更新v(s ):</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/f8ea84e4b293810e8679bec157b29dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*yb3xo8noYgkeOD0Du_sOEQ.png"/></div></figure><p id="c125" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们不用等到这一集结束才得到G_t，而是看一看<strong class="jp ir">贝尔曼期望方程</strong>:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/7eb06001b68bf261c620082ecc0ad4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*GHLbAMhUFrPOhp7vPon0Dw.png"/></div></figure><p id="3c71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">v_π(s)就是G_t的期望值，我们知道可以将G_t分解如下:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/caf501d3d75a8d346b31276cf14762cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*NVZ_uBcOgjBmIzyMo2bFMw.png"/></div></figure><p id="7a8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">即时报酬R_t+1和G_t+1，这正好是下一个状态<em class="km"> s </em>的贴现状态值:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/ffb8ec09837e87bffccf6715e6c58ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*O_o2lqE-U9JHx3N8zf8CyQ.png"/></div></figure><p id="cf97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们最终用期望的即时回报加上期望的贴现状态值来代替G_t，这是遵循π策略的下一个状态的值。</p><p id="ee13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以，现在在贝尔曼<em class="km">期望</em>等式中，我们有一个期望。这意味着我们考虑了所有可能状态的概率，以及通过遵循策略π从s得到平均值可能得到的回报。但是在无模型的环境中，我们事先不知道转移概率。</p><p id="85d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用该知识来修改上面用于增量蒙特卡罗方法的公式，以更新v_π(s)。我们只需从贝尔曼期望方程中去掉期望，并用G_t代替:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/2abb6e61f08f87a64ee6859b2b54380a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wqAN8Hd0c7B6yMTmflx-A.png"/></div></div></figure><p id="621d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该公式告诉我们如何在采取每一步后更新v_π(s ),而不是等待获得完整的G_t。代理按照策略π从状态s_t采取动作a。通过采取该动作，代理最终处于状态s_t+1。对于那个状态，我们有一个估计，一个猜测，它的状态值v_π(s_+1)。我们用它来更新vπ。</p><p id="2e44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">右边是我们对s的当前状态值的估计，在TD学习中R_t+1 + γv_π(s_t+1)称为<strong class="jp ir"> TD目标</strong>。这样做的原因是，我们希望我们的当前状态更接近目标。我们通过获取TD目标和当前状态的差值并将其加到我们的当前值上来实现这一点。</p><p id="31e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种使用不完整情节和对未来状态值的估计来逐步更新状态值的技术被称为<strong class="jp ir">自举</strong>。在每次迭代的每一步，我们更新每个v_π(s)的估计值。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/2268eb9772280a6b6730e37fbf1ed356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oiuXwlnnnXnlJsMZnulTBw.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">TD(0): Instead of considering the complete trajectory, only one step is considered to update the current state-value.</figcaption></figure><p id="4c7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但现在你会说:但这怎么能行得通呢，沃尔特？我的意思是我们只考虑一种可能的结果，然后直接更新vπ(s)。我们最终可能进入的所有其他状态都不会被认为是更新状态值！</p><p id="1e94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是的，Billy，你是对的，但是如果我们像在蒙特卡洛那样运行很多很多次，那么代理可能会选择不同的路线并在不同的州结束。通过多次重复这个过程，我们越来越接近真实的状态值。</p><p id="0f73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也把步长α放回到公式中。α是一个介于0和1之间的值，它描述了添加的值对v(s)的旧值的影响程度。它告诉我们离目标有多远:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mj"><img src="../Images/c7ffb0c60c07e3fec4f4a9c6efecd76a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vxg3UTIpbWESy1wAEWHEhw.png"/></div></div></figure><h1 id="c573" class="kn ko iq bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">偏差和方差权衡</h1><p id="199c" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">偏差和方差指的是我们更新v(s)时的<strong class="jp ir">目标</strong>的估计值。在TD的情况下，它是TD目标R_t+1 + γv_π(s_t+1 ),在蒙特卡罗中，它是完整的轨迹G_t。</p><h2 id="f081" class="mk ko iq bd kp ml mm dn kt mn mo dp kx jy mp mq lb kc mr ms lf kg mt mu lj mv bi translated">偏见</h2><p id="073a" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">先说<strong class="jp ir">偏置</strong>。偏差告诉我们，目标代表了环境中真正的潜在目标。在TD(0)学习的情况下，偏差可能非常高，因为贴现状态值γv_π(s_t+1)只是一个估计值，它是不断更新的。“真实的”γvπ(s t+1)可能完全不同。</p><p id="bc80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一方面，对于蒙特卡洛来说，情况并非如此，因为我们使用估计的状态值<strong class="jp ir">而不是</strong>，而是使用到达轨迹终点后获得的G_t。而且我们知道v_π(s) = G_t，不用估计。我们不是在那里自举，只是使用我们观察到的真实回报。</p><h2 id="45c0" class="mk ko iq bd kp ml mm dn kt mn mo dp kx jy mp mq lb kc mr ms lf kg mt mu lj mv bi translated">差异</h2><p id="52c9" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">当然，对于蒙特卡洛来说，在遵循随机策略π的两个不同轨迹中，同一个状态的G_t可能完全不同。对于一个轨迹，它可能有一个很高的值，而对于另一个轨迹，它可能有一个很低的值。这叫做<strong class="jp ir">方差，</strong>嗯，因为数值变化<strong class="jp ir">。</strong>蒙特卡洛中的方差可能非常高，因为状态值取决于完整的轨迹:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/00d88f123832b72bb98c420c24eb7af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*KfV8tnoTz3qjMdF2JUckeQ.png"/></div></figure><p id="9053" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个轨迹的每个贴现R_t+i可以是不同的，因为我们有一个随机政策和一个环境，它以一定的概率将我们的代理人抛向不同的状态。两者都给轨迹的每个部分增加了一些随机性(或“噪声”)。</p><p id="b226" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，TD(0)的方差很低。为什么？因为我们只考虑一步。我们对下一个R_t+1有一些随机性，但仅此而已。在这一步之后，我们直接更新我们对状态值的估计。</p><h2 id="46f1" class="mk ko iq bd kp ml mm dn kt mn mo dp kx jy mp mq lb kc mr ms lf kg mt mu lj mv bi translated">感谢阅读！</h2><p id="a0b3" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy ln ka kb kc lo ke kf kg lp ki kj kk ij bi translated">在接下来的文章中，我将讨论TD(n)和TD(λ)。</p></div></div>    
</body>
</html>