<html>
<head>
<title>Long Short Term Memory Maths — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆数学—第二部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/long-short-term-memory-maths-part-2-e938b1b42f74?source=collection_archive---------7-----------------------#2020-09-23">https://medium.datadriveninvestor.com/long-short-term-memory-maths-part-2-e938b1b42f74?source=collection_archive---------7-----------------------#2020-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8631b067a2fa484692a92341a8f06b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7d10zUK4874FayT_AGCc2w.jpeg"/></div></div></figure><p id="3f42" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我学习神经网络背后的基础数学系列的第五篇文章。你可以在这里查看我以前的文章:</p><ol class=""><li id="09f5" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/neural-network-maths-in-5-minutes-f385eeddf783" rel="noopener">神经网络数学</a></li><li id="14c2" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/convolution-neural-network-maths-intuition-6b047cb48e90" rel="noopener">卷积神经网络数学</a></li><li id="fa70" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/recurrent-neural-network-maths-69214e4d69e1" rel="noopener">递归神经网络数学</a></li><li id="77d1" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/long-short-term-memory-maths-part-1-d99b3c3b09d0" rel="noopener">长短期记忆数学—第一部分</a></li></ol><p id="313d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上一篇文章中，我们介绍了LSTM的基础知识，并讨论了它的前向传播。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="fe49" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">修订本</h1><p id="e2d3" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">这是我们为了理解LSTM而画的图表。在这一点上，如果你还没有浏览我之前的<a class="ae lf" href="https://medium.com/datadriveninvestor/long-short-term-memory-maths-part-1-d99b3c3b09d0" rel="noopener">文章</a>，我强烈建议你在继续下一步之前查看一下。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ff73e4a0665c882cfa1cd15c2643a1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjbjBNSh9F5wH0R6K23ABQ.png"/></div></div></figure><p id="c3e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我们上次提出的公式列表。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f8a1a5339ac7f449a7655c98407fbff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*X37JdjWMv9W8a3ci6Uemmg.png"/></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="a8ce" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">反向传播</h1><p id="20c9" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">因为反向传播的目标是——如何调整变量以减少误差。</p><div class="na nb gp gr nc nd"><a href="https://www.datadriveninvestor.com/2020/07/23/learn-data-science-in-a-flash/" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd ir gy z fp ni fr fs nj fu fw ip bi translated">一瞬间学会数据科学！？数据驱动的投资者</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">在我之前的职业生涯中，我是一名训练有素的古典钢琴家。还记得那些声称你可以…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jw nd"/></div></div></a></div><p id="3c0e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，为了简化事情，我特意去除了等式中的偏差。此外，为了便于计算，我只考虑一个输入为基础的LSTM。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi gj"><img src="../Images/881dc13fbd12fbf108d96ef6a66b4aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nq0kBLPN2dmbKsAUK4lI5Q.png"/></div></div></figure><p id="1f76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，正如我们之前解释过的，通过链式法则和基本的微分学，我们应该能够区分它们。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/1e1140578a5d34f8d063d7439f885b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0HBFnW2ivPaU4q94CR5BA.png"/></div></div></figure><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/e52949a84798e91f034bebba1515522c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ocdNjuVTtCI6hPAeCMstg.png"/></div></div></figure><p id="b1f0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">注意:</strong>这些方程背后的思想只是为了得到一个链式法则的直觉，并弄清楚如何计算导数。这里可能有一些数学错误(因为，我只是一个软件工程师，而不是数学专家)。请不要直接复制它们。这些不是从任何其他站点复制粘贴的，而是手动计算的。如果有什么问题，欢迎在下面评论。</p><p id="fd2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此这些就是最终重量。</p><figure class="mv mw mx my gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/a713e64f1ff92fd968c26a81aa8786ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2eIUfVn83jzqNieg6nGVw.png"/></div></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="4a34" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">消失/爆炸渐变问题</h1><p id="6568" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">如果你看到上面的反向传播方程，你会发现它们都是多重导数的乘积。所以，消失和爆炸梯度的问题也会发生，对吗？</p><p id="ba74" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以，我为此做了很多研究，因为方程式告诉了我一些别的东西，而整个互联网也在告诉我一些别的东西。但是，显然数学赢了。</p><p id="08c6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">是的，没错。这是一个流行的神话，说因为LSTM的附加性质，这样的问题不会发生。但实际上这是可能发生的。只是因为LSTM的记忆单元和附加属性，这些梯度问题更容易管理。<a class="ae lf" href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="e483" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这总结了我对不同神经网络的一系列数学。谢谢你们坚持阅读我所有的博客。请随意评论或添加任何对你不合适的东西的反馈。毕竟，我们都在学习:)</p><p id="b2be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">访问专家视图— </strong> <a class="ae lf" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>