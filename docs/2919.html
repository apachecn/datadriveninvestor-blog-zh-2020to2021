<html>
<head>
<title>Types of regression in Machine learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的回归类型。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/types-of-regression-in-machine-learning-bd0f5c4772fc?source=collection_archive---------2-----------------------#2020-05-22">https://medium.datadriveninvestor.com/types-of-regression-in-machine-learning-bd0f5c4772fc?source=collection_archive---------2-----------------------#2020-05-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f77bee91a050a03f4f1db3391c803758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z55kLIwRPgxXwRjp"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Isaac Smith</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="450c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我写这篇文章是为了列出机器学习中可用的不同类型的回归模型，并进行简短的讨论，以帮助我们对它们的含义有一个基本的概念。</p><p id="c51c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，在进入主题之前，让我们先了解数据科学实际上是什么，以及它与回归有什么关系。</p><p id="bcf7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">众所周知，数据科学是一个主要用于借助统计学解决问题的领域，然后是机器学习算法，以帮助我们做出预测和有价值的商业决策。</p><p id="bc8f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，当我们从统计学过渡到机器学习时，我们首先应该理解的是回归。</p><p id="ac79" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">什么是回归？？回归可以说是一种找出输入变量(称为预测变量)和输出变量(也称为响应/目标变量)之间最佳关系的技术。并且最佳关系由预测值和实际值之间的最小差异来表示，该最小差异也称为残差。</p><p id="d0f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，回归通过最小化误差来指示预测值和响应变量之间的最佳关系。</p><h2 id="a3a0" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><strong class="ak">回归的类型</strong>:</h2><p id="2988" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">现在让我们了解一下回归模型是如何分成不同类型的。</p><p id="42d6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据我遇到的各种问题的经验，以下三个因素在很大程度上将每个模型分为不同的类型:</p><p id="8ac1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1 &gt;响应变量的类型(是连续变量还是离散变量)。</p><p id="5ed8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2 &gt;回归模型中使用的输入变量的数量。</p><p id="fdd3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3 &gt;最后—回归线的形状(无论是直线还是曲线)。</p><p id="a2fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">因此，最著名和最广泛使用的回归模型是:</strong></p><p id="53fc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1 &gt;线性回归。</p><p id="42f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2 &gt;逻辑回归。</p><p id="697b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3 &gt;多项式回归。</p><p id="b5e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">还有几个是:</strong></p><p id="2992" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4 &gt;逐步回归。</p><p id="4b24" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5 &gt;岭回归。</p><p id="185c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6 &gt;套索回归。</p><p id="e04f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7&gt; ElasticNet回归。</p><p id="690f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们逐一了解它们:</p><p id="b5b8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1&gt; <strong class="ki iu">线性回归</strong> —这是一种监督学习算法，主要用于预测分析，这通常意味着尝试拟合输入和输出变量之间的最佳直线，以便对我们的数据进行建模。这条最佳拟合直线也称为回归线，它使预测的误差平方和最小。线性回归的另一个最明显的特征是输出变量应该是连续的。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/3506214fc40bb36e49e249bfb8ef6df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*GnoMPH4FsH6CTzqCmAj6bA.png"/></div></figure><p id="9718" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图有助于我们理解线性回归，其中Y是输出变量，X是输入变量。</p><p id="8953" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2&gt; <strong class="ki iu">逻辑回归</strong> -这也是一种监督学习算法，但在我们想要找到事件的概率时使用。针对由分类/离散输出变量确定的分类问题的最广泛使用的算法。与输出连续变量的线性回归不同，逻辑回归通过使用逻辑sigmoid函数发送回一个概率值来转换输出，该概率值稍后将映射到两个或多个离散类。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/5ce6c3340d59b05dc1bec174b214e895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*jmcg72kVS_hx0isDSzdaQw.png"/></div></figure><p id="fcaf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑回归用“S”形曲线表示，也称为Sigmoid曲线或逻辑函数。</p><p id="59a0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3&gt; <strong class="ki iu">多项式回归</strong>:与我们试图拟合输入和输出变量之间的线性关系的线性回归不同，在多项式回归中，我们试图拟合输入和输出变量之间的非线性关系，这样做是为了增加输入变量的功效。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/196ca5a85c892669b0ab6d8871efaeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*furWUl4urYEMF-VWoJwcBg.png"/></div></div></figure><p id="ed30" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">维基百科对多项式回归的定义是:</em></p><p id="e7c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将y的期望值建模为n次多项式，从而产生一般的多项式回归模型。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/4784bb2618c033e6a9ff30a4cf28c039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*-HMw-6Hi_HQq8bdpzv0LRw.png"/></div></figure><p id="af68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它基本上是试图拟合一条曲线，而不是一条直线。多项式回归也称为多元线性回归，主要用于复杂的数据集，在这种情况下，拟合直线会变得很困难。</p><p id="029e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4&gt; <strong class="ki iu">逐步回归</strong>:当我们有大量独立变量时，这是一个用于大型数据集的回归模型，因为模型的特征选择是一个自动过程。模型基本上会选择自己的独立特征来拟合模型，并通过使用前向选择和后向排除法等方法来验证模型中每个独立变量的统计显著性，这些方法可以在minitab等统计软件包/工具的帮助下轻松实现。在R中，“olsrr”包可用于此目的，并根据R平方、调整R平方、AIC等标准选择最佳最终模型。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bb525cd5a53ffaa36adaf60bb9acfcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*2ZtiE2hRQN1wcDYj95QxuA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Stepwise regression- wikipedia</figcaption></figure><p id="35be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5&gt; <strong class="ki iu">岭回归:</strong>每个回归模型都容易出现误差，这种误差是预测值与实际值之间的差值，可以分为三类——偏倚、方差和不可约误差。</p><p id="ff56" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此<strong class="ki iu"> </strong>当数据具有高方差且多重共线性问题意味着输入变量之间存在高相关性时，则使用岭回归，因为它带有一个罚函数来最小化误差，并以较小的误差拟合模型。因为我们使用λ函数的平方项来最小化误差，所以它们也被称为正则化技术，并且类似地，当我们使用罚函数的平方时，它也被称为L2正则化或偏差的平方项。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/312b30bb9e08df57e656b515eb76f82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*X3aKcJ_Y19AaAfoFDX5vAw.png"/></div></figure><p id="1576" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">岭回归也有助于通过惩罚系数向量的L2范数(欧几里德距离)来解决过拟合，这导致β系数的“收缩”。</p><p id="c111" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6&gt; <strong class="ki iu"> Lasso回归</strong> : Lasso(最小绝对收缩和选择算子)也和岭回归一样，但是我们在这里增加了一个绝对项作为罚函数来最小化误差。拉索回归也被称为L1型正则化。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi md"><img src="../Images/0f41cc279d7d3e2d73ea5dba2e84b169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*fV4MZbwbFviXho8CZDskyg.png"/></div></figure><p id="38bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">套索回归通过惩罚系数向量的L1范数(曼哈顿距离)来帮助解决过度拟合问题，因为当λ值增加时，一些系数收缩到零。这种方式L1正则化有助于最佳特征选择，解决多重共线性问题，并且未被归零的独立特征指示它们是最重要的特征。</p><p id="ed84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7&gt; <strong class="ki iu"> ElasticNet回归</strong>:这种类型的技术可以称为岭和套索回归的混合，它使用L1和l2正则化来惩罚模型，主要用于具有多个变量的数据。</p><p id="03ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在ElasticNet回归中使用了一个称为alpha的超参数来在套索和脊之间进行选择。并且该参数可以在0和1之间优化，这将有效地缩小一些系数，从而改进特征选择过程。但是，alpha值为0将选择L2(山脊)术语，而1将选择L1(套索)术语。</p><p id="5687" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="le">总结一下</em> </strong>:有三种流行的正则化技术，每一种都旨在减小系数的大小:</p><p id="c0bc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">岭回归，惩罚系数平方和(L2惩罚)。</p><p id="6956" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拉索回归，惩罚系数的绝对值之和(L1惩罚)。</p><p id="f0e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">弹性网，凸脊和套索的组合。</p><p id="c192" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">感谢阅读！！</strong></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="c398" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">参考文献:</strong></p><p id="7d04" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/tutorial-ridge-lasso-elastic-net</a></p></div></div>    
</body>
</html>