<html>
<head>
<title>Why study Mathematics? : Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么要学数学？:Python中的机器学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/why-study-mathematics-machine-learning-in-python-588974f6ed51?source=collection_archive---------7-----------------------#2021-01-06">https://medium.datadriveninvestor.com/why-study-mathematics-machine-learning-in-python-588974f6ed51?source=collection_archive---------7-----------------------#2021-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="8934" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated">"数学不是关于数字、方程式、计算或算法:它是关于理解."威廉·保罗·瑟斯顿</p></blockquote><figure class="jz ka kb kc kd ke gh gi paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="gh gi jy"><img src="../Images/5a0525fc8f09e3b04a034e0d1533013d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6c3tjKi6rl2dHbKX55lZeg.jpeg"/></div></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://www.pexels.com/@lum3n-44775?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Lum3n</a> from Pexels</figcaption></figure><p id="0279" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">理解是成为机器学习专家的旅程中至关重要的一部分。尽管有人可能会认为学习机器学习背后的数学是不必要的，因为python提供了大量的库来执行这些数学运算，但这是一种谬误，在鼓舞人心的ML专业人士中产生了一种错误的期望感。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="7b91" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">机器学习背后的数学</h1><p id="91c7" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">如果你在阅读这篇博客之前就感到有点焦虑，不要担心。对于一些人来说，数学可能相当复杂，尤其是对于非技术背景的人来说。你甚至可以称之为“PMSD”——数学前应激障碍。</p><p id="bc5c" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">然而，在机器学习中，人们可能不需要解决数学问题的能力，相反，人们只需要知道如何在每种情况下应用它。只有当我们知道某项任务时，我们才能命令计算机去完成它。在这个博客中，我们将看到成为机器学习天才所需的各种数学方面。</p><ul class=""><li id="a6a7" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated"><strong class="ks ir">线性代数</strong></li><li id="79d5" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated"><strong class="ks ir">概率</strong></li><li id="8eed" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated"><strong class="ks ir">统计数据</strong></li><li id="ee2e" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated"><strong class="ks ir">微积分</strong></li><li id="2c9b" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated"><strong class="ks ir">杂项</strong></li></ul><figure class="nm nn no np gt ke gh gi paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="gh gi nl"><img src="../Images/bab45563679bd9dc40e4b0c76afdd0c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoMNr3ghbVREv9vFwMU0EA.png"/></div></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Core ML vs Data Science Mathematics</figcaption></figure><h1 id="45f2" class="lu lv iq bd lw lx nq lz ma mb nr md me mf ns mh mi mj nt ml mm mn nu mp mq mr bi translated">线性代数</h1><p id="dceb" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">线性代数是机器学习中使用最广泛的数学概念。从处理基本值到一起解多个方程，线性代数涵盖了数学中许多不可避免的方面。</p><h2 id="bd85" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated"><strong class="ak">标量:</strong></h2><p id="901f" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">从小的开始，我们有标量。这些是用来代表某个事物的数值。这个东西可以是任何东西，比如温度、金钱、房子大小和其他价值。标量代表量值或绝对值，因此包含在简单的算术中。</p><ul class=""><li id="a577" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated"><strong class="ks ir">操作和实施:</strong></li></ul><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="0711" class="nv lv iq oi b gy om on l oo op"># Addition<br/>20000 + 1500 = 21500</span><span id="6da6" class="nv lv iq oi b gy oq on l oo op"># Subtraction<br/>20000 - 1500 = 18500</span><span id="d763" class="nv lv iq oi b gy oq on l oo op"># Multiplication<br/>20000 * 1500 = 30000000</span><span id="d629" class="nv lv iq oi b gy oq on l oo op"># Division<br/>20000 / 1500 = 40/3 = 13.3333</span></pre><h2 id="d3a3" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated">向量:</h2><p id="842c" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">对于不同的背景，向量被认为是不同的:</p><ul class=""><li id="fbc6" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated">在计算机科学中，向量被认为是一系列数字。</li><li id="46a8" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">物理学家将向量视为一个有方向的标量。</li><li id="1e80" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">最后，数学家认为向量是两者的结合。</li></ul><p id="3503" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">显然，我们将从机器学习的角度来看待向量，并因此将它们视为一系列数字。为了便于运算，我们用行列表格的形式表示矢量。</p><ul class=""><li id="477f" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated"><strong class="ks ir">运营与实施:</strong></li></ul><p id="d3bf" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">矢量加法(点积)<br/> </strong>矢量加法不像标量加法那么直接。它实际上是计算从两个向量得到的位移。</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9f8b51370d8bbcf93cee73c75c52cb0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*C-SOOQ2v4U1PEUMxjOLIuQ.png"/></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://brilliant.org/wiki/vector-addition/" rel="noopener ugc nofollow" target="_blank">brilliant.org</a></figcaption></figure><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="a651" class="nv lv iq oi b gy om on l oo op">import numpy as np<br/>A = np.array([1,2,3])<br/>B = np.array([4,5,6])<br/>A2 = np.array([[1,2],[3,4]])<br/>B2 = np.array([[5,6],[7,8]])</span><span id="d0fb" class="nv lv iq oi b gy oq on l oo op"># Both A and B are 1D Array<br/>np.dot(A,B)<br/>np.vdot(A,B)</span><span id="ff40" class="nv lv iq oi b gy oq on l oo op"># Both A2 and B2 are 2D Array<br/>print(np.dot(A2,B2))</span><span id="9e7c" class="nv lv iq oi b gy oq on l oo op"># Preferred Functions for 2D Dot Product:<br/>print(np.matmul(A2,B2))<br/>print(A2 @ B2)</span><span id="c6c1" class="nv lv iq oi b gy oq on l oo op"># Either A or B is scaler<br/>print(np.dot(A,2))</span><span id="2548" class="nv lv iq oi b gy oq on l oo op"># Preferred Function for Scaler Multiplication with Vector:<br/>print(np.multiply(A,2))<br/>print(A*2)</span></pre><p id="a33e" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">向量乘法(叉积)<br/> </strong>向量乘法计算一个垂直于给定向量的向量。换句话说，它是垂直于由两个给定向量形成的区域的向量。</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div class="gh gi os"><img src="../Images/a3c57d57624c8d30846428df68025690.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*_y_cWZuPfW5jbncpWt-B0w.png"/></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://brilliant.org/wiki/cross-product-definition/" rel="noopener ugc nofollow" target="_blank">brilliant.org</a></figcaption></figure><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="eaad" class="nv lv iq oi b gy om on l oo op">import numpy as np<br/>A = np.array([1,2,3])<br/>B = np.array([4,5,6])<br/>A2 = np.array([[1,2],[3,4]])<br/>B2 = np.array([[5,6],[7,8]])</span><span id="1f27" class="nv lv iq oi b gy oq on l oo op"># Vector Multiplication / Cross Product<br/>print(np.cross(A,B))<br/>print(np.cross(A2,B2))</span></pre><h2 id="0aa5" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated">矩阵:</h2><p id="d940" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">矩阵是元素的矩形阵列或表格，按行和列排列。矩阵的维数可以表示为m×n。矩阵通常用于简洁地写出和处理多个线性方程，称为线性方程组。矩阵中的每个元素称为一个元素。<br/>以下是几种不同类型的矩阵:</p><ul class=""><li id="22e5" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated">行矩阵:有一行和多列</li><li id="4a99" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">列矩阵:有一列和多行</li><li id="2f26" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">方阵:m —行数= n —列数</li><li id="3549" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">单位矩阵:所有元素都是1</li><li id="9d4a" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">对角矩阵:只有对角元素是值，其余为零。</li></ul><figure class="nm nn no np gt ke gh gi paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="gh gi ot"><img src="../Images/688de9939e531033aed0475e7ce3b11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5e0ITly6_qKs_lEKMPQTFw.png"/></div></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://commons.wikimedia.org/w/index.php?curid=79728977" rel="noopener ugc nofollow" target="_blank">Svgo</a></figcaption></figure><ul class=""><li id="5d1e" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated"><strong class="ks ir">运营与实施:</strong></li></ul><p id="c2d2" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">矩阵乘法<br/> </strong>矩阵相乘时，第一个矩阵中的行的元素与第二个矩阵中相应的列相乘。如果A是一个n×m矩阵，B是一个m×p矩阵，它们相乘的结果AB是一个n×p矩阵，只有当A中的列数m等于B中的行数m时才定义这个矩阵。</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/29c903cb28785d1a9afcb16756b8a70c.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*FHzS5w6i8to4WUynqmHM-g.png"/></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://courses.lumenlearning.com/boundless-algebra/chapter/introduction-to-matrices/#:~:text=the%20jth%20column.-,Matrices%20can%20be%20used%20to%20compactly%20write%20and%20work%20with,also%20known%20as%20linear%20maps." rel="noopener ugc nofollow" target="_blank">lumenlearning.com</a></figcaption></figure><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="b7fe" class="nv lv iq oi b gy om on l oo op">import numpy as np<br/>Mat1 = np.matrix('1 2 3; 4 5 6')<br/>Mat2 = np.matrix('7 8 9; 10 11 12')<br/>Mat3 = np.matrix('7 8; 9 10; 11 12')</span><span id="68e0" class="nv lv iq oi b gy oq on l oo op"># Find shape/dimensions of a Matrix<br/>print(np.shape(Mat1))<br/>print(np.shape(Mat2))<br/>print(np.shape(Mat3))</span><span id="84df" class="nv lv iq oi b gy oq on l oo op"># Multiply two matrix<br/>np.matmul(Mat1,Mat3)</span><span id="679a" class="nv lv iq oi b gy oq on l oo op"># Common Error<br/>np.matmul(Mat1,Mat2)<br/>ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3)<br/># Simply means that the n dimension of Mat1 is not equal to m dimension of Mat2. This is a required condition for matrix multiplication.</span></pre><p id="b416" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">矩阵转置<br/> </strong>矩阵的转置是行和列值的交换。这是通过沿其主对角线反射元素来实现的。任何矩阵的转置都用上标t表示。</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/755063effbfdd6c54ef83d06d8f8be3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/1*_UMA3UJySHT81IeO_4z3Sg.gif"/></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://commons.wikimedia.org/w/index.php?curid=21897854" rel="noopener ugc nofollow" target="_blank">Lucas Vieira</a></figcaption></figure><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="c38d" class="nv lv iq oi b gy om on l oo op">import numpy as np<br/>Mat3 = np.matrix('7 8; 9 10; 11 12')</span><span id="fe09" class="nv lv iq oi b gy oq on l oo op"># Transpose using NumPy<br/>np.transpose(Mat3)</span><span id="0233" class="nv lv iq oi b gy oq on l oo op"># .T function only works on NumPy arrays<br/>Mat3.T</span><span id="9988" class="nv lv iq oi b gy oq on l oo op"># Other Function<br/>Mat3.getT()</span></pre><p id="547c" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">矩阵行列式<br/> </strong>矩阵的行列式给出矩阵中特征值的乘积。它只是告诉你矩阵的标量。</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/b6b015eeba5946ca77443934a5f8d089.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*eS-zNNIkWOWsntzk4eeSpw.png"/></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://www.cprogramcoding.com/p/box-sizing-border-box_474.html" rel="noopener ugc nofollow" target="_blank">cprogramcodeing.com</a></figcaption></figure><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="04cc" class="nv lv iq oi b gy om on l oo op">import numpy as np</span><span id="a835" class="nv lv iq oi b gy oq on l oo op"># Determinant<br/>Mat4 = np.matrix('1 2 3; 6 5 4; 8 7 9')<br/>np.linalg.det(Mat4)</span></pre><blockquote class="ox oy oz"><p id="2c19" class="kq kr pa ks b kt ku kv kw kx ky kz la pb lc ld le pc lg lh li pd lk ll lm jx ij bi translated"><strong class="ks ir">注:</strong>行列式只存在于方阵。</p></blockquote><p id="ef48" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">矩阵求逆<br/> </strong>在线性代数中，一个n乘n的方阵M称为可逆的，如果存在一个n乘n的方阵P使得:<strong class="ks ir"> MP = PM = I(单位矩阵)</strong></p><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="351b" class="nv lv iq oi b gy om on l oo op">import numpy as np</span><span id="9360" class="nv lv iq oi b gy oq on l oo op">x = np.array([[1,2],[3,4]])<br/>np.linalg.inv(x)</span></pre><p id="f0cd" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">使用矩阵求解方程</strong></p><p id="deaa" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">用矩阵解线性方程组有两种方法。</p><ol class=""><li id="08a3" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx pe nd ne nf bi translated">行列梯队法</li><li id="c060" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx pe nd ne nf bi translated">逆矩阵法</li></ol><blockquote class="ox oy oz"><p id="f9a8" class="kq kr pa ks b kt ku kv kw kx ky kz la pb lc ld le pc lg lh li pd lk ll lm jx ij bi translated"><strong class="ks ir">注意:</strong>我们不会在纸上讨论他们的工作，但是我们会看到如何使用Python中的NumPy来求解方程组。</p></blockquote><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="d90f" class="nv lv iq oi b gy om on l oo op">import numpy as np</span><span id="6ce2" class="nv lv iq oi b gy oq on l oo op">a = np.array([[3,1], [1,2]])<br/>b = np.array([9,8])<br/>np.linalg.solve(a, b)</span></pre><blockquote class="ox oy oz"><p id="11fe" class="kq kr pa ks b kt ku kv kw kx ky kz la pb lc ld le pc lg lh li pd lk ll lm jx ij bi translated"><strong class="ks ir">注:</strong> <code class="fe pf pg ph oi b"><em class="iq">tensordot()</em></code>、<code class="fe pf pg ph oi b"><em class="iq">linalg.tensorinv()</em></code>、<code class="fe pf pg ph oi b">linalg.tensorsolve()</code>可用于计算ndarrays的点积、逆、解。</p></blockquote><p id="398a" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><strong class="ks ir">特征向量:</strong></p><p id="9c15" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">设X是一个方阵。非零向量V是X的具有特征值Ev的本征向量，如果:<br/> <strong class="ks ir"> XV = EvV <br/> </strong>重新排列这个，我们可以得到一个齐次方程组<br/><strong class="ks ir">【X-EvI】V = 0<br/></strong>非平凡解只有当矩阵<strong class="ks ir"> (X-EvI) = 0时才存在。</strong></p><pre class="nm nn no np gt oh oi oj ok aw ol bi"><span id="3130" class="nv lv iq oi b gy om on l oo op">import numpy as np</span><span id="57ad" class="nv lv iq oi b gy oq on l oo op"># Eigen Values and Eigen Vector<br/>X = np.diag((1,2,3))<br/>Ev, V = np.linalg.eig(X)<br/>print(Ev)<br/>print(V)</span><span id="2b3c" class="nv lv iq oi b gy oq on l oo op"># Only Eigen Values<br/>X = np.diag((1,2,3))<br/>np.linalg.eigvals(X)</span></pre><h1 id="6624" class="lu lv iq bd lw lx nq lz ma mb nr md me mf ns mh mi mj nt ml mm mn nu mp mq mr bi translated">可能性</h1><p id="5d47" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">概率最简单的定义是某事发生的几率。也就是说，一个值是与某件事情发生的可能性联系在一起的。在我们正式定义概率之前，让我们看一下行话:</p><ul class=""><li id="a59b" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated">实验:我们想要计算发生或不发生的可能性的场景。</li><li id="6df5" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">结果:我们从场景中得到的结果。</li><li id="40a5" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">事件:实验中特定结果的情景。</li><li id="31d2" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">概率:事件发生的可能性。</li></ul><p id="1def" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">现在我们可以说:“概率是一个事件发生的可能性的度量。”</p><blockquote class="jn"><p id="ebe2" class="jo jp iq bd jq jr pi pj pk pl pm jx dk translated">概率=期望事件/总结果</p></blockquote><h2 id="1d03" class="nv lv iq bd lw nw pn dn ma ny po dp me lb pp ob mi lf pq od mm lj pr of mq og bi translated">更多术语</h2><ul class=""><li id="9ba8" class="mx my iq ks b kt ms kx mt lb ps lf pt lj pu jx nc nd ne nf bi translated">随机实验:结果无法确定预测的实验。</li><li id="a2f2" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">样本空间:随机实验的全部可能结果。</li><li id="1a8e" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">事件-分离:分离的事件没有任何共同的结果。</li><li id="6642" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">事件-联合:联合事件可以有共同的结果。</li></ul><h2 id="5ff6" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated">不同类型的概率</h2><p id="a78f" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">我们在不同的情况下处理不同类型的概率。我们将简要讨论这些类型:</p><ul class=""><li id="7bab" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated">边际概率:是指一个事件的发生不需要任何干预或依赖他人。</li><li id="88b8" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">联合概率:它是对同时一起发生的两个事件的度量。</li><li id="fe75" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">条件概率:它是一种只有在其他事件已经发生的情况下才会发生的事件的度量。</li></ul><h2 id="6887" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated">概率分布</h2><p id="4b3c" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">概率帮助我们理解数据的工作原理。它帮助数据科学家预测未来趋势。对于机器学习来说，概率分布的知识是必须的。概率分布有3种基本类型:</p><ul class=""><li id="924f" class="mx my iq ks b kt ku kx ky lb mz lf na lj nb jx nc nd ne nf bi translated">正态分布:它是表示平均值的对称性质的分布。它创建了一个<em class="pa">钟形曲线</em>。</li><li id="4225" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">中心极限定理:它指出，如果样本量足够大，任何独立的随机变量的均值的抽样分布将是正态或接近正态的。</li><li id="94e4" class="mx my iq ks b kt ng kx nh lb ni lf nj lj nk jx nc nd ne nf bi translated">概率密度:它是关于一个连续的随机变量在给定值上发生的相对可能性。</li></ul><p id="9ab7" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">所有这些概念对机器学习都至关重要。然而，在这篇博客中，我们将讨论最常用的最大似然定理——贝叶斯定理。</p><h2 id="126c" class="nv lv iq bd lw nw nx dn ma ny nz dp me lb oa ob mi lf oc od mm lj oe of mq og bi translated">贝叶斯定理</h2><p id="83ec" class="pw-post-body-paragraph kq kr iq ks b kt ms kv kw kx mt kz la lb mu ld le lf mv lh li lj mw ll lm jx ij bi translated">贝叶斯定理用于计算两个事件的条件概率。它基于与实验相关的条件的先验知识来计算事件发生的概率。贝叶斯定理的计算方法如下:</p><figure class="nm nn no np gt ke gh gi paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="gh gi pv"><img src="../Images/700a65c1d9f07a4412c591bad7161d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eDCBxkWKYkmWuVL6DQFNPw.jpeg"/></div></div><figcaption class="kl km gj gh gi kn ko bd b be z dk">Image Source: <a class="ae kp" href="https://luminousmen.com/" rel="noopener ugc nofollow" target="_blank">Luminousmen.com</a></figcaption></figure><p id="0164" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated">上面的信息图很好地展示了我们如何计算贝叶斯定理。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="f7ab" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm jx ij bi translated"><em class="pa">本博客涵盖了前两部分——线性代数和机器学习背后的数学概率。它涵盖了这些主题的基础和重要部分。本文档旨在提供相关的理论知识。强烈推荐多读线性代数和概率方面的知识，以获取更深入的知识。</em></p><blockquote class="jn"><p id="3b2b" class="jo jp iq bd jq jr pi pj pk pl pm jx dk translated">感谢阅读。<br/>别忘了点击👏！</p></blockquote><p id="db0d" class="pw-post-body-paragraph kq kr iq ks b kt pw kv kw kx px kz la lb py ld le lf pz lh li lj qa ll lm jx ij bi translated"><strong class="ks ir">进入专家视图— </strong> <a class="ae kp" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="ks ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>