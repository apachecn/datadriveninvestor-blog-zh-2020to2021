<html>
<head>
<title>Edge AI - Computer vision inference on the edge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">边缘人工智能——边缘的计算机视觉推理</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/edge-ai-computer-vision-inference-on-the-edge-part-2-2-aaddfae870f0?source=collection_archive---------2-----------------------#2020-08-11">https://medium.datadriveninvestor.com/edge-ai-computer-vision-inference-on-the-edge-part-2-2-aaddfae870f0?source=collection_archive---------2-----------------------#2020-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="0030" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">卡斯滕·莫宁博士</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/721c7cb08e8c6e86cb6a002acb78ef43.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*fX8lbYYT-10XBaKZVbjZEw.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Object detection on video frames using YOLOv3-tiny on a Raspberry Pi with an Intel Neural Compute Stick 2. Input video source: <a class="ae la" href="https://bitbucket.org/merayxu/multiview-object-tracking-dataset/src/master/" rel="noopener ugc nofollow" target="_blank">CAMPUS dataset</a> for multi-view object tracking [1]</figcaption></figure><p id="f754" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi lb translated">这是关于在边缘上运行人工智能应用的两部分系列的最后一部分，特别是在英特尔神经计算棒2加速的Raspberry Pi 3上运行计算机视觉推理。</p><p id="3940" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看看<a class="ae la" href="https://medium.com/datadriveninvestor/edge-ai-computer-vision-on-the-edge-dfa4ad604651" rel="noopener">第1部分</a>获取:</p><ol class=""><li id="be69" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn lp lq lr ls bi translated">edge AI硬件加速器和开发板选项概述，</li><li id="6730" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">关于如何将带有板载摄像头的Raspberry Pi配置为edge AI就绪设备的指南，</li><li id="9381" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">关于如何在Raspberry Pi上安装英特尔OpenVINO toolkit的指南，例如，在您的笔记本电脑上安装，以便为在Raspberry Pi上执行计算机视觉推理做好准备。</li></ol><p id="6588" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二部分建立在步骤2和3的基础上。虽然第1部分以运行在Raspberry Pi上的OpenVINO人脸检测演示应用程序结束，但本文将讨论在这样的边缘设备上开发您自己的计算机视觉定制解决方案。更具体地说，我将带您在一个Raspberry Pi上完成YOLOv3-tiny对象检测模型的定制实现，如第1部分所述。(关于YOLOv3物体探测模型的细节可以在<a class="ae la" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">这里</a>找到。)但是，首先，让我们仔细看看开源的英特尔OpenVINO工具包，特别是用于Raspbian操作系统的OpenVINO工具包<a class="ae la" href="https://docs.openvinotoolkit.org/latest/openvino_docs_install_guides_installing_openvino_raspbian.html" rel="noopener ugc nofollow" target="_blank">。🗹</a></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="907d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">英特尔OpenVINO工具包</strong></p><p id="7f14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">引用官方OpenVINO toolkit文档:“open vino toolkit是一个全面的工具包，用于快速开发模拟人类视觉的应用程序和解决方案。基于卷积神经网络(CNN)，该工具包扩展了英特尔硬件上的CV工作负载，最大限度地提高了性能。”</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/8c335d29d9a251a7c92e406c66b4fa73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*Mx8YSTErCLcU2Hnu7rShSQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">OpenVINO toolkit high-level capabilities (Source: <a class="ae la" href="https://docs.openvinotoolkit.org/" rel="noopener ugc nofollow" target="_blank">OpenVINO</a>)</figcaption></figure><p id="739f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">进一步来说，该工具包:</p><ul class=""><li id="4ff7" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn mg lq lr ls bi translated">“在边缘实现基于CNN的深度学习推理</li><li id="8a41" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">支持在英特尔CPU、英特尔集成显卡、英特尔FPGA、英特尔神经计算棒2和采用英特尔m ovidius VPUs的英特尔视觉加速器设计上的异构执行</li><li id="f7f0" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">通过易于使用的计算机视觉函数库和预先优化的内核加快上市时间</li><li id="7fcf" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">包括对计算机视觉标准的优化调用，包括OpenCV和OpenCL。"</li></ul><p id="696c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除其他外，这意味着该工具包特别适合在边缘设备上开发计算机视觉推理应用，如通过采用英特尔Movidius VPU的英特尔Neural Compute Stick 2 USB加速器棒增强的Raspberry Pi。</p><p id="5ada" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡如果你可能想知道:“OpenVINO”代表“开放视觉推理和神经网络优化”，表明它的深度学习计算机视觉重点。⚡</p><p id="ade0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">OpenVINO工具包的主要元素包括:</p><ul class=""><li id="6dc7" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn mg lq lr ls bi translated">模型优化器，</li><li id="c6db" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">推理机和</li><li id="b052" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">开放式模型动物园。</li></ul><p id="1935" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型优化器采用在Caffe、TensorFlow、MXNet、Kaldi或ONNX框架中预训练的深度学习计算机视觉模型，并将它们转换为OpenVINO中间表示(IR)，这是一种使用在边缘设备上运行的推理引擎针对模型执行优化的简化模型表示。推理引擎使用一个通用的API为CPU和GPU以及VPU硬件加载和推断这些IR文件。</p><p id="fb38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae la" href="https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/pretrained-models.html" rel="noopener ugc nofollow" target="_blank">开放模型动物园</a>提供免费下载的预训练模型，可用于加速开发过程，而不必首先训练自己的模型，然后需要使用模型优化器将其转换为中间表示，然后才能用于所考虑的边缘设备上的推理引擎。</p><p id="ec1d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当不使用来自开放模型动物园的预训练模型时，使用OpenVINO工具包在边缘部署深度学习模型的典型工作流如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/bc203a4834f25625bc10abae1b73cff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oqcW7zUPF00PqzkK.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Typical workflow for deploying a deep learning model using the OpenVINO toolkit (Source: <a class="ae la" href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Introduction.html" rel="noopener ugc nofollow" target="_blank">OpenVINO</a>)</figcaption></figure><p id="5995" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，开放模型Zoo模型的优势在于已经被转换为OpenVINO中间表示，但是中间表示是什么呢？</p><p id="e962" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">IR描述了输入模型的结果，该输入模型已经在以下一种或多种技术的帮助下针对边缘推断进行了优化:</p><ul class=""><li id="d5d3" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn mg lq lr ls bi translated">量化，即模型权重和偏差的数值精度的降低，</li><li id="304d" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">模型层融合，即将多个模型层合并为一个，</li><li id="8f99" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">冻结，即删除仅对模型训练有用的元数据和操作(仅在TensorFlow模型的情况下)。</li></ul><p id="9ab6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当然，您也可以完全自由地使用其他来源的模型。不过，这些必须经过OpenVINO工作流程的第二步。也就是说，您必须将模型优化器应用于输入模型，以生成其推理优化的IR。</p><p id="2f20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦IR生成，它的。xml(优化模型拓扑)和。bin(优化的模型权重和偏差)文件可以被输入到在边缘设备上运行的OpenVINO推理引擎中，以便在边缘设备上进行性能优化的推理。</p><p id="ef32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实上，在这个由两部分组成的博客系列的第1部分中的人脸检测示例中，我们已经以简单的方式完成了这个工作流程:我们从开放模型动物园下载了一个预先训练好的人脸检测模型。更具体地说，由于OpenVINO Model Downloader在用于Raspbian OS的OpenVINO toolkit中不可用，我们将IR文件手动下载到Raspberry Pi上，并将它们输入到推理引擎中，用于在输入图像上生成边界框。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/aa51c3449d721b57f2597178ab563c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*q1fw00v7SFM52kk_P_CqdQ.png"/></div></figure><p id="5660" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在将采取稍微更具挑战性的路线，通过模型优化器将预训练的公共模型，YOLOv3 [2]的“微小”版本，转换成IR。这就引出了一个关键点:由于OpenVINO推理引擎代表了用于Raspbian OS的OpenVINO工具包中唯一的OpenVINO主元素，所以模型转换需要在Raspberry Pi环境之外完成，例如，在您的主机上，这意味着您也需要在您的主机上安装OpenVINO。在OpenVINO <a class="ae la" href="https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html" rel="noopener ugc nofollow" target="_blank"> Linux </a>、<a class="ae la" href="https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_macos.html" rel="noopener ugc nofollow" target="_blank"> macOS </a>和<a class="ae la" href="https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html" rel="noopener ugc nofollow" target="_blank"> Windows </a>安装指南的帮助下，这应该不是太大的挑战。</p><p id="434d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡对于Windows的OpenVINO toolkit，您可以不使用Microsoft Visual Studio和CMake安装步骤，因为在下面，我们将只使用模型优化器。但是，如果您想在Windows机器上安装完整的OpenVINO，当然强烈建议您完成所有的安装步骤。⚡</p><p id="3355" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，您已经为在您的Raspberry Pi上定制实现YOLOv3-tiny [2]模型做好了准备。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="a7cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将预训练的YOLOv3-tiny模型转换为OpenVINO中间表示</strong></p><p id="a9ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在您的主机上安装OpenVINO之后，让我们生成YOLOv3-tiny模型的IR。YOLOv3-tiny的网络架构针对低性能设备进行了执行优化，但代价是降低了预测精度:它只使用了19个卷积层，而不是标准YOLOv3模型的53个卷积层。因此，它的执行效率比标准YOLOv3模型高得多，但准确性也低得多，平均精度(mAP)为33.1% ，而标准YOLOv3模型的mAP值为51–57%。</p><p id="fed8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了将YOLOv3-tiny模型转换为IR格式，我们将主要遵循OpenVINO <a class="ae la" href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_YOLO_From_Tensorflow.html" rel="noopener ugc nofollow" target="_blank">指南来转换YOLO模型</a>。</p><p id="b75b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第一步</strong>:克隆下面的GitHub库，得到一个基于TensorFlow的YOLOv3-tiny实现。</p><p id="2aa1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡原则上，您也可以使用任何其他基于TensorFlow的YOLOv3-tiny模型实现，但当您尝试使用以下步骤将其转换为中间表示时，可能会遇到困难。⚡</p><p id="2a74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">git clone <a class="ae la" href="https://github.com/mystic123/tensorflow-yolo-v3.git" rel="noopener ugc nofollow" target="_blank">https://github.com/mystic123/tensorflow-yolo-v3.git</a></code></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mr"><img src="../Images/8f4fd507124f4e761f47d64a2ffc4b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ej-uC6JR2j6gqcJTLtYpQQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">YOLOv3 repository cloning</figcaption></figure><p id="57b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第二步</strong>:下载一组类标签，例如以<a class="ae la" href="https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names" rel="noopener ugc nofollow" target="_blank"> coco.names </a>的形式，其中包括80个常见的对象类，从人到汽车和动物，或者提供您自己的一组您想要使用YOLOv3-tiny进行推理的类标签，前提是YOLOv3-tiny已经对您选择的类进行了预训练。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/7a7d15babc6d5773d7b7626d40d3b33b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7An-z8VfizYBUKRlGKvew.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">YOLOv3 repository contents and coco.names file</figcaption></figure><p id="43a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤3 </strong>:下载预先训练好的<a class="ae la" href="https://pjreddie.com/media/files/yolov3-tiny.weights" rel="noopener ugc nofollow" target="_blank"> YOLOv3-tiny权重</a>，或者自己训练YOLOv3-tiny并使用得到的模型权重。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/94c63ceee89424feb7d004992ece489f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWHDx30A1SeVq1JUY6-pEQ.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Pre-trained YOLOv3-tiny weights (yellow) and the frozen model definition (red)</figcaption></figure><p id="1dba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤4 </strong>:将YOLOv3模型转换为协议缓冲文件格式(。pb)以获得用于推断的简化的，即“冻结的”模型定义。</p><p id="9f38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">python convert_weights_pb.py --class_names coco.names --data_format NHWC --weights_file yolov3-tiny.weights --tiny</code></p><p id="526e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">statemenet末尾的<code class="fe mn mo mp mq b">--tiny</code>参数指示转换器生成YOLOv3-tiny版本的冻结张量流图。</p><p id="003f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">步骤5 </strong>:将冻结的模型定义转换为IR格式。在此步骤之前，不要忘记执行位于<code class="fe mn mo mp mq b">$OPENVINO_INSTALL_DIR/bin/</code>中的OpenVINO <code class="fe mn mo mp mq b">setupvars</code>环境设置脚本。</p><p id="e08f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">python mo_tf.py --input_model frozen_darknet_yolov3_model.pb --tensorflow_use_custom_operations_config &lt;path/&gt;yolo_v3_tiny.json --batch 1 --generate_deprecated_IR_V7</code></p><p id="5a64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mn mo mp mq b">yolo_v3_tiny.json</code>模型配置文件的OpenVINO <code class="fe mn mo mp mq b">&lt;path/&gt;</code>应该是这样的:</p><p id="594c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">$OPENVINO_INSTALL_DIR/deployment_tools/model_optimizer/extensions/front/tf/</code></p><p id="a356" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡<code class="fe mn mo mp mq b">--generate_deprecated_IR_V7 </code>参数强制模型优化器生成旧的IR版本7。这是由于在第1部分的<a class="ae la" href="https://medium.com/datadriveninvestor/edge-ai-computer-vision-on-the-edge-dfa4ad604651" rel="noopener">中已经提到的与IR版本10相关的</a><a class="ae la" href="https://software.intel.com/en-us/node/849460" rel="noopener ugc nofollow" target="_blank">兼容性错误</a>。如果不使用这个参数，您转换的模型将在推断过程中失败，并显示一些难以处理的错误消息。⚡</p><p id="9007" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡如果你想使用你转换的模型运行OpenVINO演示，请注意你可能需要添加<code class="fe mn mo mp mq b">--reverse_input_channel</code>选项，因为OpenVINO演示通常期望颜色通道是BGR而不是通常的RGB顺序。参考官方OpenVINO演示文档，查看演示的具体输入要求。⚡</p><p id="eb65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">结果，我们得到了边优化的YOLOv3-tiny网络拓扑(.xml)以及权重和偏差项(。bin) IR文件。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mu"><img src="../Images/8f13f3f167e2ab5eeb98fa2e2a1294eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZTDfsvBjL9TSvGzosUjbLg.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">YOLOv3-tiny Intermediate Representation files</figcaption></figure><p id="562e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了测试生成的IR文件的有效性，您可能需要在您的主机上运行open vino<a class="ae la" href="https://docs.openvinotoolkit.org/latest/_demos_python_demos_object_detection_demo_yolov3_async_README.html" rel="noopener ugc nofollow" target="_blank">object detection yolo v3 demo</a>。在这种情况下，请考虑上面的<code class="fe mn mo mp mq b">--reverse_input_channel</code>评论。</p><p id="fa2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">样本输入视频的结果如下所示。<a class="ae la" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">正如在使用YOLOv3-tiny进行推断时所预期的那样</a>，帧速率很容易在非边缘设备上达到接近实时的推断，代价是部分相当差的汽车检测和边界框精度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/5ff422e3d27a76978fa8444f459621e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*vpGLHYqt1bCWwgfV1RC6RA.gif"/></div></figure><p id="ff6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要结束这个准备阶段，只需将IR文件传输到您的Raspberry Pi。🗹</p><p id="cc3b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这让我们将转换后的YOLOv3-tiny模型合并到一个在Raspberry Pi上运行的定制开发的应用程序中。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="201c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">边缘上使用YOLOv3-tiny的物体检测应用</strong></p><p id="ffcf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的应用程序由三个主要元素组成，一般来说，它们代表了大多数基于OpenVINO的edge应用程序:</p><ul class=""><li id="03ba" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn mg lq lr ls bi translated">推理机模型加载</li><li id="e177" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">基于应用程序的输入处理和推理请求</li><li id="a4b4" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn mg lq lr ls bi translated">基于App的推理结果和输出处理</li></ul><p id="43a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这反映在我们的Raspberry PI应用程序的以下代码结构中:</p><ol class=""><li id="ee48" class="lk ll it js b jt ju jx jy kb lm kf ln kj lo kn lp lq lr ls bi translated">用于将IR加载到推理引擎并执行推理的助手类</li><li id="06e1" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">由视频和摄像机输入预处理、帧提取和推理以及视频输出处理组成的核心例程</li><li id="46e5" class="lk ll it js b jt lt jx lu kb lv kf lw kj lx kn lp lq lr ls bi translated">yolov 3-微型模型参数化类</li></ol><p id="0f2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用推理引擎API的Python包装器。代码库可以在GitHub的<a class="ae la" href="https://github.com/cm230/Computer-Vision-On-The-Edge" rel="noopener ugc nofollow" target="_blank">这里</a>找到。关于YOLO推理处理的元素在很大程度上与例如来自<a class="ae la" href="https://docs.openvinotoolkit.org/latest/omz_demos_python_demos_object_detection_demo_yolov3_async_README.html" rel="noopener ugc nofollow" target="_blank"> OpenVINO YOLO橱窗</a>的<code class="fe mn mo mp mq b">object_detection_demo_yolov3_async.py</code>内的相应部分一致。因此，为了简单明了，在下面，我只讨论特定于OpenVINO的元素。</p><p id="ba87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">先来快速看一下Python <code class="fe mn mo mp mq b">class YoloParams</code>形式的YOLOv3-tiny参数化。它的值对应于最初的<a class="ae la" href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg" rel="noopener ugc nofollow" target="_blank">暗网参数化，</a>所以原则上这并不奇怪。然而，选择“正确的”锚定值集(第9–10行)既是一门科学，也是一门艺术，此处使用的特定锚定值集可能不是下面进一步考虑的特定对象检测场景的最佳选择。所以，如果你也对YOLO锚的参数选择感到疑惑，请看这里的<a class="ae la" href="https://github.com/pjreddie/darknet/issues/568" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="a181" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由助手<code class="fe mn mo mp mq b">class Network</code>提供的特定于OpenVINO的方法在这个由两部分组成的博客系列的上下文中更加相关:<code class="fe mn mo mp mq b">load_model</code>方法实例化了推理引擎核心类的一个对象，<code class="fe mn mo mp mq b">IECore</code>(下面的第24行)。这个类在支持的设备之间提供了一个抽象层，隐藏了任何设备细节。因此，该类的方法通常期望实际使用的设备作为参数，这就是为什么设备参数包含在参数列表<code class="fe mn mo mp mq b">load_model</code>中的原因。由于我们将英特尔神经计算棒2作为推理设备，因此该参数的默认值被设置为“MYRIAD”。</p><p id="e6b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之前创建的红外文件形式的yolo v3-微小物体检测模型被读入一个<code class="fe mn mo mp mq b">IENetwork</code>物体(第29行)。<code class="fe mn mo mp mq b">IENetwork</code>类支持读取和操作模型参数，如批量大小、数值精度、模型形状和各种层属性。该功能用于将批量大小设置为1(第31行)，因为我们将一次对一帧执行推断。通过查看模型的输入拓扑，在第33行中也断言了这个单一输入需求。如果成功，模型的可执行版本在第36行被实例化，并被加载到无数设备中用于推断。最后，第39–40行在<code class="fe mn mo mp mq b">IENetwork</code>对象的帮助下为模型输入和输出层创建可迭代对象。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="7801" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡:由于我们正在处理各种各样的设备，为了保持简单，我选择了反对对不支持的网络层进行通常的安全检查，这在使用CPU设备进行视觉推断时是需要的:由于进入IR文件生成的模型优化器预处理是设备不可知的，所以优化的神经网络可能包含不被正在使用的特定设备CPU支持的层。因此，在进行基于CPU的推断之前，需要检查是否有任何不支持的层，并且如果遇到不支持的层，实际上将退出进一步的处理，以防止任意结果或系统崩溃。由于上述代码行也支持基于CPU的推理，如果您想使用Raspberry Pi的CPU而不是英特尔Neural Compute Stick 2来试验推理，请查看，例如，<code class="fe mn mo mp mq b">object_detection_demo_yolov3_async.py</code>了解所需的额外层支持相关代码行。⚡</p><p id="80b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就<code class="fe mn mo mp mq b">class Network</code>中的其他方法而言，<code class="fe mn mo mp mq b">get_input_shape</code>方法使用<code class="fe mn mo mp mq b">IENetwork</code>对象返回模型输入层的形状。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="5640" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">方法<code class="fe mn mo mp mq b">async_inference</code>获取一个输入请求ID，并为该请求执行异步推断。因为我们没有进行真正的异步处理，而是一次将一帧推送到推理引擎，所以请求ID被设置为0。真正的异步处理需要为不同的推理请求使用不同的id，将额外的帧推送到推理引擎，同时它可能仍然在处理以前的请求。然后，这些差异请求id允许区分各种推断结果，并执行单独需要的进一步处理。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="0942" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我们正在执行异步处理，<code class="fe mn mo mp mq b">wait</code>方法支持在尝试检索任何推理结果之前等待该过程完成。请注意，这个状态检查需要参考正确的请求ID来完成，因此，在本例中为0。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="c0ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">方法<code class="fe mn mo mp mq b">extract_output</code>然后返回由针对可执行网络的推理请求生成的推理输出。请求索引需要再次设置为0，以引用正确的推理请求。<a class="ae la" href="https://docs.openvinotoolkit.org/2018_R5/_ie_bridges_python_docs_api_overview.html" rel="noopener ugc nofollow" target="_blank"> wait函数的value -1 </a>确保一旦流程完成就返回状态。它有效地阻止了处理，直到超时或者结果可用，无论哪一个先出现。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="f09f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于各种与OpenVINO相关的助手方法已被排除在外，因此视频推断核心例程<code class="fe mn mo mp mq b">infer_video</code>的讨论将留在<a class="ae la" href="https://github.com/cm230/Computer-Vision-On-The-Edge" rel="noopener ugc nofollow" target="_blank"> GitHub存储库</a>的tinyYOLOv3.py文件中，该文件使用助手方法获取视频或摄像机输入，以便生成屏幕和视频文件推断结果。</p><p id="8afe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">infer_video</code>的OpenVINO相关初始化内容包括借助<code class="fe mn mo mp mq b">Network class</code>(下面第7行)的推理机初始化，接着是模型的输入形状检索(第18行)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="1ff6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">net</code>对象随后用于对一个单独的预处理视频帧发起异步推理请求(下面的第3行),并在等待异步推理完成后(第8行),使用<code class="fe mn mo mp mq b">extract_output</code>助手方法检索推理结果(第9行)。然后，该输出被解析为适用于YOLOv3模型的情况。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="d19c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡为了稍后将推断结果写入视频中，在<code class="fe mn mo mp mq b">infer_video</code>方法中，我使用了OpenCV VideoWriter类和一个。avi输出文件扩展名和MP42编解码器:<code class="fe mn mo mp mq b">codec = cv2.VideoWriter_fourcc("M","P","4","2").</code>为特定设备设置寻找编解码器和文件扩展名的“正确”组合可能有些令人沮丧。例如，参见Pyimagesearch的Adrian Rosebrock撰写的<a class="ae la" href="https://www.pyimagesearch.com/2016/02/22/writing-to-video-with-opencv/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。它为您提供了一些提示，告诉您对于您的特定设置，哪些组合可能是有效的。此外，它建议在您的Raspberry Pi上安装用于FFMPEG的Python绑定。这在我的情况下相当有效。然而事实上。安装FFMPEG库后，avi文件扩展名与MP42编解码器的组合适用于我的设置，但这并不意味着它也适用于您的设置，您可能会发现自己不得不经历许多组合，直到获得有效的视频输出。⚡</p><p id="61f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">⚡请注意，<code class="fe mn mo mp mq b">infer_video</code>方法还利用Pyimagesearch 的<a class="ae la" href="https://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/" rel="noopener ugc nofollow" target="_blank"> imutils包中的video包来计算处理管道支持的每秒帧数(FPS)。⚡</a></p><p id="7f00" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在准备在Raspberry Pi上测试我们的应用程序，使用MP4输入视频和Raspberry Pi的板载摄像头场景。</p><p id="71e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于MP4输入文件，下面的命令启动应用程序，读入输入文件，生成一个屏幕窗口，以检测到的对象周围的边界框的形式显示推理结果，包括相应的标签，并将这些结果写入。avi输出文件:</p><p id="0a14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">python tinyYOLOv3.py --m "frozen_darknet_yolov3_model.xml" --i "test_video.mp4" --l "coco.names"</code></p><p id="351a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，您可能需要添加模型的特定路径、输入视频和类标签文件参数。所用测试文件[1]的输出视频如下所示。由于英特尔神经计算棒2的帮助，Raspberry Pi管理着大约3 FPS的可观帧速率。如果没有这个硬件加速器，处理速度会慢得像爬行一样，因为Raspberry Pi CPU大部分时间都在解码MP4输入。不出所料，YOLOv3-tiny模型的性能优势是以较低的对象检测准确性为代价的:虽然人和自行车被检测得相当好，但其他对象如狗和球根本没有被检测到。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/721c7cb08e8c6e86cb6a002acb78ef43.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*fX8lbYYT-10XBaKZVbjZEw.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Object detection on video frames using YOLOv3-tiny on a Raspberry Pi with an Intel Neural Compute Sitck 2. Input video source: <a class="ae la" href="https://bitbucket.org/merayxu/multiview-object-tracking-dataset/src/master/" rel="noopener ugc nofollow" target="_blank">CAMPUS dataset</a> for multi-view object tracking [1]</figcaption></figure><p id="d26e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当涉及到对实时摄像机输入的推断时，launch命令看起来非常相似，当然，除了没有提供输入视频文件之外，因此应用程序默认为摄像机输入处理，即:</p><p id="f603" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mn mo mp mq b">python tinyYOLOv3.py --m "frozen_darknet_yolov3_model.xml" --l "coco.names"</code></p><p id="1095" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，您可能需要添加模型和类标签文件参数的特定路径。帧速率增加到4到5 FPS之间。这是因为MP4输入文件处理被计算要求较低的相机馈送处理所取代。类似于输入视频场景，诸如遥控器和咖啡杯之类的某些物体被相当可靠地检测到，而例如办公椅根本没有被检测到。</p><p id="96e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据应用场景的不同，这些相对较低的精度可能就足够了。如果不是这样，也可以使用更精确但通常执行效率较低的型号，如<a class="ae la" href="https://docs.openvinotoolkit.org/latest/omz_models_public_mobilenet_ssd_mobilenet_ssd.html" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>单次检测器。🗹</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/da9c7a80fae7db7b8f9646f49e1eaec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*bv6zqGG7mn7TUq_2H2sjCQ.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Object detection on camera frames using YOLOv3-tiny on a Raspberry Pi with an Intel Neural Compute Stick 2</figcaption></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="13e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">结束了。</strong></p><p id="6ea8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae la" href="https://medium.com/datadriveninvestor/edge-ai-computer-vision-on-the-edge-dfa4ad604651" rel="noopener">第1部分</a>中，从概述Edge AI硬件加速器和开发板以及Raspberry Pi配置和英特尔OpenVINO安装指南开始，包括一个对象检测演示应用，我们已经成功地定制实现了一个基于YOLOv3-tiny的对象检测应用，该应用在配备英特尔神经计算棒2的Raspberry Pi edge设备上处理视频文件或摄像头输入。代码库可以在<a class="ae la" href="https://github.com/cm230/Computer-Vision-On-The-Edge" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="c58c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管英特尔OpenVINO软件包有些复杂，但总体而言，它代表了一种在边缘设备上实现人工智能应用的出色而简单的方式，尤其是在利用开放模型动物园中预先训练和转换的模型时。</p><p id="1b1e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于这些相当近期的发展，边缘人工智能应用场景的领域已经开放，不胜枚举，例如，从智能家居或智能无人机设备到手势识别和基于视觉传感器的自动化场景。所需要的只是相对低成本的设备，如树莓Pi和硬件加速器、免费的OpenVINO库、一些想象力和一点点毅力。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="186a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献<br/></strong>【1】徐玉燕，刘小玲，秦，朱少春，“基于场景的跨视角人物跟踪方法”，人工智能大会，2017</p><p id="c525" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2] J .雷德蒙和a .法尔哈迪，“约洛夫3:增量改进”，技术报告，arXiv:1804.02767，2018年4月</p></div></div>    
</body>
</html>