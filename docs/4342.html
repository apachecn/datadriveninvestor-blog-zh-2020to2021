<html>
<head>
<title>Extreme Learning Machines I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极限学习机I</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/extreme-learning-machines-82095ee198ce?source=collection_archive---------5-----------------------#2020-08-03">https://medium.datadriveninvestor.com/extreme-learning-machines-82095ee198ce?source=collection_archive---------5-----------------------#2020-08-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="62e4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第一部分:引言:我们为什么需要榆树？</h2></div><p id="03e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大约在2005年，光和新加坡南洋理工大学的一组研究人员推出了一种新颖的机器学习方法。</p><p id="f989" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种新提出的学习算法趋向于达到<strong class="kh ir">最小的训练误差</strong>，<strong class="kh ir">获得最小的权值范数</strong>和<strong class="kh ir">最佳的泛化性能</strong>，<strong class="kh ir">运行极快</strong>，为了与其他流行的SLFN学习算法相区别，它被称为极限学习机(ELM)。</p><p id="1878" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法主要解决神经网络的训练时间比所需时间慢得多的问题，其主要原因是通过使用这种学习算法来迭代地调整网络的所有参数。这些基于慢梯度的学习算法被广泛用于训练神经网络。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/e2ace67b8cf68901dc5b4d0346e9eebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*EV6z8iUbyJtHmOf3.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Important features of ELM <a class="ae ln" href="https://www.ntu.edu.sg/home/egbhuang/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c488" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在深入研究ELM如何工作以及它为什么这么好之前，让我们看看基于梯度的神经网络是如何工作的。</p><h1 id="daa7" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">基于梯度的神经网络演示</h1><p id="39c2" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">简单地说，下面是单层前馈神经网络中遵循的步骤:</p><p id="cf67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步:评估Wx + B</p><p id="44d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">步骤2:应用激活函数g(Wx + B)并计算输出</p><p id="dda7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三步:计算损失</p><p id="1743" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">步骤4:计算梯度(使用德尔塔规则)</p><p id="6db7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重复</p><p id="f251" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种向前和向后传播的方法涉及大量的计算。如果输入大小很大，或者如果有更多的层/节点，则训练会花费大量的时间。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/3bbb55b4c2d3117709d2428002c500ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WxRAshGcoOdgWaQU.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">fig.1. 3-layered Neural Network</figcaption></figure><p id="17e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上述示例中，我们可以看到，对于4节点输入，我们需要W1 (20个参数)、W2 (53个参数)和W3 (21个参数)，即总共94个参数。并且参数随着输入节点的增加而迅速增加。</p><p id="dcad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们举一个使用MNIST数据集对数字进行影像分类的真实例子:</p><p id="4bd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> MNIST的例子</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/fd407328ca9d213e466709926154617d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DOFF75TodNXEx7Xr.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk"><a class="ae ln" href="https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d" rel="noopener" target="_blank">Source</a></figcaption></figure><p id="d5f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这具有28×28的输入大小，即784个输入节点。对于它的架构，我们先考虑两层<strong class="kh ir"> 128个节点</strong>和<strong class="kh ir"> 64个节点</strong>，再分成10类。那么参数将是:</p><ul class=""><li id="5f2e" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">第一层(784，128) = 100352个参数</li><li id="0a61" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">第二层(128，64) = 8192个参数</li><li id="3e81" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">输出层(64，10) = 640个参数</li></ul><p id="751d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将为我们提供总共109184个参数。并且通过反向传播重复调整权重增加了大量训练时间。</p><p id="9aeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这只是一个28x28的图像，考虑用10000个特征来训练它更大的输入大小。训练时间简直失控。</p><h1 id="df32" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">结论:</h1><p id="8d9c" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">在前向神经网络的几乎所有实际学习算法中，传统的反向传播方法要求在每个反向传播步骤中调整所有这些权重。</p><p id="430a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数时候，基于梯度下降的策略已经被用于前馈神经网络的各种学习算法中。然而，很明显，基于梯度下降的学习策略平方测量通常非常慢，因为不适当的学习步骤或可能只是收敛到局部最小值。并且这种学习算法需要许多迭代学习步骤以获得更高的学习性能。</p><p id="7f61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这使得训练比要求的慢得多，这已经成为各种应用的主要瓶颈。</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="4532" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本系列下一篇:<strong class="kh ir">第二部分:算法</strong><a class="ae ln" href="https://medium.com/@prasad.kumkar/extreme-learning-machines-9c8be01f6f77" rel="noopener">https://medium . com/@ Prasad . kumkar/extreme-learning-machines-9 c8 be 01 F6 f 77</a></p><h1 id="da67" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">参考资料:</h1><p id="fb88" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">[1]黄，张广斌，朱，秦宇，肖，徐志军.(2004).一种新的前馈神经网络学习方案。IEEE神经网络国际会议-会议录。2.985–990第二卷。</p><p id="ce07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]https://www.ntu.edu.sg/home/egbhuang/<a class="ae ln" href="https://www.ntu.edu.sg/home/egbhuang/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>