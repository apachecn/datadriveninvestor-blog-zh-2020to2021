<html>
<head>
<title>Object-Oriented Programming to tune ML Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调整ML模型的面向对象程序设计</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/object-oriented-programming-to-tune-ml-model-2c6922f5077f?source=collection_archive---------2-----------------------#2020-04-21">https://medium.datadriveninvestor.com/object-oriented-programming-to-tune-ml-model-2c6922f5077f?source=collection_archive---------2-----------------------#2020-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f585" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为不同的超参数调整技术创建ML类的示例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9b6e42b4f450108088b20c32fddcf92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8-cozpbICG78MxviylRhQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo on VectorStock.com/24059883</figcaption></figure><p id="ec53" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章是为新的和有抱负的数据科学家准备的，展示了使用Python中的面向对象编程来调优机器学习(ML)模型。因此，如果你是新手，并且越来越喜欢这个令人惊叹的领域，你应该已经意识到你的学习曲线中的某些里程碑，首先你会刷你的统计技能，然后学习像numpy和pandas这样的工具的可视化和数据转换，最后会创建和测试你的机器学习模型。下一步是熟悉面向对象编程(OOP ),并开始编写Python脚本。到本文结束时，您将对参数调优和创建ML模型的类对象有所了解。这篇文章分为两部分，所以你可以随意跳到你认为合适的部分(查看<a class="ae lr" href="https://github.com/kshitijmamgain/Mlclass/blob/master/ML_class_demonstration.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)。</p><ol class=""><li id="a228" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">贝叶斯优化简介</li><li id="e188" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">用于超参数调整的ML对象类</li></ol><h2 id="7762" class="mg mh iq bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">贝叶斯优化简介</h2><p id="15be" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">不久前，超参数的选择被称为“艺术”，有些人会说它需要技巧来调整，而其他人会说它是基于直觉。此外，ML算法越复杂，需要调整的超参数就越多。在这种情况下，一种选择是用每一种可能的调整组合(也称为网格搜索，也称为强力法)来调整模型。或者另一种方法是用试错法测试超参数，并随机选取参数集。后一种方法优于前一种方法，因为有更好的机会识别性能良好的参数。随机搜索和网格搜索都可以在sklearn库中使用，并带有交叉验证。</p><div class="ne nf gp gr ng nh"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">认知计算——一套被广泛认为是……</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv kp nh"/></div></div></a></div><p id="3cab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">幸运的是，有更好的方法来调整超参数，这比贝叶斯优化等随机搜索更有效，贝叶斯优化通过估计和更新描述目标函数潜在值的概率分布来获得最佳超参数值。简而言之，这意味着当算法为超参数的特定值找到一个好的结果时，它会加强对该值的搜索。我们将讨论使用这种方法的两个python库<a class="ae lr" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">hyperpt</strong></a>和<a class="ae lr" href="https://optuna.readthedocs.io/en/stable/index.html#" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> Optuna </strong> </a>。</p><p id="1a63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Hyperopt有优秀的<a class="ae lr" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank">教程</a>，我们将在LightGBM模型上演示它的使用。梯度提升树算法是构建监督学习模型的常用算法，LightGBM是其中的一种。我们将在sklearn中使用乳腺癌数据集</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="59a5" class="mg mh iq nx b gy ob oc l od oe"># importing the essential libraries<br/>import numpy as np<br/>import pandas as pd<br/>import lightgbm as lgb<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import f1_score, confusion_matrix, classification_report</span><span id="ffd3" class="mg mh iq nx b gy of oc l od oe"># importing the dataset<br/>from sklearn.datasets import load_breast_cancer<br/># importing the hyperopt methods<br/>from hyperopt import fmin, hp, tpe, Trials, STATUS_OK</span><span id="776a" class="mg mh iq nx b gy of oc l od oe">dataset = load_breast_cancer()</span><span id="b2dc" class="mg mh iq nx b gy of oc l od oe">X = dataset.data<br/>y = dataset.target</span><span id="92a9" class="mg mh iq nx b gy of oc l od oe">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 2/3, random_state = 1)</span></pre><p id="b20a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从Hypeopt库中调用了四个方法— <em class="og"> fmin、hp、tpe和STATUS_OK。</em>第一种，<strong class="kx ir"> fmin </strong>是最小化目标函数损失的方法。如果我们希望我们的模型以更高的精度运行，那么我们希望最小化的是(1-精度)。同样，如果我们希望最小化f1得分，我们将最小化的损失是(1-f1得分),如下所示:</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="846b" class="mg mh iq nx b gy ob oc l od oe">#lgb model takes param as dict<br/>param = {'objective': 'binary', 'learning_rate': 0.5,'reg_alpha': 0.5, 'reg_lambda': 0.5}<br/>model = lgb.train(params, lgb.Dataset(X_train, label = y_train))<br/># lgb mode.predict gives predict probabilities<br/>pred=h_model.predict(X_test)<br/># defining predicted label based on 0.5 threshold<br/>y_pred = np.where(pred&gt;0.5,1,0)</span><span id="3075" class="mg mh iq nx b gy of oc l od oe"># claculating the f1 score<br/>f1sc = f1_score(y_test, y_pred<br/>&gt;&gt;0.9561752988047808<br/>loss = 1-f1sc<br/>&gt;&gt;0.0438247011952192‬</span></pre><p id="a304" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上述代码中，超点fmin方法可用于优化损失，即有助于提高f1得分。为了创建这样的函数，我们希望它采用不同的超参数作为输入，并在F1score中测试改进。因此，我们可以做到以下几点:</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="7c5a" class="mg mh iq nx b gy ob oc l od oe">def objective(params):</span><span id="1826" class="mg mh iq nx b gy of oc l od oe"># an objective function<br/>  h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))<br/>  pred=h_model.predict(X_test)<br/>  y_pred = np.where(pred&gt;0.5,1,0)<br/>  f1sc = f1_score(y_test, y_pred)<br/>  loss = 1 — f1sc<br/>  return {‘loss’: loss, , 'status' : STATUS_OK}</span></pre><p id="8afe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个目标函数将在任何被定义为损失的基础上被优化。接下来，我们需要一个参数空间，从中选择超参数的值。这样的搜索空间由<strong class="kx ir"> hp: </strong>定义</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="9402" class="mg mh iq nx b gy ob oc l od oe"># quniform: quantile uniform distribution (discrete); <br/>space = {<br/>         ‘lambda_l1’: hp.uniform(‘lambda_l1’, 0.0, 1.0),<br/>         ‘lambda_l2’: hp.uniform(“lambda_l2”, 0.0, 1.0),<br/>         ‘learning_rate’ : hp.loguniform(‘learning_rate’,                                                                                                      np.log(0.05), np.log(0.25)),<br/>         ‘objective’ : ‘binary’}<br/>trials = Trials()</span></pre><p id="c5c6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> tpe </strong>方法具有从上面定义的空间进行搜索的算法，并且<strong class="kx ir">试验</strong>方法创建数据库以记录试验。最后是<strong class="kx ir"> STATUS_OK </strong>，它是目标函数强制返回的，用于存储运行的成功。fmin方法将存储从超参数空间中选择的最佳结果参数。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="eb38" class="mg mh iq nx b gy ob oc l od oe">best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=1000)<br/>best<br/>output&gt; {'lambda_l1': 0.0021950856879321273,  'lambda_l2': 0.004537789645112988,  'learning_rate': 0.10215913897624063,  'num_leaves': 176.0}</span></pre><p id="ef87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，当调用fmin方法时，我们还必须指定最大评估次数或空间搜索次数，以预测最佳参数。</p><p id="ac4e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Optuna库也使用贝叶斯优化来优化超参数。首先，我们在单个函数中定义了目标函数和超参数空间。第二，与远视不同，远视只“最小化”Optuna中的目标函数，如果我们希望最大化或最小化目标，我们可以定义它。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="ce52" class="mg mh iq nx b gy ob oc l od oe">import optuna</span><span id="f7c4" class="mg mh iq nx b gy of oc l od oe">def optuna_obj(trial):<br/>  '''Defining the parameters space inside the function for optuna optimization'''</span><span id="ffef" class="mg mh iq nx b gy of oc l od oe">  params = {‘num_leaves’: trial.suggest_int(‘num_leaves’, 16, 196, 4),<br/>  ‘lambda_l1’: trial.suggest_loguniform(‘lambda_l1’, 1e-8, 10.0),<br/>  ‘lambda_l2’: trial.suggest_loguniform(“lambda_l2”, 1e-8, 10.0),<br/>  ‘learning_rate’ : trial.suggest_loguniform(‘learning_rate’, 0.05, 0.25)}</span><span id="cd1c" class="mg mh iq nx b gy of oc l od oe">  o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))</span><span id="d78d" class="mg mh iq nx b gy of oc l od oe">pred=o_model.predict(X_test)<br/>  y_pred = np.where(pred&gt;0.5,1,0)<br/>  f1sc = f1_score(y_test, y_pred)<br/>  loss = 1 — f1sc<br/>  return loss</span><span id="2a30" class="mg mh iq nx b gy of oc l od oe">study = optuna.create_study(direction=’minimize’)<br/>study.optimize(optuna_obj, n_trials=2000)</span></pre><p id="5511" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，我们使用“最小化”作为方向，因为我们想要一个类似于Hyperopt的输出。我们也可以在Optuna中返回方向为“最大”的f1分数。优化的超参数存储在研究的best_params属性中。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="e89a" class="mg mh iq nx b gy ob oc l od oe">study.best_params<br/>&gt;&gt; {'lambda_l1': 9.81991399439663e-07,  'lambda_l2': 4.23211064923651,  'learning_rate': 0.1646975912311242,  'num_leaves': 64}</span></pre><p id="c183" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们应该观察到我们的初始参数和来自远视和Optuna的最佳参数都是不同的。虽然手动定义的参数不同是可以理解的，但是Optuna和Hyperopt最佳参数不同的原因可能在于最大评估和内部算法的优化方法。新参数确实提高了f1的分数</p><h2 id="99d5" class="mg mh iq bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">用于超参数调整的ML对象类</h2><p id="1a04" class="pw-post-body-paragraph kv kw iq kx b ky mz jr la lb na ju ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">本节的重点是演示如何创建一个分类器类。有大量关于OOP编程的资源，但是完整类的演示是有限的。我们将使用上面的例子来创建一个类。在构建一个类时，可视化你的类的结构是很重要的，这里我们将创建一个简单的流程，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/f568c40a5226238d6379dbdfe1044f3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3ofbSy8Is9vQvdjmhOMvg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Structure of defined ML class</figcaption></figure><p id="64c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述类的整体结构如下所示:</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="7e2f" class="mg mh iq nx b gy ob oc l od oe">class Mlclass():<br/>  def __init__(self,...):<br/>    ...<br/>  def tuning(self, ...):<br/>    ...<br/>  def hyperopt_method(self):<br/>    ...<br/>  def optuna_method(self):<br/>    ...<br/>  def train(self):<br/>    ...<br/>  def evaluate(self):<br/>    ...</span></pre><p id="9b23" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个基本结构将使我们很好地理解不同类型的类方法，有一些方法如tuning、train和evaluate将由用户调用，而其他方法如hyperopt和optuna将在类中使用，而不需要用户调用它们。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="ff30" class="mg mh iq nx b gy ob oc l od oe">class MLclass():<br/>  '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''</span><span id="34db" class="mg mh iq nx b gy of oc l od oe">  def __init__(self, x_train, y_train):<br/>    '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object<br/>    Parameters<br/>    ----------<br/>    x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.</span><span id="7e23" class="mg mh iq nx b gy of oc l od oe">    y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''</span><span id="cdf3" class="mg mh iq nx b gy of oc l od oe">    self.x_train = x_train<br/>    self.y_train = y_train<br/>    self.train_set = lgb.Dataset(data=train_X, label=train_y)</span></pre><p id="a864" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ML类方法首先用训练数据集和目标初始化。从<em class="og"> class开始定义一个类。</em>类内部定义的函数被称为<em class="og">方法</em>。我们的第一个方法是<em class="og"> __init__ </em>，用于初始化类。这里，我们希望用数据集和目标来初始化我们的类，因此我们将输入参数指定为“x_train”和“y_train”。方法中的<em class="og"> self </em>用于将函数与实例关联起来。使用“自我”作为变量的前缀也使得类变量特定于该实例。调用这个类非常简单:</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="3f42" class="mg mh iq nx b gy ob oc l od oe">#defining a unique class object<br/>obj = MLclass(X_train, y_train)</span></pre><p id="8c96" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦初始化了类方法，我们将添加用于Hypeorpt优化的方法。我们希望用户输入优化类型作为Hypeorpt，然后调整模型。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="3e70" class="mg mh iq nx b gy ob oc l od oe">def tuning(self, optim_type):<br/>  '''Method takes the optimization type and tunes the model'''<br/>  #call the optim_type: Hyperopt or Optuna<br/>  optimization = getattr(self, optim_type)<br/>  return optimization()</span><span id="b2ff" class="mg mh iq nx b gy of oc l od oe">def hyperopt_method(self):<br/>  # This method is called by tuning when user inputs   'hyperopt_method' while calling the tuning method<br/>  #define the hyperopt space<br/>  space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),<br/>  'lambda_l2': hp.uniform("lambda_l2", 0.0, 1.0),<br/>  'learning_rate' : hp.loguniform('learning_rate',<br/>np.log(0.05), np.log(0.25)),<br/>  'objective' : 'binary'}</span><span id="0726" class="mg mh iq nx b gy of oc l od oe">  # define algorithm and trials inside the class<br/>  algo, trials= tpe.suggest, Trials()</span><span id="aa5c" class="mg mh iq nx b gy of oc l od oe">  #Call the fmin from inside the class<br/>    best=fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=1000)<br/>  self.params = best<br/>  return best, trials</span><span id="80b3" class="mg mh iq nx b gy of oc l od oe">def objective(self, params):<br/>  # same objective function with added self<br/>  h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))<br/>  pred=h_model.predict(X_test)<br/>  y_pred = np.array(list(map(lambda x: int(x), pred&gt;0.5)))<br/>  f1sc = f1_score(y_test, y_pred)<br/>  loss = 1 - f1sc<br/>  return {'loss': loss,'status' : STATUS_OK}</span><span id="b843" class="mg mh iq nx b gy of oc l od oe">#call the tuner method<br/>obj.tuning('hyperopt_method')</span><span id="6c87" class="mg mh iq nx b gy of oc l od oe">return optimization()</span><span id="56ef" class="mg mh iq nx b gy of oc l od oe">def hyperopt_method(self):</span><span id="0549" class="mg mh iq nx b gy of oc l od oe"># This method is called by tuning when user inputs 'hyperopt_method' while calling the tuning method</span><span id="73e5" class="mg mh iq nx b gy of oc l od oe">#define the hyperopt space</span><span id="ad87" class="mg mh iq nx b gy of oc l od oe">space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),</span><span id="010f" class="mg mh iq nx b gy of oc l od oe">'lambda_l2': hp.uniform("lambda_l2", 0.0, 1.0),</span><span id="6b73" class="mg mh iq nx b gy of oc l od oe">'learning_rate' : hp.loguniform('learning_rate',</span><span id="3c17" class="mg mh iq nx b gy of oc l od oe">np.log(0.05), np.log(0.25)),</span><span id="f066" class="mg mh iq nx b gy of oc l od oe">'objective' : 'binary'}</span><span id="9947" class="mg mh iq nx b gy of oc l od oe"># define algorithm and trials inside the class</span><span id="c780" class="mg mh iq nx b gy of oc l od oe">algo, trials= tpe.suggest, Trials()</span><span id="24e2" class="mg mh iq nx b gy of oc l od oe">#Call the fmin from inside the class</span><span id="a9f2" class="mg mh iq nx b gy of oc l od oe">best = fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=1000)</span><span id="d190" class="mg mh iq nx b gy of oc l od oe">self.params = best</span><span id="406a" class="mg mh iq nx b gy of oc l od oe">return best, trials</span><span id="846f" class="mg mh iq nx b gy of oc l od oe">def objective(self, params):</span><span id="18db" class="mg mh iq nx b gy of oc l od oe"># same objective function with added self</span><span id="ad21" class="mg mh iq nx b gy of oc l od oe">h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))</span><span id="7625" class="mg mh iq nx b gy of oc l od oe">pred=h_model.predict(X_test)</span><span id="f0f7" class="mg mh iq nx b gy of oc l od oe">y_pred = np.array(list(map(lambda x: int(x), pred&gt;0.5)))</span><span id="1868" class="mg mh iq nx b gy of oc l od oe">f1sc = f1_score(y_test, y_pred)</span><span id="29e9" class="mg mh iq nx b gy of oc l od oe">loss = 1 - f1sc</span><span id="54ae" class="mg mh iq nx b gy of oc l od oe">return {'loss': loss,'status' : STATUS_OK}</span><span id="455b" class="mg mh iq nx b gy of oc l od oe">#call the tuner method<br/>obj.tuning('hyperopt_method')</span></pre><p id="8bf2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当在上面的代码中调用调优方法时，将执行超点优化。让我们为Optuna定义一个类似的方法</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="9416" class="mg mh iq nx b gy ob oc l od oe">def optuna_method(self):<br/>  study = optuna.create_study(direction=’minimize’)<br/>  study.optimize(optuna_obj, n_trials=2000)<br/>  self.params = study.best_params<br/>  return study</span><span id="f06c" class="mg mh iq nx b gy of oc l od oe">def optuna_obj(self, trial):<br/>  '''Same optuna objective with parameters space inside the function for optuna optimization'''</span><span id="aeca" class="mg mh iq nx b gy of oc l od oe">  params = {‘num_leaves’: trial.suggest_int(‘num_leaves’, 16, 196, 4),<br/>  ‘lambda_l1’: trial.suggest_loguniform(‘lambda_l1’, 1e-8, 10.0),<br/>  ‘lambda_l2’: trial.suggest_loguniform(“lambda_l2”, 1e-8, 10.0),<br/>  ‘learning_rate’ : trial.suggest_loguniform(‘learning_rate’, 0.05, 0.25)}</span><span id="be82" class="mg mh iq nx b gy of oc l od oe">  o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))</span><span id="6ff6" class="mg mh iq nx b gy of oc l od oe">  pred=o_model.predict(X_test)<br/>  y_pred = np.array(list(map(lambda x: int(x), pred&gt;0.5)))<br/>  f1sc = f1_score(y_test, y_pred)<br/>  loss = 1 — f1sc<br/>  return loss</span><span id="23ab" class="mg mh iq nx b gy of oc l od oe">#calling tuner with optuna<br/>obj.tuning('optuna_method')</span></pre><p id="5ec1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了使用最佳参数进行评估，我们定义了另一个变量self。<em class="og"> params </em>将为该实例定义，并可由尚未定义的训练方法访问。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="dd8e" class="mg mh iq nx b gy ob oc l od oe">def train(self):<br/>  """This function evaluates the model on best parameters"""<br/>print("Model will be trained on the following parameters: \n{}".format(self.params))</span><span id="dcb7" class="mg mh iq nx b gy of oc l od oe">  #train the model with best parameters<br/>  self.gbm = lgb.train(self.params, self.train_set)</span><span id="9d9f" class="mg mh iq nx b gy of oc l od oe">obj.train()<br/>&gt;&gt; Model will be trained on the following parameters: <br/>&gt;&gt;{'lambda_l1': 9.81991399439663e-07,  'lambda_l2': 4.23211064923651,  'learning_rate': 0.1646975912311242,  'num_leaves': 64}</span></pre><p id="a365" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦模型被训练，我们可以通过给定测试数据集和测试目标作为参数来评估它。</p><pre class="kg kh ki kj gt nw nx ny nz aw oa bi"><span id="adaf" class="mg mh iq nx b gy ob oc l od oe">def evaluate(self, x_test, y_test):<br/>  # predict the values from x_test<br/>  pred = self.gbm.predict(x_test)<br/>  y_pred = np.where(pred&gt;0.5,1,0)<br/>  #print confusion matrix<br/>  print(confusion_matrix(y_test,y_pred)<br/>  #print classification report<br/>  print(classification_report(y_test, y_pred)</span></pre><p id="e260" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过这种方式，我们展示了如何创建一个ML类及其不同的方法。我们应该理解，创建一个ML模型是一个迭代的过程，包括用特征工程训练模型、数据预处理和用不同算法测试。创建类函数有助于更快地调试代码，节省重写代码的时间，并且只需稍加修改就可以再次运行它们。</p><p id="8ad8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望这篇文章对你有用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oi oj l"/></div></figure></div></div>    
</body>
</html>