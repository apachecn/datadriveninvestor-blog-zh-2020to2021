<html>
<head>
<title>Softmax Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Softmax回归</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/softmax-regression-bda793e2bfc8?source=collection_archive---------0-----------------------#2020-05-16">https://medium.datadriveninvestor.com/softmax-regression-bda793e2bfc8?source=collection_archive---------0-----------------------#2020-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5db6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Python从头开始构建Softmax回归模型！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a4f50f76be63f9414c5533e69cf65e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*D6IqfpomdDtB00pVhQVWXw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">MNIST Handwritten Digits Dataset.</figcaption></figure><p id="8dcb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我之前的<a class="ae ln" href="https://medium.com/datadriveninvestor/fundamental-of-logistic-regression-4244cbc9aa3c" rel="noopener">文章</a>中，我们了解了用于二元分类的<strong class="kt ir">逻辑回归</strong>。然而，在实际应用中，可能有两个以上的类别需要分类，例如，数字分类。在这种情况下，我们称之为多项逻辑回归或也称为<strong class="kt ir"> Softmax回归</strong>。</p><h2 id="ead4" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">Softmax方程的推导</h2><p id="4e09" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">考虑一个涉及k个类别的分类问题。<br/>设<strong class="kt ir"> x </strong>为<strong class="kt ir">特征向量</strong>和<strong class="kt ir"> y </strong>为对应类，其中<strong class="kt ir">y</strong>T17】∈{ 1，2，…，k} 。</p><p id="aea1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，我们将对给定x，<strong class="kt ir"> P(y|x) </strong>的y的概率进行建模，这是给定特征时y成为任一类别的概率向量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/4648ac352ee6b8c4ba9bb43b8c123e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*l0d0aXvUeMOrzMbmlhMJ3A.png"/></div></figure><blockquote class="mn mo mp"><p id="f8d2" class="kr ks mq kt b ku kv jr kw kx ky ju kz mr lb lc ld ms lf lg lh mt lj lk ll lm ij bi translated">回想一下，在<a class="ae ln" href="https://medium.com/datadriveninvestor/fundamental-of-logistic-regression-4244cbc9aa3c" rel="noopener">逻辑回归</a>中，y=1相对于y=0的<strong class="kt ir">对数比被假设为与<strong class="kt ir">自变量x </strong>成线性关系。</strong></p></blockquote><p id="1d7d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用相同的类比，我们可以假设y=i相对于y=k的<strong class="kt ir">对数奇数与<strong class="kt ir">独立变量x </strong>具有线性关系。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/af9f5124bc530c44a0747974340821be.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*XFgzn3YXj9t4CSJ2Peclag.png"/></div></figure><p id="52d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于j=1，2，3，…，k的P(y=j|x)之和等于1，所以:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/1c462f061383460bef51a2413828361e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*pORePhqo82kiebDJObL5lw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2cced9a5e6c2463edb467b6810d4e8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*Fw9fou_Xu3xr_T6Wj2L5yg.png"/></div></figure><p id="e7e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过替换:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/66e0f8aed06bd1fa6893f41f21600064.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*q4ZVdTSQR_-HYoe4PACnrg.png"/></div></figure><p id="d65e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面推导出的方程被称为<strong class="kt ir"> Softmax函数</strong>。从推导中可以看出，给定x，y=i的概率可以用softmax函数来估计。</p><p id="9670" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">模型概要:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6b21dbf9796e5bfa4c341e2c9e5691a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*O5y9Krr06Or50s-KHCe1mg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">weight vector associated with class g.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4531d644619e92784465f947ff05d96c.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*qTUjCnb3VqIHcVah4E6XPA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">weight matrix where each element corresponds to a feature of a class.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/1de27cac4b3ba20f7c5dea8f7dc5503a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3b4YEB9DHk9MvsjLYDvHuw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Figure: illustration of the softmax regression model.</figcaption></figure><p id="9634" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">利用输出概率向量，我们可以将输入分类为具有最高概率的类。</p><h2 id="8f27" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">最大似然估计</h2><p id="a397" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">在我们继续之前，让我们先介绍一下<strong class="kt ir">指示器函数</strong>，如果自变量为真，它输出1，否则它将输出0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6fd3affbe845b8f33f47c5a389018c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*O9iLHVV6j06HBWPluaHWqQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Indicator function.</figcaption></figure><p id="5a39" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了得到训练数据的似然性，我们需要计算给定x⁽ⁱ⁾的y=y⁽ⁱ⁾对于i=1，2，3，…，m的所有概率(m是训练数据的总数)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ca4321f10e61d4611737e33a45ecc0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*JA9RY4ImXsoCYF-Wll-Mmw.png"/></div></figure><p id="6418" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">利用P(y⁽ⁱ⁾|x⁽ⁱ⁾的表达式，我们可以如下计算似然函数L(θ):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nh"><img src="../Images/6b0600e9cc41c8b99835cc3289a547be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k6WTJ1FNFlqtqYNQ7KTEHA.png"/></div></div></figure><blockquote class="mn mo mp"><p id="2174" class="kr ks mq kt b ku kv jr kw kx ky ju kz mr lb lc ld ms lf lg lh mt lj lk ll lm ij bi translated">似然函数是模型对样本数据的拟合优度相对于模型参数(θ)的度量。</p></blockquote><p id="8b48" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如在每个机器学习任务中一样，我们的最终目标是通过最小化损失函数来优化权重。在这种情况下，我们将似然函数作为模型性能的度量。</p><blockquote class="mn mo mp"><p id="0501" class="kr ks mq kt b ku kv jr kw kx ky ju kz mr lb lc ld ms lf lg lh mt lj lk ll lm ij bi translated">注意:可能性越高，模型越好。</p></blockquote><p id="4099" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，我们需要最大化似然函数，而不是最小化它。</p><p id="0720" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如在关于逻辑回归的文章中所提到的，优化过程通常涉及微分，这比乘法容易得多。所以，我们对似然函数取自然对数，这就是所谓的对数似然函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/103c2798ba0a282eeaf266688ccb4ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*5R8N03kwaTP_Oex__B1HBg.png"/></div></figure><p id="2d09" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">事实是<strong class="kt ir">max f(x)=–min–f(x)</strong>。因此，我们实际上可以最小化对数似然函数的负值，以避免与通常涉及最小化损失函数的其他机器学习模型的优化过程混淆。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/12be357cd99e784bb31b82caf127a57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*QYZ13E3Smq7TYB1dvr3IFg.png"/></div></figure><p id="0a3c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简化损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/320f26161e67a30d33a793407d077e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*NQ-q4p00dx_Gs-g05v3P-Q.png"/></div></figure><p id="30e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意，在最后两步中，l=1至k 的求和项σ<strong class="kt ir">1</strong>(y⁽ⁱ⁾=l)<strong class="kt ir">消失，因为它等于1，如下所述:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7d41f8b80a3a155f5d4eb15ddcfa31ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*s9iQJOu2zMrSLAGy2bXyWw.png"/></div></figure><p id="ebf9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，我们的损失函数是对数似然函数的负值。我们将使用梯度下降算法通过最小化损失函数来优化权重。</p><p id="b907" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">损失函数相对于权重矩阵任何元素的偏导数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nl"><img src="../Images/4fb61e0eee6cee4b9f57f39cff1ac8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qehk9AgPPSdzNMKIQwcdWQ.png"/></div></div></figure><p id="5460" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">梯度下降每次迭代的更新规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nm"><img src="../Images/5c2ea8f8660b354a278c37b71b49182d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40ReY8RXipr7eKHGioSE7g.png"/></div></div></figure></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h2 id="22aa" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">Python中的Softmax回归</h2><p id="acd1" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">我们将使用的数据集是<a class="ae ln" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST手写数字</a>图像，这些图像可以通过使用<a class="ae ln" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>库来加载。</p><p id="f5ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">完整的编码在下面的Python笔记本中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div></figure></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="8044" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢您的阅读，希望您喜欢这篇文章并从中获得知识。</p><div class="nw nx gp gr ny nz"><a href="https://www.datadriveninvestor.com/2020/01/16/software-development-process-how-to-pick-the-right-process/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">软件开发过程:如何选择正确的过程？数据驱动的投资者</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">软件是任何企业组织成功的生命线。没有软件的帮助，一个…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on kl nz"/></div></div></a></div><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo nv l"/></div></figure></div></div>    
</body>
</html>