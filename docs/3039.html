<html>
<head>
<title>Mastering PyTorch: A Comprehensive Guide to the Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">掌握PyTorch:基础知识综合指南</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/pytorch-the-basics-7005e71cdb83?source=collection_archive---------5-----------------------#2020-05-28">https://medium.datadriveninvestor.com/pytorch-the-basics-7005e71cdb83?source=collection_archive---------5-----------------------#2020-05-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/48a9a0da941c5a458ca2abd7303f805d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5Ush092Wgkjc8zQU.png"/></div></div></figure><p id="7268" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你对机器学习感兴趣或者正在阅读这篇文章，那么你很可能以前就听说过PyTorch。它在机器学习开发者中是一个非常著名的框架，其原因我们将在本文中讨论。此外，如果你刚刚开始学习神经网络，并且有基本的知识，这是一个完美的初学者工具。</p><p id="fdfc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好吧，让我们进入PyTorch的本质，好吗？</p><h1 id="ee4b" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Pytorch的历史</h1><p id="bacf" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">Torch最初是用Lua编写的，这种语言相当罕见，很难掌握，而且它没有python提供的一半功能。</p><p id="6b8f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在2017年，脸书的人工智能研究实验室决定建立一个与torch库功能相同的库，但要不是python，因此命名为<strong class="ka ir"> Py </strong> Torch。这在开发人员中一炮而红，使得编写代码变得更加容易和高效。</p><div class="lz ma gp gr mb mc"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">认知计算——一套被广泛认为是……</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq jw mc"/></div></div></a></div><p id="f6be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="mr">趣闻:</em></strong><em class="mr">py torch Soumith Chintala的开发者之一，是我目前就读的大学VIT的校友！</em></p><h1 id="4d41" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Pytorch vs Tensorflow</h1><p id="8864" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">PyTorch和Tensorflow之间总是有比较，我觉得这是不必要的，因为它们处理问题的方法非常不同，并且取决于数据集和算法，一个比另一个更好。然而，如果你仍然好奇，你可以在这里查看这篇非常整洁的文章。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d855442fbcbc1c35e1fd3602508e392a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*kz7mKKSTcUrzDze3juSB5w.png"/></div></figure><h1 id="5263" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Pytorch是如何成为一个好框架的？</h1><ol class=""><li id="3c2b" class="my mz iq ka b kb lu kf lv kj na kn nb kr nc kv nd ne nf ng bi translated"><strong class="ka ir">命令式编程</strong>:在PyTorch中，计算会立即运行，这意味着用户无需等待编写完整的代码，就可以检查它是否工作。这对于像我这样不耐烦的程序员来说是个好兆头，他们想在每行代码后看到结果。这也允许在python中有更灵活的编码体验，并且它更受性能驱动。</li></ol><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/08caf37e8c3cc13cf22025f87c8c8153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*0uHRxCTeOqKGDkHl.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source:<a class="ae ms" href="https://medium.com/@vincentbacalso/imperative-vs-declarative-programming-f886d3b65595" rel="noopener">https://medium.com/@vincentbacalso/imperative-vs-declarative-programming</a></figcaption></figure><p id="61ef" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。动态计算图形:</strong>动态计算图形框架是一个由库、接口和组件组成的系统，它提供了一个灵活的、可编程的运行时接口，通过连接一组有限但可能可扩展的操作来简化系统的构建和修改。所以基本上，PyTorch的工作原理是按游程定义，这对于像RNN这样规模不固定的网络来说非常有用。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/60f82de1c421e54e09217a843bdeb571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPdG5Sc6YlJa8p68yJqeWA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source: Hackernoon.com</figcaption></figure><p id="0327" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.<strong class="ka ir">亲笔签名:</strong>这个类是一个计算导数(更精确地说是雅可比向量积)的引擎。它记录了在梯度张量上执行的所有操作的图形，并创建了一个称为动态计算图的非循环图形。这个图的叶子是输入张量，根是输出张量。梯度的计算方法是从根到叶追踪图形，并使用链式法则将每个梯度相乘。</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="721c" class="ns kx iq no b gy nt nu l nv nw"><em class="mr">torch.nn.Autograd.Function</em></span></pre><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/a1e900ca6e2f73ef5d8a18dc82b6afcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yryS5Q_1yzgdPVDh6gRB3A.jpeg"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source: Udacity.com</figcaption></figure><h1 id="7297" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">Pytorch的常用功能</h1><ol class=""><li id="bd17" class="my mz iq ka b kb lu kf lv kj na kn nb kr nc kv nd ne nf ng bi translated"><strong class="ka ir"> torch.sum() </strong>:加法</li></ol><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="56f6" class="ns kx iq no b gy nt nu l nv nw">(a*b).sum()</span></pre><p id="34e6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.<strong class="ka ir"> torch.mm(): </strong>点积<strong class="ka ir">。</strong></p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="5143" class="ns kx iq no b gy nt nu l nv nw">#torch.nm(a,b)</span></pre><p id="2b8b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3。torch.randn(): </strong>生成矩阵随机数</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="225f" class="ns kx iq no b gy nt nu l nv nw">torch.manual_seed(7)# Set the random seed so things are predictable<br/>x = torch.randn(2,2)</span></pre><p id="da01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 4。torch.exp():计算指数</strong></p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="4128" class="ns kx iq no b gy nt nu l nv nw">Sigmoid=1/(1+torch.exp(-x))</span></pre><p id="1982" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 5。PyTorch提供了一个模块</strong> <code class="fe ny nz oa no b"><strong class="ka ir">nn</strong></code> <strong class="ka ir">，使得构建网络更加简单。</strong></p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="7435" class="ns kx iq no b gy nt nu l nv nw">from torch import nn</span></pre><p id="a8cb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> -nn。Linear() </strong>:这一行创建了一个线性变换的模块，𝑥𝐖+𝑏.</p><p id="f896" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">类似地，以下函数可用于计算相应的激活函数。</p><p id="4878" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> -nn。乙状结肠()</strong></p><p id="d8ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> -nn。Softmax() </strong></p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="f71c" class="ns kx iq no b gy nt nu l nv nw">self.output <strong class="no ir">=</strong> nn.Linear<br/>self.sigmoid <strong class="no ir">=</strong> nn.Sigmoid()<br/>self.softmax <strong class="no ir">=</strong> nn.Softmax(dim<strong class="no ir">=</strong>1)#columnwise</span></pre><p id="1ee0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">6.<strong class="ka ir"> nn。使用这种方法，我们可以通过操作顺序传递一个张量。</strong></p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="c5da" class="ns kx iq no b gy nt nu l nv nw">model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),<br/>                      nn.ReLU(),<br/>                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),<br/>                      nn.ReLU(),<br/>                      nn.Linear(hidden_sizes[1], output_size),<br/>                      nn.Softmax(dim=1))</span></pre><p id="8b05" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">7。损失计算</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="f1b5" class="ns kx iq no b gy nt nu l nv nw">criterion = nn.CrossEntropyLoss()<br/>criterion = nn.NLLLoss()<br/>criterion = nn.LogSoftmax()</span></pre><p id="d0cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">8。亲笔签名:亲笔签名的工作方式是跟踪对张量执行的操作，然后通过这些操作返回，计算沿途的梯度。</p><p id="5602" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> -loss.backward() </strong>计算用于参数计算的梯度。这些梯度用于通过梯度下降来更新权重。</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="d223" class="ns kx iq no b gy nt nu l nv nw">loss.backward()</span></pre><p id="5e01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> optimizer.zero_grad() </strong>将每次训练过程中的梯度归零，否则将保留先前训练批次中的梯度。</p><pre class="mu mv mw mx gt nn no np nq aw nr bi"><span id="4218" class="ns kx iq no b gy nt nu l nv nw">optimizer.zero_grad()</span></pre><h1 id="c69a" class="kw kx iq bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论</h1><p id="e81c" class="pw-post-body-paragraph jy jz iq ka b kb lu kd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv ij bi translated">我将这篇文章的重点放在PyTorch的理论和语法上，在第2部分中，我将通过开发一个神经网络在数据集上展示PyTorch。所以请继续关注，感谢您的阅读。干杯！</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="f6e5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参考:<a class="ae ms" href="https://classroom.udacity.com/courses/ud188" rel="noopener ugc nofollow" target="_blank">https://classroom.udacity.com/courses/ud188</a>——py torch简介。</p><figure class="mu mv mw mx gt jr"><div class="bz fp l di"><div class="oi oj l"/></div></figure></div></div>    
</body>
</html>