<html>
<head>
<title>Pros &amp; Cons of Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的利弊</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/pros-cons-of-dimensionality-reduction-f843cad3953a?source=collection_archive---------2-----------------------#2020-01-02">https://medium.datadriveninvestor.com/pros-cons-of-dimensionality-reduction-f843cad3953a?source=collection_archive---------2-----------------------#2020-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6ee5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于主成分分析的虹膜数据压缩和可视化</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ab2128ae526d7bd0f7be0fabbd0adbbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwenU3V3y9P4j9rfmj6krw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by <a class="ae kv" href="https://pixabay.com/users/manfredrichter-4055600/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2219594" rel="noopener ugc nofollow" target="_blank">Manfred Richter</a> from <a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2219594" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="cdcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将致力于著名的鸢尾花数据集。</p><p id="0e1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">导入必要的包。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="f6bf" class="lx ly iq lt b gy lz ma l mb mc"># Importing the necessary packages</span><span id="1ac5" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</strong></span></pre><p id="fcd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虹膜数据集从sklearn.datasets导入</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="e400" class="lx ly iq lt b gy lz ma l mb mc"># Importing the data set</span><span id="4750" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">from sklearn import datasets</strong></span><span id="9fab" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">iris_data = datasets.load_iris()</strong></span></pre><p id="0831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来了解一下‘iris _ data’。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="72ae" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">iris_data.keys()</strong></span></pre><p id="53b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出是dict_keys(['data '，' target '，' target_names '，' DESCR '，' feature_names '，' filename'])。</p><p id="fce9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据</strong>’:-顾名思义，就是数据。<br/> ' <strong class="ky ir">目标</strong>':输出标签的整数编码。0表示“setosa”，1表示“versicolor”，2表示“virginica”。<br/> ' <strong class="ky ir"> target_names </strong> ' :-输出标签即['setosa '，' versicolor '，' virginica']。<br/> ' <strong class="ky ir">描述</strong>':数据集的描述。<br/> ' <strong class="ky ir"> feature_names </strong>':输入特征，即['萼片长度'，'萼片宽度'，'花瓣长度'，'花瓣宽度']。<br/> ' <strong class="ky ir">文件名</strong> ' :-文件的路径。</p><p id="b41b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们创建一个“iris_data”的数据帧。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="70e9" class="lx ly iq lt b gy lz ma l mb mc"># Create a dataframe of the ‘iris_data’</span><span id="8003" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">iris = pd.DataFrame(data = iris_data[‘data’], columns = iris_data[‘feature_names’])</strong></span><span id="8770" class="lx ly iq lt b gy md ma l mb mc"># Displays first 5 columns of ‘iris’</span><span id="4c9b" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">iris.head()</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi me"><img src="../Images/eccaca3bf8544195ea5c597e59c1dc1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*XYlvlJcrigjsJcuxBNOatg.png"/></div></figure><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="950a" class="lx ly iq lt b gy lz ma l mb mc"># Displays properties of each column</span><span id="af79" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">iris.describe()</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/4ef101b8920ff7e937e0a5532bfda446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*EfG_RuBrd_WzsYXSOvq08w.png"/></div></figure><p id="332d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以观察到所有列的计数都是150，所有列的最小值都是非零数字，这表明<strong class="ky ir">没有丢失值</strong>。</p><h2 id="b1c9" class="lx ly iq bd mg mh mi dn mj mk ml dp mm lf mn mo mp lj mq mr ms ln mt mu mv mw bi translated">可视化数据</h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="be56" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">sns.pairplot(iris)</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/dbe4be385d488d8482d7b2e30f9504af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*jozdlqBi2hmib4jEzGvR2w.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/e4f591fabe125c6e6f7de0eab89c3b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*LEiINT6LkeCF2C-2SPHd-g.png"/></div></figure><p id="92d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">萼片长度、花瓣长度和花瓣宽度之间高度相关。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0b74" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">sns.heatmap(iris.corr())</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e743c87304ae98e3996b4b12df1cf8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*O_SUcckOhp8dKEyyK-5zcQ.png"/></div></figure><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="a42a" class="lx ly iq lt b gy lz ma l mb mc"># 'pairs' is a list of combinations of iris columns</span><span id="2fe0" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">pairs = []</strong></span><span id="7c80" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">for i in range(len(iris.columns)):<br/>  for j in range(i+1,len(iris.columns)):<br/>    pairs.append((iris.columns[i],iris.columns[j]))</strong></span><span id="c50f" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">pairs</strong></span></pre><p id="546a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出为<br/>('萼片长(cm)'，'萼片宽(cm)')，<br/>('萼片长(cm)'，'花瓣长(cm)')，<br/>('萼片长(cm)'，'花瓣宽(cm)')，<br/>('萼片宽(cm)'，'花瓣长(cm))，<br/>('萼片宽(cm)'，'花瓣宽(cm)')，<br/>('花瓣</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="7f43" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">a = 1<br/>left = 0.01</strong></span><span id="7b5e" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">plt.figure(figsize = (15,9))</strong> # Adjusting figure size</span><span id="3a9c" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">for i,j in pairs:</strong></span><span id="d05f" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">  sns.set(font_scale=1.3)</strong> # Adjusting font size<br/>  <strong class="lt ir">plt.subplots_adjust(left)</strong> # Adjusting spacing between subplots<br/> <br/>  <strong class="lt ir">plt.subplot(2,3,a)</strong> # Subplot No.</span><span id="3151" class="lx ly iq lt b gy md ma l mb mc">  <strong class="lt ir">sns.scatterplot(iris[i],iris[j],hue = iris_data[‘target’],palette= [“C0”, “C1”, “C2”])</strong> # Plotting the scatter plot<br/>  <br/><strong class="lt ir">a = a+1</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/b0016215e9bfe6e96f05601baf3731ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DrFVlRT_LhUr-jw-2ZzQ5w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Legend :- 0 denotes ‘Setosa’. 1 denotes ‘Versicolor’. 2 denotes ‘Virginica’.</figcaption></figure><p id="6dd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每个散点图中，可以看到3个分离良好的集群。橙色(“杂色”)和绿色(“海滨”)点之间有轻微的重叠。否则，集群是很好的区分。<strong class="ky ir">这表明K均值聚类模型将是iris数据集的最佳选择</strong>。因此，可视化数据不仅有助于了解数据，还有助于选择模型。</p><div class="nb nc gp gr nd ne"><a href="https://www.datadriveninvestor.com/2019/03/25/a-programmers-guide-to-creating-an-eclectic-bookshelf/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">创建折衷书架的程序员指南|数据驱动的投资者</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">每个开发者都应该有一个书架。他的内阁中可能的文本集合是无数的，但不是每一个集合…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns kp ne"/></div></div></a></div><h2 id="1119" class="lx ly iq bd mg mh mi dn mj mk ml dp mm lf mn mo mp lj mq mr ms ln mt mu mv mw bi translated">肘法</h2><p id="1180" class="pw-post-body-paragraph kw kx iq ky b kz nt jr lb lc nu ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">肘形法有助于确定最佳聚类数。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="3e94" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">from sklearn.cluster import KMeans</strong></span><span id="52c0" class="lx ly iq lt b gy md ma l mb mc"># K-Means model is run for different values of ‘number of clusters’</span><span id="a834" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">clusters = list(range(1,10))</strong></span><span id="04a0" class="lx ly iq lt b gy md ma l mb mc"># ‘inertia’ stores the inertia score of K-Means model</span><span id="ad72" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">inertia = []</strong></span><span id="00cb" class="lx ly iq lt b gy md ma l mb mc"># ‘for’ loop to run K-Means model for each value of ‘number of clusters’</span><span id="a85a" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">for i in clusters:<br/> kmeans = KMeans(n_clusters = i)<br/> kmeans.fit(iris)<br/> inertia.append(kmeans.inertia_)</strong></span><span id="7589" class="lx ly iq lt b gy md ma l mb mc"># A graph is plotted with ‘number of clusters’ on x-axis and ‘inertia score’ on y-axis</span><span id="f1a7" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">plt.figure(figsize = (10,8))<br/>plt.plot(clusters,inertia,marker = ‘o’,linestyle = ‘dashed’)<br/>plt.xlabel(‘Number of clusters’)<br/>plt.ylabel(‘Inertia score’)</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/81274899a2249cd0606faf6579b6b323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*5OOd4TYWRsEJhxL-T14HRQ.png"/></div></div></figure><p id="98d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">惯性分数急剧下降，直到“聚类数”= 3。之后没有明显的下降。因此,“聚类数”的最佳值是3。之所以称之为弯头法，是因为在最佳值时会形成弯头。</p><p id="8d7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">主成分分析(PCA)约简<br/>主成分分析</strong> ( <strong class="ky ir"> PCA </strong>)是一种统计程序，它使用正交变换将一组可能相关变量的观察值转换为一组线性不相关变量的值，称为<strong class="ky ir">主成分</strong>。在降维过程中会丢失一些信息。</p><p id="3c94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在数据集“虹膜”中，4个特征中的3个特征(“萼片长度”、“花瓣长度”和“花瓣宽度”)彼此相关。因此，主成分分析将在“虹膜”数据集上很好地工作。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="1cb2" class="lx ly iq lt b gy lz ma l mb mc"># Importing PCA</span><span id="b9c5" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">from sklearn.decomposition import PCA</strong></span><span id="8358" class="lx ly iq lt b gy md ma l mb mc"># Perform PCA</span><span id="236c" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">pca = PCA(n_components = 2)<br/>pc = pca.fit_transform(iris)</strong></span></pre><p id="f778" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有4个特征的“iris”数据集已缩减为具有2个特征的“pc”。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="5438" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">pc.shape</strong></span></pre><p id="73be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">形状为(150，2)。</p><p id="1277" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在缩减的数据集“pc”上训练模型。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9408" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">kmeans = KMeans(n_clusters = 3)<br/>kmeans.fit(pc)</strong></span></pre><p id="acab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们通过绘制颜色图来可视化简化的二维数据。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="20d9" class="lx ly iq lt b gy lz ma l mb mc"># Define step size of mesh</span><span id="64e5" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">h = 0.02</strong></span><span id="2b37" class="lx ly iq lt b gy md ma l mb mc"># Generate mesh grid</span><span id="07ce" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">x_min, x_max = pc[:,0].min() — 1, pc[:,0].max() + 1<br/>y_min, y_max = pc[:,1].min() — 1, pc[:,1].max() + 1<br/>xx, yy = np.meshgrid(np.arange(x_min,x_max,h), np.arange(y_min,y_max,h))</strong></span><span id="3b7a" class="lx ly iq lt b gy md ma l mb mc"># Obtain labels for each point in the mesh using our trained model on reduced data set</span><span id="0636" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">Z = kmeans.predict(np.c_[xx.ravel(),yy.ravel()])</strong></span><span id="6093" class="lx ly iq lt b gy md ma l mb mc"># Put the result into a color plot</span><span id="1b15" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">Z = Z.reshape(xx.shape)</strong></span><span id="154c" class="lx ly iq lt b gy md ma l mb mc"># Define color plot</span><span id="74a3" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">cmap = plt.cm.Paired</strong></span><span id="1de2" class="lx ly iq lt b gy md ma l mb mc"># Plotting color plot figure (Decision Boundary)</span><span id="b319" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">plt.clf()<br/>plt.figure(figsize=(10,9))<br/>plt.imshow(Z,interpolation = ‘nearest’,extent=(xx.min(),xx.max(),yy.min(),yy.max()),<br/> cmap = cmap,aspect = ‘auto’,origin = ‘lower’)</strong></span><span id="647a" class="lx ly iq lt b gy md ma l mb mc"># Plotting the points</span><span id="5b71" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">sns.scatterplot(pc[:,0],pc[:,1],hue = iris_data[‘target’],palette=[“C0”, “C1”, “C2”],legend = False)</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b08dc11b03b49d736f2eb6d02a2e926e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*lTwri7BI7m5sGTux78wPXw.png"/></div></figure><p id="f7e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据“iris _ data[‘target’]”信息(实际输出)，分散的点被分为3种颜色。基于训练模型的预测(预测输出),这些区域已经被分类为3种颜色。</p><p id="64aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">少数红色(杂色)和绿色(海滨)点被训练的模型错误分类。</p><h2 id="34c2" class="lx ly iq bd mg mh mi dn mj mk ml dp mm lf mn mo mp lj mq mr ms ln mt mu mv mw bi translated">“无五氯苯甲醚”和“有五氯苯甲醚”的比较</h2><p id="5567" class="pw-post-body-paragraph kw kx iq ky b kz nt jr lb lc nu ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">基于以下4个度量进行比较:-准确性、同质性得分、完整性得分和V-Measure得分。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="d9b0" class="lx ly iq lt b gy lz ma l mb mc"><strong class="lt ir">from sklearn import metrics</strong></span><span id="f50c" class="lx ly iq lt b gy md ma l mb mc"># Training model on actual data</span><span id="35df" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">kmeans1 = KMeans(n_clusters=3)<br/>kmeans1.fit(iris)</strong></span><span id="c964" class="lx ly iq lt b gy md ma l mb mc"># Training model on reduced data</span><span id="5e89" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">kmeans2 = KMeans(n_clusters=3)<br/>kmeans2.fit(pc)</strong></span><span id="703e" class="lx ly iq lt b gy md ma l mb mc"># Importing Metrics</span><span id="6fb0" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">from sklearn.metrics import homogeneity_score<br/>from sklearn.metrics import completeness_score<br/>from sklearn.metrics import v_measure_score</strong></span><span id="ed2c" class="lx ly iq lt b gy md ma l mb mc"># Calculating Accuracy</span><span id="3e71" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">models = [kmeans1,kmeans2]<br/>accuracy = []</strong></span><span id="6d42" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">for i in models:<br/>  h = 0<br/>  correct_label = 0<br/>  for j in range(i.n_clusters):<br/>    index = np.where(iris_data['target'] == h)<br/>    correct_label = correct_label + (i.labels_[index] == i.labels_[index].max()).sum()<br/>    h = h + 1<br/>  accuracy.append(correct_label/len(i.labels_))</strong></span><span id="0e38" class="lx ly iq lt b gy md ma l mb mc"># Printing the metrics</span><span id="bfad" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">k = 0<br/>for i in models:<br/>  print('Accuracy : {}'.format(accuracy[k]))<br/>  print('Homogeneity score  {}:'.format(metrics.homogeneity_score(iris_data['target'],i.labels_)))<br/>  print('Completeness score {}:'.format(metrics.completeness_score(iris_data['target'],i.labels_)))<br/>  print('V-Measure {}:'.format(metrics.v_measure_score(iris_data['target'],i.labels_)))<br/>  print('\n')<br/>  k = k+1</strong></span></pre><blockquote class="oa"><p id="11c2" class="ob oc iq bd od oe of og oh oi oj lr dk translated">(1无PCA <br/>准确度:0.7466666666666667 <br/>同质性得分:0.751485402198838<br/>完整性得分:0.766868666154</p><p id="9a4c" class="ob oc iq bd od oe ok ol om on oo lr dk translated">(2使用PCA <br/>的准确度:0.74 <br/>同质性得分:0.736419288125285 <br/>完备性得分:0.7474486585</p></blockquote><p id="b9f8" class="pw-post-body-paragraph kw kx iq ky b kz op jr lb lc oq ju le lf or lh li lj os ll lm ln ot lp lq lr ij bi translated">降维后的准确率、同质性、完备性和V-Measure评分都有小幅下降。<strong class="ky ir">这表明降低数据维度不会显著影响模型评估指标。数据的重要信息仍然被保留。降维还具有数据二维可视化的额外优势。</strong></p><p id="4ba3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新年快乐快乐阅读！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ou ov l"/></div></figure></div></div>    
</body>
</html>