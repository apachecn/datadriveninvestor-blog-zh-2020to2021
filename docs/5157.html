<html>
<head>
<title>Using Natural Language Processing for Spam Detection in Emails</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用自然语言处理检测电子邮件中的垃圾邮件</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/using-natural-language-processing-for-spam-detection-in-emails-281a7c22ddbc?source=collection_archive---------4-----------------------#2020-09-08">https://medium.datadriveninvestor.com/using-natural-language-processing-for-spam-detection-in-emails-281a7c22ddbc?source=collection_archive---------4-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本文中，我们采用了一个开源的垃圾邮件数据集，准备数据以供使用，并评估其性能。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8cb76b24f0916d39972c87b576558b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZzcKU607S3OWpn5I.jpg"/></div></div></figure><p id="214a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你有没有想过机器是如何翻译语言的？或者语音助手如何回复问题？或者邮件如何自动分类为垃圾邮件或不是垃圾邮件？</p><p id="6e78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所有这些任务都是通过自然语言处理(NLP)完成的，NLP<em class="lq">将文本处理成可以应用于未来数据的有用见解</em>。在人工智能领域，由于文本数据是上下文相关的，NLP是最复杂的研究领域之一。它需要修改以使其成为机器可解释的，并且需要多个处理阶段来进行特征提取。</p><p id="c455" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分类问题可以大致分为两类:二元分类问题和多类分类问题。二元分类意味着只有两种可能的标签类别，例如，患者的状况是癌症还是非癌症，或者金融交易是欺诈性的还是非欺诈性的。多类分类是指有两个以上标签类的情况。这方面的一个例子是将电影评论的情绪分为正面、负面或中性。</p><p id="c540" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">NLP问题有很多种，其中最常见的一种是字符串的<em class="lq">分类</em>。这方面的例子包括将电影/新闻文章分类为不同的类型，以及将电子邮件自动分类为垃圾邮件或非垃圾邮件。在本文中，我将更详细地研究最后一个例子。</p><h1 id="8a5e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">问题描述</h1><p id="9fa7" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">理解问题是解决任何机器学习问题的关键的第一步。在本文中，我们将探索和理解将电子邮件分类为垃圾邮件或非垃圾邮件的过程。这就是所谓的垃圾邮件检测，是一个二元分类问题。</p><p id="ed51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这样做的原因很简单:通过检测未经请求和不想要的电子邮件，我们可以防止垃圾邮件悄悄进入用户的收件箱，从而改善用户体验。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/3b87bf0232f2e8f1ee4ce0af529e2ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SHZ7ehjxCaS7-VBp.png"/></div></div></figure><h1 id="fbfa" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">资料组</h1><p id="5485" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">让我们从垃圾邮件检测数据开始。我们将使用来自UCI机器学习知识库的开源<a class="ae mp" href="http://archive.ics.uci.edu/ml/datasets/Spambase/" rel="noopener ugc nofollow" target="_blank">垃圾邮件数据集</a>，该数据集包含5569封电子邮件，其中745封是垃圾邮件。</p><p id="aced" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个数据集的目标变量是“spam ”,其中一封<em class="lq">垃圾邮件被映射到1 </em>,其他任何邮件都被映射到0。目标变量可以被认为是你试图预测的。在机器学习问题中，这个变量的值会被其他变量建模和预测。</p><p id="f088" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图1显示了数据的快照。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/9f663735d9fee988e05467540f5db2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*-7CWM-ExrevXBGCQ.png"/></div></figure><p id="34eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">任务:将电子邮件分类为垃圾邮件或非垃圾邮件。</p><p id="d6e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了得到我们的解决方案，我们需要理解下面的四个处理概念。请注意，这里讨论的概念也可以应用于其他文本分类问题。</p><ul class=""><li id="1ff4" class="mr ms it kw b kx ky la lb ld mt lh mu ll mv lp mw mx my mz bi translated">文本处理</li><li id="97cf" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">文本排序</li><li id="45db" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">型号选择</li><li id="f7a1" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">履行</li></ul><h1 id="8eef" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">1.文本处理</h1><p id="be3e" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">数据通常来自各种来源，格式也各不相同。因此，转换原始数据至关重要。然而，这种转换不是一个简单的过程，因为文本数据通常包含冗余和重复的单词。这意味着处理文本数据是我们解决方案的第一步。</p><p id="ff3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">文本预处理中涉及的基本步骤是，</p><ol class=""><li id="c57f" class="mr ms it kw b kx ky la lb ld mt lh mu ll mv lp nf mx my mz bi translated">清理原始数据</li><li id="d150" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp nf mx my mz bi translated">对清理的数据进行标记</li></ol><h1 id="1c3f" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">a.清理原始数据</h1><p id="72d5" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">这个阶段包括删除对文本意义没有价值的单词或字符。下面列出了一些标准的清洁步骤:</p><ul class=""><li id="7572" class="mr ms it kw b kx ky la lb ld mt lh mu ll mv lp mw mx my mz bi translated">下降箱</li><li id="88c4" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">特殊字符的删除</li><li id="1ec7" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">停用词的删除</li><li id="55d0" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">移除超链接</li><li id="d7ea" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">数字的删除</li><li id="7dd7" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">删除空白</li></ul><h2 id="fd63" class="ng ls it bd lt nh ni dn lx nj nk dp mb ld nl nm md lh nn no mf ll np nq mh nr bi translated">下降箱</h2><p id="9129" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">降低文本的大小写非常重要，原因如下:</p><ul class=""><li id="a953" class="mr ms it kw b kx ky la lb ld mt lh mu ll mv lp mw mx my mz bi translated">单词“文本”、“文本”、“文本”都给句子增加了相同的值</li><li id="55fc" class="mr ms it kw b kx na la nb ld nc lh nd ll ne lp mw mx my mz bi translated">降低所有单词的大小写对于通过减少词汇表的大小来降低维数非常有帮助</li></ul><p id="be2a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">def to_lower(word):</code><br/><code class="fe ns nt nu nv b"> result = word.lower()</code><br/>T2】</p><h2 id="0c87" class="ng ls it bd lt nh ni dn lx nj nk dp mb ld nl nm md lh nn no mf ll np nq mh nr bi translated">特殊字符的删除</h2><p id="7303" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">这是另一种文本处理技术，将有助于处理像“万岁”和“万岁！”同理。</p><p id="d6af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">def remove_special_characters(word):</code><br/><code class="fe ns nt nu nv b"> result=</code><br/><code class="fe ns nt nu nv b">word.translate(str.maketrans(dict.fromkeys(string.punctuation)))</code><br/>T6】</p><h2 id="13af" class="ng ls it bd lt nh ni dn lx nj nk dp mb ld nl nm md lh nn no mf ll np nq mh nr bi translated">停用词的删除</h2><p id="bc84" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">停用词是语言中常见的词，如“the”、“a”等。大多数时候，它们可以从文本中删除，因为它们没有提供有价值的信息。</p><p id="2175" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">def remove_stop_words(words):</code><br/><code class="fe ns nt nu nv b"> result = [i for i in words if i not in ENGLISH_STOP_WORDS]</code><br/>T9】</p><h1 id="1de3" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">移除超链接</h1><p id="3447" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">接下来，我们删除数据中的所有URL。电子邮件中很有可能会有一些网址。我们不需要它们来做进一步的分析，因为它们不会给结果增加任何价值。</p><p id="5cf1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">def remove_hyperlink(word):</code> <br/> <code class="fe ns nt nu nv b"> return re.sub(r"http\S+", "", word)</code></p><h1 id="2a3c" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">b.对清理的数据进行标记</h1><p id="e841" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">标记化是将文本分割成更小的块(称为标记)的过程。每个标记都是作为特征的机器学习算法的输入。</p><p id="73b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">keras.preprocessing.text.Tokenizer</code>是一个实用函数，它将文本标记为标记，同时只保留文本语料库中出现次数最多的单词。当我们对文本进行标记时，我们最终会得到一个庞大的单词词典，而它们并不都是必不可少的。我们可以设置'<em class="lq"> max_features </em>'来选择我们要考虑的最常用的词。</p><p id="4880" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">max_feature = 50000 #number of unique words to consider</code></p><p id="8a23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">from keras.preprocessing.text import Tokenizer</code><br/><code class="fe ns nt nu nv b">tokenizer = Tokenizer(num_words=max_feature)</code><br/><code class="fe ns nt nu nv b">tokenizer.fit_on_texts(x_train)</code><br/><code class="fe ns nt nu nv b">x_train_features = np.array(tokenizer.texts_to_sequences(x_train))</code><br/>T6】</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/2724212fc1b80823bebdf59ca97db9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZvPH8VLIX2I-LinD.png"/></div></div></figure><h1 id="c32e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">2.文本排序</h1><h1 id="9b8d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">a.填料</h1><p id="62c4" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">让所有邮件的令牌大小相等被称为<em class="lq">填充</em>。</p><div class="nx ny gp gr nz oa"><a href="https://www.datadriveninvestor.com/2020/08/30/ai-and-medical-imaging-startups-6-key-trends/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">AI和医学影像创业公司？6大趋势|数据驱动的投资者</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">“IBM Watson健康成像”是医疗保健的未来吗？谷歌详细说明了人工智能对胸部x光的分类…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><p id="fc55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们批量发送数据点输入。当输入大小不同时，信息可能会丢失。因此，我们使用填充使它们大小相同，这样便于批量更新。</p><p id="de10" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用'<em class="lq"> max_len </em>'设置所有标记化电子邮件后填充的长度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/9dc2ad3df726a017211441ab2c0dc4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PKcpigkqsJISC-Sz.png"/></div></div></figure><p id="c9c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">填充的代码段:</p><p id="eb20" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">from keras.preprocessing.sequence import pad_sequences</code> <br/> <code class="fe ns nt nu nv b">x_train_features = pad_sequences(x_train_features,maxlen=max_len)</code> <br/> <code class="fe ns nt nu nv b">x_test_features = pad_sequences(x_test_features,maxlen=max_len)</code></p><h1 id="80f4" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">b.标记编码目标变量</h1><p id="a803" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">模型期望目标变量是一个数字，而不是一个字符串。我们可以使用<code class="fe ns nt nu nv b">sklearn</code>中的标签编码器来转换目标变量，如下所示。</p><p id="eb8b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">from sklearn.preprocessing import LabelEncoder</code><br/><code class="fe ns nt nu nv b">le = LabelEncoder()</code><br/><code class="fe ns nt nu nv b">train_y = le.fit_transform(target_train.values)</code><br/><code class="fe ns nt nu nv b">test_y = le.transform(target_test.values)</code></p><h1 id="0217" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">3.型号选择</h1><p id="2c8d" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">一部电影由一系列场景组成。当我们观看一个特定的场景时，我们不会试图孤立地理解它，而是联系以前的场景来理解。以类似的方式，机器学习模型必须通过利用已经学习的文本来理解文本，就像人类神经网络一样。</p><p id="af02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在传统的机器学习模型中，我们无法存储模型的先前阶段。然而，递归神经网络(通常称为RNN)可以为我们做到这一点。下面我们来仔细看看RNNs。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1a5bc9b1ab4d07884341f5307a4d9983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/0*wUvfhkzMyvqHvjVL.png"/></div></figure><p id="f91b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">RNN有一个重复模块，它接收前一级的输入，并将其输出作为下一级的输入。然而，在RNNs中，我们只能保留最近阶段的信息。为了了解长期依赖关系，我们的网络需要记忆能力。这就是长短期记忆网络(LSTMs)来拯救我们的地方。</p><p id="9ddd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">LSTMs是RNNs的特例，它们具有与RNNs相同的链状结构，但是具有不同的重复模块结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/04a7097a9b6f76e7892d03ba3940ecf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*ltHp8AzZOBmEQGd4.png"/></div></figure><p id="d8dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了以相反的顺序执行LSTM，我们将使用双向LSTM。</p><h1 id="4750" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">4.履行</h1><h1 id="43cf" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">把...嵌入</h1><p id="bdba" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">文本数据很容易被人类理解。但是对于机器来说，阅读和分析是一项非常复杂的任务。为了完成这项任务，我们需要将文本转换成机器可以理解的格式。</p><p id="8bc3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嵌入是将格式化的文本数据转换成机器可以解释的数值/向量的过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7d7e608b72103e913b2ee62f679fd715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E5Ens4bopqFG6x_l.png"/></div></figure><p id="c6ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">import tensorflow as tf</code><br/><code class="fe ns nt nu nv b">from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D</code><br/><code class="fe ns nt nu nv b">from keras.layers import Bidirectional, GlobalMaxPool1D</code><br/><code class="fe ns nt nu nv b">from tensorflow.compat.v1.keras.layers import CuDNNGRU</code><br/><code class="fe ns nt nu nv b">from keras.models import Model</code><br/><code class="fe ns nt nu nv b">from keras import initializers, regularizers, constraints, optimizers, layers</code></p><p id="f250" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#size of the output vector from each layer</code> <br/> <code class="fe ns nt nu nv b">embedding_vector_length = 32</code></p><p id="c641" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Creating a sequential model</code> <br/> <code class="fe ns nt nu nv b">model = tf.keras.Sequential()</code></p><p id="db11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Creating an embedding layer to vectorize</code> <br/> <code class="fe ns nt nu nv b">model.add(Embedding(max_feature, embedding_vector_length, input_length=max_len))</code></p><p id="5868" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Addding Bi-directional LSTM</code> <br/> <code class="fe ns nt nu nv b">model.add(Bidirectional(tf.keras.layers.LSTM(64)))</code></p><p id="ffcd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Relu allows converging quickly and allows backpropagation</code> <br/> <code class="fe ns nt nu nv b">model.add(Dense(16, activation='relu'))</code></p><p id="db39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Deep Learninng models can be overfit easily, to avoid this, we add randomization using drop out</code> <br/> <code class="fe ns nt nu nv b">model.add(Dropout(0.1))</code></p><p id="b654" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">#Adding sigmoid activation function to normalize the output</code> <br/> <code class="fe ns nt nu nv b">model.add(Dense(1, activation='sigmoid'))</code></p><p id="777c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</code></p><p id="4fb0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">print(model.summary())</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/621f9ec649414deb4fa900c316290c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*vy6qo1vODSZP4KJt.png"/></div></figure><p id="f4a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">history = model.fit(x_train_features, train_y, batch_size=512, epochs=20, validation_data=(x_test_features, test_y))</code></p><p id="9433" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">y_predict = [1 if o&gt;0.5 else 0 for o in model.predict(x_test_features)]</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/0bfea040564f9c5fb8202ce9b5da8955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uv1XZ3jXsayf49Tq.png"/></div></div></figure><p id="0ca7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过以上操作，我们成功地将双向LSTM模型应用于我们的电子邮件数据，并在1114封电子邮件中检测出125封是垃圾邮件。</p><p id="5028" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于垃圾邮件在数据中所占的百分比通常很低，因此不建议仅通过准确性来衡量模型的性能。我们还需要使用其他性能指标来评估它，我们将在下面看到。</p><h1 id="07e0" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">性能指标</h1><p id="99b1" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">精确度和召回率是分类问题中最广泛使用的两个性能度量，用于更好地理解问题。精度是所有检索实例中相关实例的分数。精确度有助于我们理解结果有多有用。回忆是所有相关实例中相关实例的分数。回忆帮助我们理解结果有多完整。</p><p id="aa13" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">F1分数是精确度和召回率的调和平均值。</p><p id="6869" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，考虑一个搜索查询产生30个页面，其中20个是相关的，但是结果不能显示40个其他相关的结果。在这种情况下，精度是20/30，召回率是20/60。所以，我们的F1成绩是4/9。</p><p id="2a21" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用F1分数作为垃圾邮件检测问题的性能指标是一个不错的选择。</p><p id="5b72" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score</code></p><p id="439e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">cf_matrix =confusion_matrix(test_y,y_predict)</code></p><p id="4ce8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">tn, fp, fn, tp = confusion_matrix(test_y,y_predict).ravel()</code></p><p id="b173" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">print("Precision: {:.2f}%".format(100 * precision_score(test_y, y_predict)))</code> <br/> <code class="fe ns nt nu nv b">print("Recall: {:.2f}%".format(100 * recall_score(test_y, y_predict)))</code> <br/> <code class="fe ns nt nu nv b">print("F1 Score: {:.2f}%".format(100 * f1_score(test_y,y_predict)))</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/b55f6dc669c012d0aa3403e55daaab17.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/0*_qRoVGbBifVoxdOF.png"/></div></figure><p id="e68e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">import seaborn as sns</code> <br/> <code class="fe ns nt nu nv b">import matplotlib.pyplot as plt</code></p><p id="994e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b">ax= plt.subplot()</code> <br/> <code class="fe ns nt nu nv b">#annot=True to annotate cells</code> <br/> <code class="fe ns nt nu nv b">sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt='');</code></p><p id="e22b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ns nt nu nv b"># labels, title and ticks</code><br/><code class="fe ns nt nu nv b">ax.set_xlabel('Predicted labels');</code><br/><code class="fe ns nt nu nv b">ax.set_ylabel('True labels');</code><br/><code class="fe ns nt nu nv b">ax.set_title('Confusion Matrix');</code><br/><code class="fe ns nt nu nv b">ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/035ece9ac2dfe587e1586e34ae7696c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/0*hmEBs1NA4BwEt0Tg.png"/></div></figure><p id="b34e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">F1分数为94%的车型是一款不错的车型。但是，请记住，这些结果是基于我们使用的训练数据。当将这样的模型应用于真实世界的数据时，我们仍然需要随着时间的推移主动监控模型的性能。我们还可以通过添加功能和删除拼写错误的单词来响应结果和反馈，从而继续改进模型。</p><h1 id="32e2" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">摘要</h1><p id="e000" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在本文中，我们创建了一个垃圾邮件检测模型，方法是将文本数据转换为向量，创建一个BiLSTM模型，并用向量拟合该模型。我们还探索了各种文本处理技术、文本排序技术和深度学习模型，即RNN、LSTM、比尔斯特姆。你可以在我的<a class="ae mp" href="https://github.com/RamyaVidiyala/SpamDetectionInEmails" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到该项目的所有代码。</p><p id="6255" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文中学习的概念和技术可以应用于各种自然语言处理问题，如构建聊天机器人、文本摘要、语言翻译模型。我们希望以后能有更多关于这类NLP问题的文章。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><p id="80a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请务必查看下面的相关资源以获取更多技术文章，并注册<a class="ae mp" href="https://lionbridge.ai/ai-newsletter-subscription/" rel="noopener ugc nofollow" target="_blank"> Lionbridge AI简讯</a>以获取直接发送到您收件箱的采访和文章。</p><p id="ceae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="https://lionbridge.ai/articles/using-natural-language-processing-for-spam-detection-in-emails/" rel="noopener ugc nofollow" target="_blank">原创文章</a>经许可转贴。</p><p id="d9fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">访问专家视图— </strong> <a class="ae mp" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>