<html>
<head>
<title>Extreme Learning Machines II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极限学习机II</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/extreme-learning-machines-9c8be01f6f77?source=collection_archive---------1-----------------------#2020-07-04">https://medium.datadriveninvestor.com/extreme-learning-machines-9c8be01f6f77?source=collection_archive---------1-----------------------#2020-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="12b5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第二部分:算法:有何不同？</h2></div><p id="0f6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">与用于神经网络学习的传统算法(例如反向传播算法)不同，传统算法可能面临手动调整控制参数(学习速率、学习时期和其他超参数)和/或局部最小值的困难，ELM是完全自动实现的，没有任何迭代调整，也不需要用户干预。此外，ELM的学习速度与其他传统方法相比非常快。</p><p id="51c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在ELM算法中，隐节点的学习参数(包括输入权值和偏差)可以独立地随机分配，网络的输出权值可以通过简单的广义逆运算解析地计算出来。</p><p id="dc52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，ELM算法可以获得更好的泛化性能。此外，证明了具有附加或RBF激活函数的ELM算法的全局逼近能力。ELM算法已经成功地应用于许多现实世界的问题，例如分类和回归问题。</p><h1 id="8241" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">最小二乘法:</h1><p id="8e1b" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><strong class="kh ir">计算输出重量</strong></p><p id="1d35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解线性方程Ax+b=0，我们有一个不需要是方阵的A。它可以是一个<strong class="kh ir"> n </strong> x <strong class="kh ir"> m </strong>的矩阵，其中n &lt; m。这里我们有更多的未知数，而不是我们有方程，所以系统是<strong class="kh ir">欠定的</strong>。</p><p id="d738" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这些条件下，通常x有许多可能的值，使得Ax = y。我们需要找到这样一个最小(加权)欧几里德范数，即最小∣∣x∣∣.的解x</p><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="mm mn l"/></div></figure><h2 id="8dac" class="mo ll iq bd lm mp mq dn lq mr ms dp lu ko mt mu lw ks mv mw ly kw mx my ma mz bi translated">一般线性系统的最小范数最小二乘解</h2><p id="b5ab" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">对于一般线性系统Ax = y，我们说xˇ(x-hat)是最小二乘解(l.s.s ),如果</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/2e81876c420f3d2a659fa913cb55b011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxqN6h-swpEU2wqAEI0tQA.png"/></div></div></figure><blockquote class="nh"><p id="4e79" class="ni nj iq bd nk nl nm nn no np nq la dk translated">定义:x₀ ∈ Rⁿ据说是线性系统Ax = y的最小范数最小二乘解，如果对于任何y ∈ Rᵐ</p></blockquote><figure class="ns nt nu nv nw ml gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nr"><img src="../Images/18bdfb50c8488c7fb19bb04b92dce2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xh40PiQE3kQzEsFWeSlJiw.png"/></div></div></figure><h2 id="8478" class="mo ll iq bd lm mp mq dn lq mr ms dp lu ko mt mu lw ks mv mw ly kw mx my ma mz bi translated">摩尔-彭罗斯广义逆</h2><p id="a3ff" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><em class="nx">又名伪逆</em></p><blockquote class="nh"><p id="6e41" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated"><strong class="ak"> <em class="od">定义:</em> </strong>一个n × m阶矩阵G是m × n阶矩阵A的Moore-Penrose广义逆，如果</p></blockquote><figure class="ns nt nu nv nw ml gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oe"><img src="../Images/5fb0a7dc80fcca97c09d3cb561927b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DEMgQSI_sQw0nhGq7EjVw.png"/></div></div></figure><p id="62fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们一般知道，要计算一个矩阵的逆矩阵，这个矩阵应该是一个方阵。但是无论矩阵是方阵还是非方阵，都应该有一个逆矩阵。</p><p id="817f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">伪逆基本上是任何类型矩阵(方阵或非方阵)的逆。</p><h1 id="add6" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">体系结构</h1><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi of"><img src="../Images/83d1f8c5c8cf2652eee903f27e91d4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4GBP7uEAXwNvyUw7Hk3Sw.png"/></div></div><figcaption class="og oh gj gh gi oi oj bd b be z dk">Image from reference [1]</figcaption></figure><p id="e5dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于ELM算法的单层前馈神经网络由具有N个输入节点的输入批次x(批次大小为N)组成。</p><h2 id="7881" class="mo ll iq bd lm mp mq dn lq mr ms dp lu ko mt mu lw ks mv mw ly kw mx my ma mz bi translated">算法:</h2><p id="f05a" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated"><strong class="kh ir">隐藏层</strong>:使用输入权重参数(a，b)，用于计算隐藏层中的ax+b。</p><p id="66fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应用激活函数g(y ),给出g(ax+b)作为隐藏层的输出。让我们称g(ax+b)为h。</p><blockquote class="nh"><p id="cab3" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated"><strong class="ak"> H = g(ax+b) </strong></p></blockquote><p id="9c80" class="pw-post-body-paragraph kf kg iq kh b ki ok jr kk kl ol ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated"><strong class="kh ir">输出层:</strong> β是所用的输出权重，乘以H得到我们的最终输出。</p><blockquote class="nh"><p id="d021" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated"><strong class="ak"> y = Hβ </strong></p></blockquote><p id="9378" class="pw-post-body-paragraph kf kg iq kh b ki ok jr kk kl ol ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated"><strong class="kh ir">计算β </strong></p><p id="e2c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据标记的目标“T”计算β，如下所示</p><blockquote class="nh"><p id="e599" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated">β = (H.H <strong class="ak"> ᵗ </strong> )⁻。H <strong class="ak"> ᵗ.T </strong></p></blockquote><p id="1ae9" class="pw-post-body-paragraph kf kg iq kh b ki ok jr kk kl ol ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">这里的术语{ <strong class="kh ir"> (H.Hᵗ)⁻。Hᵗ是矩阵h的伪逆，我们现在用H₀.来表示</strong></p><blockquote class="nh"><p id="8662" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated">H₀ = (H.Hᵗ)⁻。Hᵗ</p><p id="adef" class="ni nj iq bd nk nl ny nz oa ob oc la dk translated">β = H₀.T</p></blockquote><p id="cdd7" class="pw-post-body-paragraph kf kg iq kh b ki ok jr kk kl ol ju kn ko om kq kr ks on ku kv kw oo ky kz la ij bi translated">在哪里，</p><ul class=""><li id="f78c" class="op oq iq kh b ki kj kl km ko or ks os kw ot la ou ov ow ox bi translated">h从隐藏层输出，</li><li id="637b" class="op oq iq kh b ki oy kl oz ko pa ks pb kw pc la ou ov ow ox bi translated">Hᵗ是h的转位</li><li id="5d42" class="op oq iq kh b ki oy kl oz ko pa ks pb kw pc la ou ov ow ox bi translated">t是目标</li></ul><h2 id="592c" class="mo ll iq bd lm mp mq dn lq mr ms dp lu ko mt mu lw ks mv mw ly kw mx my ma mz bi translated">Python实现:</h2><figure class="mh mi mj mk gt ml"><div class="bz fp l di"><div class="pd mn l"/></div></figure><p id="e08c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要初始化这个类，给它隐藏层的节点数，即hiddenSize。</p><p id="8ed3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们需要将带有标签(y_train)的输入数据(x_train)输入到训练方法中。</p><p id="21cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输入权重W和偏差‘b’随机选择适当的大小，不添加偏差是可以的。我已经应用了ReLu激活功能，尽管其他任何激活都可以。</p></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><p id="a45e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本系列上一篇:<strong class="kh ir">第一部分:背景</strong><a class="ae pl" href="https://medium.com/@prasad.kumkar/extreme-learning-machines-9f9512002205" rel="noopener">https://medium . com/@ Prasad . kumkar/extreme-learning-machines-9f 9512002205</a></p><p id="750b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本系列下一篇:<strong class="kh ir">第三部分:对比</strong><a class="ae pl" href="https://medium.com/@prasad.kumkar/extreme-learning-machines-ef3b229d63c5" rel="noopener">https://medium . com/@ Prasad . kumkar/extreme-learning-machines-ef3b 229d 63 c 5</a></p><h1 id="44c7" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">参考资料:</h1><p id="4fe9" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">[1] J. Park和J. Kim，“在线递归极限学习机及其在时间序列预测中的应用”，2017年国际神经网络联合会议(IJCNN)，安克雷奇，AK，2017，PP . 1983–1990，doi:10.1109/ij CNN . 2017 . 199900001</p><p id="6141" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]黄，g .什么是极限学习机？填补了弗兰克·罗森布拉特的梦想和约翰·冯·诺依曼的困惑之间的空白。<em class="nx">Cogn Comput</em>T22】7，263–278(2015)。<a class="ae pl" href="https://doi.org/10.1007/s12559-015-9333-0" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s12559-015-9333-0</a></p><p id="d85f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]黄、朱光斌、秦宇、萧，徐志军.(2004).一种新的前馈神经网络学习方案。IEEE神经网络国际会议-会议录。2.985–990第二卷。</p></div></div>    
</body>
</html>