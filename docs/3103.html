<html>
<head>
<title>Review On YOLOv1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLOv1综述</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/review-on-yolov1-3c85304b617d?source=collection_archive---------1-----------------------#2020-06-01">https://medium.datadriveninvestor.com/review-on-yolov1-3c85304b617d?source=collection_archive---------1-----------------------#2020-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/76e9b04d0912fbcd3a24ad2c762fd4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsiD0Hwcveq6cyJ_C2zssw.png"/></div></div></figure><p id="a187" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">YOLO代表你只看一眼。顾名思义，网络只看图像一次，以检测多个对象。在这张<a class="ae kw" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">纸上</a>的主要改进是检测速度<strong class="ka ir">(使用YOLO 45 fps</strong>，使用快速YOLO<strong class="ka ir">155 fps</strong>)。这是另一种最先进的深度学习对象检测方法，已由<strong class="ka ir"> Joseph Redmon，Santosh Divvala，Ross Girshick，Ali法尔哈迪在<strong class="ka ir"> 2016 CVPR </strong>发表。</strong></p><p id="27e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将文章分成几个小类:</p><ol class=""><li id="6d3d" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated"><strong class="ka ir">统一检测</strong></li><li id="3433" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">网络设计</strong></li><li id="dd48" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">训练、损失函数和推断。</strong></li><li id="719c" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">YOLO的局限性</strong></li><li id="f1f6" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated"><strong class="ka ir">结果</strong></li></ol></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="b5b5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">1.统一检测</h1><p id="ad90" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">在现有技术的RCNN中，首先我们生成区域提议，然后使用该区域提议来检测对象。这是一个两步的过程，因此速度较慢。YOLO通过引入统一的检测网络解决了这一问题。在YOLO，网络使用从整个图像创建的特征来预测包围盒。同时预测所有的类和包围盒。</p><div class="mv mw gp gr mx my"><a href="https://www.datadriveninvestor.com/2019/03/22/fixing-photography/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd ir gy z fp nd fr fs ne fu fw ip bi translated">修复摄影|数据驱动的投资者</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">汤姆·津伯洛夫在转向摄影之前曾在南加州大学学习音乐。作为一个…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm jw my"/></div></div></a></div><p id="6982" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在所有的目标检测算法中，传统的方法是使用滑动窗口方法。但是计算量很大。YOLO通过使用网格单元的概念克服了这个问题。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/772b4d7ba4c3905d0b8bf1b6ead74d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*FB0aF_Mj4HNqC83cTqgWww.png"/></div></figure><ul class=""><li id="de57" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated">YOLO使用一个SxS网格单元(这里S=7)。如果对象的中心落在网格单元上，则该网格单元负责检测对象。</li><li id="4a1d" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated">每个网格单元产生B (B = 2)个边界框和这些框的置信度得分/客观性得分。置信度得分表示对盒子包含对象的置信度。</li></ul><p id="3c66" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="nt">信心得分= Pr(obj)∫IoU pred truth</em></strong></p><p id="7114" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="nt"> IOU predtruth </em> </strong>表示预测框与地面真相框的IOU</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/eab30f83059eadc2878deec42fce9669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*MM5NIGUkhmkjD7Tn5HBNxQ.png"/></div></figure><ul class=""><li id="c0e3" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated">每个边界框预测由x，y，w，h坐标和置信度得分组成。(x，y)坐标表示相对于网格单元边界的盒子中心。(w，h)坐标表示相对于图像的宽度和高度。</li></ul><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/77b1b4f44903341d83a0e4e5a3e5c56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*5S40LeNO8gXgscY5hJukNw.png"/></div></figure><ul class=""><li id="fff2" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated">每个网格单元还产生每个类Pr(Class_i|Object)的条件概率。这里的班级总数是20</li></ul><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0f89eb527131a0b36f4c19e7dadac5d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*JPZoac4osHUhdADcwBsiIQ.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">output tensor</figcaption></figure><p id="aa91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们总共有<strong class="ka ir"> 7×7×(2×5+20)=1470个预测。</strong></p><ul class=""><li id="d951" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated">在训练时，我们只希望一个边界框预测器负责每个对象。我们分配一个预测器来“负责”预测一个对象，基于哪个预测具有与地面实况的最高当前IOU</li><li id="187a" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated">在测试期间，通过将条件类概率与置信度得分相乘来获得每个盒子的类特定置信度。</li></ul><p id="1ed3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"><em class="nt">pr(class _ I)∣object)×pr(object)×iou pred truth = pr(class _ I)</em></strong></p><p id="b7b7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="nt"> ×借据</em> </strong></p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="4806" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak"> 2。网络设计</strong></h1><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e81cf8b53936bbc944991d47f36974fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*9EfbOo8y8rIjzhCQ3mN5jQ.png"/></div></figure><p id="f835" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">YOLO网络架构的灵感来自谷歌网络架构。该网络有24个卷积层，后面是2个全连接层。代替GoogLeNet使用的初始模块，我们简单地使用1×1缩减层，然后是3×3卷积层。</p><p id="6d8a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">快速YOLO使用较少的卷积层(9层而不是24层)以及这些层中较少的滤波器。除了网络的规模之外，YOLO和Fast YOLO之间的所有培训和测试参数都是相同的。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="f885" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">3.训练、损失函数和推理</h1><h2 id="5004" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">3.1培训</h2><p id="1888" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">卷积层在ImageNet 1000类数据集上进行预训练。他们使用前面描述的网络的前20个卷积层，然后是一个平均池层和一个全连接层(1x1000)。</p><p id="c38f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在此之后进行检测训练，他们删除了1x1000全连接层，并添加了四个卷积层和两个随机初始化权重的全连接层。他们还将输入分辨率从224x224改为448x448，因为这有助于检测更小的物体。</p><p id="2018" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一层预测类别概率和包围盒。边界框的(x，y)坐标被参数化，因此它们的值相对于网格单元在0和1之间。高度和宽度坐标相对于图像的高度和宽度被标准化。</p><p id="85c1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">他们在最终层使用线性激活，在其他层使用泄漏relu。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/45c14a52e028968719a3f67d7e52e5a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*gc5p41BnFh4X0u_o0IEajQ.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">leaky relu</figcaption></figure><h2 id="39f5" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">3.2损失函数</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e7d3d122a8265921efce1d63975a383b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*6Ugek4zPmbp9musrIA7eTA.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">loss function</figcaption></figure><p id="905b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nt">注:1obj_ij有对象时为1，无对象时为0，同理1noob_ij无对象时为1，有对象时为0。</em></p><p id="631c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="nt"> λcoord= 5，λnoobj=0.5。</em></p><p id="4dd8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">损失函数中有5项:</p><ul class=""><li id="85d7" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated"><strong class="ka ir">第一项:</strong>x，y坐标由网格单元的偏移量参数化，其值在0和1之间。因此使用误差平方和。</li><li id="5256" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><strong class="ka ir">第二项:</strong>宽度和高度是相对于整个图像的。因此，我们不能直接使用平方误差，因为一个小的差异可以产生巨大的影响。所以这是通过求平方根，然后求平方误差部分解决的。</li><li id="9c50" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><strong class="ka ir">第三项和第四项:</strong>这是地面真值和预测包围盒之间的IOU。在许多网格单元中，将没有对象。这会让他们的信心指数降到零。这可能会在反向传播期间压倒梯度，并可能导致模型不稳定。这可以通过λnoobj =0.5来部分解决。</li><li id="00c7" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><strong class="ka ir">第5项:</strong>有对象时的类概率。</li></ul><h2 id="8131" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">3.3推理</h2><p id="4314" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">YOLO在测试时非常快，因为它只需要一个单一的网络评估，不像基于分类器的方法。在检测过程中，通常可以对同一物体进行多次检测。他们通过<strong class="ka ir">非最大抑制</strong>克服了这个问题。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/3d2c2cc4fd7d47e9cbd823b7e1026b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*cbJBHOSa9PpN34TcqrzpqQ.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">detection system</figcaption></figure><h2 id="2ccb" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">非最大抑制是如何工作的？</h2><p id="7f72" class="pw-post-body-paragraph jy jz iq ka b kb mq kd ke kf mr kh ki kj ms kl km kn mt kp kq kr mu kt ku kv ij bi translated">假设N是阈值，我们有两个列表A和B，其中A是输入建议，B是输出。然后对于图像中检测到的每个类别，</p><ol class=""><li id="8472" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">根据置信度得分按降序对提案进行排序。</li><li id="add1" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">选择具有最高置信度得分的提议，将其从A中移除，并将其添加到最终提议列表B中(最初B为空)。</li><li id="63aa" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">现在，通过计算此建议与所有其他建议的IOU(交集/并集),将此建议与所有建议进行比较。如果IOU大于阈值N，则从a中删除该建议。</li><li id="bbba" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">现在，根据置信度得分，以降序对剩余提案进行排序。现在，从A中的剩余提议中选择可信度最高的提议，将其从A中移除并添加到b中。现在，与步骤3中一样，比较这些提议，并根据需要进行操作。</li></ol><ul class=""><li id="8198" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated">重复这个过程(步骤1、2和3 ),直到a中不再有建议。</li></ul><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi or"><img src="../Images/c9e3ce960e666a338f725120674c069f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*klKwRg3hPTA0QyO2.png"/></div></div></figure><p id="ee4f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">然后，在来自PASCAL VOC 2007和2012的训练和验证数据集上对网络进行大约135个时期的训练。在2012年进行测试时，他们还包括用于培训的VOC 2007测试数据</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="ee25" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">4.YOLO的局限性</h1><ul class=""><li id="b965" class="kx ky iq ka b kb mq kf mr kj os kn ot kr ou kv ns ld le lf bi translated">模型努力探测更小的物体</li><li id="80a8" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated">模型努力检测附近的对象(如果它们也重叠)。</li><li id="06f7" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated">小边框的小误差对iou的影响更大。</li></ul></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="7115" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">5.结果</h1><h2 id="501f" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">【2007年5.1 VOC</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a11d3faa92128b76a506e1d77477b00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*co7cM9pdBfwyh7zO4BZtEA.png"/></div></figure><ul class=""><li id="a712" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv ns ld le lf bi translated"><strong class="ka ir"> YOLO </strong>在实时检测上表现非常出色。当我们考虑速度和mAp之间的权衡时，它优于快速RCNN。</li><li id="c6f7" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><strong class="ka ir">快速YOLO </strong>在52.7%贴图和155 fps的情况下也表现不错</li><li id="dcbb" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><strong class="ka ir"> YOLO VGG16 </strong>在mAp方面比YOLO表现更好，但速度较慢，因为没有1x1卷积来减小模型大小。</li></ul><h2 id="bc92" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">5.2比较快速RCNN和YOLO</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/1baf7f6e6926d9afcd2601b7495a0978.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*VzY17XBCA9LHo5xhdzNnRg.png"/></div></figure><p id="0ae7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">YOLO犯的背景错误比Fasr RCNN少。但是与快速RCNN相比，YOLO很难定位物体。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/51b676dbb21b401817030210e4936238.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*FxsDRbtWKJygIPcVWCrsRA.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">error rates</figcaption></figure><h2 id="437a" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">5.3 VOC 2012</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ox"><img src="../Images/efe2c06cb6e5f15e5eaac1165cf4f705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDCMuZ3yBGh_V2g1soC1vg.png"/></div></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">YOLO + Fast RCNN</figcaption></figure><p id="8596" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在VOC 2012测试集上，YOLO的mAP得分为57.9%。这低于当前的技术水平。但快速R-CNN + YOLO具有<strong class="ka ir"> 70.7%的mAP </strong>，这是性能最高的检测方法之一。</p><h2 id="0227" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">5.4模型精度与召回率</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/354ae753ebc5826cdbf688304d073103.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*BOsaeXl61SL2dsfr3ho1AA.png"/></div><figcaption class="nx ny gj gh gi nz oa bd b be z dk">Picasso Dataset precision-recall curves</figcaption></figure><p id="a6e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当绘制精确召回曲线时，我们可以看到YOLO执行其他对象检测。这是基于Picasso数据集的检测。</p><h2 id="2dd2" class="oc lt iq bd lu od oe dn ly of og dp mc kj oh oi mg kn oj ok mk kr ol om mo on bi translated">5.5一些可视化</h2><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/717aa70447ae280ca6796e3f68694f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BePVrSAxlTR0KckkB4mjkA.png"/></div></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="6b36" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="0d01" class="kx ky iq ka b kb mq kf mr kj os kn ot kr ou kv ns ld le lf bi translated"><a class="ae kw" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></li><li id="c7b1" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><a class="ae kw" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89" rel="noopener" target="_blank">https://towards data science . com/yolov 1-you-only-look-once-object-detection-E1 F3 ffec 8 a 89</a></li><li id="1c6e" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv ns ld le lf bi translated"><a class="ae kw" href="https://medium.com/@amrokamal_47691/yolo-yolov2-and-yolov3-all-you-want-to-know-7e3e92dc4899" rel="noopener">https://medium . com/@ amrokamal _ 47691/yolo-yolov 2-and-yolov 3-all-you-want-to-know-7e 3 e 92 DC 4899</a></li></ul><figure class="no np nq nr gt jr"><div class="bz fp l di"><div class="pa pb l"/></div></figure></div></div>    
</body>
</html>