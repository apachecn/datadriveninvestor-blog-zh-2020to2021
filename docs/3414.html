<html>
<head>
<title>BANDIT algorithm — Implemented from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BANDIT算法—从头开始实施</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/bandit-algorithm-implemented-from-scratch-38542cedafb3?source=collection_archive---------3-----------------------#2020-06-17">https://medium.datadriveninvestor.com/bandit-algorithm-implemented-from-scratch-38542cedafb3?source=collection_archive---------3-----------------------#2020-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e9d7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">窃取最多的奖励👀🏆使用强化学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a7283aafdf98dac47ab7df7e73bd265e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uU8sf_0W8HjcAJiEOoGaxg.jpeg"/></div></div></figure><p id="0d53" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这篇文章中，我回顾了强化学习和BANDIT算法的基础知识，并解释了代码实现。如果您想直接跳到代码实现，请跳过第一部分。</p><h1 id="825f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">背景</h1><p id="cc46" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">想象一下，你在赌场玩游戏，你有多台老虎机，你可以对冲你的赌注，比如说10台机器。</p><p id="ea67" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你如何知道从哪些机器开始？如果你的目标是回报最大化，你怎么知道选择哪台机器呢？</p><p id="3056" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就是强化学习中常说的多臂土匪问题。</p><p id="335a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习本身通常是关于拥有一个<strong class="kt ir">代理</strong>，与它的<strong class="kt ir">环境</strong>互动。通过与环境互动，它接收反馈，然后可以在下一个时间步骤中使用这些反馈来更好地做出决策。</p><p id="5408" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在BANDIT问题中，每台吃角子老虎机都有一个具有特定方差(标准偏差)的平均奖励，因此每次奖励都不会相同，但算法应该知道哪些机器平均比其他机器好，它可以使用这个平均值作为从特定吃角子老虎机获得的未来奖励的指标。</p><div class="ml mm gp gr mn mo"><a href="https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">AI和机器学习有什么区别？数据驱动的投资者</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">这两个主题背后有很多令人兴奋的东西，所以这是一个快速指南，介绍了它们是什么以及它们有什么…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc kp mo"/></div></div></a></div><p id="20de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，另一个问题出现了——假设强盗一次又一次地坚持同一个老虎机，只是因为它知道老虎机在迄今为止探索的所有老虎机中给出了最高的价值。如果有另一个吃角子老虎机实际上给出了更高的奖励，但算法没有研究它，因为它认为坚持使用它已经熟悉的吃角子老虎机会产生更高的奖励？</p><p id="674d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就是所谓的<strong class="kt ir">探索与利用</strong>问题——换句话说，是坚持熟悉的行为好，还是抱着更高回报的希望去探索新的行为好？</p><p id="8178" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用土匪采取的行动的数值以及估计的回报和参数来管理探索和开发之间的权衡，我们实际上可以在代码中模拟土匪问题。</p><h1 id="411a" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">代码实现</h1><p id="260d" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">让我们介绍一些术语、数学符号和代码，使所有这些更具体一些。</p><p id="051a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln"> Qt(a) </em>表示在时间步长<em class="ln"> t </em>给定行动的预测奖励，而q*(a)表示行动a的实际平均奖励。Rt表示在时间步长t的奖励</p><p id="e5aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们的目标是获得精确测量Qt(a)的最佳方法。这是通过采取行动时获得的所有先前奖励的平均值来完成的。为了优化内存，我们实际上是通过对时间步长t-1中的动作进行估计来计算时间步长t的平均值，然后将估计值和实际奖励之间的差值相加，再除以已经通过的试验次数，这与计算平均值基本相同。</p><p id="19a7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">值epsilon决定了模型探索的时间比例(它追求随机动作)和它利用的时间量(它选择具有最高估计回报的动作)。</p><p id="b21f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们用代码来看看这个。</p><p id="2b50" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是我为一个强盗从头开始构建的一些代码。请在这里随意查看我的<a class="ae nd" href="https://github.com/MukundhMurthy/BartoSutton-RL-Resources" rel="noopener ugc nofollow" target="_blank">公共github库</a>，其中包含一个Jupiter笔记本，您可以在其中试验不同的参数并可视化时间步长——奖励图。</p><p id="205c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请随意浏览这段代码，并尝试应用我们到目前为止在本文中讨论的内容——但是如果一切都没有意义，也不要担心。我会一次过几行。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="89ad" class="nj lp iq nf b gy nk nl l nm nn">def bandit(stationary, num_steps, num_runs, k=10, epsilon=0.05, Q_init_value=0, std_dev = 0.01, act_val_method = 'Sample-average', alpha = 0.1, UCB = False, c = 2):<br/>    reward_runs_list = []<br/>    optimal_runs_list = []<br/>    for j in range(num_runs):<br/>        Q_a = empty_tensor.new_full([k], Q_init_value)<br/>        rewards = torch.normal(torch.zeros(k), torch.ones(k))<br/>        N_a = torch.zeros(k)<br/>        reward_val = []<br/>        opt_val = []<br/>        max_action_val = torch.argmax(rewards).item()<br/>        assert act_val_method in ['Sample-average', 'Constant-step-size', None]<br/>        print ("Run: {0} Reward List: {1}".format(j, rewards))<br/>        for i in range(num_steps):<br/>            if not stationary:<br/>                rewards += torch.normal(torch.zeros(k), empty_tensor.new_full([k], std_dev))<br/>            if UCB:<br/>                quotient = torch.sqrt(torch.from_numpy(np.array((np.log(j+1))/((N_a+1).numpy()))))<br/>                max_action = torch.argmax(Q_a + (c * quotient))<br/>                non_changed = (Q_a==Q_init_value).nonzero()<br/>                if len(non_changed)&gt;1:<br/>                    max_action = np.random.choice((Q_a==Q_init_value).nonzero().squeeze().numpy())<br/>            elif decision(epsilon)==False:<br/>                max_action = torch.argmax(Q_a).item()<br/>                non_changed = (Q_a==Q_init_value).nonzero()<br/>                if len(non_changed)&gt;1:<br/>                    max_action = np.random.choice((Q_a==Q_init_value).nonzero().squeeze().numpy())<br/>            else:<br/>                max_action = np.random.randint(0, k)<br/>            reward = torch.normal(rewards[max_action], 1)<br/>            times_seen = N_a[max_action] <br/>            estimate = Q_a[max_action]<br/>            N_a[max_action] += 1<br/>            if (act_val_method == 'Constant-step-size'):<br/>                step_param = alpha<br/>            else:<br/>                step_param = (1/(times_seen+1))<br/>            Q_a[max_action] += (reward - estimate)*(step_param)<br/>            reward_val.append(reward.item())<br/>            opt_val.append(max_action == max_action_val)<br/>            print ("Step {0}: Action chosen:{1}   Reward:{2:.2f}    Estimate: {3:.2f}".format(i, max_action, reward.item(), estimate))<br/>        reward_runs_list.append(reward_val)<br/>        optimal_runs_list.append(opt_val)<br/>    reward_runs_array = np.array(reward_runs_list)<br/>    avg_reward_per_step = np.mean(reward_runs_array, axis=0)<br/>    opt_arr = np.array(optimal_runs_list, dtype="bool")<br/>    percent_opt_arr = np.sum(opt_arr, axis=0)/num_runs<br/>    return (avg_reward_per_step, percent_opt_arr)</span></pre><p id="c217" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先，让我们看一下函数头，对所有涉及的参数有一个清楚的了解。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="89e5" class="nj lp iq nf b gy nk nl l nm nn">def bandit(stationary, num_steps, num_runs, k=10, epsilon=0.05, Q_init_value=0, std_dev = 0.01, act_val_method = 'Sample-average', alpha = 0.1, UCB = False, c = 2):</span></pre><p id="7ff7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">涉及的参数有</p><ul class=""><li id="a18c" class="no np iq kt b ku kv kx ky la nq le nr li ns lm nt nu nv nw bi translated"><em class="ln">固定</em> —这个参数是一个布尔值，它决定了奖励是不变的，还是在强盗搜索时会改变。你可能会想象不稳定的值会使强盗更难优化奖励。</li><li id="33b7" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated"><em class="ln"> k — </em>匪徒拥有的武器数量(本质上是可能行动的数量)</li><li id="20b1" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated"><em class="ln"> num_steps </em> —强盗选择一个动作的次数</li><li id="bf05" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated"><em class="ln"> num_runs </em> —初始化并运行了<em class="ln"> num_steps </em>的不同k-armed土匪的数量</li><li id="fac8" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated"><em class="ln"> epsilon </em> —决定强盗探索的小部分时间的参数</li><li id="5db2" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated"><em class="ln"> Q_init_value </em> —每个动作的奖励的初始化估计值。将初始值设置得更高将导致更多的探索。在第一次对每个点进行采样后，估计值会急剧下降——你可以认为这是强盗失望了。它会在选择几个点之前，对几乎所有的点进行采样。</li></ul><p id="7a7a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果回报是不稳定的</p><ul class=""><li id="4d97" class="no np iq kt b ku kv kx ky la nq le nr li ns lm nt nu nv nw bi translated"><em class="ln"> std_dev </em> —奖励在时间步长之间变化的数量</li></ul><p id="862a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Act-val方法—这些方法在收到给定行为的奖励后提供评估更新。有两种主要方法——一种是样本平均法，另一种涉及恒定步长α。</p><ul class=""><li id="e1c2" class="no np iq kt b ku kv kx ky la nq le nr li ns lm nt nu nv nw bi translated">样本平均值——将奖励相加，然后除以强盗看到它们的次数(用N_a表示)</li><li id="03df" class="no np iq kt b ku nx kx ny la nz le oa li ob lm nt nu nv nw bi translated">恒定步长——不是除以N_a，而是将<em class="ln">(估计-回报)</em>值乘以恒定步长α。这允许较早的奖励比最近的奖励少计算(在不稳定的环境中特别有用)</li></ul><h1 id="8587" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">一个示例实验</h1><p id="d9b8" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">这里有一个示例实验，您可以使用BANDIT函数来进行——您可以看到初始Q值对最终奖励可视化的影响。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/5df340c379a850f17c37325fbce33e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*91Nr_whdSLIkxpKrc-B8ww.png"/></div></div></figure><p id="cba0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这里，我测试了两个不同Q值的平均回报图和最优选择图之间的差异。正如您所看到的，Q=0的曲线变得更快，但达到了更低的最佳选择百分比最大值(Q=0时为70%，Q=5时为90%)。</p><p id="9121" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从概念上讲，这是因为较大的Q允许更多的探索。</p><p id="3e46" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请随意用这个工具做自己的实验，获得强化学习基础的直觉！</p></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="72b0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln">嘿！我是Mukundh Murthy，16岁，对机器学习和药物发现的交叉领域充满热情。感谢阅读这篇文章！希望你觉得有帮助:)</em></p><p id="bbd9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln">随时查看我在Medium上的</em> <a class="ae nd" href="https://medium.com/@mukundh.murthy" rel="noopener"> <em class="ln">其他文章</em> </a> <em class="ln">和我在</em><a class="ae nd" href="https://www.linkedin.com/in/mukundhmurthy/" rel="noopener ugc nofollow" target="_blank"><em class="ln">LinkedIn</em></a><em class="ln">上联系！</em></p><p id="c718" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你想讨论以上任何话题，我很乐意与你联系！实际上，我目前正在尝试优化ML模型，以预测冠状病毒蛋白质靶点(N蛋白)的高效适体药物。(在mukundh.murthy@icloud.com给我发邮件或者在<a class="ae nd" href="https://www.linkedin.com/in/mukundhmurthy/" rel="noopener ugc nofollow" target="_blank"><em class="ln">LinkedIn</em></a><em class="ln">上给我发信息)另外，请随时查看我的网站，网址是</em><a class="ae nd" href="http://mukundhmurthy.com/" rel="noopener ugc nofollow" target="_blank"><em class="ln">【mukundhmurthy.com</em></a><em class="ln">。</em></p><p id="335c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln">如果你有兴趣关注我的进展，请在这里</em>  <em class="ln">注册我的每月简讯</em> <a class="ae nd" href="http://eepurl.com/gImYNX" rel="noopener ugc nofollow" target="_blank"> <em class="ln"/></a></p><p id="29f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">访问专家视图— </strong> <a class="ae nd" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>