<html>
<head>
<title>Recurrent Neural Networks in Deep Learning — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的递归神经网络—第1部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/recurrent-neural-networks-in-deep-learning-part-1-df3c8c9198ba?source=collection_archive---------4-----------------------#2020-03-26">https://medium.datadriveninvestor.com/recurrent-neural-networks-in-deep-learning-part-1-df3c8c9198ba?source=collection_archive---------4-----------------------#2020-03-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2fa80f372d9e9f56a464b13cb420c9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlcUA3qNUs_jUObUJG0ZVQ.jpeg"/></div></div></figure><p id="153e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">作者普里亚尔·瓦尔皮塔</strong></p><p id="8a70" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">阅读这篇文章将有助于你理解人工神经网络(ANN)的术语，ANN的缺点，RNN(递归神经网络)的架构观点，使用RNN优于ANN的优点，它们如何工作，以及如何构建一个系列模型和解决各种用例。有意地，我保持这篇文章基于理论和他们的解释，主要集中在递归神经网络(RNN)。</p><p id="4500" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇博文由两部分组成，这是第一部分。本节介绍RNN和<a class="ae kw" href="https://medium.com/@priyalwalpita/recurrent-neural-networks-in-deep-learning-part2-ce9fe1770a31" rel="noopener">，第二节将讨论RNN的类型和RNN </a>的一些实际用法。</p><p id="4429" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt" rel="noopener ugc nofollow" target="_blank">注:本文基于吴恩达博士在Coursera </a>的演讲</p><h1 id="f8f6" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">介绍</h1><h2 id="dd2b" class="lv ky iq bd kz lw lx dn ld ly lz dp lh kj ma mb ll kn mc md lp kr me mf lt mg bi translated">什么是人工神经网络(ANN)？</h2><p id="f140" class="pw-post-body-paragraph jy jz iq ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">在每一层，都有一个由若干感知器/神经元组成的群体。通常被称为前馈神经网络或ANN，因为输入只在正向上被处理。您还可以看到，ANN(或深度神经网络)由3层组成——输入、隐藏(一层或多层)和输出。输入层接受输入，隐藏层处理输入，结果由输出层生成。基本上每一层都试图保持一些权重。但是遇到安也有缺点。</p><div class="mm mn gp gr mo mp"><a href="https://www.datadriveninvestor.com/2019/02/08/machine-learning-in-finance/" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd ir gy z fp mu fr fs mv fu fw ip bi translated">金融中的机器学习|数据驱动的投资者</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">在我们讲述一些机器学习金融应用之前，我们先来了解一下什么是机器学习。机器…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd jw mp"/></div></div></a></div><h2 id="2df3" class="lv ky iq bd kz lw lx dn ld ly lz dp lh kj ma mb ll kn mc md lp kr me mf lt mg bi translated">人工神经网络(ANN)的缺点</h2><ul class=""><li id="c660" class="ne nf iq ka b kb mh kf mi kj ng kn nh kr ni kv nj nk nl nm bi translated">人工神经网络不能捕捉输入数据中的序列信息，而这些信息是处理序列数据所必需的。例如，如果两个数据元素相互关联(例如:语音识别、文本生成、文本或语音语义识别等)，我们就不能分别对待每个数据元素。</li></ul><p id="d10c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，为了克服这个缺点，我们将使用递归神经网络(RNN)。</p><p id="c54b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">让我们首先来解释一下RNN和人工神经网络之间的架构观点差异:</strong></p><p id="01ec" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">“ANN的秘密层把循环约束变成RNN”</strong></p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/f428aa0f54c90ed027ace11befcccb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4SZVPDaX-PU0AWml"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure : Main difference between ANN and RNN</figcaption></figure><p id="9dc9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RNN在隐藏态上有一个循环关系，正如你从上图中看到的。这种循环约束确保捕获输入数据中的顺序信息。</p><h2 id="00e9" class="lv ky iq bd kz lw lx dn ld ly lz dp lh kj ma mb ll kn mc md lp kr me mf lt mg bi translated">什么是递归神经网络(RNN)？</h2><p id="d501" class="pw-post-body-paragraph jy jz iq ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">在这种神经网络中，前一步的输出作为输入馈入当前步骤。传统的神经网络发生在循环神经网络发展之前，意味着所有的输入和输出都是彼此独立的，但是在诸如需要预测句子的下一个单词的情况下，需要前面的单词，因此记住前面的单词是重要的。因此，只有RNN出现后，这解决了这个问题的帮助下<strong class="ka ir">隐藏层</strong>。RNN的关键和最重要的特征是隐藏状态，它记住序列的一些细节。</p><p id="86a7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.datadriveninvestor.com/2019/02/08/machine-learning-in-finance/" rel="noopener ugc nofollow" target="_blank">https://www . datadriveninvestor . com/2019/02/08/machine-learning-in-finance/</a></p><p id="0f30" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RNN有一种“记忆”,在这种记忆中，所有关于被测量的知识都被回忆起来。为了生成输出，它对每个层使用相同的参数，因为它对所有输入或隐藏层执行相同的功能。<strong class="ka ir">与其他神经网络不同，这降低了参数的复杂性。</strong></p><h2 id="7d38" class="lv ky iq bd kz lw lx dn ld ly lz dp lh kj ma mb ll kn mc md lp kr me mf lt mg bi translated">递归神经网络(RNN)的优势</h2><p id="0d95" class="pw-post-body-paragraph jy jz iq ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">RNN捕捉输入数据中的顺序信息，即预测以下内容时文本中单词之间的联系:</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/49e5409de86a3e2e3f02b326859b83de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/0*tR63bIopzs4XlH1i"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure : RNN flow</figcaption></figure><p id="fafc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如您所见，输出(o1，o2，o3，o4)不仅取决于当前字，还取决于时间步长中的前一个字。</p><h2 id="0926" class="lv ky iq bd kz lw lx dn ld ly lz dp lh kj ma mb ll kn mc md lp kr me mf lt mg bi translated">递归神经网络(RNN)如何工作？</h2><p id="1c14" class="pw-post-body-paragraph jy jz iq ka b kb mh kd ke kf mi kh ki kj mj kl km kn mk kp kq kr ml kt ku kv ij bi translated">让我们举个例子来理解这种方法:</p><p id="71f6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在传统的神经网络中有一个隐藏层，它有自己的权重和偏差集合。分别对于权重和偏差1，假设权重和偏差是w1和b1。类似地，对于第三层，我们将有w2、b2和w3、b3。这些层也是相互分离的，这意味着先前的输出不会被记忆。然而，假设存在一个更深的网络，具有一层输入、三个隐藏层和一层输出。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/85659897340c4ecbcad8e9036586f098.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/0*r2pspWP0rbqR67xo"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure : RNN weights</figcaption></figure><p id="690e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">递归神经网络的工作方式如下:</p><p id="4411" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">1)首先，RNN将把独立激活转换成从属激活。它还向所有层分配相同的权重和偏置，这进一步降低了RNN参数的复杂性，并通过向下一层提供输入来提供用于记忆先前输出的一致框架。</p><p id="07e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2)这三层相同的权重和偏差合并成一个单一的循环结构。</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b69705d282e1cee8b7b1716bdf48553b.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/0*RZTv9fb4jhn6KBhb"/></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure : RNN state</figcaption></figure><p id="ba43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">计算当前状态的公式</strong></p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/90cc2b52e3657d2f7e4600d8e767114a.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/0*3CtWDcg_wBPuaVwM"/></div></figure><p id="4aa6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中:</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8ba80a8a224efb1697b390aab869853c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*YS2cUuNRrIWlC9mR0Xo90w.png"/></div></figure><p id="44fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">应用激活函数(tanh)的公式:</strong></p><figure class="no np nq nr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/3a1fd077474eaebe1fe418aa4025bd98.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/0*PLRzTgADIpCe1odU"/></div></div></figure><p id="bf49" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其中:</p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/84f4a2c068d88997fed8cbe7f11a40f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*0AInjeF6SXUXZzivJO6MJw.png"/></div></figure><p id="279b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">产量计算公式:</strong></p><figure class="no np nq nr gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/54b9d5e3d1a5a2ee68684a6c297dd5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/0*3Rh_AVF-VAt9Nl10"/></div></figure><figure class="no np nq nr gt jr"><div class="bz fp l di"><div class="od oe l"/></div></figure></div></div>    
</body>
</html>