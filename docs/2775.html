<html>
<head>
<title>Understanding Adaboost and Scikit-learn’s algorithm:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解Adaboost和Scikit-learn的算法:</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/understanding-adaboost-and-scikit-learns-algorithm-c8d8af5ace10?source=collection_archive---------0-----------------------#2020-05-15">https://medium.datadriveninvestor.com/understanding-adaboost-and-scikit-learns-algorithm-c8d8af5ace10?source=collection_archive---------0-----------------------#2020-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/361987cde82c56d7446251320152bd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*D4GZQuEiSCFOR3Zm.png"/></div></figure><p id="61ac" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">集成模型，尤其是助推算法，在Kaggle等在线数据科学竞赛论坛上变得非常流行。集成学习是使用一组模型进行预测的方法。今天我们将讨论一种叫做AdaBoost的集成提升算法。如果你不熟悉系综的意思，你可以参考我以前写的关于<a class="ae ks" href="https://medium.com/analytics-vidhya/ensemble-learning-methods-in-machine-learning-5d2f849192f8" rel="noopener"> <strong class="jw ir">系综模型</strong> </a>的文章，在那里我将简要介绍系综的思想和系综模型使用的4种主要技术。本文还假设您熟悉决策树，因为我们将在决策树中使用Adaboost。</p><div class="kt ku gp gr kv kw"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab fo"><div class="ky ab kz cl cj la"><h2 class="bd ir gy z fp lb fr fs lc fu fw ip bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="ld l"><h3 class="bd b gy z fp lb fr fs lc fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="le l"><p class="bd b dl z fp lb fr fs lc fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk js kw"/></div></div></a></div><p id="254d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Adaboost代表<strong class="jw ir">自适应增强。</strong>模型在集合中按顺序排列。<strong class="jw ir"> </strong>这个<strong class="jw ir"> </strong>意味着在每一步，我们都试图在我们之前模型的错误的基础上提升我们的<strong class="jw ir">弱学习者</strong>(基础模型)，因此它们合在一起就是一个<strong class="jw ir">强集成模型。</strong>每个模型都有他们擅长的问题的一部分，所以它们一起涵盖了问题的所有部分。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ll"><img src="../Images/3ffd0254bc8d91b416a5056a4adab0e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X-JEOxMIRduk-Q96.jpg"/></div></div></figure><p id="95d2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">假设您有一个数据集，您想使用AdaBoost根据天气预测一个人是否会在户外玩耍。这是Adaboost如何完成工作的概述:</p><h2 id="f056" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">步骤1:为数据集中的所有样本分配相等的权重</h2><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mn"><img src="../Images/1d674cbde4a1ea2d98f808c3a28804dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5LpVwmYyuM8QtcoV"/></div></div></figure><p id="c174" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果您注意到重量，我们的数据集中有8个样本，它们被分配了相同的重量<strong class="jw ir">1/样本数。</strong>这意味着对以往样本的正确分类与<strong class="jw ir">同等重要</strong>。</p><h2 id="5a9c" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">步骤2:使用基尼系数最低的特征创建一个决策树桩</h2><p id="7663" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">决策树树桩只是一棵有<strong class="jw ir">根和两片叶子的决策树。</strong>有1级的树。<strong class="jw ir"> </strong>这个树桩就是我们的<strong class="jw ir">弱学习者</strong>(基模)<strong class="jw ir">。</strong>首先对我们的数据进行分类的特征是使用基尼指数确定的。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e6ff0c06e332f11422546489abdbf65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/0*SmOwWS9S4lH-Xnhj"/></div></figure><p id="d24b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你愿意，你可以把你的等级提高到二级，但是去找树桩是很常见的。</p><h2 id="c620" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">第三步:评估你的残肢的性能并分配它的重量</h2><p id="deab" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">在Adaboost中，我们有一个树桩集合，在决定最终预测之前，会考虑它们的所有预测。但是一些树桩在分类我们的数据方面比我们集合中的其他树桩做得更好。对这些树桩给予更多的重视是有意义的。Adaboost通过为集合中的每个树桩分配权重来实现这一点。权重越高，树桩在最终预测中的发言权就越多<strong class="jw ir">。因此，假设我们的残肢在我们的数据中错误地分类了两个样本，则该残肢的权重计算如下</strong></p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mu"><img src="../Images/fef78e4e5dfd9d45a1b73ab9ca71ffd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A7aKUP5H2m97dvXV"/></div></div></figure><p id="cda2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<strong class="jw ir"> <em class="mv">总误差=错误分类样本的权重之和。</em> </strong>因此，如果我们的树桩得到两个错误分类的样本，使用这些样本的权重，我们得到0.5 * log(1—(1/8+1/8)/(1/8+1/8))= 0.54(使用自然对数)。</p><p id="a65c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这是我们第一个模特的体重。</p><blockquote class="mw"><p id="ae3c" class="mx my iq bd mz na nb nc nd ne nf kr dk translated">请记住，这与样品的重量不同。样本权重强调对样本进行正确分类的重要性，而模型权重用于确定模型在最终预测中获得的数量。</p></blockquote><h2 id="d545" class="lu lv iq bd lw lx ng dn lz ma nh dp mc kf ni me mf kj nj mh mi kn nk mk ml mm bi translated">步骤4:重新分配样品的重量</h2><p id="e912" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">还记得我们之前为所有样品设定了相同的重量吗？在使用我们的第一个模型分类后，我们得到了两个错误分类的样本。然后，Adaboost试图通过给这两个样本分配更高的样本权重来强调下次正确获取这两个样本的重要性。这将有助于下一个模型更加关注这两个样本的正确性。错误分类样品的新重量公式为:</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/135017f5e5ccc2c6287b0015bce0e7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*s4EBtjCDcRuVqg7eOXCcQQ.png"/></div></figure><p id="0eda" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">所以两个样本的权重都是1/8(因为这是我们第一次改变)，他们的新权重是1/8*e^amount，比如说= 0.21。这比最初的1/8要大。</p><p id="fd85" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">接下来，它减少了其他样品的重量。这是通过使用以下公式完成的:</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nm"><img src="../Images/151a6a608373b410b55a3aaab26895be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wkoccC5YlLANvKm8"/></div></div></figure><h2 id="8cdc" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">步骤5:标准化样本的权重</h2><p id="c1d4" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">现在我们有了新的重量，将重量加在一起<strong class="jw ir">用重量之和除以每个重量。</strong>这将使新权重的总和等于1。<strong class="jw ir"> </strong>归一化<strong class="jw ir">、</strong><strong class="jw ir"/>后新的权重会是什么样子。请注意，两个错误样本的权重增加了。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi nn"><img src="../Images/3bb7781d72a028623ff7188b7d5e81a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTbIhekeO3HBO9pyNJj4Sg.png"/></div></div></figure><h2 id="1b96" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">步骤6:创建一个与原始数据集大小相同的新数据集，并根据权重选取样本</h2><p id="cecd" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">这一步是关键的，因为这是下一个模型学习对前一个模型出错的样本进行分类的重要性的方法。</p><ul class=""><li id="30a9" class="no np iq jw b jx jy kb kc kf nq kj nr kn ns kr nt nu nv nw bi translated">首先，创建一个与原始数据集大小相同的空数据集。</li><li id="79f1" class="no np iq jw b jx nx kb ny kf nz kj oa kn ob kr nt nu nv nw bi translated">那么样本权重就像一个分布，像这样:</li></ul><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9cc3132fb12ca63714e9711e973b6d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*a4TjINUQPQD93E_B"/></div></figure><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a2f97bf13a48c1428afa6b8745feeabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*-_C4_SCsJBYPMK0Q"/></div></figure><ul class=""><li id="06ce" class="no np iq jw b jx jy kb kc kf nq kj nr kn ns kr nt nu nv nw bi translated">从0到1中选取一个随机数，并选择在其极限内具有该值的样本。例如，如果选择0.3，则选取第三个样本，因为0.3的值在0.167和0.416之间。将这个新样本复制到新数据集中。</li><li id="db02" class="no np iq jw b jx nx kb ny kf nz kj oa kn ob kr nt nu nv nw bi translated">重复上面的三个步骤，直到新的数据集被填充。填充新数据集后，<strong class="jw ir"> <em class="mv">将样本权重重新分配为1/样本数的相等值。</em> </strong></li></ul><p id="e9ce" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这是新数据集的样子。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi od"><img src="../Images/09bdc52dea909143fb08a379fd4c4960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3dXi75au8aDeLuEg0gaUg.png"/></div></div></figure><p id="9fd1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> <em class="mv">注意到我们之前弄错的样本是如何被更多地包含进来的吗？这将会给下一个模型一个更好的机会把它做好。有点像为错误分类创造一个大的惩罚。</em>T12】</strong></p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/cced104562fb299984c37df776b67acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*9rHyJ3NbFshX71OhCl3dYQ.gif"/></div></figure><h2 id="4380" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">第六步:重复第二步到第五步，直到我们有足够数量的模型</h2><h2 id="3d07" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">步骤7:通过多数票分配最终预测</h2><p id="a76b" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">终于！我们已经建立了足够的模型来决定预测的最终值。还记得我们是如何给每个模型分配权重的吗？我们现在要用它了。</p><ul class=""><li id="5af8" class="no np iq jw b jx jy kb kc kf nq kj nr kn ns kr nt nu nv nw bi translated">根据模型的分类拆分模型。将样本分类为“是”的模型和将同一样本分类为“否”的模型</li><li id="e111" class="no np iq jw b jx nx kb ny kf nz kj oa kn ob kr nt nu nv nw bi translated">现在一些被归类为“是”的模型和被归类为“否”的模型的权重</li><li id="190f" class="no np iq jw b jx nx kb ny kf nz kj oa kn ob kr nt nu nv nw bi translated">特定样本的最终预测Adaboost将是具有最高权重的模型的预测。因此，如果归类为“是”的模型的权重之和大于归类为“否”的模型的权重之和，则最终预测为“是”，反之亦然。</li></ul><p id="6ad9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这就对了。现在你已经知道AdaBoost是如何工作的了。现在我们来看一个代码解释，了解scikit-learn针对Adaboost的两个变异算法。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="96d4" class="om lv iq bd lw on oo op lz oq or os mc ot ou ov mf ow ox oy mi oz pa pb ml pc bi translated">Scikit-learn的SAMME和SAMME。r实现Adaboost的算法:</h1><p id="1cdd" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">让我们看看scikit-learns Adaboost分类器:</p><pre class="lm ln lo lp gt pd pe pf pg aw ph bi"><span id="24f6" class="lu lv iq pe b gy pi pj l pk pl"><strong class="pe ir"><em class="mv">class </em></strong><strong class="pe ir">sklearn.ensemble</strong>.<strong class="pe ir">AdaBoostClassifier</strong>(<strong class="pe ir"><em class="mv">base_estimator</em></strong><em class="mv">=None</em>, <em class="mv">*</em>, <strong class="pe ir"><em class="mv">n_estimators</em></strong><em class="mv">=50</em>, <strong class="pe ir"><em class="mv">learning_rate</em></strong><em class="mv">=1.0</em>, <strong class="pe ir"><em class="mv">algorithm</em></strong><em class="mv">='SAMME.R'</em>, <strong class="pe ir"><em class="mv">random_state</em></strong><em class="mv">=None</em>)</span></pre><p id="acaf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">base_estimator是你的基础模型或者弱学习者。在这里，因为我们要使用决策树的树桩，我将通过</p><p id="2a59" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">base _ estimator = decision tree classifier(max _ depth = 1)</strong></p><p id="ac17" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">而<code class="fe pm pn po pe b">n_estimators</code>是你的服装需要的模特数量。学习率将每个分类器的贡献缩小了<code class="fe pm pn po pe b">learning_rate</code>。在<code class="fe pm pn po pe b">learning_rate</code>和<code class="fe pm pn po pe b">n_estimators</code>之间有一个权衡。如果你看scikit-learn的AdaBoost文档，它给出了算法的两个选项:</p><blockquote class="pp pq pr"><p id="5002" class="ju jv mv jw b jx jy jz ka kb kc kd ke ps kg kh ki pt kk kl km pu ko kp kq kr ij bi translated"><strong class="jw ir">算法</strong> <em class="iq"> {'SAMME '，' SAMME。R'}，默认='SAMME。R' </em></p><p id="4773" class="ju jv mv jw b jx jy jz ka kb kc kd ke ps kg kh ki pt kk kl km pu ko kp kq kr ij bi translated">如果是萨姆。那就用SAMME。r实升压算法。<code class="fe pm pn po pe b">base_estimator</code>必须支持类别概率的计算。如果是“SAMME ”,则使用SAMME离散增强算法。萨姆号。r算法通常比SAMME收敛得更快，以更少的提升迭代实现更低的测试误差。</p></blockquote><p id="268e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">萨姆和萨姆。r算法是多类Adaboost函数，由朱记、萨哈伦·罗塞特、邹慧、特雷弗·哈斯蒂在一篇论文中提出。这些算法是Ababoost的主要思想的改编，用多类能力扩展它们的功能。</p><h2 id="fff4" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">SAMME:</h2><p id="0416" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">这是SAMME算法的伪代码(分阶段加法建模，因为我们更新了样本的权重)，其中wᵢ是样本权重，α是模型权重。这类似于我们谈论的方法，除了一个关键的区别。看公式估算模型权重α</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/4f1d0a2eab0114af7ddb27723cf44dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/0*xwLqM7BAhD5xFADh.png"/></div></figure><p id="e514" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这使用模型权重的加权公式。其中(K -1)中的“K”代表我们的多类问题中的类的数量，err是总误差。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/b26773eff45a29d4b8858c17c2fb2ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/0*zaSixzfj5bB35k7Z"/></div></figure><blockquote class="pp pq pr"><p id="a4c6" class="ju jv mv jw b jx jy jz ka kb kc kd ke ps kg kh ki pt kk kl km pu ko kp kq kr ij bi translated">注意:当K = 2时，SAMME算法的工作方式与我们之前看到的完全一样。</p></blockquote><h2 id="b82a" class="lu lv iq bd lw lx ly dn lz ma mb dp mc kf md me mf kj mg mh mi kn mj mk ml mm bi translated">萨姆。稀有</h2><p id="dc7c" class="pw-post-body-paragraph ju jv iq jw b jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn ms kp kq kr ij bi translated">现在来看SAMME.R的伪代码，这是SAMME算法的变种。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi px"><img src="../Images/f728491885ba4499dc77dd817ab704f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*fBaKn09BP0G8SKxP.png"/></div></figure><p id="0102" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">请注意，模型权重α不在伪代码中。这是因为SAMME。r算法给所有的模型一个相等的权重。这是如何工作的？这里是SAMME和SAMME的主要区别。r，而SAMME是离散值算法，这意味着它输出0或1。r使用类别概率。这意味着它输出样本属于一个类别的概率。报纸上的描述:</p><blockquote class="mw"><p id="bd89" class="mx my iq bd mz na nb nc nd ne nf kr dk translated">一种替代方法是使用实值置信度预测(如加权概率估计)来更新加法模型，而不是更新分类本身。</p></blockquote><p id="8ab7" class="pw-post-body-paragraph ju jv iq jw b jx py jz ka kb pz kd ke kf qa kh ki kj qb kl km kn qc kp kq kr ij bi translated">因此得名SAMME。其中R代表“实数”,因为它输出一个实数值。假设我们有一个当前估计值f(m1)(x)(前一个模型的估计值)，并通过最小化损失来寻求一个改进的估计值f(m1)(x)+h(x)(加上下一个模型的估计值)。在这里，弱学习者从加权类概率估计中学习，并且新样本根据它们被分类。正如scikit-learn的文档中提到的。r算法通常比SAMME收敛得更快，以更少的提升迭代实现更低的测试误差。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="9729" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于这个代码示例，我使用了<a class="ae ks" href="https://www.kaggle.com/c/forest-cover-type-prediction/data" rel="noopener ugc nofollow" target="_blank">森林树数据</a>。该数据集包含对科罗拉多州罗斯福国家森林四个区域的树木特征的观察。他的数据集包括树木类型、阴影覆盖范围、到附近地标(道路等)的距离、土壤类型和当地地形等信息。我们的工作是根据信息预测树的覆盖类型。我用SAMME和SAMME训练了两个模特。稀有</p><p id="34bd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://gist.github.com/AnahVeronica/da7134ffbb3b7f5109128857fa3cb79a" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/AnahVeronica/da 7134 ffbb 3 b 7 f 5109128857 fa 3c b 79 a</a></p><pre class="lm ln lo lp gt pd pe pf pg aw ph bi"><span id="e96d" class="lu lv iq pe b gy pi pj l pk pl"><strong class="pe ir">Accuracy of SAMME:</strong> <strong class="pe ir">0.618</strong><br/><strong class="pe ir">Accuracy of SAMME.R: 0.778</strong></span></pre><p id="b228" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Scikit-learn的Adaboost有两个属性<strong class="jw ir"> estimator_weights_和estimator_errors_ </strong>，可以用来查看集合中每个模型的误差和权重。如果你在SAMME上调用estimator_weights_的话。你会看到所有的估计值的权重都是1。</p><p id="39dd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">完成了。现在你知道AdaBoost是如何工作的了！太棒了。</p><blockquote class="pp pq pr"><p id="3aab" class="ju jv mv jw b jx jy jz ka kb kc kd ke ps kg kh ki pt kk kl km pu ko kp kq kr ij bi translated">注意:Adaboost也可以用于回归问题。</p></blockquote></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="ab09" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">嗨伙计们！我叫安娜·维罗妮卡。希望你觉得我的文章有用。最近，我决定使用数据科学来探索数学在现实中是如何工作的。每天我都学习新的东西并分享我的发现。如果你想看更多，请跟我来！再见！</p><figure class="lm ln lo lp gt jr"><div class="bz fp l di"><div class="qd qe l"/></div></figure></div></div>    
</body>
</html>