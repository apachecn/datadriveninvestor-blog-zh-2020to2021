# 深度强化学习在时间序列数据压缩中的应用——ICDE 2020 论文

> 原文：<https://medium.datadriveninvestor.com/application-of-deep-reinforcement-learning-in-time-series-data-compression-icde-2020-paper-5af217f7e208?source=collection_archive---------7----------------------->

![](img/3676c2eb3f999e42771f1489822c98e7.png)

*由信阳*

# 对相关人员的认可

在本文的写作过程中，彭延庆、飞刀、、、麦军、岳燮等人贡献良多。在此，我要特别感谢飞刀的指导和帮助。我也想对西丽的支持和德仕的帮助和支持表示感谢。

# 前言

> *“牛骨之间有缝隙，而一把刀子极薄。因此，用刀子切骨头很容易。”—摘自《跑丁街牛》。*

这句中国谚语的意思是，只要你明白做一项工作的规则，你就能做好这项工作。

随着移动互联网、(物联网)、5G 的应用和普及，我们正在步入数字经济时代。由此产生的海量数据将是一种客观存在，并将在未来发挥越来越重要的作用。[时间序列数据](https://www.alibabacloud.com/product/hitsdb)是海量数据的重要组成部分。除了数据挖掘、分析和预测，如何有效地压缩数据进行存储也是一个重要的讨论话题。今天，我们也处于[人工智能](https://www.alibabacloud.com/product/machine-learning) (AI)时代，深度学习已经被广泛应用。如何才能在更多的应用中使用？深度学习的本质是做决策。当我们使用深度学习来解决一个具体的问题时，找到一个突破点并创建一个合适的模型是很重要的。在此基础上，我们可以整理数据，优化损耗，最终解决问题。过去，我们利用深度强化学习在数据压缩方面进行了一些研究和探索，并取得了一些成果。我们在 ICDE 2020 研究轨道发表了一篇题为“时间序列数据库中使用机器学习的两级数据压缩”的论文，并做了口头报告。本文介绍了我们的研究和探索，希望对其他场景有帮助，至少对其他数据的压缩有帮助。

# 1.背景

## 1.1 时间序列数据

顾名思义，时序数据是指与时间序列相关的数据。这是一种常见的数据形式。下图显示了时间序列数据的三个示例:a)心电图，b)股票指数，c)特定的股票交易数据。

![](img/94ec8473f6376e4f82b6264d0f4b4bff.png)

从用户的角度来看，时间序列数据库提供了对海量数据的查询、分析和预测。但是，在底层，时序数据库必须执行大量操作，包括读写、压缩和解压缩以及聚合。这些操作是在时间序列数据的基本单元上执行的。时间序列数据一般用两个 8 字节的值统一描述，也可以简化。

可以想象，电子设备每天都会产生海量的不同类型的时间序列数据，需要巨大的存储空间。在这种情况下，压缩数据以便存储和处理是很自然的事情。那么，重点是如何更有效地压缩数据。

## 1.2 强化学习

根据样本是否具有基础事实，机器学习分为监督学习、非监督学习和强化学习。顾名思义，强化学习需要持续学习，不需要 groundTruth。在现实世界中，groundTruth 也经常不可用。比如人类的认知，大多是一个不断迭代学习的过程。从这个意义上说，强化学习是一种更适合或更受欢迎的处理现实问题的过程和方法。所以，很多人说如果深度学习逐渐像 C、Python、Java 一样成为解决特定问题的基本工具，强化学习将是深度学习的基本工具。

下图显示了强化学习的经典示意图，其中包含状态、动作和环境的基本元素。基本过程是:环境提供一种状态。代理根据状态对动作做出决定。行动在环境中起作用，产生新的状态和奖励。奖励用于指导代理对某项行动做出更好的决策。循环往复。

相比之下，普通的监督学习，可以认为是特殊的强化学习，要简单得多。监督学习有一个明确的目标，地面真理。所以相应的奖励也是一定的。

![](img/d83457b19db4305f21a5240acc780d75.png)

强化学习可以分为以下几种类型:

*   **深 Q 网(DQN):** 这种类型更符合人的直观感受逻辑。它训练一个用于评估 Q 值的网络，并且可以基于任何状态提供对应于行动的奖励。然后，它选择奖励最大的动作。在训练期间，DQN 评估反向传播的“估计 Q 值”和“真实 Q 值”的结果。这样，网络可以更准确地评估 Q 值。
*   **策略梯度:**这种类型更多的是端到端。它训练网络直接输出任何状态的最终动作。DQN 要求连续状态的 Q 值也是连续的，这不适用于像下棋这样的情况。策略梯度忽略内部流程，直接输出动作，更具普适性。然而，政策梯度增加了评估和趋同的难度。一般的训练过程是:策略梯度对一个状态随机采取多个动作，评估所有动作的结果进行反向传播。最后，网络输出一个更好的动作。
*   **影评人:**这种类型结合了前两种类型互补。一方面，它使用策略梯度网络来输出任何状态的动作。另一方面，利用 DQN 对政策梯度输出的行为进行量化评估，并以此结果指导政策梯度的更新。顾名思义，演员-评论家之间的关系类似于演员和评论家之间的关系。在训练期间，行动者(政策梯度)网络和批评家(DQN)网络同时被训练。“演员”的训练只需要遵循“批评家”的指导。演员批评家有许多变体，也是当前 DRL 理论研究的一个主要分支。

# 2.时间序列数据的压缩

毫无疑问，在现实世界中，需要对海量时间序列数据进行压缩。因此，学术界和工业界做了大量的研究，包括:

*   **Snappy:** 压缩整数或字符串。它使用长距离预测、游程编码(RLE ),并且有许多应用，包括 InfuxDB。
*   **Simple8b:** 对数据进行增量处理。如果生成的增量值中的所有增量都相同，则将对它们应用 RLE 编码。如果增量不同，基于包含 16 个条目的代码表，数字 1 到 240(根据代码表的每个数字的比特)被打包成以 8 字节为单位的数据。Simple8b 有很多应用，包括 InfuxDB。
*   **压缩规划器:**介绍一些通用的压缩工具，如 scale、delta、dictionary、huffman、run length、patched constant 等。然后，它提出静态或动态地结合这些工具来压缩数据。这个想法很新颖，但是性能没有得到证明。
*   **ModelarDB:** 专注于基于用户指定的可容忍损失的有损压缩。基本思想是保持一个小缓冲区，并检测数据是否符合某种模式(斜率的直线拟合)。)如果失败，则切换模式并开始新的 buff。ModelarDB 适用于有损物联网领域。
*   **Sprintz:** 也适用于物联网，专注于 8 位或 16 位整数的处理。它使用缩放来执行预测，并使用 RLC 来编码产生的增量并进行比特级打包。
*   **莫:**类似大猩猩，但不包括比特打包。在这种方法中，所有数据操作都是字节对齐的，这降低了压缩率，但提高了处理性能。
*   **Gorilla:** 是脸书高吞吐量实时系统中用于 sofa 的压缩算法。它用于无损压缩，广泛应用于各种领域，如物联网和云服务。Gorilla 引入了 delta-of-delta 来处理时间戳，运行 xor 来转换数据，然后使用 Huffman 进行编码和位打包。下图是大猩猩的示意图。

![](img/6c56c68f94d04ad1969063d7832b76d3.png)

还有很多相关的压缩算法。总的来说，

1.  他们使用单一模式或有限的静态模式来压缩数据。
2.  为了提高压缩率，许多压缩算法使用位打包或有损压缩。然而，它们对日益流行的并行计算并不友好。

# 3.基于深度学习的两阶段压缩算法

## 3.1 时间序列数据压缩的特点

时间序列数据来自不同的领域，如物联网、金融、互联网、商业管理和监控。因此，它们有不同的形式和特点，对数据的准确性也有不同的要求。如果只有一个统一的压缩算法可以用于非差分处理，那么这个算法应该是以 8 字节为单位描述数据的无损算法。

下图是阿里云业务中使用的时间序列数据的一些例子。无论宏观还是微观，都有各种各样的数据模式，无论是形状曲线还是数据精度都不同。因此，压缩算法必须支持尽可能多的压缩模式，这样才能选择有效且经济的压缩模式进行压缩。

![](img/2a7ea8e3de663fad295ffc28c0fcb070.png)

用于时间序列数据的大规模商业压缩算法必须具有三个重要特征:

*   **时间相关性:**时间序列数据具有很强的时间相关性，对应的数据几乎是连续的。采样间隔通常为 1 秒或 100 毫秒。
*   **模式多样性:**如上图所示，模式和特征差异很大。
*   **数据海量:**每天、每小时、每秒都需要处理海量数据。每天处理的总数据量约为 10pb。因此，压缩算法必须高效且具有高吞吐量。

## 3.2 新算法的核心概念

数据压缩的本质可以分为两个阶段。第一个阶段是转换，在这个阶段，数据从一个空间转换到另一个具有更规则排列的空间。第二阶段是增量编码，在这一阶段，可以使用各种方法来识别增量处理后得到的增量。

基于时间序列数据的特征，我们可以定义以下六种基本转换原语。它们都是可扩展的。

![](img/4e18131768d82333c7eb47bdcab071d8.png)

然后，我们可以定义以下三种基本的差分编码原语。它们都是可扩展的。)

![](img/0ac5e8c893d110b2cec0df2d00754e66.png)

接下来，我们是否应该对前面的两个压缩工具进行排序和组合？这是可行的，但是效果不好，因为模式选择和相关参数的成本比例太高。2 字节的控制信息(原语选择+原语参数)占要用 8 字节表示的数据的 25%。

因此，更好的解决方案是在抽象层表达数据特征，如下图所示。创建一个控制参数集，以更好地表达所有情况。然后，在全局(时间线)级别选择适当的参数来确定搜索空间，该搜索空间仅包含少量压缩模式，例如四个模式。在压缩时间线中每个点的过程中，遍历所有压缩模式并选择最佳压缩模式。在这个解决方案中，控制信息的比例约为 3%。

![](img/6ff0709d3a6a01f2ea79af4afe71ab97.png)

## 3.3 两阶段压缩框架:AMMMO

自适应多模中间输出的整个过程分为两个阶段。在第一阶段，确定当前时间线的一般特征，并确定九个控制参数的值。然后在第二阶段，遍历少量压缩模式并选择最佳压缩模式。

第二阶段选择图案很容易。然而，通过在第一阶段确定参数值(在这个例子中是 9 个值)来获得合适的压缩空间是具有挑战性的，因为合适的压缩空间必须从 300，000 个组合中选择(理论上。)

## 3.4 基于规则的模式和空间选择算法

我们可以设计一种算法，为所有压缩模式创建一个记分牌。然后，该算法遍历时间线中的所有点，并执行分析和记录。最终，算法可以通过统计、分析和比较来选择最佳模式。这里，这个过程涉及一些明显的问题:

*   选定的评估指标可能不合适。
*   在这个问题中，我们必须思考并手动编译程序，处理许多工作量，包括实现、调试和维护。
*   如果算法的原语和压缩模式发生变化，所有代码都必须重新构建。此外，假设前面的选择不是从理论上推导出来的，那么就需要一种自动和智能的方法来支持不断的进化。

# 4.深度强化学习

## 4.1 问题建模

我们可以将前面的模式和空间选择算法简化为下图所示的结构。这样，我们可以把问题看作是一个多目标的分类问题。每个参数都是一个目标，每个参数空间的取值范围包括可用的类。深度学习在图像分类和语义理解方面已经证明了它的高可用性。同样，我们也可以通过使用深度学习作为多标签分类过程来实现模式和空间选择。

![](img/e574c5307ecab2aa76f72984e2344402.png)

那么，我们可以使用什么样的网络呢？由于识别的主要关系包括δ/xor、移位和位掩码，卷积神经网络(CNN)是不合适的，而全连接多层感知器(MLP)是合适的。考虑时间线中的所有点。一个小时总共有 3600 x 8B 个点，这是一个巨大的数字。考虑到同一时间轴中的相似段，我们可以考虑以 32 个点为基本处理单位。

接下来，我们如何创建训练样本，如何确定样本的标签？

出于以下原因，我们在训练中引入了强化学习而不是监督学习。

*   很难创建带标签的样本:32 个样本的大小是 256 字节。理论上，样本有 256 种⁵⁶可能性。对于这些样本中的每一个，我们都需要遍历 30 万种可能性来找到最好的一个。因此，我们必须处理创建和选择样本以及创建标签的巨大工作量。
*   这不是一个常见的一类标签问题。给定一个样本，最好的结果不只有一个。相反，许多选择可能会达到相同的压缩效果。相应地，N 个类(N 是未知的)的训练可能更加困难。
*   为了解决这个问题，我们需要一个自动化的方法。参数的选择，如压缩工具，可能会扩展。如果是这样，必须重新创建整个训练样本。因此，这种情况需要一种自动方法。

那么，我们应该选择哪种类型的强化学习；DQN，政策梯度，还是演员评论家？正如我们前面分析的，DQN 不适用于奖励和行动不连续的情况。参数，如 majorMode 0 和 1，会导致完全不同的结果。因此，DQN 不适用。此外，不容易评估压缩问题，网络也不复杂。所以不需要演员-评论家。所以，我们选择了政策梯度。

政策梯度的一个常见损失是使用缓慢提高的基线作为衡量当前行动是否合理的标准。对于这个样本，政策梯度是不合适的，效果很差。这是因为样品有太多(256 ⁵⁶)的理论块态。所以，我们设计了亏损。

在我们获得每个块的参数后，我们需要考虑块之间的相关性。我们可以使用统计聚合来获得整个时间线的最终参数设置。

## 4.2 深度强化学习的网络框架

下图显示了整个网络框架。

![](img/3eafb4124e14fba9af360f42ac0c5be8.png)

在训练端，随机选择 M 个块，每个块复制 N 个副本，然后将副本输入到一个具有三个隐层的全连接网络中。区域 softmax 用于获得每个参数的各种选择的概率。然后，基于概率对每个参数的值进行采样。然后将得到的参数值传递给底层压缩算法进行压缩。最后，获得一个压缩值。比较 N 个重复块以计算损失，然后执行反向传播。损失的总体设计是:

![](img/95089eed8ccfdcb8d3ac3449d6be95f7.png)

fn(copi)描述了压缩效果，如果它高于 N 个块的平均值，则表示正反馈。Hcs(copi)表示交叉熵；获得高分的概率越高，越有把握，结果越好，反之亦然。H(cop)表示交叉熵，其被用作归一化因子以避免网络固化并执行收敛以实现局部最优。

在推理方面，可以将时间线的全部或部分块输入到网络中以获得参数值，这些参数值可以用于统计聚合以获得时间线的参数值。

# 5.结果数据

## 5.1 实验设计

我们通过从阿里云 IoT 和服务器两个大场景中随机选取共 28 个时间轴来获得测试数据。我们还选择了时间序列数据分析和挖掘领域中最常见的数据集 UCR。基本信息是:

![](img/d05934a344bce1780c82cca4954470e1.png)

我们选择 Gorilla、MO 和 Snappy 作为比较算法。由于 AMMMO 是一个两阶段压缩算法框架，第一阶段可以使用各种算法进行参数选择，所以我们选择了 Lazy(简单设置一些通用参数)、rnd1000Avg(从 1000 个随机样本中获取平均效果)、Analyze(使用手动代码)、ML(深度强化学习的一种算法)。)

## 5.2 压缩效果的比较

从整体压缩比来看，AMMMO 的两级自适应多模式压缩比显著提高了压缩效果，与 Gorila 和 MO 相比，平均压缩比提高了 50%左右。

![](img/c7a192a9226b68a11cb7629d746558a4.png)

那么，ML 的性能如何呢？下图以 ML 为单位比较了测试集 B 上的压缩效果。总的来说，ML 比 Analyze 略好，比 rnd1000Avg 好很多。

![](img/272e3a5602476a38d3dec9dfb1c344fe.png)

## 5.3 运行效率

基于 MO 的设计理念，AMMMO 去除了位打包。这使得 AMMMO 可以在 CPU 上高速运行。这也使得它非常适合并行计算平台，如 GPU。此外，AMMMO 分为两个阶段。第一阶段的性能较差，但大多数情况下，全局压缩参数可以重用。例如，特定设备最近两天的数据是可重用的。下图显示了整体性能比较。实验环境为“英特尔 CPU 8163 +英伟达 GPU P100”，AMMMO 代码为 P100。

![](img/ce5002e393cb51be8b945102858cdb3d.png)

如上图所示，AMMMO 在压缩和解压缩端都实现了 Gbit/s 级别的处理性能，性能指标值也很好。

## 5.4 算法学习的效果

深度强化学习训练的网络取得了良好的最终效果。那么，它学习到有意义的内容了吗？下表比较了三种算法在几个测试集上的性能。我们可以看到 ML 的参数选择与 Analyze 和 RandomBest 类似，尤其是在字节偏移量和 majorMode 的选择上。

![](img/64382a32af757c7e880ff443c32d3368.png)

那么，压缩全连接网络参数的表示是什么呢？我们在第一层可视化了参数热图，如下图所示。正参数值显示为红色，负参数值显示为蓝色；值越大，颜色越亮。

![](img/360a447cba7bb23e9a35e751885400b6.png)

我们可以看到，32 个点的值有规律地显示在相同字节内的竖线中。混淆发生在字节之间，我们可以考虑在相应的位置进行 delta 或 xor 计算。数字变化最大的字节 0 的参数也是活动的。

*本文观点仅供参考，不一定代表阿里云官方观点。*

# 原始来源:

[](https://www.alibabacloud.com/blog/application-of-deep-reinforcement-learning-in-time-series-data-compression---icde-2020-paper_596445) [## 深度强化学习在时间序列数据压缩中的应用——ICDE 2020 论文

### 阿里云 2020 年 7 月 28 日 641 期本文由彭延庆、飞刀、、麦军…

www.alibabacloud.com](https://www.alibabacloud.com/blog/application-of-deep-reinforcement-learning-in-time-series-data-compression---icde-2020-paper_596445)