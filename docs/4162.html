<html>
<head>
<title>Deep Learning : Artificial Neural Networks(ANN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:人工神经网络(ANN)</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/deep-learning-artificial-neural-networks-ann-9eb56cf95be8?source=collection_archive---------1-----------------------#2020-07-26">https://medium.datadriveninvestor.com/deep-learning-artificial-neural-networks-ann-9eb56cf95be8?source=collection_archive---------1-----------------------#2020-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8f842791a8af9bddcee3771b31df6a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6kMy36ia12_xabVCUeOwNA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image : TechRadar</figcaption></figure><div class=""/><h1 id="f8f8" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">深度学习:</strong></h1><p id="ab53" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">根据给定的定义。</p><blockquote class="ly lz ma"><p id="849a" class="la lb mb lc b ld mc lf lg lh md lj lk me mf ln lo mg mh lr ls mi mj lv lw lx ij bi translated"><strong class="lc jg">深度学习</strong>是人工智能(AI)中<strong class="lc jg">机器学习</strong>的一个子集，它拥有能够从非结构化或无标签的数据中<strong class="lc jg">学习</strong>的网络。又称<strong class="lc jg">深度</strong>神经<strong class="lc jg">学习</strong>或<strong class="lc jg">深度神经网络</strong></p></blockquote><h1 id="0d66" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">深度学习中的类别:</h1><p id="3a51" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">深度学习可以大致分为三大类。</p><ol class=""><li id="c1fe" class="mk ml jf lc b ld mc lh md ll mm lp mn lt mo lx mp mq mr ms bi translated">人工神经网络也称为ANN。</li><li id="1db3" class="mk ml jf lc b ld mt lh mu ll mv lp mw lt mx lx mp mq mr ms bi translated">被称为CNN的卷积神经网络</li><li id="ca77" class="mk ml jf lc b ld mt lh mu ll mv lp mw lt mx lx mp mq mr ms bi translated">称为RNN的递归神经网络。</li></ol><h1 id="d1e4" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">人工神经网络(ANN) : </strong></h1><p id="fdff" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">人工神经网络是一种信息处理系统，它受到人类生物神经系统(如大脑)处理信息的方式的启发。</p><p id="6715" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">简而言之，ANN研究的是人类大脑中神经元的概念。神经元(也称为神经元或神经细胞)是一种电可兴奋细胞，通过电信号和化学信号处理和传输信息。</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/4865af2243335484e3fd6dd80df24101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rbvjJWxn7_DpP6hDsGoHuQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">image:visiblebody</figcaption></figure><p id="6ea1" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg"> <em class="mb">在安信息是从下面提层传递过来的</em> </strong></p><ol class=""><li id="cef2" class="mk ml jf lc b ld mc lh md ll mm lp mn lt mo lx mp mq mr ms bi translated">输入图层-接受数据集的输入要素</li><li id="a07a" class="mk ml jf lc b ld mt lh mu ll mv lp mw lt mx lx mp mq mr ms bi translated">隐藏图层-该图层负责对输入数据集执行各种操作。这些(隐藏层中的圆圈)被称为神经元。</li><li id="dc5c" class="mk ml jf lc b ld mt lh mu ll mv lp mw lt mx lx mp mq mr ms bi translated">输出层-显示隐藏层执行操作的结果。</li></ol><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/380d33c1a84a03a0f359fa5f887dc2ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ok8e8fjZ8YF7QqkcafDEMg.png"/></div></div></figure><p id="06cd" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">正如你在上面的图片中看到的，我们有三个输入特征输入F1，输入F2和输入F3，它们从输入层传递到隐藏层。</p><p id="cb77" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">W1、W2、W3、W4、W5、W6、W7、W8是分配给每个到隐藏神经元的连接的不同权重。这些权重对于优化我们将在下面详细讨论的模型起着至关重要的作用。</p><p id="ef17" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">每个输入特征连接到隐层的所有神经元，隐层的输出连接到最终的输出层。</p><div class="ip iq gp gr ir ne"><a href="https://www.datadriveninvestor.com/2020/06/19/artificial-intelligence-and-the-new-frontiers-of-brainsourcing/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jg gy z fp nj fr fs nk fu fw je bi translated">人工智能和“智力资源”的新领域|数据驱动的投资者</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">“智力来源”是最近由赫尔辛基大学的研究人员开发的一项技术，它使用…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ix ne"/></div></div></a></div><h1 id="c5ac" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">人工神经网络中神经元的工作:</strong></h1><p id="ae2b" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">每个神经元执行两个主要操作。</p><p id="3333" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">操作1 </strong>:包含所有权重乘以输入特征的总和，并在其上添加偏差。例如，在我们的线性回归中，我们使用下面的公式来寻找最佳拟合线<strong class="lc jg"> y=m * x+c </strong></p><p id="d6bf" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">这里m是我们所有的权重，x是我们所有的输入特征，c是偏差。</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/978ccdda0494475c555c306d42993104.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*_a52FX88wZ6P2YQ-NTKXTg.png"/></div></figure><p id="4653" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">这同样适用于所有其他神经元。</p><p id="3f15" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">操作2 : </strong></p><p id="251c" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">Neuron在<strong class="lc jg">操作1 </strong>的输出上应用激活函数来转换值。</p><p id="cfd2" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">很少有著名的激活函数是<strong class="lc jg"> Sigmoid </strong>和<strong class="lc jg"> Relu </strong>。</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/b39eaeadf45176e5af69452ba7bb0df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29VH_NiSdoLJ1jUMLrURCA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image :Towards Data Science</figcaption></figure><p id="51bd" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">一旦每个神经元下的操作完成，神经元的输出连同新的权重一起被传递到输出层。</p><p id="c72a" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">下图中，</strong></p><p id="0acb" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">隐藏神经元HL_1的输出是O1，其与权重W7一起被传递到输出层，神经元HL_2的输出是O2，其与权重W8一起被传递到输出层。</p><p id="c550" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">这个预测输出比如OP1与实际输出进行比较。</p><p id="82b6" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">从<strong class="lc jg">输入层到隐藏层再到输出层的整个信息流称为1正向传播。</strong></p><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/2dcaed82c0ea7872815cc229603114e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSjrM4rEfQik5-OSLzRPBQ.png"/></div></div></figure><p id="6cab" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">损失函数:</strong>当我们有单一预测产量时，我们将与实际产量进行比较并计算损失</p><p id="ca93" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">成本函数:</strong>当我们有多个预测输出时，我们将与实际输出进行比较，并计算损失(通过对所有损失求和)</p><p id="682c" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">反向传播</strong>是当预测输出不等于实际输出时，通过使用优化器来更新权重以减少损失(达到全局最小值)的过程。几个著名的优化器是梯度下降，随机梯度下降，小批量S.G.D .等。</p><figure class="mz na nb nc gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/05a7b1de4f018ea495f76e7adc9bc2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nAUzoWNPMkIYUeDlDRfdGw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">image gradient descent: medium</figcaption></figure><p id="bb6f" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">历元</strong>:一次完整的正向传播和反向传播称为1个历元。</p><p id="40ef" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">结论</strong>:人工神经网络对于获得良好的模型精度极其重要，通常在数据集中有数百万条记录时使用。我将在以后的文章中详细介绍每一层以及所有的激活函数和优化器。</p><p id="0756" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">希望你喜欢我的文章。请鼓掌👏(50次)激励我继续写下去。</p><p id="06f8" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">请写下您的疑问和意见，并分享您的反馈。</p><p id="20ee" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">想要连接:</p><p id="773b" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">联系方式:<a class="ae nx" href="https://www.linkedin.com/in/anjani-kumar-9b969a39/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/anjani-kumar-9b969a39/</a></p><p id="2c0d" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated">如果你喜欢我在Medium上的帖子，并希望我继续做这项工作，请考虑在<a class="ae nx" href="https://www.patreon.com/anjanikumar" rel="noopener ugc nofollow" target="_blank"> <strong class="lc jg"> patreon </strong> </a>上支持我</p><p id="7217" class="pw-post-body-paragraph la lb jf lc b ld mc lf lg lh md lj lk ll mf ln lo lp mh lr ls lt mj lv lw lx ij bi translated"><strong class="lc jg">访问专家视图— </strong> <a class="ae nx" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="lc jg">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>