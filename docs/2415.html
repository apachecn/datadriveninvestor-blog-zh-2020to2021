<html>
<head>
<title>Gradient Descent Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/gradient-descent-algorithm-b4c5afb4eb98?source=collection_archive---------6-----------------------#2020-04-29">https://medium.datadriveninvestor.com/gradient-descent-algorithm-b4c5afb4eb98?source=collection_archive---------6-----------------------#2020-04-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ba8d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索梯度下降算法的三种变体。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ae5480b149f8200c381e8b191c4c6fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*eOIJZJIuwySfPZ1xr4vCqw.png"/></div></figure><p id="d14f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在机器学习中，<strong class="kp ir">优化</strong>是最小化<strong class="kp ir">成本(损失)函数</strong>(选择的评估模型的度量)的过程，例如平方误差的和(<a class="ae lj" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank"> SSE </a>)和均方误差(<a class="ae lj" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>)等等。通常，成本函数以J(θ)的符号给出，其中θ是与目标机器学习模型相关联的n维权重向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/b01b1fd04fa1940068fc47978b9b12a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*xwnKQIZjJiBEShjGUbhekw.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">sum of squared errors</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/d73af7672d160537ee96824a3184ddda.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*UN5CfxaIlyuS_SXZhjfu3Q.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">mean squared errors</strong></figcaption></figure><p id="3b54" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在我们之前关于<a class="ae lj" href="https://link.medium.com/FckcOIukW5" rel="noopener">线性回归</a>的文章中，我们已经讨论了梯度下降算法在优化线性回归的权重和偏差方面的作用。事实上，梯度下降是机器学习问题中最常实现的优化算法之一。<a class="ae lj" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>是一种最小化函数的迭代优化算法。参数/权重在每次迭代中通过<strong class="kp ir">步长进行更新，该步长与当前点</strong>处函数梯度的负值成比例。</p><p id="4cf2" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了更好地理解算法，让我们考虑一个二次函数，f(x)= 2(x–0.5)+0.1，其图形如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/06aa745d7dfa1a36cbadf52198108f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQqxk1GbzgxhDEvahmVORw.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Figure:</strong> Graph of f(x)=2(x–0.5)²+0.1</figcaption></figure><p id="e6ce" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在算法描述中给出“<strong class="kp ir">步与函数在当前点</strong>的梯度的负值成正比”，实际上函数在一点的梯度是由其一阶导数给出的，因此我们可以将<strong class="kp ir">步</strong>表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/e8d5215c50728eb26071b5d0feec3252.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*d-2u2s5XtFGuTTS0X4xLSQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Gradient of f(x) is given by its first derivative</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/3ed884b1b1391f4f9c26c704529c77e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*9trIP_nDTrV-Qp_BYVYGUg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Formulation of step</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/94ef5bf7d31b90006f339751c3379381.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*ZGudlx6BcFPJhgFPlyCljQ.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Update x with step of –α(4x – 2) in each iteration</figcaption></figure><p id="2a38" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在让我们选择学习率，α=0.2，并初始化一个起始x值。通常，它被初始化为零(x=0)。</p><p id="0012" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #1迭代:<br/></strong>x = 0–0.2(4(0)-2)= 0.4<br/>f(x)= 2(0.4–0.5)+0.1 = 0.12</p><p id="4162" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #2迭代:<br/></strong>x = 0.4–0.2(4(0.4)-2)= 0.48<br/>f(x)= 2(0.48–0.5)+0.1 = 0.1008</p><p id="ec73" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #3迭代:<br/></strong>x = 0.48–0.2(4(0.48)-2)= 0.496<br/>f(x)= 2(0.496–0.5)+0.1 = 0.100032</p><p id="ab2c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #4迭代:<br/></strong>x = 0.496–0.2(4(0.496)-2)= 0.4992<br/>f(x)= 2(0.4992–0.5)+0.1 = 0.10000128</p><p id="11b6" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #5迭代:<br/></strong>x = 0.4992–0.2(4(0.4992)-2)= 0.49984(≈0.5)<br/>f(x)= 2(0.49984–0.5)+0.1 = 0.1000000512</p><p id="b714" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们可以观察到x值在每次迭代中增加，而f(x)的值向其最小值(0.1)减少(收敛)。在第5次迭代时，x大约等于0.5，这是f(x)的最小值，如上图所示。</p><blockquote class="lz"><p id="57ed" class="ma mb iq bd mc md me mf mg mh mi li dk translated">"为什么在更新过程中需要学习率？"</p></blockquote><p id="2e7b" class="pw-post-body-paragraph kn ko iq kp b kq mj jr ks kt mk ju kv kw ml ky kz la mm lc ld le mn lg lh li ij bi translated">让我们考虑不存在通过简单地设置α=1而引入的学习率。所以，<strong class="kp ir">步=–(4x–2)</strong>。</p><p id="f6fc" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">类似地，我们初始化x=0。</p><p id="0ae0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #1迭代:<br/></strong>x = 0-(4(0)–2)= 2<br/>f(x)= 2(2–0.5)+0.1 = 4.6</p><p id="75c0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #2迭代:<br/></strong>x = 2-(4(2)–2)=–4<br/>f(x)= 2(–4–0.5)+0.1 = 40.6</p><p id="f395" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #3迭代:<br/></strong>x =–4-(4(–4)-2)= 14<br/>f(x)= 2(14–0.5)+0.1 = 364.6</p><p id="9782" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #4迭代:<br/></strong>x = 14-(4(14)-2)=–40<br/>f(x)= 2(–40–0.5)+0.1 = 3280.6</p><p id="da99" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> #5迭代:<br/></strong>x =–40-(4(–40)-2)= 122<br/>f(x)= 2(122–0.5)+0.1 = 29524.6</p><p id="30cd" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这个例子中你观察到了什么？x值从负值到正值振荡，并且幅度增加，而f(x)值显著增加。这是因为在每一次迭代中，x值的更新需要很大的步长，从而导致f(x)值发散而不是收敛到最小值。因此，这就是为什么我们需要学习率(α)来重新调整步长以确保f(x)收敛到最小值的原因。</p><p id="6ce0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">希望通过这个简单的例子，你对梯度下降算法有了更清晰的了解。注意，我们刚刚研究的例子是单变量函数，然而同样的概念同样适用于多变量函数。这就是梯度下降算法的基本概念。</p><h2 id="20bc" class="mo mp iq bd lp mq mr dn ms mt mu dp mv kw mw mx my la mz na nb le nc nd ne nf bi translated">梯度下降算法的三种变体</h2><p id="4948" class="pw-post-body-paragraph kn ko iq kp b kq ng jr ks kt nh ju kv kw ni ky kz la nj lc ld le nk lg lh li ij bi translated">将均方误差(MSE)视为评估机器学习模型(如线性回归模型)的度量，因此<strong class="kp ir">成本函数</strong>为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/d73af7672d160537ee96824a3184ddda.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*UN5CfxaIlyuS_SXZhjfu3Q.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">m is the number of data points</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/9d6843c10cd5cc14303f5f59526e8749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*7UebE532ltznu6RyaeMSRg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Gradient of cost function with respect to θⱼ (where j=0, 1, 2, … , n with n=total number of independent variables)</strong></figcaption></figure><blockquote class="lz"><p id="5f61" class="ma mb iq bd mc md nm nn no np nq li dk translated">让我们看看梯度下降算法的三个变体是如何工作的。</p></blockquote><p id="8da4" class="pw-post-body-paragraph kn ko iq kp b kq mj jr ks kt mk ju kv kw ml ky kz la mm lc ld le mn lg lh li ij bi translated"><strong class="kp ir"> 1。批量梯度下降:</strong></p><p id="a116" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在批次梯度下降算法的每次迭代中，计算梯度时会考虑所有数据示例(行)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0992ac7273ec1d89d45e1b0f3cf4b140.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*i2KDXJGTkvLRo3dqZP6sUw.png"/></div></figure><p id="aed1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更新等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e3e6c6b73877737b4d2fa048420670df.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*wc3hqQoGATIzR7OVLgyUqQ.png"/></div></figure><p id="919b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，该算法可以总结如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/59ad0c09364e6b02e163ff262e8ad9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*3bU1EaoVlgxPU4fXzEHSUw.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Batch gradient descent algorithm for linear regression</figcaption></figure><p id="6047" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">优点:</p><ul class=""><li id="577e" class="nw nx iq kp b kq kr kt ku kw ny la nz le oa li ob oc od oe bi translated">这在计算上更有效，因为在考虑所有数据点的每个时期中更新仅发生一次。</li><li id="c4ab" class="nw nx iq kp b kq of kt og kw oh la oi le oj li ob oc od oe bi translated">梯度的估计更精确，因为它在计算中使用了所有的数据点。所以收敛更稳定。</li></ul><p id="1683" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">缺点:</p><ul class=""><li id="69f3" class="nw nx iq kp b kq kr kt ku kw ny la nz le oa li ob oc od oe bi translated">如果数据集非常大，它会更慢，因为它需要在一次更新中计算整个数据集的梯度。</li><li id="8736" class="nw nx iq kp b kq of kt og kw oh la oi le oj li ob oc od oe bi translated">由于在计算梯度时需要整个数据集，因此整个数据集必须适合硬件存储器，如果数据集非常大，这可能是不可行的。</li></ul><p id="0e11" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 2。随机梯度下降:</strong></p><p id="4819" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在随机梯度下降算法中，不是使用所有行的数据，而是只使用一行数据来计算梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/91b6ed8666567f09acb19383c1bc9785.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*vgcHSxhRTKF3ZEVszbjwPg.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">Note that the summation notation is dropped as compared to the one in batch gradient descent because we are using one data point in each update.</strong></figcaption></figure><p id="41bb" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更新方程式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5e08994dd559ce3fa06bad97de397372.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*G1UT472bSFQIr2QqKaOebA.png"/></div></figure><p id="7d4f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">总结算法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cecc82b97df9ceb0682a5a6a4360cf57.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ZaqHIiHfjJzX5iklxHd_Yg.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Stochastic gradient descent algorithm for linear regression</figcaption></figure><p id="2c8b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">优点:</p><ul class=""><li id="ab8c" class="nw nx iq kp b kq kr kt ku kw ny la nz le oa li ob oc od oe bi translated">因为更新发生得更频繁，所以收敛会更快。</li><li id="b697" class="nw nx iq kp b kq of kt og kw oh la oi le oj li ob oc od oe bi translated">权重的更新非常嘈杂，因为它仅用单个数据点来估计梯度。噪声更新可以帮助模型避免次优的局部最小值。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/db3d999099a1fbd8205eb60ab0a6dba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*BS5UuWEE_qXzoWBDQumgDA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Noisy cost function with respect to each update. (Source: <a class="ae lj" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a>)</figcaption></figure><p id="ebbc" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">缺点:</p><ul class=""><li id="b8e5" class="nw nx iq kp b kq kr kt ku kw ny la nz le oa li ob oc od oe bi translated">同样，如果数据集非常大，由于迭代次数非常多，因此训练时间会更长。</li><li id="0ec5" class="nw nx iq kp b kq of kt og kw oh la oi le oj li ob oc od oe bi translated">如前所述，权重的更新非常嘈杂，因此它实际上不会收敛到最小值，而是在最小值附近保持振荡。</li></ul><p id="1b6b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir"> 3。小批量梯度下降:</strong></p><p id="b544" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">正如我们所看到的，批量梯度下降和随机梯度下降都有一些缺点，因此出现了梯度下降的第三种变体，它通常比上述两种变体更快，并且与随机梯度下降相比还减少了噪声。</p><p id="3332" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">顾名思义，小批量梯度下降法使用小批量b(其中<strong class="kp ir"> 1 &lt; b &lt; m </strong>的数据来计算梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c0fa3cdd7b736e99e5bb1851bf054aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*f_xMHOqOYm6vebLbYDcGUA.png"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><strong class="bd lp">b is the batch size (number of data per batch)</strong></figcaption></figure><p id="6568" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更新方程式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4ab6a9c6ae1a62bb11921e9bbd70ce2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*INNWdXaK3jv6o_T2h-Wrsw.png"/></div></figure><p id="b0dd" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">总结算法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/009ff9b05ca26b749be1f27224b01ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*cDIn0Vcb_-Vy8_SyVDb6cw.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk">Mini-batch gradient descent algorithm for linear regression</figcaption></figure><h2 id="f30b" class="mo mp iq bd lp mq mr dn ms mt mu dp mv kw mw mx my la mz na nb le nc nd ne nf bi translated">结论</h2><p id="408b" class="pw-post-body-paragraph kn ko iq kp b kq ng jr ks kt nh ju kv kw ni ky kz la nj lc ld le nk lg lh li ij bi translated">这就是梯度下降算法的三种变体。我希望你喜欢这篇文章并从中获得知识。在不久的将来，我将对其中的三种算法做一些实验，以提供它们之间的定量比较。</p><div class="or os gp gr ot ou"><a href="https://www.datadriveninvestor.com/2020/02/22/algorithms-for-fairness/" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">公平算法|数据驱动的投资者</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">许多人都有算法偏见。软件工程师关心算法偏差，因为我们关心…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi kl ou"/></div></div></a></div><h2 id="c8ac" class="mo mp iq bd lp mq mr dn ms mt mu dp mv kw mw mx my la mz na nb le nc nd ne nf bi translated">参考</h2><p id="66d8" class="pw-post-body-paragraph kn ko iq kp b kq ng jr ks kt nh ju kv kw ni ky kz la nj lc ld le nk lg lh li ij bi translated">[1]<a class="ae lj" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pj nv l"/></div></figure></div></div>    
</body>
</html>