<html>
<head>
<title>Intuition to Pooling Layers in CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对CNN中汇集层的直觉</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/intuition-to-pooling-layers-in-cnn-a45be956a3a9?source=collection_archive---------4-----------------------#2020-08-27">https://medium.datadriveninvestor.com/intuition-to-pooling-layers-in-cnn-a45be956a3a9?source=collection_archive---------4-----------------------#2020-08-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9623" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">理解汇集层和对反向传播的影响的方法</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/2850c69a22090fb09fd23c0ae6c9c5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mVf8A-ZGG_R1MxYL"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Credits <a class="ae lc" href="https://unsplash.com/@sickhews" rel="noopener ugc nofollow" target="_blank">@sickhews</a></figcaption></figure><p id="ab3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CNN基于权重共享策略，神经元能够对数据执行卷积，卷积滤波器由权重形成。随后进行汇集操作，这减少了表示的空间大小，从而减少了网络中的计算量和参数。池层也有助于控制过度拟合。</p><blockquote class="ld"><p id="41f8" class="le lf iq bd lg lh li lj lk ll lm kk dk translated">在池层上没有学习发生</p></blockquote><p id="7a8d" class="pw-post-body-paragraph jn jo iq jp b jq ln js jt ju lo jw jx jy lp ka kb kc lq ke kf kg lr ki kj kk ij bi translated">使用像最大池、平均池甚至L2-诺姆池这样的函数来获得池单元。在池层，前向传播导致一个<em class="kl"> N </em> × <em class="kl"> N </em>池块被减少到一个单一值——即“获胜单元”的值。</p><p id="7216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在实践中，最大池层只有两种常见的变化:一种池层具有<em class="kl"> F </em> =3，<em class="kl"> S </em> =2(也称为重叠池)，更常见的是<em class="kl"> F </em> =2，<em class="kl"> S </em> =2。具有较大感受野的池大小具有太大的破坏性。</p><h2 id="cc96" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">最大池上的反向传播</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/e43c256a62eba00b36c9e816f90e633f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U0QXNgAuBo842m12.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Forward Propagation of CNN</figcaption></figure><p id="de2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解maxpooling上的反向传播，我们必须理解max(x，y)的导数。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mm"><img src="../Images/200f15b9e96783782b86260c8e0800cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WZBqD5AkXXBfyTQ5CD1Ogw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Derivative of max(x,y)</figcaption></figure><p id="e4dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也就是说，在较大的输入上,(子)渐变为1，在另一个输入上为0。直观上，如果输入为<em class="kl"> x </em> =4，<em class="kl"> y </em> =2，那么最大值为4，功能对<em class="kl"> y </em>的设置不敏感。也就是说，如果我们将它增加一个微小的量<em class="kl"> h </em>，函数将继续输出4，因此梯度为零:没有影响。</p><p id="57f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi mn translated">由于max(x，y)运算的后向传递有一个简单的解释，即只将梯度发送到前向传递中具有最高值的输入。因此，在池层的正向传递期间，通常跟踪最大激活的索引(有时也称为<em class="kl">开关</em>)，使得梯度路由在反向传播期间是有效的。</p><h2 id="ce4e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">合并图层有什么问题</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mw"><img src="../Images/125efcde6a8cdd3766f4d55c706769f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZUbFZr8Mvc1Nlipw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk"><strong class="bd lu">To a CNN, both pictures are similar, since they both contain similar elements.</strong></figcaption></figure><p id="6b7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们在谈论一个人脸检测器，那么合并图层会丢失很多有价值的信息，并且它忽略了部分和整体之间的关系，所以我们必须结合一些特征(嘴、两只眼睛、椭圆形脸和鼻子)来说这是一张脸。</p><h2 id="2e14" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">无池层的CNN</h2><p id="f8cc" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">为了减小表示的大小，他们建议偶尔在CONV层使用较大的步幅。丢弃池层也被发现在训练良好的生成模型中是重要的，例如变分自动编码器(VAEs)或生成对抗网络(GANs)。未来的架构似乎很可能很少甚至没有池层。</p><h1 id="2285" class="nc lt iq bd lu nd ne nf lx ng nh ni ma nj nk nl md nm nn no mg np nq nr mj ns bi translated">参考</h1><p id="b55f" class="pw-post-body-paragraph jn jo iq jp b jq mx js jt ju my jw jx jy mz ka kb kc na ke kf kg nb ki kj kk ij bi translated">[1] <a class="ae lc" href="http://Backpropagation In Convolutional Neural Networks" rel="noopener ugc nofollow" target="_blank">卷积神经网络中的反向传播，jefkine</a><br/>【2】<a class="ae lc" href="https://medium.com/the-bioinformatics-press/only-numpy-understanding-back-propagation-for-max-pooling-layer-in-multi-layer-cnn-with-example-f7be891ee4b4" rel="noopener">Jae Duk Seo</a><br/>【3】<a class="ae lc" href="https://medium.com/@pechyonkin?source=post_page-----b4b559d1159b----------------------" rel="noopener">Max Pechyonkin</a><br/>【4】<a class="ae lc" href="https://cs231n.github.io" rel="noopener ugc nofollow" target="_blank">cs 231n用于视觉识别的卷积神经网络</a><br/>【5】<a class="ae lc" href="https://towardsdatascience.com/part-2-gradient-descent-and-backpropagation-bf90932c066a" rel="noopener" target="_blank">梯度下降和反向传播</a></p><h2 id="ace9" class="ls lt iq bd lu lv lw dn lx ly lz dp ma jy mb mc md kc me mf mg kg mh mi mj mk bi translated">尾注</h2><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1314b393f293b3136bc3fd5e59f328c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/0*lSm8m450Myvx1qUF.png"/></div></figure><blockquote class="nu nv nw"><p id="7c6f" class="jn jo kl jp b jq jr js jt ju jv jw jx nx jz ka kb ny kd ke kf nz kh ki kj kk ij bi translated">如果你坚持到最后——请鼓掌。(具体来说是50次)<br/>这会让我有动力写更多。谢谢你。</p></blockquote></div></div>    
</body>
</html>