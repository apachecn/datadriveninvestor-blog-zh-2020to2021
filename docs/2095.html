<html>
<head>
<title>Which Reinforcement learning-RL algorithm to use where, when and in what scenario?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在哪里、什么时候、什么场景下使用哪种强化学习-RL算法？</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/which-reinforcement-learning-rl-algorithm-to-use-where-when-and-in-what-scenario-e3e7617fb0b1?source=collection_archive---------0-----------------------#2020-04-14">https://medium.datadriveninvestor.com/which-reinforcement-learning-rl-algorithm-to-use-where-when-and-in-what-scenario-e3e7617fb0b1?source=collection_archive---------0-----------------------#2020-04-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2fb3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">什么？为什么？什么时候？哪一个？强化学习算法和现有强化学习算法的快速事实。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/93b58644a4cb2390240e424502bdcf48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NW8VYp2rRU3-UjALxOY8Qw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.workfront.com%2Fblog%2Fproject-management-101-the-5-ws-and-1-h-that-should-be-asked-of-every-project&amp;psig=AOvVaw0zUAFYrKwD1BAVslL2Ufim&amp;ust=1586974663320000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCNjy67vD6OgCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">Asking the basic questions improves our understanding of the topic</a></figcaption></figure><p id="ab64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">强化学习是一个复杂的机器学习领域，超参数的微小变化可能导致模型性能的突然变化。</p><p id="f148" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们将讨论关于各种RL技术的快速事实，然后继续理解哪种算法有什么专长，哪种情况需要哪种技术。</p><h1 id="739c" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">深Q网(DQN):</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ma"><img src="../Images/3fe3b4214aa94a2188736ad345603000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZVM8FFvuwuGjaGnJ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287&amp;psig=AOvVaw0AldQqVIfxeYiMBkycsQiM&amp;ust=1587053501798000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCLixhpTp6ugCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">Psuedo code for DQN</a></figcaption></figure><ol class=""><li id="e5b2" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">基于Q网络</li><li id="6706" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">需要一个稳定的目标网络</li><li id="816e" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">需要优先体验重放，以实现高效采样</li><li id="a2f5" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">非政策方法</li><li id="f4c5" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">收敛慢但效率高</li></ol><h1 id="4134" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">普通政策梯度(VPG): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mp"><img src="../Images/a97057968bae44a1e4b755da49c33d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-cY4YArYzVVxKxTX-24Ng.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/vpg.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for VPG</a></figcaption></figure><ol class=""><li id="bd66" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">VPG是一种基于策略的算法</li><li id="fc78" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">使用策略梯度实现融合</li><li id="d25b" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">VPG可用于具有离散或连续动作空间的环境</li></ol><h1 id="e32e" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">信任区域策略优化(TRPO): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mq"><img src="../Images/01e200409b7d216337e9d82cde50126d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QzzHCiGJ-mZwiH4evhV84w.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for TRPO</a></figcaption></figure><ol class=""><li id="32dd" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">TRPO是一种策略算法</li><li id="6709" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TRPO通过采取尽可能大的步骤来更新策略，以提高性能，同时满足基于KL的关于允许新旧策略有多接近的分歧的约束</li><li id="4547" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TRPO可用于具有离散或连续动作空间的环境</li></ol><h1 id="95dc" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">最近策略优化(PPO): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mr"><img src="../Images/c40687e4bed10811239a49f325139e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VCRDNOOCqFG5TPd1SY4qEQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for PPO</a></figcaption></figure><ol class=""><li id="3b64" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">PPO是一种基于策略的算法。</li><li id="f168" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">PPO方法实现起来更简单。</li><li id="478e" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">PPO有两种变体。</li><li id="6365" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">PPO-罚-&gt;(罚KL背离)</li><li id="c5d2" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">PPO-Clip--&gt;(剪辑目标函数)</li></ol><h1 id="0da0" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">深度确定性政策梯度(DDPG): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ms"><img src="../Images/964357ebc7cd27c9e4cacb231b42a457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORRaWJhYg6RT7yeLlgrIGQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for DDPG</a></figcaption></figure><ol class=""><li id="30d5" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">DDPG是一种非策略算法</li><li id="4855" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">DDPG可以被认为是连续动作空间的深度Q学习</li><li id="aba4" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它使用非策略数据和贝尔曼方程来学习Q函数，并使用Q函数来学习策略</li><li id="b0d2" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">DDPG只能用于有连续活动空间的环境</li></ol><h1 id="f3aa" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">双延迟DDPG (TD3): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f644a0d585070ef6ecaa63c677bebb44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*DFBXZ3aXYu3cdA3rQpoGFw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/td3.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for TD3</a></figcaption></figure><ol class=""><li id="2d8f" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">TD3是一种非策略算法。</li><li id="12c5" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TD3只能用于有连续动作空间的环境。</li><li id="65fd" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">对于超参数和其他种类的调整，它经常是脆弱的。</li><li id="5ac5" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TD3学习<em class="mu">两个</em> Q函数而不是一个，并使用两个Q值中较小的一个来形成损失函数中的目标。</li><li id="b9ba" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TD3更新策略(和目标网络)的频率低于Q函数。</li><li id="d00c" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TD3向目标动作添加噪声，通过平滑Q以及动作的变化来利用Q函数误差。</li></ol><h1 id="a656" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">优秀演员兼评论家(A2C): </h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mv"><img src="../Images/6e26f28796c4704034e638b271ceca97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z68tGSLJpZ4Um8Px4L3CDA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&amp;psig=AOvVaw3PVGJko_f7Rd6cpCMIaZZy&amp;ust=1587053644120000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCICzgtjp6ugCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">Psuedo code for A2C</a></figcaption></figure><ol class=""><li id="d972" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">A2C是一种非政策方法</li><li id="8466" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它使用优势估计来计算每个行动状态对的价值主张</li><li id="af7e" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">A2C是A3C的同步版本。</li></ol><h1 id="33cf" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">异步优势演员-评论家(A3C): </strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mw"><img src="../Images/efa292aa84f0777d17089fe92fff2620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cpg7Am3CLGj2dUsh1iretA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">P<a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-actor-critic-methods-931b97b6df3f&amp;psig=AOvVaw3PVGJko_f7Rd6cpCMIaZZy&amp;ust=1587053644120000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCICzgtjp6ugCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">suedo code for A3C</a></figcaption></figure><ol class=""><li id="8715" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">A3C是一种不符合政策的方法。</li><li id="209e" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">演员和评论家以异步方式训练和更新</li><li id="a35c" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">与A3C相比，提供更快的多处理能力。</li><li id="f6d9" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">由于多个实例同时运行，收敛速度更快</li></ol><h1 id="a9f6" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">软演员兼评论家(SAC): </h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mx"><img src="../Images/db621f395f5c1697ecd60d41a52a54e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKUH8w_HGgntjur7qANfEA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://spinningup.openai.com/en/latest/algorithms/sac.html" rel="noopener ugc nofollow" target="_blank">Psuedo code for SAC</a></figcaption></figure><ol class=""><li id="5043" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">SAC是一种非策略算法。</li><li id="f2d7" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它以非策略的方式优化随机策略，在随机策略优化和DDPG式方法之间架起了一座桥梁。</li><li id="9944" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">它结合了削波双Q技巧。</li><li id="c3d3" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">SAC使用<strong class="jp ir">熵正则化</strong>，其中策略被训练为最大化预期回报和<a class="ae lb" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">熵</a>(策略中的随机性)之间的权衡。</li><li id="70ec" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">SAC同时学习一个策略和两个Q函数</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/78a1a83b8f0ff357f9376d6ed1cc4993.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*6tUpOT4xW2dVvr5t9NfV6g.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftenor.com%2Fview%2Fits-gonna-take-alot-of-work-but-the-work-is-the-fun-part-guys-ted-danson-michael-the-good-place-gif-16132269&amp;psig=AOvVaw2c5uR1aoDWSXHI59sfSr7y&amp;ust=1586974521439000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCPCn_ffC6OgCFQAAAAAdAAAAABAQ" rel="noopener ugc nofollow" target="_blank">Keep revising until the concept is clear people</a></figcaption></figure><p id="7181" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后介绍了最流行的RL算法和它们的关键特性，这些特性揭示了它们的用途和新颖性。</p><p id="371e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们将看到哪种算法应该在什么场景中使用，以及哪种算法在给定特定问题陈述的情况下提供最佳性能。我们将根据动作空间的类型和多重处理的能力来划分问题。</p><blockquote class="mz na nb"><p id="3acb" class="jn jo mu jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated"><strong class="jp ir">离散动作——单个流程:</strong></p></blockquote><p id="956f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DQN与一些补充包括(目标网络，双DQN，优先体验重播和决斗DQN)和宏基(演员评论家与体验重播)。<br/> DQN通过重放缓冲提供更好的采样效率，但收敛时间更长。</p><blockquote class="mz na nb"><p id="d8d1" class="jn jo mu jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated"><strong class="jp ir">离散动作—多重处理:</strong></p></blockquote><p id="3947" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PPO、A2C、ACKTR(使用克罗内克因子信任区域的演员兼评论家)和宏基。PPO是首选，因为它提供了这里提到的更快的收敛-&gt;(<a class="ae lb" href="https://openai.com/blog/openai-baselines-ppo/#ppo" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/openai-baselines-ppo/#ppo</a>)</p><blockquote class="mz na nb"><p id="4dcb" class="jn jo mu jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated"><strong class="jp ir">连续动作—单个流程:</strong></p></blockquote><p id="e9d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SAC和TD3</p><blockquote class="mz na nb"><p id="85d9" class="jn jo mu jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated"><strong class="jp ir">连续动作—多重处理:</strong></p></blockquote><p id="672b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PPO、TRPO和A3C。A3C的训练速度更快，但PPO的收敛速度更快，而TRPO在某些点上很吃力。</p><h1 id="3f9e" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">结论:</h1><p id="6847" class="pw-post-body-paragraph jn jo iq jp b jq nf js jt ju ng jw jx jy nh ka kb kc ni ke kf kg nj ki kj kk ij bi translated">因此，在这篇文章中，我们了解了每个基于RL的算法的独特方面，从策略梯度到Q学习方法，还涵盖了演员评论方法。一些关键要点:</p><ol class=""><li id="b825" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated">可以观察到，PPO提供了比其他技术更好的收敛性和性能速率，但是对变化敏感。</li><li id="cec6" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">单独的DQN是不稳定的，并且给出较差的收敛性，因此需要几个附加元件。</li><li id="37b1" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">由于熵因子正则化，SAC对于基于能量的优化技术非常有效</li><li id="cd55" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">TD3和TRPO在连续动作空间中工作良好，但是缺乏更快的收敛速度</li><li id="6c87" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">当大计算能力可用并且需要引入相似环境下迁移学习的概念时，A3C非常有用。</li></ol><h1 id="014a" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><span class="l nk nl nm bm nn no np nq nr di">B</span>T19】onus部分:</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a7b37e30d8fb01c072096e73f2564c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/1*XHfcnfDRccnuV4FefwU7iA.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fgfycat.com%2Fslightfarawayballpython&amp;psig=AOvVaw0H7fh8MIb3gX5rEOP89gL5&amp;ust=1586989355511000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCMD_hJn66OgCFQAAAAAdAAAAABAJ" rel="noopener ugc nofollow" target="_blank">Bonus section -&gt; Might wanna try training Mario gym environment using RL</a></figcaption></figure><p id="7243" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有一个类别没有被发现，那就是如何处理目标或情景环境。</p><p id="51b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">她的(事后经验回放)为完成轨迹的部分分配小奖励，并帮助代理更快地学习与最终目标相关的时间信息。</p><p id="d1a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> GAEL(广义优势估计学习)</strong>帮助RL代理从专家玩家或专家/预先学习的轨迹中学习。这有助于提供基于模仿的学习，并提高收敛和样本效率。使用GAEL来诱导半监督学习，然后进一步改进专家策略。</p><blockquote class="mz na nb"><p id="9718" class="jn jo mu jp b jq jr js jt ju jv jw jx nc jz ka kb nd kd ke kf ne kh ki kj kk ij bi translated">注意——GAEL和HER将用作PPO/A2C/TRPO/TD3顶部的包装材料，而不是单独使用</p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/d20c0a7239fb99da526bcbd89c16cbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*KowWujusMfDjlkjeeOFEBg.gif"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk"><a class="ae lb" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fgiphy.com%2Fgifs%2Fcongratulations-congrats-xT0xezQGU5xCDJuCPe&amp;psig=AOvVaw1SvMlBKYfEIGmc4qMOzwCG&amp;ust=1586990648581000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCPD574H_6OgCFQAAAAAdAAAAABAI" rel="noopener ugc nofollow" target="_blank">You did great. Best wishes for your future work and RL </a>training</figcaption></figure><p id="0237" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望这篇博文对你有所帮助。</p><h1 id="f39b" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">创建此帖子所用的资源:</h1><ol class=""><li id="5122" class="mb mc iq jp b jq nf ju ng jy nt kc nu kg nv kk mg mh mi mj bi translated"><a class="ae lb" href="https://stable-baselines.readthedocs.io/en/master/index.html" rel="noopener ugc nofollow" target="_blank">https://stable-baselines . readthedocs . io/en/master/index . html</a></li><li id="1345" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated"><a class="ae lb" href="https://openai.com/blog/openai-baselines-ppo/#ppo" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/openai-baselines-ppo/#ppo</a></li><li id="37de" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated"><a class="ae lb" href="https://openai.com/blog/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/</a></li><li id="0be9" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated"><a class="ae lb" href="https://spinningup.openai.com/en/latest/" rel="noopener ugc nofollow" target="_blank">https://spinningup.openai.com/en/latest/</a></li></ol><h2 id="6c78" class="nw ld iq bd le nx ny dn li nz oa dp lm jy ob oc lq kc od oe lu kg of og ly oh bi translated">学习强化学习的有用资源:</h2><ol class=""><li id="3e71" class="mb mc iq jp b jq nf ju ng jy nt kc nu kg nv kk mg mh mi mj bi translated"><a class="ae lb" href="https://sites.google.com/view/deep-rl-bootcamp/home" rel="noopener ugc nofollow" target="_blank">https://sites.google.com/view/deep-rl-bootcamp/home</a></li><li id="32e1" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">大卫·西尔弗的RL课程</li><li id="1914" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">通过OpenAI旋转RL</li><li id="069c" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">代码实现的稳定基线</li><li id="adfd" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated">代码实现的ChainerRL</li></ol><p id="cc2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">.</p></div></div>    
</body>
</html>