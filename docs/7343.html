<html>
<head>
<title>High-Quality Background Removal Without Green Screens</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">没有绿屏的高质量背景消除</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/high-quality-background-removal-without-green-screens-8e61c69de63?source=collection_archive---------1-----------------------#2020-12-04">https://medium.datadriveninvestor.com/high-quality-background-removal-without-green-screens-8e61c69de63?source=collection_archive---------1-----------------------#2020-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="688d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这种新的背景去除技术可以从单个输入图像中提取一个人，而不需要实时的绿色屏幕！</h2></div><blockquote class="ki kj kk"><p id="0aa5" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">最初发表于<a class="ae li" href="https://www.louisbouchard.ai/remove-background/" rel="noopener ugc nofollow" target="_blank"> louisbouchard.ai </a>，前两天在<a class="ae li" href="https://www.louisbouchard.ai/tag/artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">我的博客</a>上读到的！</p></blockquote><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/22a032ee9f81e3c383ce0e8d15202dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-x8PPzSo4eJ3OZ4x2uf0w.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Image by Author</figcaption></figure><h2 id="4899" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">介绍</h2><p id="f3c9" class="pw-post-body-paragraph kl km it ko b kp mv ju kr ks mw jx ku mi mx kx ky mm my lb lc mq mz lf lg lh im bi translated">人体抠图是一项非常有趣的任务，目标是找到照片中的任何人，并从照片中移除背景。由于任务的复杂性，这真的很难实现，必须找到具有完美轮廓的人。在这篇文章中，我回顾了多年来使用的最佳技术和2020年11月29日发表的一种新颖的方法。许多技术正在使用基本的计算机视觉算法来完成这项任务，例如GrabCut算法，它速度极快，但不是很精确。</p><h2 id="5a80" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">grab cut<a class="ae li" href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html" rel="noopener ugc nofollow" target="_blank">【4】</a></h2><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi na"><img src="../Images/0d70328520085b3f895f9cf36d39dd55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*6kfHAV8beUHSDmO9O9ndHA.gif"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Image by Author</figcaption></figure><p id="02ab" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">该GrabCut算法基本上使用高斯混合模型来估计前景项目和背景的颜色分布。我们在感兴趣的对象(前景)上绘制一个矩形，并通过绘制算法未能向前景添加像素或从前景中移除一组像素的部分来迭代地尝试改善结果。这就是为什么我们经常使用“绿色屏幕”，帮助算法只移除绿色像素，而将其余的像素留在最终结果中。但是，当我们无法进入这样一个绿色屏幕时，结果就不那么好了。</p><div class="nb nc gp gr nd ne"><a href="https://www.datadriveninvestor.com/2020/11/27/deep-learning-amid-increased-physician-administrative-workload/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">医生管理工作量增加时的深度学习|数据驱动的投资者</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">行政工作量是我们这个时代大多数医生所经历的众多负担之一。医生，尤其是…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lt ne"/></div></div></a></div><h2 id="4585" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">深度图像抠图<a class="ae li" href="https://sites.google.com/view/deepimagematting" rel="noopener ugc nofollow" target="_blank">【3】</a></h2><p id="859d" class="pw-post-body-paragraph kl km it ko b kp mv ju kr ks mw jx ku mi mx kx ky mm my lb lc mq mz lf lg lh im bi translated">现代深度学习和我们GPU的能力使我们有可能创建更强大的应用程序，但这些应用程序还不完美。这里最好的例子是深度图像抠图，由Adobe Research在2017年做出。这种模型的一个版本目前在大多数网站上被用来自动移除图片的背景。不幸的是，这种技术需要两个输入:一个图像和它的三分图。三分图基本上是图像在三个层次上的表示:背景、前景和像素被认为是前景和背景的混合的区域。看起来像这样。</p><div class="lk ll lm ln gt ab cb"><figure class="nt lo nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/27ca1058e57afaf8b58438864bafeffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uSEFiTHbZBYTKYddz2PBVg.png"/></div></figure><figure class="nt lo nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/3f9c9a131029ec99a568ef32c34f472d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*B5Xig0_1ZAB0GorHvsFc9g.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk nz di oa ob">A red fox (left) and its trimap (right). Image by Author</figcaption></figure></div><p id="075c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">为了使用深度图像抠图技术成功地去除背景，我们需要一个强大的网络，能够比较准确地定位这个人。然后，我们产生一个分割，其中相当于人的像素被设置为1，图像的其余部分被设置为0。接下来，我们使用基本的计算机视觉转换从这个分割创建三分图。我们首先通过腐蚀来减小分割对象的尺寸，为未知区域留下一点空间，迭代地移除对象轮廓处的一些像素。之后，我们添加第三部分，这是未知区域，通过扩大对象，在轮廓周围添加像素。产生这样的结果。这个三分图是一个发送到深度图像抠图模型与原始图像，你得到你的输出。你可以看到这项技术需要多大的计算能力。使用两个强大的模型，如果你想达到比较准确的结果。</p><div class="lk ll lm ln gt ab cb"><figure class="nt lo oc nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/fb2697e62b59833df95cdb715444780e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*aA3R_NrNvXX5ua-9K73ocQ.png"/></div></figure><figure class="nt lo oc nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/abdc74192360fd21d7d2e8da583ded98.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*4qNIuooYeCNewXS8hd_uHA.png"/></div></figure><figure class="nt lo oc nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><img src="../Images/35429f75173d0af1a44ffbd7cd29b6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*B5Xig0_1ZAB0GorHvsFc9g.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk od di oe ob">Trimap progression. Segmentation of the object (left), erode the segmentation (middle), add the unknown region with dilations (right). Image by Author.</figcaption></figure></div><p id="3f1a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">正如你刚刚在封面图片上看到的，当前最先进的方法非常准确，但它们需要几秒钟，有时甚至几分钟才能找到单幅图像的结果。你可以想象处理整个视频需要多长时间。</p><h2 id="2f90" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">MODNet<a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></h2><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi of"><img src="../Images/5b3f97c49b59b8db8f767a19987cb29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIiFVeXralcVtJEQ_1Xu1g.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Human Matting framework. “a) We train MODNet on the labeled dataset to learn matting sub-objectives from RGB images. b) To adapt to real-world data, we finetune MODNet on the unlabeled data by using the consistency between sub-objectives. c) In the application of video human matting, our OFD trick can help smooth the predicted alpha mattes of the video sequence.” from <a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">Ke, Z. (2020) [1]</a></figcaption></figure><p id="60f5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">幸运的是，这项新技术可以从单个输入图像中处理人体抠图，而不需要绿色屏幕或三分图，实时速度高达每秒63帧！他们称他们的网络为:MODNet。这是一个轻量级的抠图目标分解网络。我们将进一步详述。他们以监督和自我监督的方式训练他们的网络。受监督的方法接受输入，并学习根据相应的基础事实去除背景，就像通常的网络一样。然后，是自我监督的训练过程。这被称为自我监督，因为这个网络不能访问它被训练的视频的基本事实。它使用未标记的数据，并可以访问上一步中找到的信息，这些信息是网络的参数。它基本上采用了第一个网络所学习的内容，并理解了每一帧中对象之间的一致性，以正确地去除背景。这两段训练是在MODNet架构上进行的。MODNet基本上由三个主要分支组成。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi og"><img src="../Images/7fbc3f41f14804d631e98a044f64db4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qM3tp6tzM0wIpXAR2jkG5w.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">MODNet architecture. Image from <a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">Ke, Z. (2020) [1]</a></figcaption></figure><p id="e420" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">有一个低分辨率分支估计人类语义。然后，基于这些结果、原始图像和图像的地面真相，高分辨率分支专注于检测精确的人体边界。最后，添加一个融合分支，也由整个地面真相遮罩监督，以预测阿尔法遮罩的最终结果，该结果将用于移除输入图像的背景。这种网络架构速度更快，因为它首先发现语义估计本身，使用低分辨率分支内的基本解码器，使其处理速度更快。可以看到，网络基本上主要由下采样、卷积和上采样组成。可以使用任意的CNN架构，在这种情况下，他们使用MobileNetV2，因为它是为移动设备设计的。与其他最先进的架构相比，它是一个小型网络，效率极高。如果你不熟悉卷积神经网络，或者CNN，我邀请你观看我制作的解释它们是什么的视频。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="e88a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">下采样和在高分辨率分支中使用较少的卷积层是为了减少计算时间。这个融合分支只是一个CNN模块，用于结合语义和细节，如果我们想要语义周围的精确细节，就必须进行上采样。最后，使用深受深层图像抠图纸启发的损失来测量结果。它计算输入图像和从地面真实前景和地面真实背景获得的合成图像之间的绝对差。现在，这个网络的架构还有最后一步。如果我们回到这里的完整架构，我们可以看到他们应用了所谓的一帧延迟。该方法利用前一帧和后一帧的信息来确定在前景和背景之间徘徊的未知像素。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oj"><img src="../Images/5b70bff2468ab7ef65eb1853a5a31428.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mhl08ZZHuRm2WBewyHzvkg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Image from <a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">Ke, Z. (2020) [1]</a></figcaption></figure><p id="a554" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">在这里，您可以看到一个例子，其中前景在三个连续的帧中稍微向左移动，像素没有对应于它应该对应的像素，红色像素在第二次迭代中闪烁。</p><p id="d7cc" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">然后，你得到了提取前景物体的最终结果，在这个例子中是一个人，你可以添加许多不同的背景。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ok"><img src="../Images/9ec5549fc721c00ed637527b1cc6199c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AvMzPAwcDuIu-QZTi0Brgw.gif"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Results example using MODNet. Image via <a class="ae li" href="https://github.com/ZHKKKe/MODNet" rel="noopener ugc nofollow" target="_blank">MODNet’s GitHub project [2]</a>.</figcaption></figure><h2 id="8f2f" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">结论</h2><p id="1e4d" class="pw-post-body-paragraph kl km it ko b kp mv ju kr ks mw jx ku mi mx kx ky mm my lb lc mq mz lf lg lh im bi translated">当然，这只是这篇新论文的一个简单概述。我强烈推荐阅读论文<a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>来更深入地了解这项新技术。正如他们在自己的页面上写的那样，代码和预训练的模型也将很快在他们的Github<a class="ae li" href="https://github.com/ZHKKKe/MODNet" rel="noopener ugc nofollow" target="_blank">【2】</a>上可用。两者都在下面的参考文献中链接。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="2fef" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku mi kw kx ky mm la lb lc mq le lf lg lh im bi translated">如果你喜欢我的工作并想支持我，我会非常感谢你在我的社交媒体频道上关注我:</p><ul class=""><li id="189f" class="os ot it ko b kp kq ks kt mi ou mm ov mq ow lh ox oy oz pa bi translated">支持我的最好方式就是跟随我上<a class="ae li" href="https://medium.com/@whats_ai" rel="noopener"><strong class="ko iu"/></a>。</li><li id="92d4" class="os ot it ko b kp pb ks pc mi pd mm pe mq pf lh ox oy oz pa bi translated">订阅我的<a class="ae li" href="https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg" rel="noopener ugc nofollow" target="_blank">T5】YouTube频道T7】。</a></li><li id="6b64" class="os ot it ko b kp pb ks pc mi pd mm pe mq pf lh ox oy oz pa bi translated">在<a class="ae li" href="https://www.linkedin.com/company/what-is-artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> LinkedIn </strong> </a>上关注我的项目</li><li id="62fa" class="os ot it ko b kp pb ks pc mi pd mm pe mq pf lh ox oy oz pa bi translated">一起学习AI，加入我们的<a class="ae li" href="https://discord.gg/SVse4Sr" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> Discord社区</strong> </a>，<em class="kn">分享你的项目、论文、最佳课程，寻找Kaggle队友，等等！</em></li></ul></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><h2 id="18e5" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">参考</h2><p id="484a" class="pw-post-body-paragraph kl km it ko b kp mv ju kr ks mw jx ku mi mx kx ky mm my lb lc mq mz lf lg lh im bi translated">[1]柯，z .等人，<a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank">实时人体抠图真的需要绿屏吗？(2020) </a>，<a class="ae li" href="https://arxiv.org/pdf/2011.11961.pdf" rel="noopener ugc nofollow" target="_blank"/><br/>【2】柯，z .，<a class="ae li" href="https://github.com/ZHKKKe/MODNet" rel="noopener ugc nofollow" target="_blank"> GitHub for实时人体抠图真的需要绿屏吗？</a>(2020)<a class="ae li" href="https://github.com/ZHKKKe/MODNet" rel="noopener ugc nofollow" target="_blank"/><br/>【3】徐，n .等人<a class="ae li" href="https://sites.google.com/view/deepimagematting" rel="noopener ugc nofollow" target="_blank">深度图像抠图— Adobe研究</a>(2017)<a class="ae li" href="https://sites.google.com/view/deepimagematting" rel="noopener ugc nofollow" target="_blank"/><br/>【4】<a class="ae li" href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html" rel="noopener ugc nofollow" target="_blank">GrabCut算法由OpenCV</a><a class="ae li" href="https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="779c" class="lz ma it bd mb mc md dn me mf mg dp mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated">访问专家视图— <a class="ae li" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank">订阅DDI英特尔</a></h2></div></div>    
</body>
</html>