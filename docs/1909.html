<html>
<head>
<title>Easy Implementation of the Decision Tree with Python &amp; Numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python和Numpy轻松实现决策树</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae?source=collection_archive---------0-----------------------#2020-04-05">https://medium.datadriveninvestor.com/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae?source=collection_archive---------0-----------------------#2020-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20a5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于这种流行算法的简单而快速的阅读！</h2></div><p id="0666" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树是许多数据科学家最重要的算法之一，他们几乎每天都要适应Xgboost和其他基于树的算法。理解这种机器学习算法的基本思想和实现是至关重要的，以便建立更准确和更高质量的模型。在本文中，我将尝试用Python解释和实现基本的决策树分类器算法。我将使用著名的<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>来训练和测试模型。我们可以开始了吗？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/923b616ec0b22d159febbf0e33b68a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stLZzBVhgdwTgVQW6wi7pg.jpeg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Source <a class="ae le" href="https://unsplash.com/@niko_photos" rel="noopener ugc nofollow" target="_blank">niko photos</a>, via <a class="ae le" href="https://unsplash.com/photos/tGTVxeOr_Rs" rel="noopener ugc nofollow" target="_blank">unsplash</a> (CC0)</figcaption></figure><p id="58c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">准备虹膜数据集</strong></p><p id="0e96" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们加载数据集并显示数据帧的前几行。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/5f5c3f64a547618df3956874ae3f4fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*XUKOR1OtCe3BLOExKqNFIg.png"/></div></figure><p id="5cfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还将展示数据的配对图。这可以通过<a class="ae le" href="https://seaborn.pydata.org/generated/seaborn.pairplot.html" rel="noopener ugc nofollow" target="_blank"> seaborn.pairplot </a>函数来完成。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/046075a0a3c8711a99ab8c329d58458d.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*YwYq_ovUiwxv3gS4E5wZzw.png"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/73fe53959e0ec1b2fdd217b001df421d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CJop4qxXd2fe-ieBPm5jg.png"/></div></div></figure><p id="f22d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在pairplots上，你可以看到“花瓣长度”和“花瓣宽度”变量很好地分离了目标特征。</p><div class="ly lz gp gr ma mb"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd iu gy z fp mg fr fs mh fu fw is bi translated">认知计算——一套被广泛认为是……</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp lp mb"/></div></div></a></div><p id="ae8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，让我们将数据分成训练和测试部分。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0ca13549384ee0e0dc48499d60d75738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*KnpGyT8mKHiBmdutPpWiFw.png"/></div></figure><p id="52d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，当数据集准备好了，让我们转到决策树模型背后的理论和直觉。</p><p id="d271" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">决策树算法背后的直觉</strong></p><p id="3625" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我将使用CART模型来构建决策树，这个模型不是别的，而是一个简单的二叉树，就像这样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8d5c1ddfff6991806c6e60590a6b8973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*BXQYg743ilN-hC-X.jpg"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><a class="ae le" href="https://www.apa.org/science/about/psa/2018/04/classification-regression-trees" rel="noopener ugc nofollow" target="_blank">apa.org</a></figcaption></figure><p id="ff42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练一个简单的决策树模型的想法非常简单。在构建每个树节点时，我们执行算法，用以下伪代码表示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ms"><img src="../Images/07080b48078804aeec55670c4d453dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_jzABZLHPcfDei6BIbqQw.png"/></div></div></figure><p id="bd6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将按照这个算法递归地向我们的树添加新的节点，直到我们满足一个停止标准。在我的实现中，我将使用以下停止规则:<strong class="kk iu"> max_depth </strong>，<strong class="kk iu"> min_samples_in_leaf </strong>，<strong class="kk iu"> min_samples_split </strong>。</p><p id="80a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">示例:</p><p id="1ef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="mt">max _ depth = 3</em></strong>—树深度不能大于3</p><p id="a6ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="mt">min _ samples _ in _ leaf = 2</em></strong>—一个树节点不能少于2个样本。</p><p id="7026" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="mt">min _ samples _ split = 2</em></strong>—树节点中不能少于2个样本，我们在寻找最佳分裂列和阈值时。</p><p id="00c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，每一步我都在计算<strong class="kk iu">杂质</strong>值，那是什么？在本次实施中，我将使用<strong class="kk iu">基尼系数</strong>进行计算，计算公式如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fa37592406ccbd9795d3d459887a6734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*wXnnCmFHpb1uKh4HJAeQZA.png"/></div></figure><p id="e89e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> probas </strong>这里可以定义如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/929b711cf414b8c93c19820374fd2536.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*Yw2t6NBSGdRchbp_-n2erQ.png"/></div></figure><p id="f36c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，为了在上面的伪代码中执行<strong class="kk iu">find infinity</strong>函数，我应该执行以下操作:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mw"><img src="../Images/515a60598aeb03cfdd273387b402f9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXg4HC8OvJCv0cyI0593wA.png"/></div></div></figure><p id="ffc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这里做的是根据给定的阈值和列分割数据，我计算左右子节点的<strong class="kk iu"> G1 </strong>和<strong class="kk iu"> G2 </strong>。你可能已经注意到，在上面的伪代码中，我已经计算了Split之前的<strong class="kk iu">impurity</strong>值。在这个函数中，我们将使用它来计算<strong class="kk iu"> impurityAfterSplit </strong>值。</p><p id="95a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，总而言之，所有的训练程序，下面的函数将为我们训练一个决策树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mx"><img src="../Images/0bfc7d17dce150ab781f0756649f7748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XEQqMTUNp_hc0kolyLgPfw.png"/></div></div></figure><p id="e8c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，当我解释了购物车决策树的直觉后，让我们用Python和Numpy实现它！</p><p id="7634" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用Python和Numpy实现决策树</strong></p><p id="87be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先创建两个类，一个用于决策树中的节点，另一个用于决策树本身。</p><p id="88b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的节点类将如下所示:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a8eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于决策树类，我现在将创建一个框架，我们将继续填充它。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="d13b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先实现<strong class="kk iu"> gini </strong>和<strong class="kk iu"> nodeProbas </strong>方法，到目前为止我已经在我的伪代码中很好地解释了它们。这里没什么特别的:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5d23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">calcImpurity函数只是这两个函数的包装器。我们首先计算nodeProbas，然后将它们传递给基尼函数，这可以编码如下:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="63e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，是时候编写<strong class="kk iu"> calcBestSplit </strong>函数了。如上所述，我在这里所做的就是查看所有的对(列，来自数据[列]的值)，对于每个这样的对，我计算信息增益，最后只选择具有最佳信息增益的对。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a012" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">知道了如何在一个节点中找到最佳分裂，让我们实现构建决策树本身的函数。这里，我用一个根节点初始化树，然后递归地将数据分成左右两个子节点，直到满足我们的一个停止标准。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="801b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里的<strong class="kk iu"> Fit </strong>方法很简单，我只需要初始化根节点并运行<strong class="kk iu"> buildDT </strong>函数。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e86c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一步是实现预测方法。当一个对象<strong class="kk iu"> x </strong>在每一步到达树的根节点时，它与计算的(<strong class="kk iu">列</strong>，<strong class="kk iu">阈值</strong>)节点值进行比较，直到它以如下方式到达根节点:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a727c7ed25b7e7d5f4199b5b81c3a900.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*3nO_WvWXf-7b9eN3VsKWTg.png"/></div></figure><p id="0896" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总而言之，对于一个对象，预测看起来如下:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="154a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算所有对象的预测，我们只需要遍历它们，并对每个对象调用<strong class="kk iu"> predictSample </strong>函数。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="1193" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来这就是我们简单的决策树模型所需的所有代码。让我们用<a class="ae le" href="http://sklearn.tree.DecisionTreeClassifier" rel="noopener ugc nofollow" target="_blank">sk learnings的决策树分类器</a>来测试一下。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nb"><img src="../Images/f48b7e2633f878403cde192ba53f3c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfDeijRE_iBDZMx_dsJ9yg.png"/></div></div></figure><p id="05cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，Sklearns的实现不仅更快，因为他们的树构建过程更有效，而且Sklearns模型的准确性更高。总而言之，对于一个简单的决策树实现来说，这并不坏！你可以在下面找到整个Jupyter笔记本。感谢您的阅读:)</p><p id="e7aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">你可以在我的</strong> <a class="ae le" href="http://artkulakov.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">网站</strong> </a>上查看其他帖子</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="nc mz l"/></div></figure></div></div>    
</body>
</html>