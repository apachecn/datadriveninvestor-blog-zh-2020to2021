<html>
<head>
<title>5 AI/ML Research Papers on Object Detection You Must Read</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5篇关于物体探测的AI/ML研究论文，必读</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/5-ai-ml-research-papers-on-object-detection-you-must-read-1ad636b66697?source=collection_archive---------4-----------------------#2020-09-19">https://medium.datadriveninvestor.com/5-ai-ml-research-papers-on-object-detection-you-must-read-1ad636b66697?source=collection_archive---------4-----------------------#2020-09-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8311" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">伟大的论文…</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c3d7508daab053a9aeff94bb9aa92811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wQ99iTQe_0Qc44hI.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Pic credits: Pinterest</figcaption></figure><h1 id="0ca6" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">检测器:使用递归特征金字塔和可切换阿特鲁卷积检测对象</h1><p id="daeb" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作者:乔思远、陈良杰、艾伦·尤尔</p><h1 id="68b4" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="e5cd" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">许多现代物体检测器通过使用两次观察和思考的机制而表现出优异的性能。在本文中，我们探讨了这一机制的骨干设计对象检测。在宏观层面，我们提出了一个递归的特征金字塔，它将来自特征金字塔网络的额外反馈连接合并到自底向上的主干层中。在微观层面，我们提出了可切换的阿特鲁卷积，它以不同的速率对特征进行卷积，并使用切换函数收集结果。将它们组合在一起产生检测器，这显著提高了对象检测的性能。在COCO test-dev上，DetectoRS实现了最先进的54.7%的对象检测盒AP、47.1%的实例分割掩模AP和49.6%的全景分割PQ。</p><p id="57a1" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">论文可以在这里找到:</em> </strong></p><p id="1517" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">【https://arxiv.org/pdf/2006.02334v1.pdf T4】</p><p id="2bd2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">代码可以在这里找到:</em> </strong></p><div class="mq mr gp gr ms mt"><a href="https://github.com/joe-siyuan-qiao/DetectoRS" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">乔-思远-乔/探测仪</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">06/30/2020]检测器现在由MMDetection正式支持。非常感谢@xvjiarui、@ZwwWayne和@ hellock</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh kp mt"/></div></div></a></div></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="c96e" class="kv kw iq bd kx ky np la lb lc nq le lf jw nr jx lh jz ns ka lj kc nt kd ll lm bi translated">IterDet:拥挤环境中目标检测的迭代方案</h1><p id="9dbf" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作者:Danila Rukhovich，Konstantin Sofiiuk，Danil Galeev，Olga Barinova，Anton Konushin</p><h1 id="7f02" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="0aee" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">基于深度学习的检测器通常产生一组冗余的对象边界框，包括同一对象的许多重复检测。然后使用非最大抑制(NMS)来过滤这些框，以便为每个感兴趣的对象选择一个边界框。这种贪婪方案是简单的，并且为孤立的对象提供了足够的精度，但是在拥挤的环境中经常失败，因为需要为不同的对象保留盒子并且抑制重复检测。在这项工作中，我们开发了一个替代的迭代方案，其中在每次迭代中检测一个新的对象子集。在前一次迭代中检测到的盒子在下一次迭代中被传递给网络，以确保同一物体不会被检测两次。这种迭代方案可以应用于一级和两级对象检测器，只需对训练和推理过程进行较小的修改。我们在四个数据集上使用两种不同的基线检测器进行了大量的实验，并显示出相对于基线的显著改善，从而在CrowdHuman和WiderPerson数据集上实现了最先进的性能。</p><p id="5077" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">论文可以在这里找到:</em> </strong></p><p id="e686" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae mp" href="https://arxiv.org/pdf/2005.05708v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2005.05708v1.pdf</a></p><p id="8e99" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">代码可以在这里找到:</em> </strong></p><div class="mq mr gp gr ms mt"><a href="https://github.com/saic-vul/iterdet" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">saic-vul/iterdet</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">这个项目包含了实现IterDet对象检测方案的代码，正如我们在论文…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nu l ne nf ng nc nh kp mt"/></div></div></a></div></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="17bd" class="kv kw iq bd kx ky np la lb lc nq le lf jw nr jx lh jz ns ka lj kc nt kd ll lm bi translated">用于目标检测的单发细化神经网络</h1><p id="5b36" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作者:、、文、、、李</p><h1 id="2884" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="3463" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">对于对象检测，两阶段方法(例如，更快的R-CNN)已经实现了最高的精度，而一阶段方法(例如，SSD)具有高效率的优势。为了继承两者的优点，同时克服它们的缺点，本文提出了一种新的基于单次触发的检测器，称为RefineDet，它比两阶段方法获得了更好的精度，并且保持了与一阶段方法相当的效率。RefineDet由两个相互连接的模块组成，即锚点细化模块和对象检测模块。具体来说，前者旨在1。过滤掉负面锚以减少分类器的搜索空间，以及2 .粗略调整锚点的位置和大小，以便为后续回归变量提供更好的初始化。后一个模块将细化的锚作为来自前一个模块的输入，以进一步改进回归并预测多类标签。同时，我们设计了一个传输连接块来传输锚点细化模块中的特征，以预测对象检测模块中对象的位置、大小和类别标签。多任务丢失功能使我们能够以端到端的方式训练整个网络。在PASCAL VOC 2007、PASCAL VOC 2012和MS COCO上进行的大量实验表明，RefineDet实现了一流的检测精度和高效率。</p><p id="e9f6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">论文可以在这里找到:</em> </strong></p><p id="b643" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae mp" href="https://arxiv.org/pdf/1711.06897v3.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.06897v3.pdf</a></p><p id="799c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">代码可以在这里找到:</em> </strong></p><div class="mq mr gp gr ms mt"><a href="https://github.com/sfzhang15/RefineDet" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">sfzhang15/RefineDet</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">我们提出了一种新的基于单次触发的检测器，称为RefineDet，它比两阶段方法获得了更好的精度</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nv l ne nf ng nc nh kp mt"/></div></div></a></div></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="1638" class="kv kw iq bd kx ky np la lb lc nq le lf jw nr jx lh jz ns ka lj kc nt kd ll lm bi translated">体素网:基于点云的三维物体检测的端到端学习</h1><p id="7e54" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作者:尹舟，Oncel Tuzel</p><h1 id="3ce0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="3eae" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">三维点云中物体的精确检测是许多应用中的核心问题，例如自主导航、家务机器人和增强/虚拟现实。为了将高度稀疏的激光雷达点云与区域建议网络(RPN)连接起来，大多数现有工作都集中在手工制作的要素表示上，例如，鸟瞰图投影。在这项工作中，我们消除了对3D点云的手动特征工程的需要，并提出了体素网，这是一种通用的3D检测网络，它将特征提取和包围盒预测统一到单个阶段、端到端可训练的深度网络中。具体而言，体素网将点云划分为等距的3D体素，并通过新引入的体素特征编码(VFE)层将每个体素内的一组点转换为统一的特征表示。这样，点云被编码为描述性的体积表示，然后连接到RPN以生成检测。在KITTI汽车检测基准上的实验表明，VoxelNet的性能远远优于最先进的基于激光雷达的3D检测方法。此外，我们的网络学习了具有各种几何形状的物体的有效区分表示，从而基于唯一的激光雷达在行人和骑自行车者的3D检测中产生了令人鼓舞的结果。</p><p id="b851" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">论文可以在这里找到:</em> </strong></p><p id="39d9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">【https://arxiv.org/pdf/1711.06396v1.pdf T4】</p><p id="97c4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">代码可以在这里找到:</em> </strong></p><div class="mq mr gp gr ms mt"><a href="https://github.com/charlesq34/pointnet" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">charlesq34/pointnet</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">由来自斯坦福大学的Charles R. Qi，，Kaichun Mo，Leonidas J. Guibas创建。这项工作是基于我们的…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nw l ne nf ng nc nh kp mt"/></div></div></a></div><div class="mq mr gp gr ms mt"><a href="https://github.com/qianguih/voxelnet" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">qianguih/voxelnet</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">这是一个非官方的体素网实现:端到端学习的点云为基础的三维物体检测在…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nx l ne nf ng nc nh kp mt"/></div></div></a></div></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><h1 id="13da" class="kv kw iq bd kx ky np la lb lc nq le lf jw nr jx lh jz ns ka lj kc nt kd ll lm bi translated">用CNN检测艺术品中的人物</h1><p id="0b24" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">尼古拉斯·韦斯特莱克，彼得·霍尔蔡洪平</p><h1 id="d04a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="1bb8" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">CNN极大地提高了照片中物体检测的性能。然而，对艺术品中物体检测的研究仍然有限。我们在具有挑战性的数据集People-Art上展示了最先进的性能，该数据集包含来自照片、漫画和41种不同艺术运动的人物。我们通过为这项任务微调CNN来实现这种高性能，因此也证明了在照片上训练CNN会导致照片的过度适应:只有前三层或四层从照片转移到艺术作品。虽然CNN的性能是最高的，但它仍然低于60%的AP，这表明需要进一步的工作来解决交叉描绘问题。</p><p id="c431" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">论文可以在这里找到:</em> </strong></p><p id="773b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae mp" href="https://arxiv.org/pdf/1610.08871v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1610.08871v1.pdf</a></p><p id="1f5a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> <em class="mo">代码可以在这里找到:</em> </strong></p><div class="mq mr gp gr ms mt"><a href="https://github.com/BathVisArtData/PeopleArt" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">BathVisArtData/PeopleArt</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">人物-艺术数据集人物-艺术数据集是一个来自照片和艺术作品的图像数据集，具有地面真实边界…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="ny l ne nf ng nc nh kp mt"/></div></div></a></div></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="152a" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">参考文献和致谢—</p><div class="mq mr gp gr ms mt"><a href="https://arxiv.org/" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">arXiv.org</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">arXiv是一个免费的分发服务和开放存取的档案库，包含1，721，837篇学术文章，涉及领域包括…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>