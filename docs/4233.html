<html>
<head>
<title>Gradient Descent: How Neural Networks Learns</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降:神经网络如何学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/gradient-descent-how-neural-networks-learns-ce79d99d7771?source=collection_archive---------8-----------------------#2020-07-29">https://medium.datadriveninvestor.com/gradient-descent-how-neural-networks-learns-ce79d99d7771?source=collection_archive---------8-----------------------#2020-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f287" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">在本文中，我们将了解神经网络中的梯度下降。我们将介绍基本原理及其工作原理。所以让我们先了解一下。</em></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="ab gu cl kr"><img src="../Images/221a15ce65fb686054e56e6217430487.png" data-original-src="https://miro.medium.com/v2/format:webp/1*LBJeamY7PqOBSK0w0Ci2iw.jpeg"/></div></figure><blockquote class="ku kv kw"><p id="e54b" class="jn jo kl jp b jq jr js jt ju jv jw jx kx jz ka kb ky kd ke kf kz kh ki kj kk ij bi translated"><strong class="jp ir">梯度下降</strong>是一种优化算法，用于通过沿梯度负值定义的最陡下降方向迭代移动来最小化某个函数。在机器学习中，我们使用梯度下降来更新模型的参数。参数是指线性回归中的系数和神经网络中的权重。</p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="ab gu cl kr"><img src="../Images/1810ad038f8b7d9eb9c19d3e9f41e536.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Su7d1-ec375MpT746wzwYw.png"/></div></figure><p id="be28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在成本函数的上下文中考虑三维图形。我们的目标是从右上角的山(高成本)移动到左下角的深蓝色的海(低成本)。箭头表示从任意给定点开始的最陡下降方向(负梯度)——尽可能快地降低成本函数的方向</p><p id="38f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从山顶开始，我们沿着负梯度指定的方向开始第一步下坡。接下来，我们重新计算负梯度(传递新点的坐标),并在它指定的方向上再走一步。我们迭代地继续这个过程，直到我们到达图表的底部，或者到达我们不能再向下移动的点——局部最小值。</p><h1 id="0308" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">学习速度:</strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/c8860b910fcfdee31abf632013f3d1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*YxljGzBY4EHKAt7H"/></div></figure><blockquote class="ku kv kw"><p id="eea4" class="jn jo kl jp b jq jr js jt ju jv jw jx kx jz ka kb ky kd ke kf kz kh ki kj kk ij bi translated">达到最小值或底部所需的步长称为<strong class="jp ir">学习速率</strong>。随着学习速度的提高，我们每一步可以走更多的路，但是我们有可能会越过最低点，因为山坡的坡度是不断变化的。由于学习率很低，我们可以自信地向负梯度的方向前进，因为我们经常重新计算它。低学习率更精确，但是计算梯度很费时间，所以我们要花非常长的时间才能追根究底。</p></blockquote><h1 id="2145" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">成本函数</strong></h1><blockquote class="ku kv kw"><p id="31d6" class="jn jo kl jp b jq jr js jt ju jv jw jx kx jz ka kb ky kd ke kf kz kh ki kj kk ij bi translated">一个<strong class="jp ir">损失函数(误差函数)/成本函数</strong>告诉我们，我们的模型对于给定的一组参数的预测“有多好”。成本函数有自己的曲线和自己的梯度。这条曲线的斜率告诉我们如何更新我们的参数，使模型更加准确。</p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/3c6939aec17665992bb92a93ce56ef16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*2GOuCGB19pLNY8vd.jpg"/></div></figure><ol class=""><li id="2bd7" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk mf mg mh mi bi translated"><strong class="jp ir">向哪个方向下降(考虑前两个图)？</strong></li><li id="3d6e" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk mf mg mh mi bi translated"><strong class="jp ir">下降多少？步长应该是多少？</strong></li></ol><p id="5126" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初，我们采取大的步骤，逐渐地，我们开始一小步一小步地达到全局最小值。如果我们不随时间改变步长，我们可能会超过全局最小值，我们的模型可能永远不会收敛。因此，我们需要使用自适应学习速率，而不是使用恒定的学习速率，以便我们能够根据条件改变步长。梯度下降的味道很多，我们就讨论其中的几种吧。</p><h1 id="a3df" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">批量梯度下降(BGD) </strong></h1><p id="3f07" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在批量梯度下降中，我们在一次迭代中处理整个训练数据集。一旦一个历元完成，权重就被更新。</p><h1 id="505d" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">随机梯度下降(SGD) </strong></h1><p id="021e" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在随机梯度下降中，我们在每次迭代中处理来自训练数据集的单个观察值(而不是整个数据集)。我们计算误差、梯度和新权重，并针对训练数据集中的每个观察值不断更新模型。</p><h1 id="8482" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">小批量梯度下降(MBGD) </strong></h1><p id="2a80" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在小批量梯度下降中，我们在每次迭代中处理训练数据集的一个小子集。换句话说，我们可以说这是BGD和SGD之间的妥协。</p><p id="66ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:批量大小是一个非常重要的超参数。它可能因数据集而异。因此，决定批量是非常关键的一步。</p><h1 id="a1e1" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">新币的变种</strong></h1><p id="429f" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">1.基于动量(内斯特罗夫动量)</p><p id="008c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.基于自适应学习率(Adagrad、AdaDelta、RMSprop)</p><p id="85b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.动量和自适应学习率的组合(Adam)</p><h1 id="0b29" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">气势</strong></h1><p id="dda9" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">动量有助于在相关方向上加速SGD。所以，考虑每个参数的动量是个好主意。它具有以下优点:</p><p id="f283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.避免局部极小值:由于动量增加了速度，因此增加了步长，优化器将不会陷入局部极小值。</p><p id="fd6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.更快的收敛:动量使得收敛更快，因为它由于获得的速度而增加步长。</p><h1 id="7c66" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">内斯特罗夫势头</strong></h1><p id="d66d" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">它找出当前的动量，并在此基础上近似下一个位置。然后，它计算梯度w.r.t下一个近似位置，而不是计算梯度w.r.t当前位置。这个东西可以防止我们走得太快，并提高响应速度，从而显著提高SGD的性能。</p><h1 id="828e" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">阿达格拉德</strong></h1><p id="cd20" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">它主要关注自适应学习速率，而不是动量。</p><p id="cd80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在标准SGD中，学习率总是不变的。这意味着，不管坡度如何，我们都必须以相同的速度前进。这在现实生活中似乎不切实际。如果我们知道应该减速还是加速，会发生什么？如果我们知道应该在这个方向加速，在那个方向减速，会发生什么？使用标准SGD是不可能的。</p><p id="ed29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Adagrad不断更新学习率，而不是使用恒定的学习率。它累加所有梯度的平方和，并使用它来归一化学习率，因此现在学习率可以根据过去梯度的表现而变小或变大。</p><p id="4141" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它使学习率适应参数，对与频繁出现的特征相关联的参数执行较小的更新(即，低学习率)，对与不频繁出现的特征相关联的参数执行较大的更新(即，高学习率)。因此，它非常适合处理稀疏数据。</p><h1 id="a732" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak"> AdaDelta和RMSprop(均方根prop) </strong></h1><p id="2f87" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">AdaDelta和RMSprop是Adagrad的扩展。</p><p id="b705" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如在Adagrad部分所讨论的，Adagrad累加所有梯度的平方和，并使用它来归一化学习速率。因此，阿达格拉德遇到了一个问题。问题是阿达格勒的学习率持续下降，在某一点上学习几乎停止。</p><p id="44a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了处理这个问题，AdaDelta和RMSprop衰减过去累积的梯度，因此只考虑过去梯度的一部分。现在，我们不考虑所有过去的梯度，而是考虑移动平均。</p><h1 id="2a6c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">亚当(自适应矩估计)</strong></h1><p id="d969" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">Adam是最好的梯度下降优化器，被广泛使用。它利用动量和适应性学习的力量。换句话说，亚当是有动量的RMSprop或AdaDelta。</p><p id="a6d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，梯度计算如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/ec7e11076ee87bb32c1d13b722f1605d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J-eDxGeEgpf-MVN3.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Gradient in RMSprop</figcaption></figure><p id="9c56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，权重和偏差矩阵更新如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nc"><img src="../Images/7e8c2e4571f86d69fc820581f8dbb025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mWyCnbPRrNWzdvEw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Weight and bias update in RMSprop</figcaption></figure><p id="1f92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意<em class="kl">β2</em>是一个新的超参数(不要与动量的<em class="kl">β</em>相混淆)。另外，<em class="kl">ε</em>是一个非常小的值，以防止被0除。</p><p id="6370" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，结合动量和RMSprop，Adam引入了四个超参数:</p><ul class=""><li id="466a" class="ma mb iq jp b jq jr ju jv jy mc kc md kg me kk nd mg mh mi bi translated">学习率<em class="kl">α</em></li><li id="3d1e" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk nd mg mh mi bi translated"><em class="kl">β</em>来自动量(通常为0.9)</li><li id="aaeb" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk nd mg mh mi bi translated">来自RMSprop的<em class="kl"> beta2 </em>(通常为0.999)</li><li id="75dc" class="ma mb iq jp b jq mj ju mk jy ml kc mm kg mn kk nd mg mh mi bi translated"><em class="kl">ε</em>(通常为1e-8)</li></ul><p id="73d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，您通常不需要调整<em class="kl"> beta </em>、<em class="kl"> beta2 </em>和<em class="kl"> epsilon </em>，因为上面列出的值通常会工作得很好。为了加速训练，只需要调整学习速率。</p><p id="1b8e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论:</strong>上述大部分梯度下降方法已经在TensorFlow、Keras、Theano、Caffe等流行的深度学习框架中实现。然而，Adam是目前推荐使用的默认算法，因为它利用了动量和自适应学习特性。</p><blockquote class="ku kv kw"><p id="2129" class="jn jo kl jp b jq jr js jt ju jv jw jx kx jz ka kb ky kd ke kf kz kh ki kj kk ij bi translated">请注意，更好的方法只不过是更快的方法。给定更多的时期，其他方法可以给出更好的准确度分数。不过，这个练习的目的是评估每种方法的速度，Adam显然是赢家。</p></blockquote><h1 id="09e9" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">局部和全局最小值</strong></h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ne"><img src="../Images/35fe6ca0122685839a3f06da13246303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1iHAmxEOIZwOPXZw.png"/></div></div></figure><p id="c500" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如上所述，梯度下降优化器的任务是找出参数的最佳权重。但有时，它可能会找到小于最佳值的权重，从而导致模型不准确。理想情况下，我们的SGD应该达到全局最小值，但有时它会陷入局部最小值，并且很难知道我们的SGD是处于全局最小值还是陷入局部最小值。</p><p id="f894" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何避免局部极小？</strong></p><p id="6fe4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">局部最小值是梯度下降的一个主要问题。超参数调整在避免局部极小值方面起着至关重要的作用。这个问题没有通用的解决方案，但是我们可以使用一些方法来避免局部极小值。</p><p id="c6d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.增加学习率:如果算法的学习率太小，那么SGD更有可能陷入局部极小值。</p><p id="b0f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.在更新权重时添加一些噪声:在权重中添加随机噪声有时也有助于找出全局最小值。</p><p id="1972" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.分配随机权重:使用随机起始权重进行重复训练是避免这个问题的流行方法之一，但是它需要大量的计算时间。</p><p id="abfe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.使用大量隐藏层:层中的每个隐藏节点以不同的随机起始状态开始。这允许每个隐藏节点收敛到网络中的不同模式。参数化该大小允许神经网络用户潜在地在单个神经网络中尝试数千个(或数百亿个)不同的局部最小值。</p><p id="8759" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">5.最重要的一点:使用基于动量和自适应学习的SGD:如上所述，不要使用传统的梯度下降优化器，而是尝试使用Adagrad、AdaDelta、RMSprop和Adam等优化器。Adam使用动量和自适应学习速率来达到全局最小值。</p><p id="61eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有时局部最小值和全局最小值一样好</p><blockquote class="ku kv kw"><p id="b52f" class="jn jo kl jp b jq jr js jt ju jv jw jx kx jz ka kb ky kd ke kf kz kh ki kj kk ij bi translated">通常，并不总是需要达到真正的全局最小值。通常认为大多数局部最小值具有接近全局最小值的值。有很多论文和研究表明，有时达到全局最小值并不容易。因此，在这些情况下，如果我们设法找到一个和全局最小值一样好的最优局部最小值，我们应该使用它。</p></blockquote><p id="97eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！将来我也会写更多的<strong class="jp ir"/><strong class="jp ir">文章</strong>。<a class="ae nf" href="https://medium.com/@vijay_choubey" rel="noopener"> <strong class="jp ir">跟随</strong> </a> me上<a class="ae nf" href="https://medium.com/@vijay_choubey" rel="noopener"> <strong class="jp ir">中</strong> </a>了解他们。我也是一名自由职业者，如果有一些数据相关项目的自由职业工作，请随时通过<a class="ae nf" href="https://www.linkedin.com/in/vijay-choubey-3bb471148/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Linkedin </strong> </a>联系。没有什么比做真正的项目更好的了！</p><h1 id="7b80" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">如果你喜欢这篇文章，请鼓掌！</h1></div></div>    
</body>
</html>