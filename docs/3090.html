<html>
<head>
<title>Understanding Bigrams: Cipher Decryption with Language Modeling and Genetic Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解二元模型:用语言建模和遗传算法进行密码解密</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/understanding-bigrams-cipher-decryption-with-language-modeling-and-genetic-algorithm-1283ee0b3b80?source=collection_archive---------2-----------------------#2020-05-31">https://medium.datadriveninvestor.com/understanding-bigrams-cipher-decryption-with-language-modeling-and-genetic-algorithm-1283ee0b3b80?source=collection_archive---------2-----------------------#2020-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4e31" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="d8e5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">这个项目如何帮助我理解那些传说中的单词二元模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6af25a18dfd04d7c628bcab0cb820b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEjWWtqY5HUwAlNDDcfPQw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@fantasyflip?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Philipp Katzenberger</a> on <a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c22e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">是时候开始一些自然语言处理的新的学习冒险了。</p><p id="9a25" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">什么？这难道不是由垃圾邮件检测项目启动的吗？正如你可能会想到的，密码解密可能是一个奇怪的地方开始一个NLP系列。这个起点与我从更深层次理解嵌入这个词的愿望有着密切的联系。在SpaCy和NLTK等开源NLP包的帮助下，我们能够从一个巨大的训练文本语料库中生成一个功能齐全的单词嵌入，只需一行代码——对我来说就像魔术一样。虽然当我们希望在相对较短的时间内完成一些复杂的任务，而不必太担心第一步的细节时，利用这些工具的力量绝对是非常好的，但对于那些仍然试图掌握自然语言处理这一迷人领域的本质的人来说，这肯定不是理想的。在<a class="mn mo ep" href="https://medium.com/u/5eeae1d5b388?source=post_page-----1283ee0b3b80--------------------------------" rel="noopener" target="_blank">懒惰的程序员</a>开发的一门令人惊叹的课程的帮助下(链接到课程<a class="ae lh" href="https://www.udemy.com/course/data-science-natural-language-processing-in-python/" rel="noopener ugc nofollow" target="_blank">这里</a>，我能够实现这个有趣的项目，使我能够完全理解构建大多数单词嵌入的底层算法——二元模型。</p><p id="5f02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这怎么可能呢？先简单说一下什么是密码解密，以及它的应用。</p><h2 id="6f21" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">什么是密码解密？</h2><p id="aeb4" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">密码解密，简单来说就是一个代入的过程。英语中有26个字母——这些字母组成单词，而单词组成表达意思的序列。如果我们随意打乱这些句子中字母的顺序，这些序列的意思就会变得模糊不清。然而，如果我们知道这些随机洗牌的精确顺序，我们将能够逆转这些字母的洗牌，使句子再次有说服力。这个混洗过程的字母映射是密码解密映射。下面是一个说明性的例子。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/e5041141fe7ac2827bd72aae33ff789e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P2Z7RmPE2zUGnUYl.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Source: Matt Crypto from <a class="ae lh" href="https://en.wikipedia.org/wiki/File:Caesar_substition_cipher.png" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="c9e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们用<strong class="lk jd"> <em class="nn">替换<strong class="lk jd"><em class="nn">a</em></strong>d</em></strong><strong class="lk jd"><em class="nn">b</em></strong>用<strong class="lk jd"> <em class="nn"> e </em> </strong>，…单词<strong class="lk jd"> <em class="nn">苹果</em> </strong>就会变成<strong class="lk jd"> <em class="nn"> dssoh </em> </strong>。由于我们记住了这种一对一的关系，我们将能够通过反向映射回到苹果。</p><h2 id="68d6" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">与自然语言处理的关系</h2><p id="6d11" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">这和NLP有什么关系？好吧，假设你不知道上面的映射，我们怎么猜映射是什么？事实证明，我们可以采用迭代方法来解决这个问题——从随机初始化的映射开始，迭代地最小化从真实映射生成的解密结果和从估计映射生成的结果之间的差异。这个差异度量就是这个解密单词在英语中出现的“可能性”的对数可能性。构建这个评估指标需要我们生成所有26个字母的所有字母组合的所有可能性—<strong class="lk jd"><em class="nn">【aa】</em></strong>，<strong class="lk jd"> <em class="nn"> ab </em> </strong>，<strong class="lk jd"><em class="nn">AC</em></strong>…<strong class="lk jd"><em class="nn">yz</em></strong>，<strong class="lk jd"> <em class="nn"> zz </em> </strong>。一些组合比其他组合更有可能，这种可能性将从一些巨大的文本语料库中推断出来。哦，等等，那就是我们所说的<strong class="lk jd">二元模型</strong>。</p><h2 id="6d9c" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">该项目的总体结构</h2><p id="6c94" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">现在我们理解了这些关系，我们可以谈谈这个项目是如何构建的。我们首先从一些包含真实语言单词的巨大文本语料库中构建这个语言模型，并生成这些二元模型的所有频率。这个模型会被保存到一个名为<code class="fe no np nq nr b">LanguageModel</code>的python模块中，以便经常访问，评估加密过程的正确性。然后我们将构建真正的加密映射、加密和解密方法，存储在一个名为<code class="fe no np nq nr b">Encoder</code>的python模块中。最后，我们还将构建一个模块<code class="fe no np nq nr b">GeneticAlgorithm</code>，它处理这个加密映射的迭代猜测，并选择更好、更正确的加密映射，直到选择出最佳映射。</p><p id="502d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能对这个项目的面向对象方法有疑问——因为可读性和构建难度会比函数方法显著增加。然而，使用OOP可以让我们拥有一个只包含几行代码的用户界面。</p><p id="8147" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">文件的总体项目结构如下</p><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="0a06" class="mp mq it nr b gy nw nx l ny nz">/cipher_decryption</span><span id="3757" class="mp mq it nr b gy oa nx l ny nz">   |--/model<br/>      |--language_model.py<br/>      |--encoder.py<br/>      |--genetic_algorithm.py</span><span id="7fe4" class="mp mq it nr b gy oa nx l ny nz">   |--/notebooks<br/>      |--Cipher Decryption.ipynb</span><span id="53d3" class="mp mq it nr b gy oa nx l ny nz">   |--cipher.py</span></pre><p id="43e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过设置上面的文件夹层次结构，我们将能够像下面这样导入模块——我们都非常熟悉的方式。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="b7b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe no np nq nr b">Cipher Decryption.ipynb</code>包含勘探和评价结果，在这里可以找到<a class="ae lh" href="https://nbviewer.jupyter.org/github/chen-bowen/Natrual_Language_Processing_Projects/blob/master/cipher/notebook/Cipher%20Decryption.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="5ec0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">想想我们刚刚完成了什么——我们基本上重新创建了我们习惯使用的NLP包，只是这次我们自己构建了这些包。事实上，这通常是软件项目在真实行业中的构建方式——下游用户应该能够拥有一个简单的用户界面，而不必太担心他/她试图使用的组件的细节。在下一节中，让我们深入研究上面提到的这3个模块的一些实现细节。</p><h2 id="c12c" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">语言模型</h2><p id="35ad" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">语言模型模块包含可用一行代码访问的二元模型概率</p><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="ffa7" class="mp mq it nr b gy nw nx l ny nz">LanguageModel().log_M</span></pre><p id="b023" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个<code class="fe no np nq nr b">log_M</code>属性包含英语中任何两个字母组合出现的对数概率。由于有26个字母，<code class="fe no np nq nr b">log_M</code>的维数为26 × 26。一个说明性的例子<code class="fe no np nq nr b">M</code>如下所示(所有假设值)，</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="8fc9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">参考上面的例子，我们得到看到“aa”组合的概率是0.04，而看到“be”组合的概率是0.07。但是我们如何产生这些价值呢？</p><p id="7746" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些值是通过计数生成的。对于我们的文本语料库中任何单词的每个双字符组，我们只需计算出现的次数，并将该次数除以文本语料库中的总单词数—这将产生一个介于0和1之间的数字，它满足概率的定义。</p><p id="2b87" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面的代码片段生成这个组合矩阵，并将其保存为<code class="fe no np nq nr b">LanguageModel</code>对象的属性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="1429" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为什么这两个字母的组合能正确地建立起典型的英语句子？它是基于一个叫做<strong class="lk jd"> <em class="nn">马尔可夫性质</em> </strong>的假设——它声明字母的下一次出现只依赖于前一个字母，而不是之前的任何字母。用数学术语来说，</p><pre class="ks kt ku kv gt ns nr nt nu aw nv bi"><span id="63b7" class="mp mq it nr b gy nw nx l ny nz">P(letter n | letter n-1, letter n-2, ...) = P(letter n | letter n-1)</span></pre><p id="6865" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一特性实质上表明，我们可以通过单词中两个字母组合的概率来恰当地模拟整个单词的概率。</p><p id="8772" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe no np nq nr b">LanguageModel</code>也有两种方法可以很容易地获得单词和句子的对数属性，这是简单的迭代循环，计算单词中所有字母组合的对数概率，并将它们相加以获得总的对数概率。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="402e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了得到句子的对数概率，我们可以简单地多次调用上面的<code class="fe no np nq nr b">get_log_word_probability</code>，并对结果求和。</p><p id="b02e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如您可能已经看到的，我们已经使用计算训练文本语料库中出现的二元模型的比例的方法构建了自己的二元模型概率矩阵。尽管有其他变化，大多数二元模型——不考虑字符或单词水平——都是以类似的方式构建的。此外，我们将在实例化<code class="fe no np nq nr b">LanguageModel</code>类时调用<code class="fe no np nq nr b">build</code>方法(如上所示),这允许<code class="fe no np nq nr b">LanugageModel</code>自动计算属性<code class="fe no np nq nr b">log_M</code>,并使其可供下游使用。此模块的完整实现在此处链接<a class="ae lh" href="https://github.com/chen-bowen/Language_Modeling_in_Cipher_Decryption/blob/master/cipher_decryption/model/language_model.py" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="4c85" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">编码器</h2><p id="f628" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">编码器模块将通过随机打乱26个字母来生成加密的真实映射，并保持一对一的关系。这个功能是由私有方法<code class="fe no np nq nr b">build_cipher_mapping</code>完成的</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="725b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还将构建执行输入句子编码/解码任务的方法</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="d919" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe no np nq nr b">Encoder</code>的完整实现如这里的<a class="ae lh" href="https://github.com/chen-bowen/Language_Modeling_in_Cipher_Decryption/blob/master/cipher_decryption/model/encoder.py" rel="noopener ugc nofollow" target="_blank">链接所示。</a></p><h2 id="a82d" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">遗传算法</h2><p id="5595" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">在<code class="fe no np nq nr b">LanguageModel</code>和<code class="fe no np nq nr b">Encoder</code>模块的实用方法的帮助下，<code class="fe no np nq nr b">GeneticAlgorithm</code>模块包含猜测和改进估计加密图的迭代过程。一般过程如下，</p><ol class=""><li id="9dee" class="od oe it lk b ll lm lo lp lr of lv og lz oh md oi oj ok ol bi translated">多次随机猜测加密映射并将其保存到一个集合中</li><li id="ae2b" class="od oe it lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated">使用<code class="fe no np nq nr b">Encoder</code>模块中的<code class="fe no np nq nr b">decode</code>方法，根据步骤1中生成的加密映射对编码的输入消息进行解密</li><li id="e97c" class="od oe it lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated">通过使用<code class="fe no np nq nr b">LanguageModel</code>模块的<code class="fe no np nq nr b">get_sentence_log_probability</code>方法计算信息的对数似然来评估每个解码信息。从最高到最低排列解密消息的所有对数似然值</li><li id="cbd1" class="od oe it lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated">选择具有最高对数似然值的前5个加密映射。随机交换这5个加密映射中的1对映射，并将结果保存到集合中。从步骤2开始重复该过程</li></ol><p id="462b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">经过几次迭代，更“正确”的加密图将被保留下来——这将使我们越来越接近真正的加密图。这个迭代过程在名为<code class="fe no np nq nr b">train</code>的类方法中实现</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="0661" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe no np nq nr b">GeneticAlgorithm</code>模块的完整实现见<a class="ae lh" href="https://github.com/chen-bowen/Language_Modeling_in_Cipher_Decryption/blob/master/cipher_decryption/model/genetic_algorithm.py" rel="noopener ugc nofollow" target="_blank">这里</a>的链接。</p><h2 id="184b" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">结果</h2><p id="4e24" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">迭代过程中的解密进度会是怎样的？提醒一下，下面是我们输入的来自小说<strong class="lk jd"> <em class="nn">福尔摩斯</em> </strong>的消息，</p><p id="d24d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我在街上闲逛，不出我所料，在花园的一堵墙边的一条小巷里有一个马厩。我帮助马夫夫妇给他们的马擦身，作为交换，我得到了两便士，一杯对半牌香烟，两包粗毛烟草，以及我所能得到的关于阿德勒小姐的尽可能多的信息，更不用说附近的六个人了，我对他们一点也不感兴趣，但我不得不听他们的传记。”</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/92b6a669e70db9f22d122ccca3cb0470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0JL2oJibOuQ_ZaZ6loM1A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">decryption evolution over 150 iterations</figcaption></figure><p id="27db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上所示，解密后的信息从完全不可读变成非常非常接近真实的信息。我们算法唯一弄错的两个字符是<strong class="lk jd"> <em class="nn"> k </em> </strong>和<strong class="lk jd"> <em class="nn"> z — </em> </strong>作为单词<strong class="lk jd"> <em class="nn">打</em> </strong>解密为<strong class="lk jd"> <em class="nn"> doken </em> </strong> <em class="nn">。</em>由于算法偏向于更常见的二元模型，二元模型<strong class="lk jd"> <em class="nn"> ok </em> </strong>确实比<strong class="lk jd"><em class="nn">oz</em></strong><em class="nn"/>更常见，从而产生更高的对数似然。</p><h2 id="16dd" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">万岁！结论</h2><p id="cbf3" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">是啊，我们刚刚经历了很多有趣的事情。随着密码加密的应用，我们已经创建了自己的语言模型，它包含了所有26 × 26可能二元模型的所有概率——具有真正直观的用户界面。我们自己构建二元模型矩阵将允许我们获得对最经典的NLP技术之一的更多见解，这对于我们学习更复杂的技术(如Word2Vec和Glove)将是巨大的。</p><p id="b7df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我希望你能和我一样享受这个过程。下次见！</p></div></div>    
</body>
</html>