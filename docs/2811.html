<html>
<head>
<title>Gradient Boosting For Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归的梯度推进</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/gradient-boosting-for-regression-539fa8aa4b00?source=collection_archive---------1-----------------------#2020-05-17">https://medium.datadriveninvestor.com/gradient-boosting-for-regression-539fa8aa4b00?source=collection_archive---------1-----------------------#2020-05-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9cf6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度推进是一种用于回归和分类问题的机器学习集成技术，通过集成几个弱学习器尤其是决策树来产生输出。</p><p id="5dab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度推进可以简化为三句话:</p><ol class=""><li id="bbc6" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">要优化的损失函数</li><li id="8f5a" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">预测能力差的学习者</li><li id="36c9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">最终模型增加了这些弱学习者，以最小化损失并做出更好的预测</li></ol><p id="4283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的算法看起来如下:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi kz"><img src="../Images/a1042bf3b9852292e38d5cddd16bfd61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxLUho43QmljxmZOyDBJew.png"/></div></div><figcaption class="ll lm gj gh gi ln lo bd b be z dk"><a class="ae lp" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_boosting</a></figcaption></figure><p id="b1bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不要害怕！我会用例子详细解释每一步。</p><p id="a130" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将从回归的角度解释梯度推进。考虑一个简单的回归问题，我们希望在给定身高和性别的情况下预测体重。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/66d0479791a817b0a6f9a848f7a5552d.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*CZUGvmMp6ZNfDFoFYNazLA.png"/></div></figure><p id="def7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们选择均方误差作为损失函数。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/9b3f2e109a444e4b76cb588f8378f605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*AtKStXEii-C5ya3eUvWbpg.png"/></div></figure><p id="301c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:重要的是我们的损失函数是可微的。</p><p id="364d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">步骤1:用常数值F0(x) </strong>初始化我们的模型</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/c69c0795800c28bc783b62922f0fe6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*-a454yoSNbl81Z-5fXA0Wg.png"/></div></figure><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="d962" class="ly lz iq lu b gy ma mb l mc md">F0(x) = argmin  [(1/2)(88-predicted)^2 + (1/2)(76-predicted)^2 +<br/>          γ                          (1/2)(56 - predicted)^2 ]</span></pre><p id="6ad2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了解决优化问题，我们将梯度设置为零并求解</p><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="a49d" class="ly lz iq lu b gy ma mb l mc md">dL(yi,F0(x))<br/>----------- = 0<br/>F0(x)</span><span id="6030" class="ly lz iq lu b gy me mb l mc md">-(88- predicted) - (76-predicted) - (56-predited) = 0</span><span id="ac8b" class="ly lz iq lu b gy me mb l mc md">=&gt; predicted = (88+76+56+3)/3  </span><span id="6d86" class="ly lz iq lu b gy me mb l mc md">=&gt; F0(x) = 73.3</span></pre><p id="8458" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">第二步:</strong></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi mf"><img src="../Images/b2bbd9d7ea3819304f4544981f705a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ad4lZSgUiRrpH-Hf7c7MA.png"/></div></div></figure><p id="249c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个循环，其中M代表树的总数。通常我们考虑M= 100。因此，对于每棵树，我们做如下。</p><p id="b071" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2.1计算psudo残差</strong></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0baa58f79e7dcfac4eb26a9c09cf1912.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*u3Q538UfdZdyuCatERl29A.png"/></div></figure><p id="e63f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是损失函数wrt预测值的导数。</p><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="c4c5" class="ly lz iq lu b gy ma mb l mc md">rim = -(observed - predicted)</span></pre><p id="17d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们有，</p><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="8434" class="ly lz iq lu b gy ma mb l mc md">r11 = (88-73.3) = 14.6<br/>r12 = (76-73.3) = 2.7<br/>r13 = (56-73.3) = -17.3</span></pre><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/16657c91d13d2c42905195a7d2f3c7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*KG9ec6Pe7djlAkqWAuBadQ.png"/></div></figure><p id="de2d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:GBDT算法是以这个等于预测负梯度的psudo残差命名的。</p><p id="0329" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.2将一个基础学习器(决策树)拟合到这个psudo残差中，并将末端叶节点命名为Rjm，其中j=1，2…Jm </p><p id="8fd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里我将创建一个深度为1的决策树(stump ),因为我的例子很小。通常对于梯度推进，我们会考虑更深的决策树。我们通常不使用树桩。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/914ec6952d26a461600ecc348d2d0f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*eCswPof5I3qzYjfBu40Ejg.png"/></div></figure><p id="ac7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设这是我们创建的决策树。如果你不知道如何构造决策树，你可以参考我的文章  中的<a class="ae lp" href="https://medium.com/datadriveninvestor/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38" rel="noopener"> <strong class="jp ir"> <em class="mj">用实例演示构造决策树。现在标记终端区域。这部分非常简单，因为叶子是末端区域。</em></strong></a></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ac0d0e037f48d086e6893ebe4af115a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*96q5KLi6xUG0VNry4bBIcA.png"/></div></figure><p id="05e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2.3。对于j = 1…Jm计算γjm </strong></p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/4f64e761791f842349a8d840ff7ba5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*J0C2WzFwql3SIWxqbxQwSQ.png"/></div></figure><p id="2285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这一步中，我们将计算每片叶子的输出值。每个叶子的输出值是使损失函数最小化的伽马值。这类似于我们初始化F0(x)的第一步。但是在这一步中，我们将考虑我们之前的预测(Fm-1(xi))。</p><p id="846a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Rij的xi元素意味着该特定叶节点中的所有元素。我们将使用拉格朗日乘数来解决这个优化问题。</p><p id="45bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于左叶节点R1，1</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d3b2653010eaf56bdaa9c515718b24a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*tazHyDHavQVKmXZjOhU0XQ.png"/></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/45891847154ae47667f48ef010711e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*bT590ZcxY1XDPtXuMPSqIw.png"/></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/933cc9ae6ad059a3d76802324b36db00.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*YJzZaeOC8966ZAGuVUxRWg.png"/></div></figure><p id="32dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们用链式法则求解伽马。我们有</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/52385de0d7db23db9a8651bddf86857c.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*xKbR1vdLy9a5AXjpKFfB5w.png"/></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/c9e2a25c2c8f4ca13b4017a8be514881.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*uziSCnxPt3Z4S1V9nWdJ-g.png"/></div></figure><p id="5042" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，我们求解叶节点R1，2</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/4653d7bd082f825f1313588133c57baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*MR_jcRuGRzOWVip6s0IEhw.png"/></div></figure><p id="91f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们有y1=88，y2=76，Fm-1(x1)和Fm-1(x2 ) = 73.3这是我们之前的预测。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4c0fc695494f2011701404ec1ef91bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*lWMGDB-ujoKQxw7y5gELww.png"/></div></figure><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/c8f4cca0f56dcb181556a3d4ff8f2e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*fOtzCdtCEtnutPm1UkyobQ.png"/></div></figure><p id="0d04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意:在回归问题中，残差的平均值以rjm值结束。</p><p id="0dc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了决策树:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/939edcd516f2eae3055892b1a21f3c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*7sICv2oS-fIIRur7QXkQaQ.png"/></div></figure><p id="6f3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.4.更新Fm(x)</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/1159dba0366d0542cfa8d1da4d093ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*puXq9r5lRzx5iWskavHSiQ.png"/></div></figure><p id="3955" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，为了得到F1(x ),我们将先前的预测与叶节点的rjm值乘以学习率相乘。这里我们取学习率为0.1</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/536d16660b12fd0caa4bcef8b2a1ccbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*QfhNMTBFHTIwhi-FAGZcgQ.png"/></div></figure><p id="5bc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的新预测是:</p><pre class="la lb lc ld gt lt lu lv lw aw lx bi"><span id="c512" class="ly lz iq lu b gy ma mb l mc md">For x1, r11 = 73.3 + (0.1*-17.3) = 71.6<br/>For x2, r21 = 73.3 + (0.1*8.7) = 74.2 <br/>For x3, r21 = 73.3 + (0.1*8.7) = 74.2</span></pre><p id="7ee7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于这些预测构建下一棵树</p><h1 id="cb84" class="mw lz iq bd mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns bi translated">预测阶段</h1><p id="f3d3" class="pw-post-body-paragraph jn jo iq jp b jq nt js jt ju nu jw jx jy nv ka kb kc nw ke kf kg nx ki kj kk ij bi translated">假设我们的训练结束了，我们想对一个数据进行预测。假设我们构建了两个决策树，即M=2。实际上，我们通常会有M = 100。</p><p id="5af2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们要预测体重的人身高1.3，男性。</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi ny"><img src="../Images/7ffe2c1dc8152d7baf15edb8ab74b286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOIVhXN6za3IS37adh1BWQ.png"/></div></div></figure><p id="2ef2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经预测了权重为73.3+(0.1 *-17.3)+(0.1 *-15.6)= 70。</p><p id="ca06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我想你对GBDT回归已经有了清晰的直觉。我写这篇文章的灵感来自于Josh Starmer的StatQuest。如果这个概念仍然不清楚，你可以参考他的视频，其中解释了更多的例子。直到那时快乐的机器学习！</p><h2 id="78d8" class="ly lz iq bd mx nz oa dn nb ob oc dp nf jy od oe nj kc of og nn kg oh oi nr oj bi translated">参考:</h2><p id="1ad2" class="pw-post-body-paragraph jn jo iq jp b jq nt js jt ju nu jw jx jy nv ka kb kc nw ke kf kg nx ki kj kk ij bi translated">【https://www.youtube.com/watch?v=2xudPOBz-vs】T2&amp;list = plblh5 jkooluictaglrohqduf _ 7q 2 gfu JF&amp;index = 45</p><p id="3d73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【https://en.wikipedia.org/wiki/Gradient_boosting T4】</p></div></div>    
</body>
</html>