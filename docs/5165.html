<html>
<head>
<title>ELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ELECTRA:有效地学习编码器，准确地分类令牌替换</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/electra-efficiently-learning-an-encoder-that-classifies-token-replacements-accurately-59253abd5f25?source=collection_archive---------12-----------------------#2020-09-08">https://medium.datadriveninvestor.com/electra-efficiently-learning-an-encoder-that-classifies-token-replacements-accurately-59253abd5f25?source=collection_archive---------12-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eb51" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解基于变压器的自监督架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/52f5ee36f3a0edc34c6cac2c5b688b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qDSwRuu3fGrB6NLT"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@overdriv3?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">A. L.</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="044b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP的最新进展建议主要使用因果语言建模目标或去噪自动编码目标(例如，掩蔽语言建模目标)来训练语言模型。该框架根据上述目标之一对模型进行自我监督的预训练，然后根据特定的下游目标对模型进行微调。像<a class="ae ky" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener">伯特</a>、<a class="ae ky" href="https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946" rel="noopener">罗伯塔</a>、<a class="ae ky" href="https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710" rel="noopener" target="_blank"> XLNet </a>、<a class="ae ky" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">艾伯特</a>、<a class="ae ky" href="https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e" rel="noopener" target="_blank"> T5 </a>等车型。接受过此类目标的培训，并已达到各自基准的最高水平。</p><p id="8135" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论由<a class="ae ky" href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener ugc nofollow" target="_blank"> Google AI </a>提出的一种相当独特的方法，用于本文中的语言模型预训练，<a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank"> ELECTRA:将文本编码器预训练为鉴别器而不是生成器</a>。我们将首先浏览现有的预培训方法，然后深入讨论ELECTRA的方法。</p><p id="f3d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意</strong>这些模型实现了变压器。艾尔。)建筑。想了解更多关于变形金刚的信息，请阅读本文<a class="ae ky" href="https://towardsdatascience.com/transformers-explained-65454c0f3fa7" rel="noopener" target="_blank">。</a></p><h1 id="17a5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">现有的培训前方法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/0a81418d24482d47e311699936b60203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kbtYfJeWejFX7HBH.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Existing Pre-training Approaches via <a class="ae ky" href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener ugc nofollow" target="_blank">Google AI Blog</a></figcaption></figure><p id="1306" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些方法可以大致分为:</p><ol class=""><li id="bddd" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated"><strong class="lb iu">语言建模或因果语言建模目标</strong>(左):在该目标中，在自回归设置中训练语言模型，即对于记号序列，我们最大化记号<strong class="lb iu"> <em class="mx"> x_t </em> </strong>出现在第“<strong class="lb iu"> <em class="mx"> t </em> </strong>”位置的概率，给定序列中该记号之前的所有记号。形式上最大化:<strong class="lb iu"><em class="mx">P(x _ t | x _(I&lt;t))</em></strong>。</li><li id="3a79" class="mo mp it lb b lc my lf mz li na lm nb lq nc lu mt mu mv mw bi translated"><strong class="lb iu">去噪自动编码或模糊地屏蔽语言建模目标</strong>(右):在该目标中，在自动编码设置中训练语言模型，即，对于记号序列，给定序列<strong class="lb iu"> <em class="mx"> x_hat </em> </strong>中的所有记号，我们最大化记号<strong class="lb iu"> <em class="mx"> x_t </em> </strong>出现在第<strong class="lb iu"> <em class="mx"> t </em> </strong>位置的概率。形式上最大化:<strong class="lb iu"> <em class="mx"> P(x_t | x_hat) </em> </strong>。</li></ol><div class="nd ne gp gr nf ng"><a href="https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">人工智能、深度学习和医疗实践|数据驱动的投资者</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">人工智能和深度神经学习的效用看起来可能是合法和有前途的，特别是…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu ks ng"/></div></div></a></div><blockquote class="nv nw nx"><p id="e3ba" class="kz la mx lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4" rel="noopener"><em class="it">【GPT】</em></a><em class="it"/><a class="ae ky" href="https://medium.com/swlh/openai-gpt-2-language-models-are-multitask-learners-1c6d42d406ae" rel="noopener"><em class="it">【GPT-2】</em></a><em class="it"/><a class="ae ky" href="https://towardsdatascience.com/xlnet-autoregressive-pre-training-for-language-understanding-7ea4e0649710" rel="noopener" target="_blank"><em class="it">XLNet</em></a><em class="it">接受语言建模目标训练</em><a class="ae ky" href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener"><em class="it">BERT</em></a><em class="it"/><a class="ae ky" href="https://medium.com/dataseries/roberta-robustly-optimized-bert-pretraining-approach-d033464bd946" rel="noopener"><em class="it">RoBERTa</em></a><em class="it">接受去噪自动编码目标训练</em></p></blockquote><h1 id="8ef9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">伊利克特拉</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1d659804bd6b8a89c3852faff62d44fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/0*0SBMn50jTdXDoo2J.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Replaced Token Detection via <a class="ae ky" href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener ugc nofollow" target="_blank">Google AI Blog</a></figcaption></figure><p id="d553" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ELECTRA论文提出了一个<strong class="lb iu">替换记号检测</strong>的目标，其中<strong class="lb iu">不是屏蔽输入，然后训练模型来预测正确的记号，而是用不正确的记号替换序列中的原始记号</strong>，但是它们对于给定的序列是有意义的(例如在上面的动画中， <strong class="lb iu">把</strong>、<strong class="lb iu">熟了的</strong>替换成<strong class="lb iu"> a </strong>、<strong class="lb iu">吃了</strong>，虽然不正确，但还是有道理的)，然后训练模型对<strong class="lb iu">分类</strong>如果给定的令牌是“<strong class="lb iu"> <em class="mx">原</em> </strong>或者“<strong class="lb iu"> <em class="mx">替换了</em> </strong>”。</p><p id="a540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型由一个<strong class="lb iu">生成器</strong>和一个<strong class="lb iu">鉴别器网络</strong>组成，如<a class="ae ky" href="https://towardsdatascience.com/a-comprehensive-guide-to-generative-adversarial-networks-gans-fcfe65d1cfe4" rel="noopener" target="_blank">生成对抗网络</a> (GANs)。相反，训练程序是<strong class="lb iu">而不是</strong> <strong class="lb iu">对抗</strong>就像甘斯一样。在本文的后面，我们将简要介绍ELECTRA不是GAN的几个原因。</p><h2 id="8480" class="oc lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated">发电机</h2><p id="51d9" class="pw-post-body-paragraph kz la it lb b lc oo ju le lf op jx lh li oq lk ll lm or lo lp lq os ls lt lu im bi translated">这本质上是一个<strong class="lb iu">小型的屏蔽语言模型</strong>。因此，输入最初被[屏蔽]记号破坏，然后，生成器被训练来从原始序列预测正确的记号(或如前所述有意义的似是而非的记号)。</p><p id="4f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，拥有小型生成器背后的逻辑与模型的计算复杂性有关。</p><blockquote class="nv nw nx"><p id="e191" class="kz la mx lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">如果生成器和鉴别器大小相同，训练ELECTRA每一步的计算量将是仅用屏蔽语言建模训练的两倍</p><p id="a4cd" class="kz la mx lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">— <a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">电子纸</a></p></blockquote><p id="faba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形式上，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/d8f74accb5ed8fbe04c3723f239b89ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/0*7npk9Z1FYHhVDo4j.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Masked Language Modeling (MLM) Objective via <a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><h2 id="4ff7" class="oc lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated">鉴别器</h2><p id="3260" class="pw-post-body-paragraph kz la it lb b lc oo ju le lf op jx lh li oq lk ll lm or lo lp lq os ls lt lu im bi translated">这实质上是一个变换器编码器，并且被训练为最大化将被替换的标记分类为“<strong class="lb iu"> <em class="mx">被替换的</em> </strong>”并且将原始标记分类为“<strong class="lb iu"> <em class="mx">原始</em> </strong>”的可能性。</p><p id="9bfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">形式上，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/cb13fcc03c5ac686494afca461ad2298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uR5SdQMo_16s8bm_.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Replaced Token Detection Objective via <a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><h2 id="8521" class="oc lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated">组合模型</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/6bfaca1f9eb5c914c29c73241928ff10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5iedOoRDB6MThUZJ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">ELECTRA via <a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><p id="31df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用两者对组合模型进行预训练；替换令牌检测目标上的生成器和鉴别器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c6ebde763f417dcba796f8287e155eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*Od1muCkSsP-JtNd2.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Combined Loss via <a class="ae ky" href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><p id="1cf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">发生器在预训练</strong>后被丢弃，鉴别器用于下游任务的微调。</p><blockquote class="ox"><p id="89ec" class="oy oz it bd pa pb pc pd pe pf pg lu dk translated">因此，<strong class="ak">鉴别器是ELECTRA。</strong></p></blockquote><h1 id="ed6a" class="lv lw it bd lx ly lz ma mb mc md me mf jz ph ka mh kc pi kd mj kf pj kg ml mm bi translated">伊莱克特拉怎么不是甘人？</h1><p id="c4b1" class="pw-post-body-paragraph kz la it lb b lc oo ju le lf op jx lh li oq lk ll lm or lo lp lq os ls lt lu im bi translated">虽然模型架构和/或培训目标可能会让人想起GANs，但ELECTRA不是！以下是ELECTRA与GANs相比的一些特点:</p><ol class=""><li id="6d91" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">如果生成器碰巧生成了正确的令牌，则该令牌被认为是“真实的”而不是“伪造的”。</li><li id="d54f" class="mo mp it lb b lc my lf mz li na lm nb lq nc lu mt mu mv mw bi translated">以最大似然性训练生成器，而不是以对抗性训练来欺骗鉴别器。</li><li id="e74a" class="mo mp it lb b lc my lf mz li na lm nb lq nc lu mt mu mv mw bi translated">由于我们采样令牌作为发生器输出并将其馈送给鉴别器，所以不可能通过这个采样步骤反向传播损失，而这是对抗训练所必需的。</li></ol><h1 id="5667" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="5820" class="pw-post-body-paragraph kz la it lb b lc oo ju le lf op jx lh li oq lk ll lm or lo lp lq os ls lt lu im bi translated">在本文中，我们介绍了一种新的语言模型预训练方法。我们还讨论了伊莱克特拉和甘斯的相似之处，以及它们之间的区别。</p><p id="d330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个<a class="ae ky" href="https://github.com/google-research/electra" rel="noopener ugc nofollow" target="_blank">链接指向ELECTRA的GitHub资源库</a></p><p id="f894" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个API引用<a class="ae ky" href="https://huggingface.co/transformers/model_doc/electra.html" rel="noopener ugc nofollow" target="_blank">使用huggingface transformers </a>实现ELECTRA</p><h1 id="f17f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><div class="nd ne gp gr nf ng"><a href="https://openreview.net/forum?id=r1xMH1BtvB" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">ELECTRA:预先训练文本编码器作为鉴别器，而不是...</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">ELECTRA:预训练文本编码器作为鉴别器而不是生成器；大卫:一个文本编码器…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">openreview.net</p></div></div><div class="np l"><div class="pk l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">使用ELECTRA进行更有效的NLP模型预训练</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">语言预训练的最新进展导致了自然语言处理领域的实质性进展…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">ai.googleblog.com</p></div></div><div class="np l"><div class="pl l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://medium.com/swlh/bert-pre-training-of-transformers-for-language-understanding-5214fba4a9af" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">伯特:语言理解变形金刚的前期训练</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="pm l nr ns nt np nu ks ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://medium.com/dataseries/openai-gpt-generative-pre-training-for-language-understanding-bbbdb42b7ff4" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd iu gy z fp nl fr fs nm fu fw is bi translated">开放式GPT:语言理解的生成性预训练</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">了解基于变压器的自监督架构</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="pn l nr ns nt np nu ks ng"/></div></div></a></div><p id="0233" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">访问专家视图— </strong> <a class="ae ky" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>