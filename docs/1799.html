<html>
<head>
<title>From Word Embeddings to Sentence Embeddings — Part 3/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从单词嵌入到句子嵌入——第三部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-3-3-e67cc4c217d7?source=collection_archive---------4-----------------------#2020-04-01">https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-3-3-e67cc4c217d7?source=collection_archive---------4-----------------------#2020-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d80d7accaecdbd1ee379a3c61e9a16a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clgRm-spNMKT_8IKXae_OA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Designed by <a class="ae kc" href="https://br.freepik.com/fotos-gratis/letras-formando-as-palavras-progresso-crescimento-e-sucesso_1330222.htm" rel="noopener ugc nofollow" target="_blank">Freepik</a></figcaption></figure><p id="7d62" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[关于这篇和更多的帖子，请查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="ccc6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你好。这篇文章是关于<strong class="kf ir">句子嵌入</strong>的三部分系列文章的最后一篇。如果你没有读过第一部分或第二部分，你可以在这里找到它们<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917" rel="noopener">和</a><a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1" rel="noopener"/>。</p><p id="325f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将解释创造句子嵌入的最先进的(SOTA)方法。</p><div class="lb lc gp gr ld le"><a href="https://www.datadriveninvestor.com/2020/01/16/software-development-process-how-to-pick-the-right-process/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">软件开发过程:如何选择正确的过程？数据驱动的投资者</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">软件是任何企业组织成功的生命线。没有软件的帮助，一个…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jw le"/></div></div></a></div><h1 id="9b66" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">句子-伯特</h1><p id="6213" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">句子-Bert目前(2020年4月)是用于创建句子嵌入的<strong class="kf ir"> SOTA算法</strong>。它是由Nils Reimers和Iryna Gurevych [1]在2019年提出的，它利用BERT模型创建了更好的句子嵌入，考虑了文本和许多以前时间步长的上下文中的长期依赖关系。</p><p id="a481" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了理解句子-BERT体系结构，必须解释几个概念。</p><h2 id="c4f2" class="mw lu iq bd lv mx my dn lz mz na dp md ko nb nc mh ks nd ne ml kw nf ng mp nh bi translated">注意机制</h2><p id="c9be" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">如<a class="ae kc" href="https://dev.to/diogodanielsoaresferreira/from-word-embeddings-to-sentence-embeddings-part-2-3-1db9" rel="noopener ugc nofollow" target="_blank">第2部分</a>所述，LSTMs有一个隐藏向量，代表输入当前状态的内存。然而，对于长输入，例如长句，向量不能给出正确预测下一个状态所需的所有信息。<strong class="kf ir"> LSTM可能会出错，因为在实践中，由于表示状态的隐藏向量的大小的瓶颈，它只具有有限数量的后退的信息</strong>。</p><p id="85b8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这个问题，2014年引入了注意力机制([3]、[4])。代替单个向量，<strong class="kf ir">该模型在决定预测什么之前可以访问所有先前的隐藏状态</strong>(在【4】中可以找到更好的解释)。注意力有助于解决长期依赖问题。</p><h2 id="75e7" class="mw lu iq bd lv mx my dn lz mz na dp md ko nb nc mh ks nd ne ml kw nf ng mp nh bi translated">变压器</h2><p id="bcb7" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">LSTMs的另一个问题是<strong class="kf ir">训练的时间</strong>。因为输出总是依赖于之前的输入，所以训练是按顺序进行的，花费的时间太多。谷歌大脑和多伦多大学在2017年提出的Transformer架构[5]，<strong class="kf ir">展示了如何在一个可以并行化的神经架构中使用注意力机制，在机器翻译任务中花费更少的时间进行训练并获得更好的结果</strong>。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/c4e94d74e941c03288802ac3199561ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y6V9oR417eSEpb1I.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1 — Transformer architecture. (Source: [5])</figcaption></figure><p id="c956" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图1显示了变压器的完整架构。详细的解释可以在[6]中找到。该架构由两部分组成:编码器和解码器。<strong class="kf ir">编码器对输入的表示进行编码，而解码器试图根据编码器的表示和先前的输出输出一个概率</strong>。编码器和解码器的基本构造块是前馈层和自关注层。自我关注层着眼于整个输入，并试图关注最重要的部分，而不是依赖于单一的隐藏状态表征。</p><p id="d7be" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">transformer体系结构应用于句子翻译的结果是对以前最先进模型的巨大改进。transformer彻底改变了NLP领域，因为现在可以在合理的时间内训练大型数据集，而模型不太容易出现长期依赖问题。然而，我们为什么要坚持只使用一个变压器呢？<strong class="kf ir">如果我们堆叠许多变压器，并对它们进行稍微不同的训练以获得更好的性能，会发生什么？</strong></p><h2 id="a54b" class="mw lu iq bd lv mx my dn lz mz na dp md ko nb nc mh ks nd ne ml kw nf ng mp nh bi translated">伯特</h2><p id="10b1" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">BERT于2019年年中由谷歌人工智能语言团队[7]提出，并兑现了使用变形金刚创建一个比所有前辈都好得多的通用语言理解模型的承诺，在NLP发展中向前迈出了一大步。当它被提出时，它在诸如问题回答或语言推理的任务中实现了SOTA结果，只是对其架构进行了微小的修改。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ni"><img src="../Images/223af1fe194039725e5171bc3f90082e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uIqGYpqSP4UWFy6I.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2 — BERT Architecture for pre-training and fine-tuning. (Source: [7])</figcaption></figure><p id="3bb7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图2显示了BERT架构。它主要由一个<strong class="kf ir">多层双向变换器编码器</strong>组成(大模型由24层变换器块组成)，其中输入是每个令牌在输入中的嵌入。</p><p id="715d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个架构的一个重要方面是<strong class="kf ir">双向性，这使得BERT能够学习前向和后向依赖性</strong>。这是通过用两个不同的目标任务预先训练BERT来实现的:</p><ul class=""><li id="d176" class="nn no iq kf b kg kh kk kl ko np ks nq kw nr la ns nt nu nv bi translated"><strong class="kf ir">掩蔽语言模型</strong>，也称为完形填空任务，使BERT能够学习双向依存关系。它不是预测句子中的下一个单词，而是随机屏蔽一定比例的输入标记，并预测那些被屏蔽的标记。这迫使模型不仅要学习向前，还要学习标记之间的向后依赖性。</li><li id="7fee" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated"><strong class="kf ir">下一句预测</strong>给模型输入两句话，预测这两句话是否相邻。该任务迫使模型学习两个句子之间的关系，这不是语言建模直接捕获的。</li></ul><p id="b08e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在获得无监督数据的预训练模型后，微调部分可以适应不同的NLP任务，只需改变模型的输入和输出即可。论文中一个有趣的结论是<strong class="kf ir">变压器层数越多，下游任务</strong>的结果越好。</p><h2 id="e7fa" class="mw lu iq bd lv mx my dn lz mz na dp md ko nb nc mh ks nd ne ml kw nf ng mp nh bi translated">句子-伯特法</h2><p id="eafd" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">最后，我们得出了句子-伯特法。</p><p id="5427" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使用BERT计算两个句子之间的相似度，需要将两个句子输入到网络中。由于BERT的复杂性，计算10000个句子的相似性需要大约65个小时的计算。</p><p id="4d04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">句子-BERT是一种创建语义有意义的句子嵌入的方法，可以与余弦相似度进行比较，保持BERT的准确性，但将寻找最相似对的工作从65小时减少到5秒</strong>。</p><figure class="nj nk nl nm gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2c2e96ea3a8256b87489b69e618a6da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*m9lS18AYoOITaz8g.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3 — Sentence-BERT training architecture. (Source:[1])</figcaption></figure><p id="04af" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图3描述了句子-BERT体系结构。训练目标(同样用于推断)是根据句子的相似性对句子进行分类:蕴涵(句子相关)、矛盾(句子矛盾)或中性(句子不相关)。一对句子被馈送给BERT，接着是一个池层(它可以是最大池、均值池或使用BERT输出中的CLS令牌)，它将为每个句子生成一个嵌入。这两种嵌入与它们的差异连接在一起，并被馈送到3路softmax层。这种训练模式通常被称为暹罗网络[8]。</p><p id="e83d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过用这种架构微调Bert权重，生成的<strong class="kf ir">嵌入适合句子相似度</strong>，通过BERT发送句子并应用池操作。</p><h1 id="a910" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">比较算法</h1><p id="d3e1" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">现在我们已经看到了创建句子表示的四种算法(查看前面的算法<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917" rel="noopener">这里</a>和<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1" rel="noopener">这里</a>，让我们测试它们，看看结果。这个<a class="ae kc" href="https://github.com/diogodanielsoaresferreira/document_representations_tests/blob/master/Sentence%20Similarity.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>包含了四种方法的测试。</p><p id="dcfa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个新闻数据集，用四种方法创建新闻描述的表示。给定用户插入的查询，它将生成该查询的表示，并将它与所有新闻表示进行比较。该比较是使用余弦相似性来完成的。前五个最相似的新闻描述被打印到笔记本上。我们来分析一些结果。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="3370" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题是“民主党在选举中赢了共和党。”所有的方法都产生了好的结果，但是似乎<strong class="kf ir">inferent和sentent-Bert有更好的匹配</strong>。再来看另一个结果。</p><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="0e03" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于前三种方法，这个查询特别困难。<strong class="kf ir">只有一句话——伯特似乎产生了正确的结果</strong>。</p><p id="9de2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在笔记本<a class="ae kc" href="https://github.com/diogodanielsoaresferreira/document_representations_tests/blob/master/Sentence%20Similarity.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>看到更多有趣的结果。</p><p id="63f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还测试了其他产生句子表达的算法，但在这篇文章中没有探讨。如果你想知道更多，我建议你看看<strong class="kf ir">通用句子编码器【9】，Skip-thought【10】或者fast sent【11】</strong>。</p><p id="6604" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，有各种算法来创建句子表示。除了性能，考虑它们的速度和内存需求也很重要。</p><p id="a42a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">句子嵌入是一个开放的研究领域，在过去的几年里取得了很大的进展，行业应用程序，如聊天机器人和搜索引擎，对它们的需求越来越大。跟上研究领域的最新发展是很重要的。</p><p id="dccf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您坚持阅读本系列关于句子嵌入的最后一部分！查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a>类似帖子。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><h1 id="33c1" class="lt lu iq bd lv lw ol ly lz ma om mc md me on mg mh mi oo mk ml mm op mo mp mq bi translated">参考</h1><ul class=""><li id="f576" class="nn no iq kf b kg mr kk ms ko oq ks or kw os la ns nt nu nv bi translated">[1]: Nils Reimers和Iryna Gurevych:《句子-BERT:使用暹罗BERT-Networks的句子嵌入》，2019；<a class="ae kc" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank"> arXiv:1908.10084 </a>。</li><li id="47a7" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[2]: Dzmitry Bahdanau，Kyunghyun Cho和Yoshua Bengio:《通过共同学习对齐和翻译进行神经机器翻译》，2014年；<a class="ae kc" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> arXiv:1409.0473 </a>。</li><li id="3148" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[3]: Minh-Thang Luong，Hieu Pham和Christopher D. Manning:“基于注意力的神经机器翻译的有效方法”，2015；<a class="ae kc" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank"> arXiv:1508.04025 </a>。</li><li id="471a" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[4] — Jay Alammar，<a class="ae kc" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">可视化一个神经机器翻译模型(Seq2seq模型的力学注意)</a>。</li><li id="a517" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[5] —阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯和卢卡斯·凯泽:《注意力是你所需要的一切》，2017；<a class="ae kc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762 </a>。</li><li id="97ee" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[6] —杰·阿拉玛，<a class="ae kc" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《图解变压器》</a>。</li><li id="bc1e" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[7] — Jacob Devlin，Ming-Wei Chang，Kenton Lee和Kristina Toutanova:《BERT:用于语言理解的深度双向转换器的预训练》，2018；<a class="ae kc" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> arXiv:1810.04805 </a>。</li><li id="c8dc" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[8] — Jane Bromley、Isabelle Guyon、Yann LeCun、Eduard Sackinger和Roopak Shah:“<a class="ae kc" href="https://papers.nips.cc/paper/769-signature-verification-using-a-siamese-time-delay-neural-network.pdf" rel="noopener ugc nofollow" target="_blank">使用暹罗时间交易神经网络</a>进行签名验证”，1994年。</li><li id="762a" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[9] — Daniel Cer、Yang、Sheng-yi Kong、Nan Hua、Nicole Limtiaco、Rhomni St. John、Noah Constant、Mario Guajardo-Cespedes、Steve Yuan、Chris Tar、Yun-Hsuan Sung、Brian Strope和Rey Kurzweil:《通用句子编码器》，2018；<a class="ae kc" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank"> arXiv:1803.11175 </a>。</li><li id="9962" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[10] — Ryan Kiros、Yukun Zhu、Ruslan Salakhutdinov、Richard S. Zemel、Antonio Torralba、Raquel Urtasun和Sanja Fidler:“跳过思维向量”，2015年；<a class="ae kc" href="https://arxiv.org/abs/1506.06726" rel="noopener ugc nofollow" target="_blank"> arXiv:1506.06726 </a>。</li><li id="7f91" class="nn no iq kf b kg nw kk nx ko ny ks nz kw oa la ns nt nu nv bi translated">[11] — Felix Hill，Kyunghyun Cho和Anna Korhonen:“从未标记数据中学习句子的分布式表示”，2016年；<a class="ae kc" href="https://arxiv.org/abs/1602.03483" rel="noopener ugc nofollow" target="_blank"> arXiv:1602.03483 </a>。</li></ul><figure class="nj nk nl nm gt jr"><div class="bz fp l di"><div class="ot od l"/></div></figure></div></div>    
</body>
</html>