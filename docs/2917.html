<html>
<head>
<title>Complete Guide On Linear Regression Vs. Polynomial Regression With Implementation In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现线性回归与多项式回归的完整指南</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/complete-guide-on-linear-regression-vs-polynomial-regression-with-implementation-in-python-964c64c28aa8?source=collection_archive---------0-----------------------#2020-05-22">https://medium.datadriveninvestor.com/complete-guide-on-linear-regression-vs-polynomial-regression-with-implementation-in-python-964c64c28aa8?source=collection_archive---------0-----------------------#2020-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e604a56d8d891c9f73c77a629f7bb79e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnC73P_m0-Qg3Npj1LlTZA.png"/></div></div></figure><p id="c111" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我以前的文章中，我写了如何为我们的数据集绘制回归线。太酷了，对吧！但是还有一个问题，我们的模型越来越不精确。我们<strong class="ka ir">的最终目标</strong>始终是建立一个精确度最高、误差最小的模型。因此，在本文中，我们将了解如何通过使用曲线来实现最适合我们数据的多项式回归。</p><p id="06c5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在此之前，这里有一些基本的多项式函数及其图形绘制。这将有助于您更好地理解对特定数据集使用哪个多项式。</p><p id="38a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">享受这篇文章吧！</strong></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/2a08700589415e2f0c11a4649fdffb40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*4J2Bm1g7_DR47u-g.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Comparison Between Models</figcaption></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h2 id="c704" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated">多项式函数及其图形；</h2><p id="7311" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">(1)曲线图为<strong class="ka ir"> Y=X </strong>:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/4c2963a65e4bb525da12298ce911c3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*tNh9Z3TVr9VWBA3tHiPMLA.png"/></div></figure><p id="9388" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(2)曲线图为<strong class="ka ir"> Y = X </strong>:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/394407289b8afd52270551b71c002f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*2TYNJ8ZBRWxbZyZnbQAhsw.png"/></div></figure><p id="0f98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(3)曲线图为<strong class="ka ir"> Y = X </strong>:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/52551dde711591d99a757903e2acc74c.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*uSS6bSh-IGKgaap69oLBEw.png"/></div></figure><p id="55d2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(4)多项式多于一个的图:<strong class="ka ir"> Y = X +X +X </strong>:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/3366fccf235dfc9dfbaf2e805ca77fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*0mH0OCkMIK9gl24kEvLAvg.png"/></div></figure><p id="95f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在该图中，您可以看到红点显示的是Y=X +X +X的图形，蓝点显示的是Y=X的图形。在这里，您可以清楚地看到，最大功率会影响我们的图形形状。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="c741" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，让我们开始酷的部分吧！:)</p><p id="6e43" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">好了，现在我们来看看为什么我们应该使用多项式(非线性)回归。这里我将举一个例子，对于第一部分，我将找到线性回归线并计算误差，对于第二部分，我将使用多项式回归，并找到最适合它的曲线并计算误差。然后，我们将比较这两个误差，看看哪个模型表现更好。</p><div class="mo mp gp gr mq mr"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab fo"><div class="mt ab mu cl cj mv"><h2 class="bd ir gy z fp mw fr fs mx fu fw ip bi translated">认知计算——一套被广泛认为是……</h2><div class="my l"><h3 class="bd b gy z fp mw fr fs mx fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="mz l"><p class="bd b dl z fp mw fr fs mx fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf jw mr"/></div></div></a></div><p id="c199" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我采用这个多项式函数来生成数据集，因为这是一个示例，我将向您展示何时使用多项式回归。我要加一些噪点，这样看起来更真实！</p><p id="d90e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们要用<a class="ae ng" href="https://medium.com/@shuklapratik22/multivariable-linear-regression-using-normal-equation-707d19f1c325" rel="noopener"> <strong class="ka ir">正规方程</strong> </a>实现线性回归和多项式回归。你可以<a class="ae ng" href="https://www.youtube.com/channel/UCpRJj1vjQsjCTB3S5ANvNvg" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">点击这里</strong> </a>查看各种机器学习算法的详细讲解视频。</p><p id="855e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们向前看，</p><p id="99e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正常方程如下:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8d024ad226fc80eee1dff1b52fc731a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/0*l_t5kBIlZdMp2FMm.png"/></div></figure><p id="f996" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的等式中:</p><p id="0714" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">θ:最佳定义的假设参数。</p><p id="76d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">x:输入每个实例的特征值</p><p id="e990" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">y:每个实例的输出值</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="fd44" class="ni ln iq bd lo nj nk nl lr nm nn no lu np nq nr lx ns nt nu ma nv nw nx md ny bi translated">简单线性回归:</h1><p id="882c" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated"><strong class="ka ir">简单线性回归的假设函数:</strong></p><p id="2d07" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">y =β_ 0+β_ 1 * x</strong></p><p id="ce60" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">我们来编码:</strong></p><p id="0a95" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(1)导入所需的库:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5fe477d0a95ee3dd3d5e6786d0691dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Ir2F6mETRZ1YtQ30v_yEyg.png"/></div></figure><p id="ce79" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(2)数据集生成:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/aef03c0bb3a44cec2e4103db179fbec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*47YtZNoWrz4Rwn5fc9drVw.png"/></div></figure><p id="0d7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(3)x的形状:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/98305cf7142eb86541ff7472f35f9cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*jy6V5ONV2Hl0DBq7aoMPtA.png"/></div></figure><p id="8197" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(4)主矩阵的第1列:</p><p id="8b32" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里，列-1将始终是β_ 0系数的值，该值将始终为1。但是要创建一个矩阵，我们需要把它看作一列。为了更好的理解<a class="ae ng" href="https://youtu.be/wmmUJnmwQho" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">，点击这里</strong> </a></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0580534fd406985410e953f2e5810894.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*vL0LpQCWHOOWo9uYs_fvPg.png"/></div></figure><p id="a095" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(5)x _ bias的形状:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cdc897b04c35e5dfe6aa9f5862a8c7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*mGuXxS1bxsp9xzTXbDAflw.png"/></div></figure><p id="30fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(6)因为我们需要用x_bias附加x，所以它必须具有相同的形状:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4997526b8c4fbb347402bdc47c1104a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*Qb67c3jpKpnlj0vqDhMWyQ.png"/></div></figure><p id="75fc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(7)最终矩阵x:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5d1bb7f34251186edf5312e9f8d61ead.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*3GGM8rmE2mOleQAS12EQIw.png"/></div></figure><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="ab gu cl og"><img src="../Images/4369c1d3a572fa8b6d612e4b0d81f653.png" data-original-src="https://miro.medium.com/v2/format:webp/1*KFp6eTZWm7ORjuyaPONhgQ.png"/></div></figure><p id="4cd9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(8)矩阵转置:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7663eadbad8681abfe87ea375f663c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*jZMhZFbS3tHMnyxPeMbETg.png"/></div></figure><p id="3e50" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(9)矩阵乘法:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b9d62badc989102421ca11d8b61e628a.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*AFrGDUQoi1XOx2Bkx2mxEA.png"/></div></figure><p id="995d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(10)矩阵的逆矩阵:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/23d9543e9d6b8cd07aa3bd124e227db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*JRUISKeBu9j-BACUhyT6CA.png"/></div></figure><p id="e756" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(11)矩阵乘法:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5fac7cc10a5207cd3097cfae05552134.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*IYXIcoFA7UYP0T_2L_fwQw.png"/></div></figure><p id="f29e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(12)求系数:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/41fe0faefe0a6bc6aab39278c6a72cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*8nkKRTSdBmHgc5z0a4Nqsg.png"/></div></figure><p id="fe5b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(13)绘制回归线:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/18b46f32d9181a046be1ea165e15d56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*MD0K8aLtTeR35ledp_jlSQ.png"/></div></figure><p id="ce7c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(14)预测功能:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi om"><img src="../Images/92e6e73eb96e161e502137ad1b9508fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*yTjofSD-a8Z-4WpuEj065A.png"/></div></figure><p id="2dce" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(15)计算均方误差:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2b4abc02ee25498474a327a9078e1bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*fedl_kRcLhtNf82sPZfkqg.png"/></div></figure><p id="6fb9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以这里我们可以看到误差很高。让我们看看如果使用多项式方程会发生什么。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="6f40" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正常方程如下:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8d024ad226fc80eee1dff1b52fc731a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/0*l_t5kBIlZdMp2FMm.png"/></div></figure><h1 id="692b" class="ni ln iq bd lo nj oo nl lr nm op no lu np oq nr lx ns or nu ma nv os nx md ny bi translated">多项式回归:</h1><h2 id="a6a7" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated">多项式回归的假设函数:</h2><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/fa9b05cd5dc4446703ce2598a6e2bc8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*6mtLX-c3TEppNyKLYjnY0Q.png"/></div></figure><p id="414f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在哪里，</p><p id="7eba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">贝塔_0，贝塔_1，…是我们需要找到的系数。</p><p id="67e1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">x，x，x是我们数据集的特征。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/6550e9e3733c33c4fd894caf45cc091e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*QwtMEHsFq4CmvwcEAeuZbA.png"/></div></figure><p id="a993" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">咱们来码:</strong></p><p id="7bdb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(1)导入所需的库:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/5615ed92e42f2a0a15ee17d776a591a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*0LswXWrO1G-1ToOUOaMrQg.png"/></div></figure><p id="db97" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(2)生成数据点:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4a27ad9a5e7c81b3fff44f5710f4bb1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*FOJFfd5E6D1XY9obdGwNHA.png"/></div></figure><p id="fc37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(3)初始化x，x，x向量:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/055ca8522da22c322d8a50f4a2bbb618.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*Vc-fmU1KH_HgvhOn11YGGQ.png"/></div></figure><p id="2849" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(4)X矩阵的第1列:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/00bf3bad7dd5a090d9c4ec0ea301562a.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*Ii4DOxaQx3DfiVirBIXd4A.png"/></div></figure><p id="647d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(5)形成完整的x矩阵:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a478a7a14f532038096df29bb2fb9f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*jTm8eJ7wprbs60ce2KbtEg.png"/></div></figure><p id="c06d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(6)矩阵的转置:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/adc84612146ece1f4a1dab88408a6301.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*prOQaqDaPbyRWxZF1VZRlg.png"/></div></figure><p id="c41a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(7)矩阵乘法:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/f38555c7d9efe900d199a2b9d7033859.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*TkoipQLNmj_ThnfA5J37WA.png"/></div></figure><p id="f922" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(8)矩阵的逆矩阵:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/96b10f524c3a89012f7af62703a2ef60.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*R6kwJRg-6E6oBzjLVC5KIg.png"/></div></figure><p id="fb28" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(9)矩阵乘法:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/78bd17f981744f35defd9e289bbee748.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*U7T5vgYT00Z7Tfz2tQ32hA.png"/></div></figure><p id="982a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(10)系数值:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/1b355bdf8d316838007e033af5b10626.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*IRTwSIka3HXxFfwUB0Ig5Q.png"/></div></figure><p id="5905" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(11)将系数存储在变量中:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/86bb8df94ca366a60a984b8fa83875de.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*AnPpcIjW2ceCYLMmfkOm_Q.png"/></div></figure><p id="3d2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(12)用曲线绘制数据:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/4569d7ee8a0681dd7380b9836c396e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*snkD2Y_X5PAIdNkqAv9mzA.png"/></div></figure><p id="b2b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(13)预测功能:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/ea5009547f4ef8f9da89b6c424408cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*VZ7Ep4TsCw63AkHadInMww.png"/></div></figure><p id="071c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(14)误差函数:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/fd6450193ec6d5da53ea4fa31ad800a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*YgpxBLQw1UbU-8tYVNdC6g.png"/></div></figure><p id="592e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(15)计算误差:</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/04230d70109df987843727c1ccf00b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*-7d0JUaap03iwsfv8zDz9A.png"/></div></figure><p id="6129" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里你可以看到误差<strong class="ka ir">明显低于线性回归中的误差</strong>。</p><p id="af02" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以我们可以说它做了一件相当好的工作。因此，总之，我们可以说，如果我们的数据集遵循曲线趋势，那么我们可以使用多项式回归获得更好的结果和准确性。</p><p id="5757" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望你们喜欢这篇文章。如果你喜欢，请点击拍手图标。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h2 id="bf2d" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated">向前看，</h2><p id="32c5" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">在下一篇文章中，我将展示如何用python实现其他多项式回归。</p><p id="2b9a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我所有的文章都在我的博客上:<br/><a class="ae ng" href="http://patrickstar0110.blogspot.com" rel="noopener ugc nofollow" target="_blank"><strong class="ka ir">patrickstar0110.blogspot.com</strong></a></p><p id="a2af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">在我的youtube频道观看带有解释和推导的详细视频:</strong></p><p id="9a74" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【https://www.youtube.com/channel/UCpRJj1vjQsjCTB3S5ANvNvg<strong class="ka ir">T21</strong>T24】</p><p id="ae6b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (1)简单线性回归解释及其推导:</strong><br/><a class="ae ng" href="https://youtu.be/1M2-Fq6wl4M" rel="noopener ugc nofollow" target="_blank">https://youtu.be/1M2-Fq6wl4M</a></p><p id="d051" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (2)如何从零开始计算线性回归中模型的精度:</strong><br/><a class="ae ng" href="https://youtu.be/bM3KmaghclY" rel="noopener ugc nofollow" target="_blank">https://youtu.be/bM3KmaghclY</a></p><p id="60ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (3)使用Sklearn的简单线性回归:</strong><br/><a class="ae ng" href="https://youtu.be/_VGjHF1X9oU" rel="noopener ugc nofollow" target="_blank">https://youtu.be/_VGjHF1X9oU</a></p><p id="6f1f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (4)机器学习数学(矩阵)讲解:</strong><br/><a class="ae ng" href="https://youtu.be/1MASyeyAydw" rel="noopener ugc nofollow" target="_blank">https://youtu.be/1MASyeyAydw</a></p><p id="3829" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (5)正规方程的完整数学推导:</strong><br/><a class="ae ng" href="https://youtu.be/E7Q4UP6bNmc" rel="noopener ugc nofollow" target="_blank">https://youtu.be/E7Q4UP6bNmc</a></p><p id="7c25" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> (6)利用正规方程实现简单线性回归:【https://youtu.be/wmmUJnmwQho】<br/><a class="ae ng" href="https://youtu.be/wmmUJnmwQho" rel="noopener ugc nofollow" target="_blank"/></strong></p><h2 id="8e40" class="lm ln iq bd lo lp lq dn lr ls lt dp lu kj lv lw lx kn ly lz ma kr mb mc md me bi translated">阅读我的其他文章:</h2><p id="3a8f" class="pw-post-body-paragraph jy jz iq ka b kb mf kd ke kf mg kh ki kj mh kl km kn mi kp kq kr mj kt ku kv ij bi translated">(1)从零开始线性回归:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/linear-regression-from-scratch-a3d21eff4e7c" rel="noopener">https://medium . com/@ shuklapratik 22/Linear-Regression-从零开始-a3d21eff4e7c </a></p><p id="b154" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(2)暴力线性回归:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/linear-regression-line-through-brute-force-1bb6d8514712" rel="noopener">https://medium . com/@ shuklapratik 22/Linear-Regression-line-Through-Brute-Force-1 bb6d 8514712</a></p><p id="5814" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(3)线性回归完全推导:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/linear-regression-complete-derivation-406f2859a09a" rel="noopener">https://medium . com/@ shuklapratik 22/Linear-Regression-Complete-Derivation-406 f 2859 a09a</a></p><p id="0d9e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(4)简单线性回归实现从头开始:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/simple-linear-regression-implementation-from-scratch-cb4a478c42bc" rel="noopener">https://medium . com/@ shuklapratik 22/Simple-Linear-Regression-Implementation-从头开始-cb4a478c42bc </a></p><p id="2fbd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(5)从零开始简单线性回归:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/simple-linear-regression-implementation-2fa88cd03e67" rel="noopener">https://medium . com/@ shuklapratik 22/Simple-Linear-Regression-implementation-2fa 88 CD 03 e 67</a></p><p id="53f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(6)梯度下降及其数学:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/what-is-gradient-descent-7eb078fd4cdd" rel="noopener">https://medium . com/@ shuklapratik 22/what-is-Gradient-Descent-7eb 078 FD 4c DD</a></p><p id="8c8f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(7)从无到有梯度下降的线性回归:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/linear-regression-with-gradient-descent-from-scratch-d03dfa90d04c" rel="noopener">https://medium . com/@ shuklapratik 22/Linear-Regression-With-Gradient-Descent-From-Scratch-d 03 DFA 90d 04 c</a></p><p id="51b6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(8)线性回归误差计算技巧:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/error-calculation-techniques-for-linear-regression-ae436b682f90" rel="noopener">https://medium . com/@ shuklapratik 22/Error-Calculation-Techniques-For-Linear-Regression-AE 436 b 682 f 90</a></p><p id="d504" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(9)机器学习用矩阵介绍:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/introduction-to-matrices-for-machine-learning-8aa0ce456975" rel="noopener">https://medium . com/@ shuklapratik 22/机器学习用矩阵介绍-8aa0ce456975 </a></p><p id="71e0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(10)了解线性回归中正规方程背后的数学(完全推导)<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/understanding-mathematics-behind-normal-equation-in-linear-regression-aa20dc5a0961" rel="noopener">https://medium . com/@ shuklapratik 22/Understanding-Mathematics-Behind-Normal-Equation-In-Linear-Regression-aa 20 dc5 a 0961</a></p><p id="95aa" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(11)使用正规方程(矩阵)实现简单线性回归<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/implementation-of-simple-linear-regression-using-normal-equation-matrices-f9021c3590da" rel="noopener">https://medium . com/@ shuklapratik 22/Implementation-Of-Simple-Linear-Regression-Using-Normal-Equation-matrix-f 9021 c 3590 da</a></p><p id="5302" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(12)多变量线性回归实现:<br/><a class="ae ng" href="https://medium.com/@shuklapratik22/multivariable-linear-regression-using-normal-equation-707d19f1c325" rel="noopener">https://medium . com/@ shuklapratik 22/Multivariable-Linear-Regression-using-normal-equation-707d 19 f1c 325</a></p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/9c3d4e80ad5f35bf332885ac145b5075.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*oybYx3mI8YtjzOmW.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Thank You!!</figcaption></figure><figure class="kx ky kz la gt jr"><div class="bz fp l di"><div class="pl pm l"/></div></figure></div></div>    
</body>
</html>