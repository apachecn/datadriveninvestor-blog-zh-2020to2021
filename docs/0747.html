<html>
<head>
<title>How Neural Nets fell in love with Computer Hardware</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络如何爱上计算机硬件</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-neural-nets-fell-in-love-with-computer-hardwares-79a4617c8f3a?source=collection_archive---------12-----------------------#2020-02-14">https://medium.datadriveninvestor.com/how-neural-nets-fell-in-love-with-computer-hardwares-79a4617c8f3a?source=collection_archive---------12-----------------------#2020-02-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4d24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章是一个关于神经网络的爱情故事，她是科技行业的热门女演员。她是如何爱上这个多年来一直是技术计算骨干的家伙的。一颗硅心的家伙。</p><h2 id="f566" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">为什么神经网络和硬件以前从未相处过？</strong></h2><p id="fc5a" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">神经网络出现在文献中已经超过80年了，但在过去的5-6年里，研究和商业领域出现了巨大的繁荣，因为我们都知道的原因是缺乏计算能力。直到1999-2000年，计算机科学家才在一系列应用(医学领域)中使用GPU来加速他们的计算，这导致了<a class="ae lj" href="https://www.nvidia.com/content/GTC-2010/pdfs/2275_GTC2010.pdf" rel="noopener ugc nofollow" target="_blank">通用GPU计算的出现。</a></p><h2 id="8dc2" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">神经网络的解剖</h2><ul class=""><li id="6d99" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">神经网络源于图论，属于图结构学习领域。处于初级水平的神经网络具有完全连接层、卷积层、递归层、子采样层、标准化层和分类层。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/bb2eedecb68dfaf48e0a47b1bfe489aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wvp_OH0_CWS8u1cXlgZCJw.jpeg"/></div></figure><p id="1f9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些层堆叠在一起，形成一个网络架构。我们现在知道的一些著名的网络架构有LeNet、Alexnet、VGGNet、Resnet、InceptionNet、PyramidNet、XceptionNet和ZFNet。</p><ul class=""><li id="5974" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">除了这些基于卷积层的架构之外，我们还有用于顺序数据的基于连接的循环网络，一些著名的网络包括基于RNN GRU的单向网络、基于and some的单向网络和基于data and的定向网络Transformers。</li><li id="1b90" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">相比之下，循环神经网络(RNNs)，其中长短期记忆网络(LSTMs)是一种流行的变体，具有内部记忆，允许长期依赖影响输出。在这些网络中，一些中间操作生成的值存储在网络内部，并作为其他操作的输入，与后面的输入一起处理。</li><li id="d46b" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">为了执行学习，神经网络使用称为反向传播的一阶微分算法，该算法基本上使用微分学的链规则来找出每一层的权重随着作为输出的预测值的变化而变化的速率。</li></ul><p id="8c40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">网上有很多关于这些网络的文献可以研究，所以我就不赘述了。我将继续考虑，你知道机器学习和深度学习在不同层次方面的基础。</p><p id="5015" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">在本文中，我们将关注前馈网络，因为<br/>1)RNNs中的主要计算仍然是加权和<br/>(即矩阵向量乘法)，这由<br/>前馈网络涵盖；2)到目前为止，很少有人关注<br/>专门针对rnn的硬件加速。</strong></p><p id="92bc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们关注一种叫做卷积神经网络的神经网络来开始这个爱情故事。</p><h2 id="8a98" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">流行的CNN网络</h2><p id="f188" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated"><strong class="jp ir"> LeNet </strong></p><ul class=""><li id="b594" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">LeNet是1989年引入的第一个CNN方法之一。它是为28 × 28的灰度图像的数字分类任务而设计的。最著名的版本LeNet-5包含两个CONV层和两个FC层。</li><li id="f235" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">每个CONV层使用大小为5 × 5的滤波器(每个滤波器一个通道)，第一层有6个滤波器，第二层有16个滤波器。每次卷积后使用2 × 2的平均池，并使用一个sigmoid来表示非线性。</li><li id="3377" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">LeNet总共需要60，000个权重和341，000个<br/>乘加(MAC)每张图像。</strong> LeNet导致了CNN的第一次商业成功，因为它被部署在自动取款机上来识别支票存款的数字。</li></ul><p id="9b9a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Alexnet </strong></p><ul class=""><li id="f206" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">AlexNet [3]是第一个赢得2012年ImageNet挑战赛的CNN。它由五个CONV层和三个FC层组成。在每个CONV层中，有96到384个滤波器，滤波器大小从3 × 3到11 × 11不等，每个滤波器有3到256个通道。</li><li id="cee4" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">在第一层中，滤镜的三个通道对应于输入图像的红色、绿色和蓝色分量。在每一层中使用ReLU非线性。3 × 3的最大池应用于层<br/> 1、2和5的输出。</li><li id="1004" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">为了减少计算量，在网络的第一层使用步长4。AlexNet在max pooling之前在第1层和第2层引入了LRN的使用，尽管LRN在后来的CNN模型中不再流行。</li><li id="9d9b" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">将AlexNet与LeNet区分开来的一个重要因素是<br/>重量的数量要大得多，并且每层的形状都不相同。为了减少第二CONV层中的权重和计算量，第一层的96个输出声道被分成第二层的两组48个输入声道，使得第二层中的滤波器只有48个声道。类似地，第四层和第五层中的权重也被分成两组。</li><li id="116c" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">AlexNet总共需要6100万个砝码和7.24亿台MAC来处理一张227 × 227的输入图像。</li></ul><p id="24de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> VGGNet </strong></p><ul class=""><li id="53bc" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">VGG-16深入到16层，包括13个CONV层和3个FC层。为了平衡更深入的成本，较大的过滤器(例如5 × 5)由多个较小的过滤器(例如3 × 3)构成，这些过滤器具有较少的权重，以实现相同的有效感受野，如下图所示。因此，所有CONV图层都具有相同的3 × 3滤镜大小。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d8a2ba2611bf9910c2f803892d07b683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*PsvllwB__wjKpHdXTr0oKw.jpeg"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Decomposing larger filters into smaller filters. Constructing a 5x5 support for 3x3 filters.</figcaption></figure><ul class=""><li id="2b25" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">总的来说，VGG-16需要1.38亿个砝码和15.5G MACs来处理一个224 × 224的输入图像。VGG有两种不同的型号:VGG-16(在此描述)和VGG-19。VGG-19给出了比VGG-16低0.1%的top-5错误率，代价是多1.27倍的MAC。</li></ul><p id="a7ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">谷歌网</strong></p><ul class=""><li id="09c2" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">GoogLeNet更深入，有22层。它引入了一个由并行连接组成的初始模块，而以前只有一个串行连接。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/9730eccb63578ef9f10baef882489f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*yDlPZk6L6tN5WAbBbJCsyA.jpeg"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Inception module from GoogleNet</figcaption></figure><ul class=""><li id="1b35" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">不同尺寸的滤波器(即1 × 1、3 × 3、5 × 5)以及3 × 3最大池用于每个并联连接，其输出连接成模块输出。使用多种过滤器大小具有在多种尺度下处理输入的效果。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d6bf4d616d4cf5cbc9911182854c15ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*s63GklwTGtlW5vms14Dzjw.jpeg"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Constructing a 5×5 support from 1×5 and 5×1 filter. Used in GoogleNet/Inception v3 and v4.</figcaption></figure><ul class=""><li id="910b" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">为了提高训练速度，Google-net的设计使得在训练过程中为反向传播存储的权重和激活度都可以放入GPU内存中。</li><li id="40c4" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">为了减少权重的数量，1 × 1滤波器被用作减少每个滤波器的通道数量的“瓶颈”。这22层包括三个CONV层，随后是九个inceptions层(每个层有两个CONV层深)和一个FC层。</li></ul><p id="4856" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ResNets </strong></p><ul class=""><li id="ab82" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">ResNet，也称为Residual Net，使用剩余连接可以更深入(34层或更多)。这是ImageNet挑战赛中第一个超过人类水平准确性的参赛DNN，前5名的错误率低于5%。深度网络的挑战之一是训练期间梯度的消失:随着误差通过网络<br/>反向传播，梯度缩小，这影响了在非常深的网络的早期层中更新<br/>权重的能力。</li><li id="c5f8" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">快捷模块学习残差映射[F(x)= H(x)-x]，而不是学习权重层F(x)的函数。最初F(x)为零，取恒等式连接；然后在训练过程中逐渐使用通过权重层的实际前向连接。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/8d1898f6d14aee48bc710221caff27a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*CqIZmbiRUDD_kxjFgAAcrA.jpeg"/></div><figcaption class="mk ml gj gh gi mm mn bd b be z dk">Shortcut module from ResNet</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/2cc72a2171be4a0eafa5ed8e139e9619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NpR67da0OiboUVcwIzC8KQ.jpeg"/></div></div></figure><h1 id="6483" class="mw km iq bd kn mx my mz kq na nb nc kt nd ne nf kw ng nh ni kz nj nk nl lc nm bi translated">用于DNN处理的硬件</h1><ul class=""><li id="9f7d" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">Conv和FC层的基本概念都是乘法和累加(<a class="ae lj" href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation" rel="noopener ugc nofollow" target="_blank"> MAC </a>)运算。需要对这些MAC操作进行优化，以便更快地计算DNNs。</li><li id="6bb6" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">有两种硬件架构，空间和时间架构。时态架构主要出现在CPU和GPU中，并采用各种技术来提高并行性，如向量(<a class="ae lj" href="https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data" rel="noopener ugc nofollow" target="_blank"> SIMD </a>)或并行线程(<a class="ae lj" href="https://medium.com/@valarauca/wtf-is-a-simd-smt-simt-f9fb749f89f1" rel="noopener"> SIMT </a>)。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e4638be158c0498062251ce44fd85b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*yubMuYpNTiDs96g_B_KSfA.jpeg"/></div></figure><ul class=""><li id="6295" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated"><strong class="jp ir">时序架构</strong>对大量<a class="ae lj" href="https://study.com/academy/lesson/arithmetic-logic-unit-alu-definition-design-function.html" rel="noopener ugc nofollow" target="_blank">alu</a>使用集中控制。这些alu只能从内存层次结构中获取数据，并直接相互通信。<br/>对于CPU和GPU等时态架构，内核上的计算变换可以减少乘法次数，提高吞吐量。</li><li id="9ebf" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">空间架构</strong>使用数据流处理，即alu形成一个处理链，以便它们可以直接将数据从一个传递到另一个。空间架构通常用于ASICs和FPGAs中的dnn。<br/>为空间加速器。我们将讨论数据流如何增加存储器层次结构中低成本存储器的数据量，从而降低能耗。</li></ul><h2 id="891e" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">加速CPU和GPU平台上的内核计算</h2><ul class=""><li id="0de3" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">CPU和GPU使用SIMD和SIMT等并行化技术来并行执行MAC。所有alu共享相同的控制和存储器。</li><li id="f559" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">在这些平台上，操作被映射到矩阵乘法(内核计算)。</li><li id="79ca" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">下面是矩阵和向量乘法以及矩阵和矩阵乘法的图示。<br/>下面的输入张量的高度代表通道的数量，宽度代表每个通道的二维特征图中的元素数量。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi no"><img src="../Images/87b4d6e88dbcb70753a0bcc1835e34ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*_mdt46ho8g2Vc2AU6_HC5w.jpeg"/></div></figure><ul class=""><li id="ec9a" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated"><strong class="jp ir">卷积层的矩阵乘法:<br/> </strong>输入的特征映射矩阵中有冗余数据。这要么导致存储效率低下，要么导致复杂的内存访问模式。<br/>我们使用<a class="ae lj" href="https://en.wikipedia.org/wiki/Toeplitz_matrix" rel="noopener ugc nofollow" target="_blank">托普利兹</a>矩阵来有效映射conv层中的冗余特征。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi np"><img src="../Images/60e1b7868ca8c4d01da690f499dc70f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*yyxI-gqqQEJnMswHAupH2g.jpeg"/></div></figure><ul class=""><li id="89ab" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">这些平台上的矩阵乘法可以通过对数据应用计算变换来进一步加速，以减少乘法次数，同时给出相同的逐位结果。</li><li id="68e5" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">快速傅立叶变换:<br/></strong><a class="ae lj" href="https://www.google.com/search?q=fast+fourier+transform+matrix+multiplication&amp;oq=fast+fourier+transform+for+matri&amp;aqs=chrome.2.69i57j0l7.10781j1j7&amp;sourceid=chrome&amp;ie=UTF-8#kpvalbx=_b54-Xq3NKP2P4-EP0JOpuA418" rel="noopener ugc nofollow" target="_blank">快速傅立叶变换</a>将乘法次数从<br/> O( No *Nf)减少到O( No *log2*No)，其中<br/>输出大小为No× N o，滤波器大小为Nf×N f。<br/>我们对滤波器和输入特征图进行FFT，然后在频域中执行乘法，然后将逆FFT应用于<br/>所得乘积，以在空间域中恢复输出特征图。<br/>傅里叶变换的问题:<br/>1)FFT的好处随着滤波器尺寸的增大而减小。<br/>2)FFT的大小取决于输出特征图的大小，它通常比滤波器大得多。<br/> 3)频域中的系数是复数。因此，虽然FFT减少了计算，但它需要更大的存储容量和带宽。</li><li id="588e" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><a class="ae lj" href="https://blog.usejournal.com/understanding-winograd-fast-convolution-a75458744ff" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">铜匠-威诺格拉算法</strong> </a> <strong class="jp ir"> : <br/> </strong>该算法是快速矩阵乘法方法之一。Winograd的算法将变换应用于特征图和滤波器，以减少卷积所需的乘法次数。Winograd是逐块应用的，乘法运算的减少因滤波器和块大小而异。较大的块大小以较高复杂度的变换为代价导致乘法的较大减少。一个特别吸引人的滤波器尺寸是3 × 3，在计算一个2 × 2 <br/>输出的块时，可以将乘法次数减少2.25倍。</li><li id="4141" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><a class="ae lj" href="https://stanford.edu/~rezab/classes/cme323/S16/notes/Lecture03/cme323_lec3.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">斯特拉森算法</strong> </a> <strong class="jp ir"> : <br/> </strong>斯特拉森算法也被探索用于减少DNNs中的乘法次数。它以递归方式重新安排矩阵乘法的计算，将乘法次数从O( N)减少到O( N ^2.807)。然而，Strassen的好处是以增加存储需求和有时降低数值稳定性为代价的。</li></ul><p id="8a63" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">一般来说，FFT用于滤波器&gt;5×5<br/>wino grad，Strassen用于滤波器&lt;3×3</strong></p><h2 id="094b" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">面向加速器的高能效数据流</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4b0ef5ad7b4ebde00c59fe7eda8a0c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*1SWWNyrNvish94sLQJDZ5Q.jpeg"/></div></figure><ul class=""><li id="151e" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">DNNs的处理瓶颈在于内存访问。每个MAC需要三个存储器访问过滤器权重、fmap激活、用于存储器读取的部分求和以及用于存储器写入的一个操作。</li><li id="2045" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">目标是开发一种架构，减少从DRAM访问存储器的次数。因为DRAM存储器访问是能量昂贵的。</li><li id="0997" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">如上所述的诸如空间架构之类的加速器通过引入具有不同能量成本的不同级别的本地存储器层级来降低数据移动的能量成本。</li><li id="fa09" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">连接DRAM和内部PE(处理元件)网络的全局缓冲器的大小增加了。它可以在alu和每个PE内的寄存器文件之间传递几千字节大小的数据。这种多级存储器体系通过提供低成本数据访问来帮助提高能量效率。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a4b42b30cca03ca9840cfcb0ce1cc3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*Y23QKUWeGBFKyX7zG6n-2g.jpeg"/></div></figure><ul class=""><li id="e570" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">上图显示了从不同级别到ALU的内存访问的能耗，DRAM到ALU的能耗是ALU访问的200倍。</li></ul><p id="be8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是不同的空间架构，用于提高内存访问的能效。</p><ol class=""><li id="ab08" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk ns lq lr ls bi translated"><strong class="jp ir">权重固定(WS): <br/> </strong>通过最大化PE对寄存器文件中权重的访问，最小化读取权重的能耗。每个权重从每个PE的DRAM访问到RF，并在那里保持不变以供进一步访问。它最大化权重的卷积和滤波器重用。输入和部分和必须在空间数组和全局缓冲区中移动。</li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b2e675e770cb35cf7e561cf6d7fe4788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*-raeu27rU5i-BFRhtHxUPg.jpeg"/></div></figure><p id="0743" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir">输出固定(OS): </strong></p><ul class=""><li id="db64" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">它将相同输出激活<br/>值的部分和的累加保持在RF本地。为了在RF中保持部分和的累积稳定，一种常见的实现方式是使输入激活流过PE阵列，并将权重广播给阵列中的所有PE。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/456dd915fb6e15edd272d57e7bef1062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*nolPe9xPTMmxgg09jo_FJQ.jpeg"/></div></figure><ul class=""><li id="457c" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">有多种输出稳定变量，即OSa、OSb和OSc。变体OS A以CONV层的处理为目标，因此一次专注于来自相同信道的输出激活的处理，以便最大化卷积数据重用机会。变体OS C以FC层的处理为目标，并专注于从所有不同的通道生成<br/>输出激活，因为每个通道只有一个输出激活。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nv"><img src="../Images/35f5608f8dd7669d949196e95afb1f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*xzusWuvaSp6R5KFHAHeczg.jpeg"/></div></div></figure><p id="5545" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.<strong class="jp ir">没有本地重用(NLR): </strong></p><ul class=""><li id="2ce7" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">在这种体系结构中，我们有一个更大的全局缓冲区，没有本地存储添加到PE中。这种架构侧重于增加存储容量和最小化片外存储器带宽。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/590d009d6f5c20bdc8b2b0e7bad7a377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*RiXugl0-qALw4VPyaKGQOg.jpeg"/></div></figure><p id="41b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.<strong class="jp ir">行固定(RS): </strong></p><ul class=""><li id="aa32" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">在中提出了一种行静态数据流，旨在最大化所有类型数据(权重、像素、部分和)在RF级别的重用和累积，以提高整体能效。</li><li id="9531" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">它在PE的RF内保持滤波器权重行静止，然后将输入激活流式传输到PE。PE一次对每个滑动窗口执行MAC，这仅使用一个存储空间来累加部分和。由于在不同的滑动窗口之间存在输入激活的重叠，因此输入激活可以被保存在RF中并被重新使用。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/614d290ac671ec66926a973b6ae55961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*_IwnslZLsE_JPJ4a-zwCLg.jpeg"/></div></figure><p id="c8b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">不同数据流的能量比较</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ny"><img src="../Images/c9bf81f4b96f5b6d79e8643be5bd68b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*RXkadksjh3xnDEgNTTEktg.jpeg"/></div></div></figure><ul class=""><li id="9578" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">WS和OS数据流分别具有访问权重和部分和的最低能耗。然而，RS数据流具有最低的总能耗，因为它针对整体能效进行了优化，而不是仅针对特定的数据类型。</li></ul><h2 id="d04e" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">内存中的处理/内存中的逻辑</h2><ul class=""><li id="0649" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">虽然存在空间架构来使存储器访问高效。还有另一种体系结构，它将内存移动到靠近处理单元的位置，这样数据的移动可以最小化。使用混合信号电路设计和先进的存储技术可以帮助我们减少数据移动。</li></ul><p id="0df7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">嵌入式DRAM (eDRAM) </strong></p><ul class=""><li id="973a" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">它带来了高密度的片上存储器，以避免切换片外电容的高能源成本。eDRAM的密度<br/>比SRAM高2.85倍，能效比DRAM <br/> (DDR3)高321倍。</li><li id="1d4e" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">在DNN处理中，eDRAM可用于在芯片上存储数十兆字节的权重和激活，以避免芯片外访问。eDRAM的缺点是密度比片外DRAM低，会增加<br/>芯片成本。</li></ul><p id="2f18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3D内存</strong></p><ul class=""><li id="62cd" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">DRAM也可以使用硅通孔(<a class="ae lj" href="https://semiengineering.com/knowledge_centers/packaging/advanced-packaging/through-silicon-vias/" rel="noopener ugc nofollow" target="_blank">tsv</a>)堆叠在芯片顶部。这项技术通常被称为3-D存储器，并且已经以<a class="ae lj" href="https://www.allaboutcircuits.com/industry-articles/hybrid-memory-cubes-what-they-are-and-how-they-work/" rel="noopener ugc nofollow" target="_blank">混合存储立方体</a> (HMC)和高带宽存储器(HBM)的形式商业化。相对于现有的2-D dram，三维存储器提供了高一个数量级的带宽，并减少了高达5倍的访问能量，因为tsv的电容低于典型的片外互连。</li><li id="38a7" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><a class="ae lj" href="https://web.stanford.edu/~mgao12/pubs/tetris.asplos17.pdf" rel="noopener ugc nofollow" target="_blank">俄罗斯方块</a>探索了HMC与<a class="ae lj" href="https://dl.acm.org/doi/abs/10.1145/3007787.3001177" rel="noopener ugc nofollow" target="_blank"> Eyeriss空间架构</a>和行静态数据流的使用。它提出为计算分配比片上存储器更多的区域(即，更大的PE阵列和更小的全局缓冲器)，以便利用HMC的低能量和高吞吐量特性。它还调整数据流，以适应HMC存储器和较小的片上存储器。与采用传统二维DRAM的基准系统相比，俄罗斯方块的能耗降低了1.5倍，吞吐量增加了4.1倍</li></ul><p id="0d35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">另一种减少数据移动的方法是让计算机靠近内存，而不是让内存靠近计算机。</strong></p><p id="bcd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">乘法和累加操作可以直接集成到SRAM阵列的位单元中。</p><p id="61ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">非易失性电阻存储器</strong></p><ul class=""><li id="9171" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">乘法和累加操作也可以直接集成到高级非易失性高密度存储器中，将其用作可编程电阻元件，通常称为<a class="ae lj" href="https://www.memristor.org/reference/research/13/what-are-memristors" rel="noopener ugc nofollow" target="_blank">忆阻器</a>。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5ae80b5c023fe9206c27c8dadee12198.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*KwBBCqvnRgVs-0taLGwB-Q.jpeg"/></div></figure><p id="f434" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如图所示，以电阻的电导作为权重，电压作为输入，电流作为输出，执行乘法运算。加法是通过用基尔霍夫电流定律对不同忆阻器的电流求和来完成的。这是权重静态数据流的最终形式，因为权重总是保持在适当的位置。这种方法<br/>的优势包括降低能耗，因为计算嵌入在内存中，减少了数据移动，以及增加密度，因为内存和计算可以以类似于DRAM的密度密集封装。</p><ul class=""><li id="dae5" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">有几种流行的非易失性电阻存储器件，如<a class="ae lj" href="https://www.enterprisestorageforum.com/storage-hardware/phase-change-memory.html" rel="noopener ugc nofollow" target="_blank">相变存储器</a> (PCM)、电阻RAM ( <a class="ae lj" href="https://www.webopedia.com/TERM/R/resistive_memory_reram_rram.html" rel="noopener ugc nofollow" target="_blank"> RRAM </a>或ReRAM)、导电桥RAM ( <a class="ae lj" href="http://iopscience.iop.org/article/10.1088/0268-1242/31/11/113001/ampdf" rel="noopener ugc nofollow" target="_blank"> CBRAM </a>)和自旋转移力矩磁性<br/> RAM ( <a class="ae lj" href="https://www.mram-info.com/stt-mram" rel="noopener ugc nofollow" target="_blank"> STT-MRAM </a>)。</li><li id="4651" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">用非易失性电阻存储器进行处理有几个缺点。首先，它会受到模拟处理精度降低和ADC/DAC开销的影响。第二，阵列大小受到连接电阻器件的导线的限制；具体来说，对于大型阵列(如1000 × 1000)，导线能量占主导地位，沿导线的IR压降会降低读取精度。第三，对电阻器件进行编程的写入能量可能<br/>很昂贵，在某些情况下需要多个脉冲。最后，阻性器件也会受到器件间干扰的影响。</li></ul><h1 id="19e9" class="mw km iq bd kn mx my mz kq na nb nc kt nd ne nf kw ng nh ni kz nj nk nl lc nm bi translated"><strong class="ak">DNN模型和硬件的代码设计</strong></h1><ul class=""><li id="2d68" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">在网络的计算效率和准确性之间总是有一个折衷。当开发这样的网络时，在任何一个方面都有一个折衷。</li><li id="69c9" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">因此，我们的目标不仅是大幅降低能耗<br/>和增加吞吐量，还包括最大限度地降低精度。</li><li id="ae03" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">协同设计方法可以大致分为以下几类<br/>:<br/>；降低运算和操作数的精度(这<br/>包括从浮点到定点、<br/>降低位宽、非均匀量化和<br/>权重共享)；<br/>减少操作数量和模型大小(此<br/>包括压缩、修剪、<br/>和紧凑网络架构等技术)。</li></ul><h2 id="e5a4" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">降低精度</h2><ul class=""><li id="0f11" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated"><strong class="jp ir">量化</strong>涉及将数据映射到一个较小的量化级别集合。最终目标是最小化来自量化级别的重构数据和原始数据之间的误差。</li><li id="0b47" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">量化级数反映了精度，并最终反映了表示数据所需的位数(通常是级数<br/>的对数2)；因此，精度降低是指减少级数，从而减少位数。</li><li id="93ad" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">量化基本上包括减少比特数来表示权重值(最初)，但是现在焦点已经转移到量化激活函数上。</li><li id="8417" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">目前，大多数研究都是针对权重的量化和推理的激活，因为梯度训练对量化技术高度敏感。</li></ul><h2 id="76a3" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">均匀量化</strong></h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/00fb95e9ebc5fceb93628780d9b03440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*eu3J7FT6ycYRfMpoDPJdTg.png"/></div></figure><ul class=""><li id="2c4d" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">如上图所述，浮点数是用比特来表示的。它有三个部分，尾数、指数和符号。在32位浮点表示中，1位分配给符号，8位分配给指数，23位分配给尾数。</li><li id="b4fc" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">N位定点数可以用(1)s×m×2 f<br/>表示，其中s是符号位，m是(N-1)位尾数，f决定小数点的位置并作为比例因子。</li><li id="6794" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">上图第二部分显示了<strong class="jp ir">动态定点表示</strong>，其中小数点可以根据f的值移动。在中，我们使用7位作为尾数，如果我们使用f=3，则我们使用3位作为小数，4位作为整数。</li><li id="2083" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">动态定点表示允许f根据期望的动态范围根据<br/>变化。这对于DNNs很有用，因为权重和激活的动态范围可能非常不同。此外，动态范围也可以随着层和层类型而变化。使用动态定点，可以将权重的位宽减少到8位，将激活的位宽减少到10位，而无需对权重进行任何微调。通过微调，权重和激活都可以达到8位。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ob"><img src="../Images/28ac11f0580b1ee22bfadb5f462f0ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvrUuLjeo3EVFIOgu9eTgg.png"/></div></div></figure><p id="f23e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上表显示了权重和激活的位数的不同精度方法，还指出了精度降低时的精度损失。</p><p id="e835" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定点展示的能耗和面积影响</strong></p><blockquote class="oc od oe"><p id="958b" class="jn jo of jp b jq jr js jt ju jv jw jx og jz ka kb oh kd ke kf oi kh ki kj kk ij bi translated">8位定点加法比32位定点加法消耗的能量少3.3倍(面积少3.8倍)，比32位浮点加法消耗的能量少30倍(面积少116倍)。定点加法的能量和面积与位数近似成线性比例<br/>。</p><p id="1c75" class="jn jo of jp b jq jr js jt ju jv jw jx og jz ka kb oh kd ke kf oi kh ki kj kk ij bi translated">8位定点乘法器比32位定点乘法器消耗能量少15.5倍(面积少12.4倍)，比32位浮点乘法器消耗的能量少18.5倍(面积少27.5倍)。定点乘法的能量和面积与位数近似成二次方比例。</p></blockquote><p id="3c87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">然而，应当注意，从浮点变为定点，而不减少位宽，并不减少存储器的能量或面积成本。</strong></p><p id="fae7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">二进制量化</strong></p><ul class=""><li id="974d" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">精度甚至可以更积极地降低到一位；这个研究领域通常被称为二元网络。二进制权重(即1和1)使用二进制权重将MAC中的乘法运算简化为加法和减法运算。</li></ul><p id="5896" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">三进制量化</strong></p><ul class=""><li id="8f06" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">三元量化将权重限制为3个值-1，0，+1。虽然<br/>与二进制<br/>权重相比，这需要每权重一个额外的比特，但是可以利用权重的稀疏性来降低<br/>计算和存储成本，这可以潜在地抵消<br/>额外比特的成本。</li><li id="0f12" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">已经开发了许多使用二进制和三进制量化技术的网络，例如<a class="ae lj" href="https://arxiv.org/abs/1605.04711" rel="noopener ugc nofollow" target="_blank">三进制权重网络</a>(TWNs)<a class="ae lj" href="https://arxiv.org/abs/1612.01064" rel="noopener ugc nofollow" target="_blank">训练三进制量化(TTQ) </a>、<a class="ae lj" href="https://arxiv.org/abs/1606.05487" rel="noopener ugc nofollow" target="_blank">约丹</a>使用二进制权重，而<a class="ae lj" href="https://arxiv.org/pdf/1806.07550.pdf" rel="noopener ugc nofollow" target="_blank">布雷因</a>使用二进制权重和激活。SRAM工作中的计算也使用二进制权重。最后，名义上受spike启发的<a class="ae lj" href="http://www.research.ibm.com/articles/brain-chip.shtml" rel="noopener ugc nofollow" target="_blank"> TrueNorth芯片</a>可以使用TrueNorth的量化权重，通过二进制激活和三进制权重来实现精度降低的神经网络。</li></ul><h2 id="b5e8" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">非均匀量化</strong></h2><ul class=""><li id="a650" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">已经表明，权重和激活的分布是不均匀的，因此，当级别之间的间隔变化时，不均匀量化可以潜在地提高准确性。</li></ul><p id="e848" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">对数域量化</strong></p><ul class=""><li id="30b2" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">如果基于对数分布来分配量化级别，则权重和激活在不同级别上更平均地分布，并且每个级别被更有效地使用，从而导致更少的量化误差。例如，使用4 b和统一量化<br/>会导致27.8%的精度损失，相比之下，VGG-16的log base-2量化会导致5%的精度损失。</li><li id="924f" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><a class="ae lj" href="https://arxiv.org/abs/1702.03044" rel="noopener ugc nofollow" target="_blank">增量网络量化(INQ) </a>通过将大小权值分成不同的组，然后迭代量化和重新训练权值，可以进一步降低精度损失。</li></ul><p id="2a13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">学习量化或权重分配</strong></p><ul class=""><li id="4bbe" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">权重共享强制几个权重共享一个值；这减少了过滤器或层中唯一权重的数量。可以使用散列函数或k-means算法对权重进行分组，并且为每组分配一个值。</li><li id="cfcc" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">然后构建码本，以将每组权重映射到其共享值。<br/>因此，为滤波器中的每个位置存储码本中对应组的索引，而不是权重值。</li><li id="6457" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">注意，与之前的量化方法不同，权重共享方法不会降低MAC计算本身的精度<br/>，只会降低权重存储要求。</li></ul><h2 id="b87a" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated"><strong class="ak">减少操作次数和模型尺寸</strong></h2><p id="8e45" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">对于减少运算次数和模型大小的方法已经有了大量的研究。</p><p id="290f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">利用激活统计</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi oj"><img src="../Images/90724b728ba2969e502a69a7d456f9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGxnlDReBFD2seMq5NEd9w.jpeg"/></div></div></figure><ul class=""><li id="aeb9" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">ReLU是DNNs中常用的非线性形式，它将所有负值设置为零。因此，ReLu操作后的输出激活是稀疏的。Alexnet中的特征图稀疏度在19%到63%之间，如边上的图表所示。</li><li id="bd45" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">可以利用这种稀疏性，通过压缩节省能量和面积，特别是对于昂贵的片外DRAM访问。</li><li id="67d9" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">除了压缩之外，还可以修改硬件，以便<br/>跳过读取权重和执行零值激活的MAC，从而降低45%的能源成本。除了选通读取和MAC计算，硬件还可以跳过该周期，将吞吐量提高1.37倍。</li></ul><p id="113b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">网络修剪</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7ada5d9021f083b475e41f2e1f105b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*m1VaRk5_iJwq3TFWeoHD7w.jpeg"/></div></figure><ul class=""><li id="aec4" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">为了使网络训练更容易，网络通常被过度参数化。因此，网络中的大量权重是冗余的，并且可以被移除(即设置为零)。这个过程称为网络修剪。</li><li id="a538" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">低显著性权重被移除，剩余的权重被微调；重复这一过程，直到达到所需的重量减轻和精度。</li><li id="3773" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">可以使用能量评估方法来估计DNN能量，该能量考虑了来自存储器分级结构的不同级别的数据移动、MAC的数量以及数据稀疏性。</li><li id="2dbf" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">能量感知修剪</strong>可用于根据能量修剪权重，以将AlexNet所有层的总能量减少3.7倍，比基于幅度的方法效率高1.74倍。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/69d68130d2ad70a44e1a742e3f339e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*AMX6b6IcHrT68rFUyljLtA.jpeg"/></div></figure><ul class=""><li id="b821" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">一个感兴趣的领域是如何在修剪后最好地存储稀疏权重。当DNN处理作为矩阵向量乘法执行时，一个挑战是确定如何以压缩格式存储稀疏权重矩阵。压缩可以按行或列顺序应用。<strong class="jp ir">压缩稀疏行(CSR) </strong>格式通常用于执行稀疏矩阵-向量乘法。然而，由于矩阵的每一行都是稀疏的，即使只使用了输入向量的一个子集，也需要多次读取输入向量。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0b44d69729cf7f1f9677a641da22189b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*kz7xEk1lgPw65z0thnSpzA.jpeg"/></div></figure><ul class=""><li id="145d" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated"><strong class="jp ir">可使用压缩稀疏列(CSC)格式</strong>，其中输出更新数次，一次仅读取输入向量的一个元素。如果输出小于输入，或者在DNN的情况下，如果滤波器的数量没有明显大于滤波器中的权重数量[C × R × S]，CSC格式将提供比CSR更低的整体存储器带宽。因为这通常是真的，CSC可以是稀疏DNN处理的有效格式。</li><li id="5a01" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">最近的工作还探索了使用结构化修剪来避免对定制硬件的需要。结构化修剪涉及修剪<br/>组权重(也称为粗粒度修剪)，而不是修剪单个权重(也称为细粒度修剪)。</li><li id="d9f1" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">结构化修剪的好处如下:1)得到的权重可以更好地与现有通用硬件中的数据并行架构(例如，SIMD)一致，这导致更有效的处理；以及2)它分摊了跨一组权重发信号通知非零权重的<br/>位置所需的开销成本，这改进了压缩，从而降低了存储成本。</li></ul><p id="916f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">紧凑型网络架构</strong></p><ul class=""><li id="4a9b" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated"><strong class="jp ir">训练前</strong>:在最近的DNN车型中，宽度和高度较小的过滤器使用得更频繁，因为将几个过滤器串联起来可以模拟更大的过滤器。例如，一个5 × 5卷积可以用两个3 × 3卷积代替。或者，一个N × N卷积可以分解成两个1-D卷积，一个<br/> 1 × N和一个N × 1卷积。这基本上施加了二维滤波器必须是可分离的限制，这是图像处理中的常见约束。<br/>同样，一个三维卷积可以用一组二维卷积(即仅应用于一个输入通道)来代替，后面是1 × 1三维卷积，如Xception和MobileNets所示。二维卷积和1 × 1 -3D卷积的顺序可以互换。<br/> 1 × 1卷积层也可用于减少给定层的输出特征图中的通道数量，这减少了滤波器通道的数量，从而减少了<br/>下一层中滤波器的计算成本。</li><li id="8df8" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">训练后:</strong>张量分解可用于分解已训练网络中的滤波器，而不会影响精度。它将层中的权重视为四维张量，并将其分解为更小张量的组合(即几层)。然后可以应用低秩近似，以精度下降为代价进一步提高压缩率，精度下降可以通过微调权重来恢复。<br/><a class="ae lj" href="https://www.tensorlab.net/doc/cpd.html" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"/></a>正则多元分解，奇异值分解的高阶扩展，可以通过各种方法求解，或者非线性最小二乘法。将CP分解与低秩近似相结合，在CPU上实现了4.5倍的加速。</li></ul><p id="5280" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">知识蒸馏</strong></p><ul class=""><li id="ad9b" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">这种技术试图从经过训练的更大的网络(教师)中创建一个更小的网络(学生)。因此，学生网络可以达到用相同的数据集直接训练时无法达到的精度。</li></ul><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d304e31d64554bc13b643d637f5ce060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*9_-kiIUO-3DpIkYmpekpvA.jpeg"/></div></figure><ul class=""><li id="f0a3" class="lk ll iq jp b jq jr ju jv jy mb kc mc kg md kk lp lq lr ls bi translated">对于该知识提炼方法，教师DNN(或教师DNN的集合)的班级分数被用作目标，并且目标是最小化目标和学生DNN的班级分数之间的平方差。类分数而不是类概率被用作目标，因为softmax层通过将相应的类概率推向0来消除小类分数中包含的重要信息。或者，如果softmax被配置为生成更平滑的类别概率分布，更好地保留小类别分数，则类别概率可以<br/>用作目标。</li></ul><h1 id="849d" class="mw km iq bd kn mx my mz kq na nb nc kt nd ne nf kw ng nh ni kz nj nk nl lc nm bi translated"><strong class="ak">如何量化硬件和神经网络之间的化学作用(基准评估指标)？</strong></h1><ul class=""><li id="823c" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">当在电池容量有限的嵌入式设备(例如，智能手机、智能传感器、无人机和可穿戴设备)的边缘处理dnn时，或者在由于冷却成本而具有严格功率上限的数据中心的云中处理dnn时，能量和功率都很重要。由于延迟、隐私或通信带宽限制，对于某些应用，边缘处理优于云。</li><li id="05f5" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">高吞吐量</strong>是为导航和机器人等交互式应用提供实时性能所必需的。随着可视化数据量呈指数级增长，高吞吐量大数据分析变得非常重要，尤其是在需要根据分析采取行动的情况下。</li><li id="02df" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated"><strong class="jp ir">低延迟</strong>是实时交互应用所必需的。延迟衡量像素到达系统和产生结果之间的时间。延迟以秒为单位，而吞吐量以操作数/秒为单位。</li></ul><blockquote class="oc od oe"><p id="0a11" class="jn jo of jp b jq jr js jt ju jv jw jx og jz ka kb oh kd ke kf oi kh ki kj kk ij bi translated">硬件成本在很大程度上取决于片上存储量和内核数量。典型的嵌入式处理器具有大约几百千字节的有限片上存储。由于片内存储器数量和外部存储器带宽之间存在权衡，因此两个指标都应该报告。同样，内核数量和吞吐量之间也存在相关性。此外，虽然一个芯片上可以构建多个内核，但应该报告在给定时间内实际可以使用的内核数量。</p></blockquote><h2 id="0201" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">DNN模型的指标</h2><ul class=""><li id="c4f7" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">该模型在ImageNet等数据集上前5名错误方面的准确性。</li><li id="b4a0" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">应报告模型的网络架构，包括层数、过滤器尺寸、过滤器数量和通道数量。</li><li id="d386" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">权重的数量会影响模型的存储要求，因此应予以报告。</li><li id="bac9" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">应报告需要执行的MAC数量，因为它在一定程度上表明了给定DNN的操作数量和潜在吞吐量。</li></ul><h2 id="f159" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">DNN硬件的指标</h2><ul class=""><li id="4c22" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">应提供DNN模型规范，包括测量期间硬件支持哪些层和位精度。</li><li id="2ad1" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">需要提到DRAM或片外访问的数量。可以根据每次推理在片外读取和写入的数据总量进行报告。</li><li id="275f" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">延迟和吞吐量应该按照批处理大小和各种DNN模型的实际运行时间来报告，这说明了映射和内存带宽的影响。</li><li id="0e7c" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">芯片的成本取决于面积效率，面积效率考虑了存储器(例如寄存器或SRAM)的大小和类型以及控制逻辑的数量。</li></ul><p id="7261" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我想我已经给了你们很多需要消化的内容。但肯定的是，这个故事只是冰山一角，在这个领域已经有了很多有前途的东西，使人工智能计算最接近人类互动。</p><h2 id="f2d9" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">承认</h2><ul class=""><li id="19b7" class="lk ll iq jp b jq le ju lf jy lm kc ln kg lo kk lp lq lr ls bi translated">硬件调查论文:<a class="ae lj" href="https://people.csail.mit.edu/emer/papers/2017.12.pieee.DNN_hardware_survey.pdf" rel="noopener ugc nofollow" target="_blank">https://people . csail . MIT . edu/emer/papers/2017.12 . pieee . dnn _ hardware _ survey . pdf</a></li><li id="6a67" class="lk ll iq jp b jq me ju mf jy mg kc mh kg mi kk lp lq lr ls bi translated">https://www.youtube.com/watch?v=WbLQqPw_n88<a class="ae lj" href="https://www.youtube.com/watch?v=WbLQqPw_n88" rel="noopener ugc nofollow" target="_blank"/></li></ul></div></div>    
</body>
</html>