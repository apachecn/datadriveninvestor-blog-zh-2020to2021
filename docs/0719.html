<html>
<head>
<title>Machine Learning: Getting Started With K-Nearest Neighbor.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:K近邻入门。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/machine-learning-getting-started-with-k-nearest-neighbours-6851280d4c93?source=collection_archive---------2-----------------------#2020-02-13">https://medium.datadriveninvestor.com/machine-learning-getting-started-with-k-nearest-neighbours-6851280d4c93?source=collection_archive---------2-----------------------#2020-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/08b35d1dca0b54cc091808276c9767df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*az0eLms286TQ__19_tI-Ew.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Google</figcaption></figure><div class=""/><p id="2fd7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">我们将从这篇文章中学到什么:</strong></p><ol class=""><li id="c0e0" class="la lb jf ke b kf kg kj kk kn lc kr ld kv le kz lf lg lh li bi translated">K-NN是什么？</li><li id="86c5" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">决策面如何随着K值的变化而变化</li><li id="1104" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">K-NN的过拟合和欠拟合</li><li id="abc1" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">K-NN中使用的距离类型</li><li id="1286" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">K-NN的时空复杂度</li><li id="139a" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">回归的K-NN</li><li id="7c57" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">K-NN在现实世界中的应用</li></ol><p id="e5aa" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">K-最近邻或K-NN是一种基本、简单但功能强大的机器学习算法之一。K-NN可用于分类和回归。在这篇博客中，我们将主要关注分类部分，因为K-NN主要用于分类。</p><p id="569a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">什么是K-NN？它是做什么的？这个算法是如何工作的？</strong></p><p id="3093" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">简而言之，给定一个用于分类的数据集，K-NN试图在类之间绘制一个边界表面来分隔它们。现在，当一个查询点被提供给K-NN时，它计算该查询点和周围K个最近的数据点之间的距离，并且基于大多数类点，给定的查询点被分类。</p><h1 id="5618" class="lo lp jf bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">示例:</h1><figure class="mn mo mp mq gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/34b8921e0de9c90476e66177c6225a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*U9dOThJUekSHmdXrsSbr_A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Google</figcaption></figure><p id="d8a1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从上图可以看出:</p><p id="c9bd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">图1: </strong>我们的数据中有两类，分别是星形和三角形。数据集中引入了一个新点，我们需要对这个新点进行分类。</p><p id="d252" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">图片2: </strong>计算距离我们新数据点最近的点之间的距离。离新数据点最近的点是相邻的数据点。</p><p id="f4fe" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">图3: </strong>假设我们已经给定了K值为3。所以这里考虑最近的三个相邻点。在最近的三个相邻点中，我们认为大多数点是三角形(三角形= 2，星形=1)。因此，我们的新数据点现在被归类为三角形。</p><p id="5973" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">简单对吧？？</strong>恭喜，现在你已经理解了K近邻的基本功能。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9fac0f331e3e0583eb836e8f132971cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*yL9LTNwamvdc3O2GKu0P6g.gif"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Google</figcaption></figure><p id="58c8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">你可能会有疑问。<strong class="ke jg">我们如何挑选K的正确值？</strong></p><p id="551a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里K-NN中的K是一个<strong class="ke jg"> <em class="ms">超参数</em> </strong>。通常我们使用奇数作为K的值，因为算法基于多数来决定查询点的类别。k可以是3，5，7，…但不能是1。我们在选择K值时需要小心，因为它可能会导致过拟合或欠拟合的问题。k既不能太小，也不能太大。</p><p id="c84e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">那么，<strong class="ke jg">什么是过拟合和欠拟合呢？？</strong></p><p id="4044" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在理解过拟合和欠拟合的概念之前，我们先来理解一下决策曲面的概念。</p><p id="3437" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">决策面:</strong>它可以被定义为一个在给定的数据集中分离类的面。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mt"><img src="../Images/97d173b8a0f3f1f8a8e95007abfc6579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3x2mf6gZPdAmGuHLY9zEMA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Google</figcaption></figure><p id="62b0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上图中分隔三个类别的表面称为决策表面或决策边界。</p><p id="e175" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当K值改变时，决策面如何改变:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mu"><img src="../Images/6b35f632852b78dddbd4e68668a3e52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWTAwUQrimxdgRXICZYnjw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">source: Google</figcaption></figure><p id="921c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> K = 1 </strong></p><p id="2a7d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上图所示，当K=1时，我们的算法试图确保它创建一个完全分隔两个类的表面。假设，我们在黄色区域中有一个紫色点P1，当在黄色区域中给定一个查询点，并且该查询点的最近邻居是P1(黄色区域中的紫色点)时，该查询点被分类为紫色点，尽管该查询点周围的所有点都是黄色点。由于K的值=1，我们不考虑其他邻居。这里，查询点可能被错误分类。</p><p id="c454" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当K-NN算法创建了如图1所示的完全分离两个类的表面时，那么可以说它正在试图<strong class="ke jg">过度拟合</strong>。</p><p id="fb01" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> K = 5，7，15..</strong></p><p id="2f57" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上图所示，随着K值的增加，决策面变得更加平滑。</p><p id="eeb3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> K = n </strong></p><p id="c43e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">类似地，当我们在给定的数据集中有n个数据点并且K = n时，无论我们的查询属于哪个类，我们的算法返回的查询点都是具有最高个数据点的类。这里，这被认为是K-NN中的<strong class="ke jg">欠拟合</strong>。</p><h1 id="ab22" class="lo lp jf bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">K-NN如何计算两点之间的距离？</h1><p id="5906" class="pw-post-body-paragraph kc kd jf ke b kf mv kh ki kj mw kl km kn mx kp kq kr my kt ku kv mz kx ky kz ij bi translated">K-NN中的距离可以通过三种方式计算:</p><ol class=""><li id="ddd4" class="la lb jf ke b kf kg kj kk kn lc kr ld kv le kz lf lg lh li bi translated">欧几里得距离</li><li id="dccb" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">曼哈顿距离</li><li id="c280" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">闵可夫斯基距离</li></ol><p id="4b6c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">欧几里德距离(L2): </strong></p><p id="fe10" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">向量的L2范数通常被称为两个数据点或两个向量之间最短直线的距离。两点X1和X2之间的欧几里德距离可以通过给定的公式计算:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/c3bcf7a9ef1c98272600076354c0f937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K49opfj3w4b7RQt8MwbIlw.jpeg"/></div></div></figure><p id="0d9f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">其中X1和X2属于d维。</p><p id="f705" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">曼哈顿距离(L1): </strong></p><p id="df4d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通常被称为向量的L1范数。两点X1和X2之间的曼哈顿距离可以通过以下公式计算:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/c8a43998b0321b868bc00a2c7dd7b4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8NQSHQcyzs8A54US4GGrQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd lq">P.S.: it is the sum of all the absolute differences between the two points in d-dimensions. |X1i-X2i|***</strong></figcaption></figure><p id="56a5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">其中X1和X2属于d维。</p><p id="4eaa" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">闵可夫斯基距离(LP): </strong></p><p id="e12e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通常称为向量的LP范数。两点X1和X2之间的闵可夫斯基距离可以通过以下公式计算:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/92e26df0137d686c0a62569723932d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgqLlsgclkNEYkSc-NlS6g.jpeg"/></div></div></figure><p id="8c09" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果P的值是2，那么闵可夫斯基距离与欧几里德距离相同。</p><p id="9236" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果P的值为1，那么闵可夫斯基距离与曼哈顿距离相同。</p><p id="ef3c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在实现K-NN算法时，欧几里德距离被用作默认距离。</p><p id="eed3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">K-NN的空间和时间复杂度:</strong></p><p id="18ed" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">K-NN的时间和空间复杂度较高，被认为是其局限性之一。</p><p id="28a8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">空间复杂度:O(nd)</p><p id="6b50" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">时间复杂度:O(nd)</p><p id="0bd6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">偏差&amp;方差权衡:</strong></p><p id="db67" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果K值很低，模型会尝试过度拟合。</p><p id="0c95" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果K值很高，模型会尝试进行欠拟合。</p><p id="6dd0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这都是为了K-NN分类。现在让我们来看看K-NN是如何实现回归的。</p><p id="f805" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">用于回归的K-NN:</strong></p><p id="e3b3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如我们前面讨论的，在K-NN分类中，在计算查询点和K个最近邻点之间的距离之后，该算法基于多数投票对查询点进行分类。</p><p id="7374" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">类似地，在K-NN回归中，我们计算查询点和K个最近邻点之间的距离，现在我们计算所有最近邻点的平均值/中值，而不是进行多数投票。</p><p id="e938" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">注意:平均值会受到异常值的影响，但是，中位数受异常值的影响最小。</p><p id="ce41" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">K近邻在现实世界中的应用:</strong></p><ol class=""><li id="d6cb" class="la lb jf ke b kf kg kj kk kn lc kr ld kv le kz lf lg lh li bi translated">K-NN可以用来对文本进行分类。</li><li id="7945" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">股票市场预测是KNN最核心的财务任务之一。</li><li id="563d" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">推荐系统也是K-NN可以应用的一个领域。</li><li id="05bc" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated">K-NN在医学领域也有很多应用。</li></ol><p id="dead" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">关于K-NN的更多信息，可以参考:</strong></p><ol class=""><li id="90db" class="la lb jf ke b kf kg kj kk kn lc kr ld kv le kz lf lg lh li bi translated">【https://scikit-learn.org/stable/modules/neighbors.html T4】</li><li id="9172" class="la lb jf ke b kf lj kj lk kn ll kr lm kv ln kz lf lg lh li bi translated"><a class="ae nd" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . neighbors . kneighborsclassifier . html</a></li></ol><h1 id="1462" class="lo lp jf bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">如果你学到了一些新的东西或者喜欢阅读这篇文章，请分享给其他人看。也可以随意发表评论。</h1><figure class="mn mo mp mq gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7b322e48a1d73b1b36e0cd28279d5c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/1*upds9oU3Sf4AGMZCCJ6s0g.gif"/></div></figure><figure class="mn mo mp mq gt is"><div class="bz fp l di"><div class="nf ng l"/></div></figure></div></div>    
</body>
</html>