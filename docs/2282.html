<html>
<head>
<title>Fundamental of Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归基础</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/fundamental-of-linear-regression-8306aee2a15d?source=collection_archive---------3-----------------------#2020-04-22">https://medium.datadriveninvestor.com/fundamental-of-linear-regression-8306aee2a15d?source=collection_archive---------3-----------------------#2020-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7dff" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Python从头开始构建一个简单的线性回归模型！</h2></div><h2 id="b4ce" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">开始之前，让我们设定目标:</h2><ol class=""><li id="0570" class="lb lc iq ld b le lf lg lh ko li ks lj kw lk ll lm ln lo lp bi translated">了解线性回归的基本原理。</li><li id="ed52" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">理解用梯度下降算法优化线性回归模型的参数。</li><li id="14f0" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">用Python从头开始构建一个简单的线性回归模型。</li><li id="9f52" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">在Sklearn库中实现线性回归模型。</li></ol><p id="bca5" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">根据维基百科的定义，监督学习是一种机器学习任务，它学习一种基于一组输入输出数据将输入映射到输出的函数。线性回归是最简单的监督学习算法之一，它是从统计学“借用”来的。</p><p id="8661" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">在我们深入线性回归的数学和算法之前，让我们考虑一个简单的问题:</p><p id="e484" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">假设我们有如下表所示的工资和工作经验数据:</p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="ak">Source: </strong><a class="ae mv" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ak">https://www.kaggle.com/</strong></a></figcaption></figure><blockquote class="mw mx my"><p id="92c8" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">数据电子表格可在此下载<a class="ae mv" href="https://docs.google.com/spreadsheets/d/1mROKl21pI-z2dc-hCqRJlhwjbSVyb_JS4pHHlIR-FOc/export?gid=0&amp;format=csv" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3634ef190a5103e3ae5b3392579e950f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*wtVfF0wB2ZfWI70g89HKQA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Plot of the data in Spreadsheet</strong></figcaption></figure><p id="7878" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">现在，假设你正在寻找一份工作，并希望在发送简历之前预测期望的薪水。显然，很难根据上面的情节做出预测。所以，这里有<strong class="ld ir">线性回归</strong>来帮你解决。如前所述，线性回归是将<strong class="ld ir">独立</strong>(又名<strong class="ld ir">预测器/输入/特征</strong>)变量映射到<strong class="ld ir">相关</strong>(又名<strong class="ld ir">响应/输出/目标</strong>)变量的函数。在这种情况下，自变量是经验，因变量是工资。我注意到在这种情况下只有一个独立变量。这就是我们所说的<a class="ae mv" href="https://en.wikipedia.org/wiki/Simple_linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">简单线性回归</strong> </a>。</p><div class="ng nh gp gr ni nj"><a href="https://www.datadriveninvestor.com/2020/03/24/encoder-decoder-sequences-how-long-is-too-long/" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">编码器解码器序列:多长是太长？数据驱动的投资者</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">在机器学习中，很多时候我们处理的输入是序列，输出也是序列。我们称这样的一个…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ne nj"/></div></div></a></div><blockquote class="mw mx my"><p id="ff12" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">现在让我们看看简单线性回归背后的数学！</p></blockquote><p id="6774" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">相信你对“<strong class="ld ir"> y = mx + c”，</strong>很熟悉吧？这是一个包含两个变量的线性方程。现在，看上面数据的曲线图。你能从上面的图表中注意到经验和薪水之间的线性关系吗？</p><p id="c501" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">设<strong class="ld ir"> x </strong>为经验，<strong class="ld ir"> y </strong>为工资。通过将y近似为x的线性函数，</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/f9522e08e2fd764303d97e2f395229fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*tiIN9A0kddpVEoKqVlydpg.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Simple linear regression which involved only one independent variable</strong></figcaption></figure><p id="ba42" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">或者更一般地说</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/adc6afc9b7ae7890eaa6f0365b17235e.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*NXOtDwejZkmvF7pSR0eNsg.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">General equation for linear regression where n is the number of independent variables</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/2cb2ebe914f59ee410915a1305602215.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*7ANbJrIjXOJrfL_uOzQKmQ.png"/></div></figure><p id="5d1c" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">这里，h(x)是将输入x映射到输出y的预测模型。θj是线性回归模型的参数(权重)。(注:θo通常被称为偏差项)<br/>因此，现在我们的目标是获得权重和偏差的最佳值，这将最小化<strong class="ld ir">实际值(y) </strong>和<strong class="ld ir">预测值(h(x)) </strong>之间的误差。选择误差平方和作为评估该模型的度量。误差平方和函数称为<strong class="ld ir">成本(</strong>又称<strong class="ld ir">损失)函数</strong>，J(θ)。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7e0fd1c098671c8ae944e4ae59bd487e.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*4aTYDCKYOAd3EbLQzSYGzA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Note: The superscript (i) here is simply the index of data, and has nothing to do with mathematical exponent. And, m is the number of data, which is 30 in this case.</strong></figcaption></figure><p id="5118" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">“为什么平方误差？”</strong></p><blockquote class="mw mx my"><p id="44ad" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">因为有正负误差会相互抵消。所以这里的“平方”是必要的，以避免正负抵消。</p></blockquote><p id="bc5f" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">“还有，为什么求和前面有1/2？”</strong></p><blockquote class="mw mx my"><p id="ebed" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">这只是因为数学上的方便，当我们求一阶导数时，2会被1/2抵消。我们将在下面的梯度下降算法中看到它。</p></blockquote><p id="6e24" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">在定义了成本函数J(θ)之后，我们将通过优化算法来最小化J(θ)。</p><h2 id="4907" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">梯度下降算法</h2><p id="7ae4" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko oc mc md ks od mf mg kw oe mi mj ll ij bi translated"><a class="ae mv" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">梯度下降</strong> </a>是一种最小化函数的迭代优化算法。为了通过梯度下降算法找到函数的最小值，我们通过与函数在当前点的梯度的负值成比例的步长迭代地更新θj。让我们来看看相关的方程和可视化。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi of"><img src="../Images/7e7b474345a960ce941627123813a54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*RaRVFHhUzvgOyTKEzqgU3w.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Parameter θj is updated in every iteration according to this equation.</strong></figcaption></figure><p id="9c20" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">梯度下降算法如下图所示。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi og"><img src="../Images/27f47a93433b6c683e51cc3609a5eecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/1*e88JKNWAFok3vpjeuPfHig.gif"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Source: </strong><a class="ae mv" href="https://donsoft.io/deep-learning-with-rnns/images/gradient_descent_cropped.gif" rel="noopener ugc nofollow" target="_blank"><strong class="bd kh">https://donsoft.io/deep-learning-with-rnns/images/gradient_descent_cropped.gif</strong></a></figcaption></figure><p id="bd2c" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">为了计算J(θ)的梯度以更新权重θj，我们对θj取偏导数:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e1b8147b6e7eb9d82ebd24727822ae3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*0UB9p66AKuL1bh-l9fry-w.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Look at second step to third step, the term 2 and 1/2 cancelled out each other as mentioned before.</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/fb715e8e0e9bef3c7244b74c00ad888b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*WbR_hQXteLDL4A_coQjQOA.png"/></div></figure><p id="de4a" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">迭代算法:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3e6e370f7107c08a5c061b01ee0b3536.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*bhAJA5DL1HpW92nyX-eZnw.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">This is known as Batch Gradient Descent</strong></figcaption></figure><blockquote class="mw mx my"><p id="faa2" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">还有两种其他类型的梯度下降，即随机梯度下降(SGD)和小批量梯度下降，这将在即将发布的帖子中介绍。</p></blockquote><h2 id="8a72" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">Python实现</h2><p id="2dc2" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko oc mc md ks od mf mg kw oe mi mj ll ij bi translated">好了，算够了。现在，我们到了激动人心的部分，我们将用Python代码构建一个线性回归模型。还记得我们在本帖开头看到的数据吗？我们将使用它来训练我们的线性回归模型，根据多年的经验来预测工资。当您浏览代码时，您可能想知道什么是' @ '操作符。嗯，简单来说就是矩阵乘法。供您参考，Python 3.5在2014年引入了<a class="ae mv" href="https://www.python.org/dev/peps/pep-0465/" rel="noopener ugc nofollow" target="_blank">操作符</a>。</p><p id="0ad3" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">导入相关库:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="64a3" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><code class="fe ol om on oo b"><a class="ae mv" href="https://pypi.org/project/pandas/" rel="noopener ugc nofollow" target="_blank">Pandas</a></code>库将用于从csv文件中读取数据。由于我们将根据矩阵形式执行算法，因此需要<code class="fe ol om on oo b"><a class="ae mv" href="https://pypi.org/project/pandas/" rel="noopener ugc nofollow" target="_blank">NumPy</a></code>库来帮助计算。最后，<code class="fe ol om on oo b"><a class="ae mv" href="https://pypi.org/project/matplotlib/" rel="noopener ugc nofollow" target="_blank">matplotlib</a></code>用于绘制结果以便更好的可视化。</p><p id="d480" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">定义梯度下降算法的功能:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="5df5" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">首先，我们初始化两个数组，即<code class="fe ol om on oo b"><strong class="ld ir">loss_history</strong></code>和<code class="fe ol om on oo b"><strong class="ld ir">gradient_history</strong></code>，以保存每次迭代的历史损失值和梯度值。对于每次迭代，每个数据的预测值被计算为一个数组(<code class="fe ol om on oo b"><strong class="ld ir">h</strong></code>)。'【T6]'的操作可以如下图所示，其中θ是权重的向量。结果是每个x值的预测y值的向量。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7851f20951082a04cc3f6b9d07c99f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*DV-8DdgU-TLh4x8Sy7uFbQ.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">There are 30 pairs of (x, y) in our data, so the superscript is indicating the index of the respective data.</strong></figcaption></figure><p id="c22d" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">然后，通过从预测值中减去实际y值来计算每个数据的误差。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/923ce8c604eb7b67916f1507144ecd72.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*qOeqiQbHoX04pD1lM5nLBA.png"/></div></figure><p id="b373" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">所以，在我们的Python代码中，<code class="fe ol om on oo b"><strong class="ld ir">h-y</strong></code> <strong class="ld ir"> </strong>代表长度为30的错误向量:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi or"><img src="../Images/80433207efa99c525f48a7025df02216.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*fJ_Fec8gMH8LKkznhe9FYw.png"/></div></figure><p id="607c" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">损失或成本(<code class="fe ol om on oo b"><strong class="ld ir">J</strong></code>)通过使用之前定义的等式来计算(<code class="fe ol om on oo b"><strong class="ld ir">np.sum(error ** 2) / 2</strong></code>)，该等式为:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7adfa1f76537ab52fe98dde404ce240c.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*m5-sdqHRz4wQ_-THpqNBXA.png"/></div></figure><p id="855b" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">然后，基于之前导出的等式计算损失(成本)函数的梯度:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/794c6fedc7825c763c27e33591750379.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*GsYLBWE_FfqsuJ0g0sTVKw.png"/></div></figure><p id="1348" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">因此，在我们的Python代码中，<code class="fe ol om on oo b"><strong class="ld ir">gradients = x.T @ e</strong></code>是分别计算关于θ0和θ1的损失(成本)函数梯度的运算:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/dfedc1ad8b417909f3971f2c8a22a56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*CXtaiNAWyrNSm4gvetxRkw.png"/></div></figure><p id="22eb" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">最后，我们更新权重(<code class="fe ol om on oo b"><strong class="ld ir">weights -= alpha * gradients</strong></code>)。</p><p id="22d1" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">加载数据:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="3e45" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">请注意，y值被除以1000，因此我们在优化后不会获得非常大的权重值。这样做时，需要格外小心，无论何时我们做预测，结果必须乘以1000才能得到正确的预测。</p><p id="e625" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">运行梯度下降算法获得权重的最优值:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="9cb1" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">训练前权重初始化为0。学习率设置为0.001，迭代次数设置为500。</p><p id="ff3b" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">剧情:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><h2 id="e03d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果</h2><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/5e81379c4cab823fbf8c715278df6940.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*UlHJQTHB2X-USfKTwHJM9g.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">A part of output in terminal during the training (gradient descent optimization) process.</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi ow"><img src="../Images/a010a15b83719f0e5a8f2c01a6b4a90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gdEKHnQ-BZR8X7z9HBfVCQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">Figure showed that a linear line (computed through gradient descent algorithm) is fitted according to the dataset.</strong></figcaption></figure><p id="b9b5" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">在获得优化的权重之后，预测模型可以写成简单的线性方程<code class="fe ol om on oo b"><strong class="ld ir">h = 24.802723 + 9.596797x</strong></code>。<br/>现在假设我有3年的工作经验:<br/>所以，我的期望工资，h = 24.802723 + 9.596797(3)，也就是$53.593114‬‬k.</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pb"><img src="../Images/d8fd488052d332ed25d5f162f4282115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mm6Q-wQcPj6FgjyNrUeuQ.png"/></div></div></figure><p id="8fbc" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从上面的损失与迭代的关系图(仅显示前100次迭代)中，我们可以观察到损失非常迅速地向最小值收敛。这是因为在这种情况下，学习率0.001被认为很高。现在，让我们改变学习速度并观察结果。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pc"><img src="../Images/6f53130c2ff9fcb41d86bf73262afa9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fl7Tm9rcqDvqo61vf37EJw.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">learning rate = 0.0001 (low)</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pb"><img src="../Images/d8fd488052d332ed25d5f162f4282115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mm6Q-wQcPj6FgjyNrUeuQ.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">learning rate = 0.001 (high)</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pd"><img src="../Images/06f7dae943a0147fbf0c22f8c9408680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bkNEAzMgdIWzGHN85j60A.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">learning rate = 0.002 (too high)</strong></figcaption></figure><p id="3c6a" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从图中可以看出，当学习率为0.0001时，损失值非常缓慢地向最小值收敛。当学习率增加到0.001时，损失值以更高的速率向最小值收敛。然而，当学习率增加到0.002时，损失值根本没有向最小值收敛！这是因为梯度从正值到负值振荡(幅度越来越大！)在整个迭代中。让我们观察下面的梯度与迭代的关系图，以便有更好的视觉效果。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/e183368c4be303643f263c4891685daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*zM1aSXZgZNrZw7x477r0zQ.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">graph of gradients against each iteration (learning rate = 0.0001)</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pf"><img src="../Images/05bb238c135ce2922a3ccecd21380fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*bCENIyf64iiCaKTfX9gDlg.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">graph of gradients against each iteration (learning rate = 0.001)</strong></figcaption></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/287cb2cbf362a02f77bf5b25db5c3916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*kcw6Oqr5vL8Sr68eRmjCIA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><strong class="bd kh">graph of gradients against each iteration (learning rate = 0.002)</strong></figcaption></figure><p id="82ac" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">“在学习率= 0.002的情况下，为什么梯度在开始时看起来直到第60次迭代才发生变化？”</strong></p><blockquote class="mw mx my"><p id="27e6" class="lv lw mz ld b le lx jr ly lg lz ju ma na mb mc md nb me mf mg nc mh mi mj ll ij bi translated">实际上，梯度确实从一开始就在振荡，但是由于图形的缩放比例而无法观察到。让我们观察前25次迭代的放大图，如下所示。</p></blockquote><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/db7fc18ff76a60a60b21b6db4d0bdbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*3SiSM9CPg2mQxMxVsUxnbA.png"/></div></figure><h2 id="ad43" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">Sklearn线性回归的实现</h2><p id="6427" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko oc mc md ks od mf mg kw oe mi mj ll ij bi translated">现在，让我们尝试使用一个机器学习库，即<code class="fe ol om on oo b"><a class="ae mv" href="https://pypi.org/project/scikit-learn/" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">scikit-learn</strong></a></code>。据官网介绍，该库提供了简单高效的预测数据分析工具。</p><p id="3e35" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">导入相关库:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="e584" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">加载数据:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="9e38" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">权重和偏差的优化:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="9eda" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">绘制结果:</strong></p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="ok mq l"/></div></figure><p id="751b" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">结果:</strong></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ox oy di oz bf pa"><div class="gh gi pi"><img src="../Images/7042cc6b81656ef425f655e7ea6291b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3zSaaYwWsc8F5HxS2iRGg.png"/></div></div></figure><p id="9918" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">正如我们所看到的，通过使用scikit-learn库训练的模型(线性方程)与我们从零开始获得的模型大致相似。但是，需要注意的一点是，scikit-learn库中实现的<code class="fe ol om on oo b"><strong class="ld ir">LinearRegression</strong></code>是使用<a class="ae mv" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">普通最小二乘法</strong> </a>计算的，这是一个封闭形式的解，而不是迭代梯度下降算法。</p><h2 id="3a0f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h2><p id="6db5" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko oc mc md ks od mf mg kw oe mi mj ll ij bi translated">希望到现在为止，您已经理解了线性回归以及梯度下降算法的基础，并且能够用Python构建一个简单的线性回归模型。在这篇文章的例子中，只有一个特征(独立变量)用于预测输出。那么，如果我们有一个包含映射到一个或多个输出的多个要素的数据集呢？在这种情况下，我们称之为多元线性回归。处理多元线性回归的概念与简单线性回归的概念相同。</p><h2 id="b19f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参考</h2><ol class=""><li id="e78c" class="lb lc iq ld b le lf lg lh ko li ks lj kw lk ll lm ln lo lp bi translated"><a class="ae mv" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Supervised_learning</a></li><li id="f969" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599<a class="ae mv" href="https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599" rel="noopener ugc nofollow" target="_blank"/></li></ol><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="pj mq l"/></div></figure></div></div>    
</body>
</html>