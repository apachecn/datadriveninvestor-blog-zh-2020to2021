<html>
<head>
<title>Distributed Data Processing with Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark的分布式数据处理</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1?source=collection_archive---------0-----------------------#2020-08-03">https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1?source=collection_archive---------0-----------------------#2020-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e247" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用通用分布式数据处理引擎进行数据处理。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2b0313651805b6a815029f25b07baa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fjl10HIHSTrujTTv11zoSw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@scottwebb?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Scott Webb</a> on <a class="ae le" href="https://unsplash.com/s/photos/spread?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="cfcf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="http://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">Apache Spark</em></strong></a><em class="lf">，用Scala编写，是一个</em> <strong class="js iu"> <em class="lf">通用分布式数据处理引擎</em> </strong> <em class="lf">。或者换句话说:加载大数据，以分布式方式对其进行计算，然后存储它。</em></p><p id="055e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Spark提供Java、Scala、Python和R的高级API，以及支持通用执行图的优化引擎。它还支持一套丰富的高级工具，包括用于SQL和结构化数据处理的</em><a class="ae le" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"><em class="lf">Spark SQL</em></a><em class="lf">，用于机器学习的</em><a class="ae le" href="http://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"><em class="lf">ml lib</em></a><em class="lf">，用于图形处理的</em><a class="ae le" href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="noopener ugc nofollow" target="_blank"><em class="lf">GraphX</em></a><em class="lf">，以及Spark流。</em></p><p id="1e74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">要运行Spark，你可以旋转你的</em> <strong class="js iu"> <em class="lf">自己的集群</em> </strong> <em class="lf">或者使用</em> <strong class="js iu"> <em class="lf">亚马逊EMR </em> </strong> <em class="lf">带/不带</em> <strong class="js iu"> <em class="lf">亚马逊胶水</em> </strong> <em class="lf">，或者使用</em><strong class="js iu">【Google data proc</strong><em class="lf">。</em></p><p id="2ccb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Apache Spark包含用于数据分析、机器学习、图形分析和流式实时数据的库。Spark一般比Hadoop </em>  <em class="lf">快</em> <strong class="js iu"> <em class="lf">。这是因为Hadoop将中间结果写入磁盘(即大量的</em> <strong class="js iu"> <em class="lf"> I/O操作</em> </strong> <em class="lf">)，而Spark则尽可能将中间结果保存在内存中(即</em> <strong class="js iu"> <em class="lf">内存计算</em> </strong> <em class="lf">)。而且，Spark提供的</em> <strong class="js iu"> <em class="lf">懒评</em> </strong> <em class="lf">的运算和</em> <strong class="js iu"> <em class="lf">的优化就在最终结果</em> </strong> <em class="lf">之前；Sparks维护了一系列要执行的转换，除非我们试图获得结果，否则不会实际执行这些操作。通过这种方式，Spark能够在查看所需的整体转换时找到最佳路径(例如，将向数据集的每个元素添加数字</em> <code class="fe lg lh li lj b"><em class="lf">5</em></code> <em class="lf">和</em> <code class="fe lg lh li lj b"><em class="lf">20</em></code> <em class="lf">的两个独立步骤减少为向数据集的每个元素添加</em> <code class="fe lg lh li lj b"><em class="lf">25</em></code> <em class="lf">的一个步骤，或者不实际对数据集的一部分执行操作，该部分最终将在最终结果中被过滤掉)。这使得Spark成为目前最受欢迎的大数据分析工具之一。</em></strong></p><div class="lk ll gp gr lm ln"><a href="https://www.datadriveninvestor.com/2020/05/15/big-data-analytics-in-telemedicine-reshaping-the-healthcare-industry/" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iu gy z fp ls fr fs lt fu fw is bi translated">远程医疗中的大数据分析重塑医疗保健行业|数据驱动的投资者</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">最近，在冠状病毒疫情的推动下，远程医疗的使用出现了大爆炸。越来越…</h3></div><div class="lv l"><p class="bd b dl z fp ls fr fs lt fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb ky ln"/></div></div></a></div><p id="645e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Hadoop将中间状态保存到磁盘，并通过网络进行通信。如果我们考虑ML模型的逻辑回归，那么每次迭代状态都保存回磁盘。这个过程很慢。然而，Spark保持所有数据<strong class="js iu">不变，并且在内存中</strong>。它使用函数式编程的思想来实现这一点，例如容错，它通过在原始数据集上重放函数转换来工作。</p><p id="499f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark <strong class="js iu">懒惰</strong>(关于转型)和渴望(关于行动)是Spark如何使用编程模型优化网络通信的。因此，Spark定义了数据集(和RDD)上的转换和操作来支持这一点。转换(比如<code class="fe lg lh li lj b">where</code>)是惰性的，因此它们的结果输出不会立即被计算出来。行动(如<code class="fe lg lh li lj b">take</code>)是热切的。他们的结果被立即计算出来。</p><p id="8351" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"/><strong class="js iu"><em class="lf">Hadoop生态系统</em> </strong> <em class="lf">包括一个分布式文件存储系统，名为</em><strong class="js iu"><em class="lf"/></strong><em class="lf">(Hadoop分布式文件系统)。</em><strong class="js iu"><em class="lf"/></strong><em class="lf">另一方面，不包括文件存储系统。您可以在HDFS上使用Spark，但这不是必须的。Spark可以从其他来源读入数据，如</em> <strong class="js iu"> <em class="lf">【亚马逊】</em> </strong> <em class="lf">。</em></p><p id="fecd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">虽然</em> <strong class="js iu"> <em class="lf"> Spark没有实现</em></strong><a class="ae le" href="https://medium.com/@goyalmunish/the-why-and-how-of-mapreduce-524f71561daf" rel="noopener"><strong class="js iu"><em class="lf">MapReduce</em></strong></a><em class="lf">，但是我们可以编写Spark程序，其行为方式类似于map-reduce范式。</em></p><p id="ae49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">对于大数据处理，最常见的数据形式是</em> <strong class="js iu"> <em class="lf">键-值对</em> </strong> <em class="lf">。事实上，在2004年的mapReduce研究论文中，设计者指出键值对是设计mapReduce的一个关键选择。在转换过程中，键通常充当一个组，在转换中根据该组计算聚合值。</em></p><p id="3c58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">与Hadoop类似，Spark中的</em> <strong class="js iu"> <em class="lf">分区</em> </strong> <em class="lf">(哈希分区器或范围分区器)可以带来巨大的性能增益，尤其是在洗牌阶段。</em></p><p id="a050" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lf">数据流</em> </strong> <em class="lf">是大数据中的一个专门话题。用例是当您想要实时存储和分析数据时，例如脸书帖子或Twitter推文。Spark有一个流库叫做</em><strong class="js iu"><em class="lf">Spark Streaming</em></strong><em class="lf">虽然没有其他一些流库普及快。其他流行的流媒体库还有</em> <a class="ae le" href="http://storm.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> <em class="lf">阿帕奇风暴</em> </strong> </a> <em class="lf">和</em> <a class="ae le" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> <em class="lf">阿帕奇弗林克</em> </strong> </a> <em class="lf">。使用火花流时，卡夫卡或Kinesis很有用。</em></p><h2 id="792e" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">方便的参考:</h2><ul class=""><li id="7cf8" class="mv mw it js b jt mx jx my kb mz kf na kj nb kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark主页</a></li><li id="aa0d" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/" rel="noopener ugc nofollow" target="_blank"> Apache Spark文档</a></li><li id="0c19" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/quick-start.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark快速入门指南</a></li><li id="32ac" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark Python API文档</a></li><li id="433f" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark SQL和数据集指南</a></li><li id="655a" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://0.0.0.0:8000/concepts_machine_learning.html#dask" rel="noopener ugc nofollow" target="_blank"> Dask </a>和Spark <a class="ae le" href="https://docs.dask.org/en/latest/spark.html" rel="noopener ugc nofollow" target="_blank">比较</a></li></ul></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="9bd3" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">本地模式下的火花</h1><p id="cc1e" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">试用Apache Spark最简单的方法就是在</em> <strong class="js iu"> <em class="lf">本地模式</em> </strong> <em class="lf">。整个处理是在一个</em><strong class="js iu"><em class="lf"/></strong><em class="lf">的单一服务器上完成的。因此，您仍然会受益于服务器中所有内核</em>  <em class="lf">的</em> <strong class="js iu"> <em class="lf">并行化，但不会受益于几个服务器。</em></strong></p><p id="ca87" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Spark运行在JVM上。它公开了一个Python、R、Scala和SQL接口。</em></p><p id="67bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">对于Python，Spark通过</em><a class="ae le" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"><em class="lf">py Spark</em></a><em class="lf">提供了</em><a class="ae le" href="http://spark.apache.org/docs/latest/api/python/index.html#" rel="noopener ugc nofollow" target="_blank"><em class="lf">Python API</em></a><em class="lf">，PyPI中有，所以可以通过</em> <code class="fe lg lh li lj b"><em class="lf">pip</em></code> <em class="lf">安装。可以导入或者直接调用为</em> <code class="fe lg lh li lj b"><em class="lf">pyspark</em></code> <em class="lf">得到一个交互shell。</em></p><pre class="kp kq kr ks gt om lj on oo aw op bi"><span id="7b9f" class="mc md it lj b gy oq or l os ot"># install pyspark<br/>pip install --upgrade pyspark</span><span id="ae4d" class="mc md it lj b gy ou or l os ot"># get pyspark help<br/>pyspark --help</span><span id="8d76" class="mc md it lj b gy ou or l os ot"># invoke pyspark interactive shell<br/>pyspark</span><span id="296f" class="mc md it lj b gy ou or l os ot"># or through python or ipython<br/>ipython<br/>&gt;&gt; import pyspark</span></pre><p id="79b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">同样，Scala和R也分别提供了</em> <code class="fe lg lh li lj b"><em class="lf">spark-shell</em></code> <em class="lf">和</em> <code class="fe lg lh li lj b"><em class="lf">sparkR</em></code> <em class="lf">的交互外壳。</em></p><p id="17fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">其他安装选项，勾选</em><a class="ae le" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"><em class="lf">spark.apache.org/downloads.html</em></a><em class="lf">。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="29b1" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">集群模式下的火花</h1><p id="61e9" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">星火</em> <a class="ae le" href="http://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">集群模式概述</em> </a> <em class="lf">解释了在集群上运行的关键概念。Spark既可以独立运行，也可以在几个现有的集群管理器(Hadoop、Apache Mesos、Kubernetes)上运行。</em></p><p id="ea07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">系统目前支持多个</em> <a class="ae le" href="https://spark.apache.org/docs/latest/spark-standalone.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">集群管理器</em> </a> <em class="lf"> : </em></p><ul class=""><li id="931e" class="mv mw it js b jt ju jx jy kb ov kf ow kj ox kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/spark-standalone.html" rel="noopener ugc nofollow" target="_blank">独立</a>—Spark附带的一个简单的集群管理器，可以轻松设置集群。</li><li id="081d" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/running-on-mesos.html" rel="noopener ugc nofollow" target="_blank"> Apache Mesos </a> —通用集群管理器，也可以运行Hadoop MapReduce和服务应用。</li><li id="8033" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/running-on-yarn.html" rel="noopener ugc nofollow" target="_blank">Hadoop YARN</a>—Hadoop 2中的资源管理器</li><li id="94b4" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated">Kubernetes  —一个用于自动化部署、扩展和管理容器化应用程序的开源系统</li></ul><p id="4aee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark被组织在一个主/工人拓扑中。在Spark的上下文中，驱动程序是主节点，而执行器节点是工作节点。每个工作节点运行相同的任务，并将结果返回给主节点。资源分配由集群管理器处理。</p><p id="059b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Spark应用程序在集群上作为独立的进程集运行，由主程序中的</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">SparkContext</em></strong></a></code> <em class="lf">对象协调(称为</em> <strong class="js iu"> <em class="lf">驱动程序</em> </strong> <em class="lf">)。</em></p><p id="49f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">具体来说，要在一个集群上运行，</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext</em></code> <em class="lf">可以连接到几种类型的</em> <strong class="js iu"> <em class="lf">集群管理器</em> </strong> <em class="lf">，它们跨应用程序分配资源。一旦连接上，Spark就会在集群中的节点上获得</em> <strong class="js iu"> <em class="lf">执行器</em> </strong> <em class="lf">，它们是为应用程序运行计算和存储数据的进程。接下来，它将您的应用程序代码(由传递给</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext</em></code> <em class="lf">的JAR或Python文件定义)发送给执行器。最后，</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext</em></code> <em class="lf">将</em> <strong class="js iu"> <em class="lf">任务</em> </strong> <em class="lf">发送给执行者运行。</em></p><p id="daba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">每个应用程序都有自己的执行程序进程，这些进程在整个应用程序运行期间保持运行，并在多个线程中运行任务。这有利于在调度端(每个驱动程序调度自己的任务)和执行端(来自不同应用程序的任务在不同的JVM中运行)将应用程序相互隔离。但是，这也意味着，如果不将数据写入外部存储系统，就不能在不同的Spark应用程序(实例</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext</em></code> <em class="lf">)之间共享数据。</em></p><p id="15cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">每个驱动程序都有一个</em><a class="ae le" href="http://0.0.0.0:8000/arch_big_data.html#spark-web-ui" rel="noopener ugc nofollow" target="_blank"><em class="lf">web UI</em></a><em class="lf">，通常是端口</em> <code class="fe lg lh li lj b"><em class="lf">4040</em></code> <em class="lf">。</em></p><p id="8344" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark可以控制应用程序之间(在集群管理器级别)和应用程序内部(如果多个计算发生在同一个<code class="fe lg lh li lj b">SparkContext</code>上)的资源分配。<a class="ae le" href="https://spark.apache.org/docs/latest/job-scheduling.html" rel="noopener ugc nofollow" target="_blank">任务调度概述</a>对此有更详细的描述。</p><p id="3b1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">虽然</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">pyspark.SparkContext</em></strong></a></code> <em class="lf">是Spark功能的主要入口点，表示与Spark集群的连接，并可用于在该集群上创建RDD和广播变量，但</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">pyspark.sql.SparkSession</em></strong></a></code> <em class="lf">是使用Dataset和DataFrame API对Spark编程的入口点。例如，</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/4bc1bd54b2dd43dded8622ee8c53dcc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkW9aKpCWvpwKeVEcSdwRA.png"/></div></div></figure><p id="617e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">这里:</em></p><ul class=""><li id="ee61" class="mv mw it js b jt ju jx jy kb ov kf ow kj ox kn nc nd ne nf bi translated">返回值是一个<code class="fe lg lh li lj b">SparkSession</code>对象。</li><li id="b10f" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">master(master)</code>设置要连接的Spark master URL，例如<code class="fe lg lh li lj b">"local"</code>在本地运行，而<code class="fe lg lh li lj b">"local[4]"</code>在本地使用<code class="fe lg lh li lj b">4</code>内核运行，或者<code class="fe lg lh li lj b">"spark://master:7077"</code>在Spark独立集群上运行。</li><li id="fefe" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">appName(name)</code>设置应用程序的名称，该名称将显示在Spark web UI中。如果没有设置应用程序名称，将使用随机生成的名称。</li><li id="be0c" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">config(key=None, value=None, conf=None)</code>设置一个配置选项。使用这种方法设置的选项会自动传播到<code class="fe lg lh li lj b">SparkConf</code>和<code class="fe lg lh li lj b">SparkSession</code>自己的配置中。</li><li id="9670" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">getOrCreate()</code>获取一个现有的<code class="fe lg lh li lj b">SparkSession</code>,或者，如果没有现有的，根据这个构建器中设置的选项创建一个新的。</li></ul></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="eac1" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">函数式编程语言和分布式系统</h1><p id="4045" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><strong class="js iu"> <em class="lf">为什么分布式系统需要函数式编程？</em>T11】</strong></p><p id="c8bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Spark使用函数式编程语言Scala。这背后的原因是函数式编程非常适合分布式系统。</em></p><p id="50df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">在功能编程中，如果</em> <code class="fe lg lh li lj b"><em class="lf">f(x) = x + 5</em></code> <em class="lf">，那么</em> <code class="fe lg lh li lj b"><em class="lf">f(3)</em></code> <em class="lf">总是</em> <code class="fe lg lh li lj b"><em class="lf">8</em></code> <em class="lf">。但是，在Python等非函数式语言中，这并不总是正确的。例如，</em></p><pre class="kp kq kr ks gt om lj on oo aw op bi"><span id="5ffd" class="mc md it lj b gy oq or l os ot">w = -2</span><span id="473a" class="mc md it lj b gy ou or l os ot">def f(x):<br/>    global w<br/>    w -= 1<br/>    return x + w + 10</span><span id="60f7" class="mc md it lj b gy ou or l os ot">f(3)<br/>f(3)</span></pre><p id="6bdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，再次运行相同的代码会产生不同的结果。这个例子中的问题很容易发现，但当您有几十台机器并行运行代码时就不那么容易了，有时如果其中一台机器出现临时问题，您需要重新开始计算。这些意想不到的副作用会导致严重的头痛。混乱来自于草率的语言。</p><p id="3d88" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"><em class="lf">DAG和lazy评价如何帮助记忆的运行:</em> </strong></p><p id="1330" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就像面包公司从母面团中复制发酵剂一样，<em class="lf">每个Spark函数都会复制其输入数据，并且永远不会更改原始父数据(即</em> <strong class="js iu"> <em class="lf">原始数据保持不变</em> </strong> <em class="lf"> ) </em>。因为Spark不会更改或改变输入数据，所以它被称为不可变的。当你有一个单一的功能时，这是有意义的，但是如果你有多个功能，那么你将多个功能链接在一起，每个功能完成一小部分工作。您经常会看到一个函数由多个子函数组成，为了让这个大函数成为对等函数，每个子函数也必须成为对等函数。Spark似乎需要为每个子功能复制一份输入数据。如果是这种情况，您的Spark程序将很快耗尽内存。</p><p id="3160" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幸运的是，Spark通过使用一种叫做 <strong class="js iu"> <em class="lf">懒求值</em> </strong>的<em class="lf">函数式编程概念避免了这一点。在Spark对程序中的数据做任何事情之前，它首先会一步一步地指出需要哪些函数和数据。这些方向就像你的面包食谱，在Spark中这被称为<strong class="js iu">有向无环图(DAG) </strong>。一旦Spark从您的代码中构建了DAG，它<em class="lf">会检查它是否可以拖延，直到最后一刻才获取数据</em>。这样，spark就有机会<strong class="js iu"> <em class="lf">优化整个转型过程</em> </strong>。如果你在做面包，你会这么做。你要抓起面粉，把它带回你的碗里，然后回到餐具室拿些糖加到碗里，然后回到橱柜拿盐，等等每一种配料。这将是烹饪相当于鞭打。取而代之的是，你在开始混合配料之前先看一下食谱，看看你能抓住什么，然后一步到位地混合在一起。事实上，你经常混合你所有的干原料，然后混合你所有的湿原料，在烘焙之前将它们混合在一起。在Spark中，这些多步组合称为阶段。</em></p><p id="544d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"><em class="lf">Python中的函数编程:</em> </strong></p><p id="5f74" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lf">匿名函数</em> </strong> <em class="lf">可以认为是编写函数式编程的Python特性。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="efdc" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">Spark中的声明式编程与命令式编程</h1><p id="3e94" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><strong class="js iu"><em class="lf"/></strong><em class="lf">声明式编程关注的是</em> <strong class="js iu"> <em class="lf">什么</em> </strong> <em class="lf">而在Spark中可以用</em><a class="ae le" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.sql" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">Spark SQL</em></strong></a><em class="lf">使用</em> <a class="ae le" href="https://spark.apache.org/docs/latest/api/sql/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf"> Spark SQL内置函数</em> </a> <em class="lf">。鉴于，</em> <strong class="js iu"> <em class="lf">命令式编程</em> </strong> <em class="lf">关心的是</em><strong class="js iu"><em class="lf"/></strong><em class="lf">如何进行，在Spark中可以用</em><a class="ae le" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">Spark data frames和Python </em> </strong> </a> <em class="lf">来完成。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="8d39" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">Spark中的数据集和数据帧</h1><h2 id="c00f" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">数据集和数据帧之间的差异</h2><p id="a9d8" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">一个</em> <a class="ae le" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> <em class="lf">数据集</em> </strong> </a> <em class="lf">是一个分布式的数据集合。数据集接口提供了</em> <a class="ae le" href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">弹性分布式数据集(RDD) </em> </a> <em class="lf">的优点和Spark SQL的优化执行引擎的优点。数据集API有Scala和Java两种版本。Python不支持数据集API。</em></p><p id="a34c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">一个</em> <strong class="js iu"> <em class="lf">数据帧</em> </strong> <em class="lf">是组织成命名列的数据集。它在概念上相当于关系数据库中的一个表或R/Python中的一个数据框，但是在底层有更丰富的优化。数据帧可以从大量的</em> <a class="ae le" href="https://spark.apache.org/docs/latest/sql-data-sources.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">来源</em> </a> <em class="lf">中构建。DataFrame API在Scala、Java、Python ( </em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">pyspark.sql.DataFrame</em></strong></a></code> <em class="lf">)和R. </em>中都有</p><p id="c93c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">注意，</em> <code class="fe lg lh li lj b"><a class="ae le" href="http://0.0.0.0:8000/arch_big_data.html#spark-in-cluster-mode" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="lf">pyspark.sql.SparkSession</em></strong></a></code> <em class="lf">是用数据集/数据帧API编程Spark的入口点。SparkSession可用于创建数据帧、将数据帧注册为表、对表执行SQL、缓存表和读取parquet文件。</em></p><h2 id="f0df" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">火花vs熊猫</h2><p id="3ce3" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">Pandas和Spark dataframes的一个关键区别就是渴望与</em> <strong class="js iu"> <em class="lf">懒惰执行</em> </strong> <em class="lf">。在PySpark中，操作被延迟，直到管道中实际需要一个结果。这种方法用于避免将完整的数据帧放入内存，并在机器集群中实现更有效的处理。有了Pandas dataframe，所有的东西都被拉入内存，每个Pandas操作都被立即应用。另一个关键区别是，Spark允许您使用分区</em> <strong class="js iu"> <em class="lf">并行化</em> </strong> <em class="lf">。</em></p><h2 id="2cae" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">镶木地板式样</h2><p id="3339" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf"> Parquet是一种</em> <strong class="js iu"> <em class="lf">列文件</em> </strong> <em class="lf">(不像CSV是基于行的存储)格式，在处理大数据时既节省时间又节省空间，是一种</em> <strong class="js iu"> <em class="lf">包含关于列数据类型</em> </strong> <em class="lf">的元数据的文件格式。例如，与使用文本相比，Parquet的Spark SQL性能平均提高了10倍，这要归功于低级读取器过滤器、高效的执行计划以及Spark 1.6.0中改进的扫描吞吐量！参考IBM的这篇</em> <a class="ae le" href="https://developer.ibm.com/hadoop/2016/01/14/5-reasons-to-choose-parquet-for-spark-sql/" rel="noopener ugc nofollow" target="_blank"> <em class="lf">文章</em> </a> <em class="lf">。</em></p><p id="d773" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">因此，如果数据是CSV格式的，那么在Spark中加载这些文件，转换并保存为parquet格式，然后在需要时使用它们是有意义的。</em></p><p id="c821" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">当使用CSV文件进入</em> <code class="fe lg lh li lj b"><em class="lf">DataFrame</em></code> <em class="lf"> s时，Spark以急切模式执行操作(即，在下一步开始执行之前，所有的数据都被加载到内存中)，而在读取拼花格式的文件时，使用</em><strong class="js iu"><em class="lf"/></strong><em class="lf">惰性方法。</em></p><p id="58c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">还要注意，在使用Spark时，对于分布式集群来说，从本地存储加载文件(或写入文件)是没有意义的。我们应该使用亚马逊S3 </em>  <em class="lf">或者HDFS</em><strong class="js iu"><em class="lf">。</em></strong></p><h2 id="d825" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated"><code class="fe lg lh li lj b">DataFrame</code></h2><p id="4eef" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf"> A </em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><em class="lf">pyspark.sql.DataFrame</em></a></code> <em class="lf">(以及</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData" rel="noopener ugc nofollow" target="_blank"><em class="lf">pyspark.sql.GroupedData</em></a></code> <em class="lf">)相当于Spark SQL中的一个关系表，可以使用</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession" rel="noopener ugc nofollow" target="_blank"><em class="lf">pyspark.sql.SparkSession</em></a></code> <em class="lf">中的各种函数创建。</em></p><p id="c5ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">使用</em> <code class="fe lg lh li lj b"><em class="lf">&lt;spark_session&gt;.read</em></code> <em class="lf">访问</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader" rel="noopener ugc nofollow" target="_blank"><em class="lf">pyspark.sql.DataFrameReader</em></a></code> <em class="lf">从外部存储系统加载</em> <code class="fe lg lh li lj b"><em class="lf">DataFrame</em></code> <em class="lf">。</em></p><p id="4750" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">使用</em> <code class="fe lg lh li lj b"><em class="lf">&lt;data_frame&gt;.write</em></code> <em class="lf">访问</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter" rel="noopener ugc nofollow" target="_blank"><em class="lf">pyspark.sql.DataFrameWriter</em></a></code> <em class="lf">写入</em> <code class="fe lg lh li lj b"><em class="lf">DataFrame</em></code> <em class="lf">到外部存储器。</em></p><p id="03fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">用</em> <code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType" rel="noopener ugc nofollow" target="_blank"><em class="lf">sql.types.StructType</em></a></code> <em class="lf">和一个</em><code class="fe lg lh li lj b"><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructField" rel="noopener ugc nofollow" target="_blank"><em class="lf">sql.types.StructField</em></a></code><a class="ae le" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructField" rel="noopener ugc nofollow" target="_blank"><em class="lf">s</em></a><em class="lf">的列表来表示一个数据类型代表一行。</em></p><p id="7997" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">例如:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/31e26e07be9554a606d8dadd505480be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQdVyiI7GkSGwXRWyJeSDg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/f55a159855b9de33a7ad4389868e0c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7iCDYdtoIpxIIOTx9GbFg.png"/></div></div></figure><p id="f3c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们继续这个例子之前，让我们简单地看一下常用函数的一些类别:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/9869eb7c78da02f67e9eb5ccf54f807e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQFlok-sgOwZtmNtdPwhNA.png"/></div></div></figure><p id="9066" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">继续这个例子..</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/b5374099767c51023ac913976902a68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xB25Lnirgkuumt2PAlASmw.png"/></div></div></figure></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="165f" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">数据存储在哪里？S3还是HDFS</h1><p id="de93" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">当你使用</em> <strong class="js iu"> <em class="lf">亚马逊S3</em></strong><em class="lf">(</em><strong class="js iu"><em class="lf">【首选</em> </strong> <em class="lf">)时，你就把数据存储从你的集群中分离出来了。其中一个缺点是，你必须通过网络下载数据，这可能是一个瓶颈(但是，Spark本身运行在AWS上就不会出现这种情况)。另一个解决方案是用HDFS把数据存储在你的Spark集群上。</em></p><p id="0e41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf"> Spark和</em><strong class="js iu"><em class="lf">HDFS</em></strong><em class="lf">被设计成可以很好地协同工作。当Spark需要来自HDFS的一些数据时，它会抓取最近的副本，从而最大限度地减少数据在网络中传输的时间。但是对HDFS来说是有代价的。你必须自己维护和修复系统。对于许多公司来说，从小公司到大公司，S3更容易，因为你不必维护一个单独的集群。此外，如果您从AWS租用集群，您的数据通常不必在网络中走得太远，因为集群硬件和S3硬件都在亚马逊的数据中心。最后，Spark足够聪明，可以下载一小块数据并处理那块数据，同时等待其余的数据下载。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="11f1" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">提交Spark申请</h1><p id="003b" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><em class="lf">Spark的</em> <code class="fe lg lh li lj b"><em class="lf">bin</em></code> <em class="lf">目录中的</em> <code class="fe lg lh li lj b"><strong class="js iu"><em class="lf">spark-submit</em></strong></code> <em class="lf">脚本用来在</em> <a class="ae le" href="https://spark.apache.org/docs/latest/submitting-applications.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">集群上启动应用</em> </a> <em class="lf">。它可以通过一个统一的接口使用所有Spark支持的</em> <a class="ae le" href="http://0.0.0.0:8000/arch_big_data.html#spark-in-cluster-mode" rel="noopener ugc nofollow" target="_blank"> <em class="lf">集群管理器</em> </a> <em class="lf">，因此您不必为每个集群管理器配置您的应用程序。</em></p><p id="0b43" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">如果你的代码依赖于其他项目，你需要将它们和你的应用一起打包，以便将代码分发到Spark集群。对于Python，可以使用</em> <code class="fe lg lh li lj b"><em class="lf">spark-submit</em></code> <em class="lf">的</em> <code class="fe lg lh li lj b"><em class="lf">--py-files</em></code> <em class="lf">参数，添加</em> <code class="fe lg lh li lj b"><em class="lf">.py</em></code> <em class="lf">、</em> <code class="fe lg lh li lj b"><em class="lf">.zip</em></code> <em class="lf">或</em> <code class="fe lg lh li lj b"><em class="lf">.egg</em></code> <em class="lf">文件，随应用一起分发。如果依赖多个Python文件，建议打包成一个</em> <code class="fe lg lh li lj b"><em class="lf">.zip</em></code> <em class="lf">或者</em> <code class="fe lg lh li lj b"><em class="lf">.egg</em></code> <em class="lf">。</em></p><p id="13bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦用户应用被捆绑，就可以使用脚本启动它。这个脚本负责设置Spark的类路径及其依赖项，并且可以支持Spark支持的不同集群管理器和部署模式:</p><pre class="kp kq kr ks gt om lj on oo aw op bi"><span id="a076" class="mc md it lj b gy oq or l os ot">./bin/spark-submit \<br/>  --class &lt;main-class&gt; \<br/>  --master &lt;master-url&gt; \<br/>  --deploy-mode &lt;deploy-mode&gt; \<br/>  --conf &lt;key&gt;=&lt;value&gt; \<br/>  ... # other options<br/>  &lt;application-jar&gt; \<br/>  [application-arguments]</span></pre><p id="57e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">一些常用的选项有:</em></p><ul class=""><li id="1ab3" class="mv mw it js b jt ju jx jy kb ov kf ow kj ox kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">--class</code>:应用程序的入口点(例如<code class="fe lg lh li lj b">org.apache.spark.examples.SparkPi</code>)</li><li id="6249" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">--master</code>:集群的<a class="ae le" href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls" rel="noopener ugc nofollow" target="_blank">主URL </a></li><li id="db53" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">--deploy-node</code>:是将您的驱动程序部署在工作节点上(<code class="fe lg lh li lj b">cluster</code>)还是本地作为外部客户端(<code class="fe lg lh li lj b">client</code>)(默认)</li><li id="8486" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">--conf</code>:任意火花配置属性<code class="fe lg lh li lj b">"key=value"</code>格式。</li><li id="0a7d" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">application-jar</code>:包含应用程序和所有依赖项的捆绑jar的路径。URL必须在集群内部全局可见，例如，所有节点上都存在的<code class="fe lg lh li lj b">hdfs://</code>路径或<code class="fe lg lh li lj b">file://</code>路径。</li><li id="cc93" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><code class="fe lg lh li lj b">application-arguments</code>:传递给主类的main方法的参数，如果有的话</li></ul><p id="1a07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">对于Python应用，只需在</em> <code class="fe lg lh li lj b"><em class="lf">&lt;application-jar&gt;</em></code> <em class="lf">处传递一个</em> <code class="fe lg lh li lj b"><em class="lf">.py</em></code> <em class="lf">文件代替JAR，并将Python </em> <code class="fe lg lh li lj b"><em class="lf">.zip</em></code> <em class="lf">、</em> <code class="fe lg lh li lj b"><em class="lf">.egg</em></code> <em class="lf">或</em> <code class="fe lg lh li lj b"><em class="lf">.py</em></code> <em class="lf">文件添加到带有</em> <code class="fe lg lh li lj b"><em class="lf">--py-files</em></code> <em class="lf">的搜索路径中。</em></p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="cba0" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">火花流</h1><p id="9d0e" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><a class="ae le" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">火花流</a></p><p id="a975" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark的局限性:Spark Streaming的延迟至少为500毫秒，因为它是对微批量的记录进行操作，而不是一次处理一条记录。诸如<a class="ae le" href="http://storm.apache.org/" rel="noopener ugc nofollow" target="_blank"> Storm </a>、<a class="ae le" href="https://apex.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apex </a>或<a class="ae le" href="https://flink.apache.org/" rel="noopener ugc nofollow" target="_blank"> Flink </a>等原生流工具可以降低延迟值，可能更适合低延迟应用。Flink和Apex也可以用于批量计算，所以如果您已经在使用它们进行流处理，就没有必要在您的技术堆栈中添加Spark。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="f721" class="ns md it bd me nt nu nv mh nw nx ny mk nz oa ob mn oc od oe mq of og oh mt oi bi translated">Spark中的调试和优化</h1><p id="43ee" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated">当您在分布式Spark集群上工作时，代码中的错误可能很难诊断。</p><p id="3a04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">通常，当一个函数被传递给在远程集群工作节点上执行的Spark操作(如</em> <code class="fe lg lh li lj b"><em class="lf">map</em></code> <em class="lf">或</em> <code class="fe lg lh li lj b"><em class="lf">reduce</em></code> <em class="lf">)时，它们对该函数中使用的所有变量的单独副本进行操作。这些变量被复制到每台机器上，并且对远程工作机器上的变量的任何更新都不会传播回驱动程序。</em></p><p id="087b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">跨任务支持通用的读写共享变量是低效的。但是，Spark确实为两种常见的使用模式提供了两种有限类型的<a class="ae le" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">共享变量</strong></a>:<a class="ae le" href="http://0.0.0.0:8000/arch_big_data.html#broadcast-variables-in-spark" rel="noopener ugc nofollow" target="_blank">广播变量</a>和<a class="ae le" href="http://0.0.0.0:8000/arch_big_data.html#accumulators-in-spark" rel="noopener ugc nofollow" target="_blank">累加器</a>。</p><h2 id="e177" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">Spark中的广播变量</h2><p id="b588" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><a class="ae le" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables" rel="noopener ugc nofollow" target="_blank">广播变量</a>允许程序员在每台机器上缓存一个只读变量，而不是随任务发送一个副本。例如，它们可用于以高效的方式为每个节点提供大型输入数据集的副本。Spark还尝试使用高效的广播算法来分发广播变量，以降低通信成本。</p><p id="0d66" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Spark动作通过一组阶段执行，由分布式“洗牌”操作分隔。Spark自动广播每个阶段内任务所需的公共数据。以这种方式广播的数据以序列化形式缓存，并在运行每个任务之前进行反序列化。这意味着，只有当跨多个阶段的任务需要相同的数据，或者以反序列化形式缓存数据很重要时，显式创建广播变量才有用。</p><p id="49c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">广播变量可以通过调用</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext.broadcast(v)</em></code> <em class="lf">从变量</em> <code class="fe lg lh li lj b"><em class="lf">v</em></code> <em class="lf">中创建。广播变量是</em> <code class="fe lg lh li lj b"><em class="lf">v</em></code> <em class="lf">的包装器，通过调用</em> <code class="fe lg lh li lj b"><em class="lf">value</em></code> <em class="lf">方法可以访问它的值。</em></p><pre class="kp kq kr ks gt om lj on oo aw op bi"><span id="8aed" class="mc md it lj b gy oq or l os ot">&gt;&gt;&gt; broadcastVar = sc.broadcast([1, 2, 3])<br/>&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;</span><span id="612a" class="mc md it lj b gy ou or l os ot">&gt;&gt;&gt; broadcastVar.value<br/>[1, 2, 3]</span></pre><p id="08f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">创建广播变量后，应该在集群上运行的任何函数中使用它来代替值<code class="fe lg lh li lj b">v</code>,以便<code class="fe lg lh li lj b">v</code>不会被多次发送到节点。此外，对象<code class="fe lg lh li lj b">v</code>在被广播后不应被修改，以确保所有节点获得相同的广播变量的值(例如，如果该变量稍后被运送到新节点)。</p><h2 id="0b27" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">火花蓄电池</h2><p id="3aaa" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated"><a class="ae le" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators" rel="noopener ugc nofollow" target="_blank">累加器</a>是只能通过关联和交换操作“相加”的变量，因此可以有效地并行支持。它们可用于实现计数器(如在MapReduce中)或求和。Spark本身支持数值类型的累加器，程序员可以增加对新类型的支持。</p><p id="aa19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为用户，可以创建命名或未命名的累加器。如下图所示，一个已命名的累加器(在本例中为计数器)将显示在修改该累加器的阶段的web UI中。Spark显示由“任务”表中的任务修改的每个累加器的值。</p><p id="610d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在UI中跟踪累加器有助于理解运行阶段的进度(注意:Python中还不支持)。</p><p id="b67f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">通过调用</em> <code class="fe lg lh li lj b"><em class="lf">SparkContext.accumulator(v)</em></code> <em class="lf">从初始值</em> <code class="fe lg lh li lj b"><em class="lf">v</em></code> <em class="lf">创建累加器。运行在集群上的任务可以使用</em> <code class="fe lg lh li lj b"><em class="lf">add</em></code> <em class="lf">方法或</em> <code class="fe lg lh li lj b"><em class="lf">+=</em></code> <em class="lf">操作符添加到集群中。但是，他们无法读取其值。只有驱动程序可以使用其</em> <code class="fe lg lh li lj b"><em class="lf">value</em></code> <em class="lf">属性读取累加器的值。</em></p><pre class="kp kq kr ks gt om lj on oo aw op bi"><span id="7e78" class="mc md it lj b gy oq or l os ot">&gt;&gt;&gt; accum = sc.accumulator(0)<br/>&gt;&gt;&gt; accum<br/>Accumulator&lt;id=0, value=0&gt;</span><span id="5109" class="mc md it lj b gy ou or l os ot">&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))<br/>...<br/>10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span><span id="2990" class="mc md it lj b gy ou or l os ot">&gt;&gt;&gt; accum.value<br/>10</span></pre><p id="894f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">累加器不改变Spark的懒评模式。如果它们在数据帧的操作中被更新，它们的值仅在数据帧作为操作的一部分被计算时才被更新。</em></p><h2 id="4fb4" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">Spark Web用户界面</h2><p id="86a8" class="pw-post-body-paragraph jq jr it js b jt mx jv jw jx my jz ka kb oj kd ke kf ok kh ki kj ol kl km kn im bi translated">每个驱动程序都有一个<a class="ae le" href="https://spark.apache.org/docs/3.0.0-preview/web-ui.html" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> web UI </strong> </a>，通常在端口<code class="fe lg lh li lj b">4040</code>上，显示关于运行任务、执行器和存储使用的信息。只需在网络浏览器中点击<code class="fe lg lh li lj b">http://&lt;driver-node&gt;:4040</code>即可访问该界面。<a class="ae le" href="https://spark.apache.org/docs/latest/monitoring.html" rel="noopener ugc nofollow" target="_blank">监控指南</a>还描述了其他监控选项。</p><p id="c655" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">web UI提供了集群的当前配置，这对于再次检查您想要的设置是否生效非常有用。</p><p id="368a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">网络用户界面也向你展示了</em> <strong class="js iu"> <em class="lf"> DAG </em> </strong> <em class="lf">，你的程序的步骤的配方。Spark应用程序由与代码相关的许多动作(如将数据帧保存到数据库或把一些记录带回给驱动程序检查)组成。工作被进一步分解成</em> <strong class="js iu"> <em class="lf">个阶段</em> </strong> <em class="lf">。阶段是相互依赖的工作单元。一个阶段内的最小单位是一个</em> <strong class="js iu"> <em class="lf">任务</em> </strong> <em class="lf">。任务是一系列火花转换，可以在我们的数据帧的不同分区上并行运行。因此，如果我们有10个分区，我们运行10个相同的任务来完成一个阶段。任务是分配给单个工作节点的步骤。在每个阶段，worker节点划分输入数据并运行该阶段的任务。</em></p><p id="978f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认情况下，Spark master使用端口<code class="fe lg lh li lj b">7077</code>与worker节点通信，端口<code class="fe lg lh li lj b">4040</code>显示活动的Spark作业，主节点的web UI位于端口<code class="fe lg lh li lj b">8080</code>(或<code class="fe lg lh li lj b">4040</code>)上。</p><p id="0b44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里有一些相关的有趣故事，你可能会觉得有帮助</p><ul class=""><li id="81e7" class="mv mw it js b jt ju jx jy kb ov kf ow kj ox kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/fluent-numpy-187cc14f2832" rel="noopener">流畅的数字</a></li><li id="eab8" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/fluent-pandas-22473fa3c30d" rel="noopener">流利的熊猫</a></li><li id="c326" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/data-streaming-with-apache-kafka-e1676dc5e975" rel="noopener">使用Apache Kafka进行数据流传输</a></li><li id="9c44" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/apache-cassandra-distributed-row-partitioned-database-for-structured-and-semi-structured-data-1dc37e72e67c" rel="noopener"> Apache Cassandra —用于结构化和半结构化数据的分布式行分区数据库</a></li><li id="674e" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/data-pipelines-with-apache-airflow-46258deb2844" rel="noopener">带有阿帕奇气流的数据管道</a></li><li id="1c49" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/designing-workflows-using-argo-9d0dc5036348" rel="noopener">使用Argo设计工作流</a></li><li id="715d" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/the-why-and-how-of-mapreduce-17c3d99fa900" rel="noopener">MapReduce的原因和方式</a></li><li id="0d46" class="mv mw it js b jt ng jx nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><a class="ae le" href="https://medium.com/@goyalmunish/observer-pattern-vs-pub-sub-pattern-7f467bcf5fe" rel="noopener">观察者模式与发布订阅模式</a></li></ul><p id="b9f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">进入专家视角— </strong> <a class="ae le" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>