<html>
<head>
<title>Fundamental of Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归基础</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/fundamental-of-logistic-regression-4244cbc9aa3c?source=collection_archive---------2-----------------------#2020-05-09">https://medium.datadriveninvestor.com/fundamental-of-logistic-regression-4244cbc9aa3c?source=collection_archive---------2-----------------------#2020-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9f42" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Python中从头开始构建逻辑回归模型！</h2></div><h2 id="c657" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">开始之前，让我们设定目标:</h2><ol class=""><li id="c676" class="lb lc iq ld b le lf lg lh ko li ks lj kw lk ll lm ln lo lp bi translated">理解逻辑回归的基本原理。</li><li id="a8b6" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">用Python从头开始构建一个简单的逻辑回归模型。</li><li id="a0c8" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll lm ln lo lp bi translated">在Sklearn库中实现logistic回归模型。</li></ol><p id="1f69" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><a class="ae mk" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">逻辑回归</strong> </a>是机器学习中的线性分类器之一。它是机器学习中最简单的分类算法之一。与预测输出为连续值的线性回归不同，逻辑回归根据概率生成分类(离散值)输出。例如，逻辑回归模型可用于预测电子邮件是垃圾邮件还是非垃圾邮件。</p><blockquote class="ml"><p id="94fa" class="mm mn iq bd mo mp mq mr ms mt mu ll dk translated">"为什么我们不能使用线性回归通过使用阈值来预测分类值？"</p></blockquote><p id="0815" class="pw-post-body-paragraph lv lw iq ld b le mv jr ly lg mw ju ma ko mx mc md ks my mf mg kw mz mi mj ll ij bi translated">让我们来看一个例子。考虑下图所示的数据集。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8886be52104e375d70285bd9bcd2a49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*pXAViTxf-kPYtxlAnn8dqA.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figure: Sample dataset that the y value either takes 0 or 1.</figcaption></figure><p id="c1b5" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">通过对数据集应用线性回归算法，我们可以得到如下所示的最佳拟合线。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/754300b3d85713ff67f927ab532c6658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2b6d1w-YzBflsdqkPtZ5tQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figure: Best-fit line generated by using linear regression algorithm.</figcaption></figure><p id="f8b3" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">通过将阈值设置为0.5，任何低于0.5的预测y值都将被视为0，否则将被视为1。我们可以观察到，在这种情况下，线性回归在分类数据方面几乎完美无缺。但是，如果我们的数据如下:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8cfbd4daabb39c4f09ad98db26cda84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*KrY0kiNhOXWteFGeJRPadA.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figure: Another sample dataset.</figcaption></figure><p id="413e" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">让我们试着在它上面实现线性回归。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nr"><img src="../Images/b8a87c58e25a81884e7245d3b3fb1cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDQ9TBKHCu1nKoUwMAD_Jw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figure: Best-fit line generated by linear regression.</figcaption></figure><p id="4a76" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">显然，在这种情况下有大量的错误分类数据，表明线性回归不适合于分类问题。</p><p id="1811" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">为了解决这个问题，通过sigmoid函数将线性回归模型压缩到[0，1]的范围内。Sigmoid函数简单地表示范围为0到1的S形函数。这一特性使其适合作为概率的估计，概率也在0到1之间。因此，我们可以根据估计的概率对数据进行分类。</p><blockquote class="ns nt nu"><p id="e8e1" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">注意，现在我们需要基于<strong class="ld ir">概率(sigmoid函数)</strong>而不是线性回归模型来优化我们的权重(θ)。</p></blockquote><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/74e11e92edb4cecb6ba3c8994d7b090c.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*RwArx8W00NRlK4tZhXipbw.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Sigmoid function.</figcaption></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/479cd125df512f70e9e3c2aa3b2440b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*WkkTDL9_d8SzFUipmzWPXQ.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd kh">Graph of sigmoid function y = 1/(1+exp(-x))</strong></figcaption></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ob"><img src="../Images/e8b2885a8a9041806c9ef5afcb5d63c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aC6c9hNDZC3XaI618t7QyA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figure: Illustration of squashing linear equation into sigmoid function.</figcaption></figure><p id="1e93" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">为了满足对sigmoid函数选择的数学好奇，让我们在下一节查看一个更简单的解释。</p><h2 id="5622" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么是乙状结肠函数？</h2><blockquote class="ml"><p id="d634" class="mm mn iq bd mo mp mq mr ms mt mu ll dk translated">"在统计学中，逻辑(logit)模型用于模拟某一类或某一事件的概率. "</p></blockquote><p id="18ac" class="pw-post-body-paragraph lv lw iq ld b le mv jr ly lg mw ju ma ko mx mc md ks my mf mg kw mz mi mj ll ij bi translated">考虑<strong class="ld ir"> m个数据</strong>有<strong class="ld ir"> n个特征(x₁，x₂，…，xₙ) </strong>，<strong class="ld ir">目标变量(y) </strong>不是0就是1。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oc"><img src="../Images/e694c0cf19e37e355d0eaaeb91f79085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgAZyaB_ezMj6i6zZmi7pg.png"/></div></div></figure><p id="ad62" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">设<strong class="ld ir"> p </strong>为<strong class="ld ir"> y=1 </strong>的概率，由此<strong class="ld ir">p/(1–p)</strong>将为对应的<a class="ae mk" href="https://www.statisticshowto.com/log-odds/" rel="noopener ugc nofollow" target="_blank">奇数</a>。</p><blockquote class="ns nt nu"><p id="35f2" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">假设:<strong class="ld ir">自变量(x) </strong>与<a class="ae mk" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">对数奇数</strong> </a>之间存在<strong class="ld ir">线性关系</strong>。</p></blockquote><blockquote class="ml"><p id="9c38" class="mm mn iq bd mo mp od oe of og oh ll dk translated">"为什么我们不能假设自变量(x)和概率(p)之间的线性关系？"</p></blockquote><blockquote class="ns nt nu"><p id="b029" class="lv lw nv ld b le mv jr ly lg mw ju ma nw mx mc md nx my mf mg ny mz mi mj ll ij bi translated">好吧，让我们检查一下。概率p的范围是[0，1]，而对数奇数的范围是(-∞，∞)。并且由于自变量(x)可以是任意实数，因此x的加权和可以在范围(-∞，∞)内，这与对数奇数的范围相吻合，而不是概率的范围。</p></blockquote><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/298a1e185190eb54e04b06fd0e3c6228.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*e3A3KioKtmQ53coDD9BFqg.png"/></div></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2cb2ebe914f59ee410915a1305602215.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*7ANbJrIjXOJrfL_uOzQKmQ.png"/></div></figure><p id="18ee" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从上面的推导中，<strong class="ld ir"> p = 1/(1+exp(-θᵀx)) </strong>也就是<strong class="ld ir"> sigmoid(逻辑)函数</strong>。</p><p id="7ec2" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">如上所述，由于sigmoid函数在范围[0，1]内，因此它符合概率的范围。因此，事件y=1的概率p可通过下式安全估算:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ok"><img src="../Images/bb64c409e4666daae97c9a5ba0483eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*SiCeKWgb3T3ct8CPlZA5gQ.png"/></div></div></figure><p id="8084" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">而事件y=0的概率是<strong class="ld ir">1–p</strong>。</p><blockquote class="ns nt nu"><p id="8ff5" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">从概率(p)中，我们可以通过应用简单的阈值来确定分类输出y，其中如果p&lt;0.5y = 0，如果p≥0.5y = 1。</p></blockquote><blockquote class="ml"><p id="cf62" class="mm mn iq bd mo mp od oe of og oh ll dk translated">“如何获得权重的最佳值？”</p></blockquote><p id="be21" class="pw-post-body-paragraph lv lw iq ld b le mv jr ly lg mw ju ma ko mx mc md ks my mf mg kw mz mi mj ll ij bi translated">为了回答这个问题，我们需要在下一节引入一个叫做可能性的术语。</p><h2 id="bbdc" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最大似然估计</h2><p id="c649" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko ol mc md ks om mf mg kw on mi mj ll ij bi translated">设p = h(x)，那么<strong class="ld ir"> P(y=1|x) = h(x) </strong>和<strong class="ld ir">P(y = 0 | x)= 1–h(x)</strong>。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oo"><img src="../Images/8ec68baba9e9542204c548e90fa2f854.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*1ygSgbDMlXGFmDpw1DpD8g.png"/></div></div></figure><p id="c91a" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">P(y|x)可以更简洁地写成:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0232a46d5b4ddf7117292b9984bb3792.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*TVC9h42hHlALEroXKR0GBA.png"/></div></figure><p id="1788" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">对于上例中的m个数据，<a class="ae mk" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">似然函数</strong> </a> <strong class="ld ir">，L(θ) </strong>可以写成:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ac2dea09aea329510d75178feeecc26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*s_3rx0KSIUjv5NbI_4LZ6w.png"/></div></figure><blockquote class="ns nt nu"><p id="def7" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">似然函数是模型对样本数据的拟合优度相对于模型参数(θ)的度量。</p></blockquote><p id="4125" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">我们现在的目标是获得权重(θ)的最佳值，这将最大化似然函数L(θ)。由于优化过程(最大化似然性)涉及微分，因此求和比乘法更容易实现。因此，我们取似然函数的自然对数(称为对数似然):</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ff62f7203f6aa05c11f3c34107603655.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*30i59M1HDU154d75yzp1tQ.png"/></div></figure><p id="80e9" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">所以现在我们的目标函数是对数似然，ℓ(θ).类似地，与线性回归一样，我们将实施梯度下降算法来搜索使ℓ(θ).最大化的权重</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi os"><img src="../Images/649e85b16d41f40dc2513a1159d1cecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*n0t-tW6DxDekogx7yInsPw.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd kh">Parameter θj is updated in every iteration according to this equation.</strong></figcaption></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/de90cbac77d680b1cd55a59add86983e.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*CfYUB-xRXlCUdui-KSs_fg.png"/></div></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/4502e0c97356d5fe04874d6ab48f378c.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*3_HFjUWDu8q_RkNB-_O1sg.png"/></div></figure><p id="3d68" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">通过使用链式法则，</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/c0c80ebc245a2aa07e528ca847a7758e.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*L4oJokqwaTmgZ48lfLXXXA.png"/></div></figure><p id="f090" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">为了计算ℓ(θ的梯度)来更新权重θj，我们对θ <strong class="ld ir"> ⱼ </strong>取偏导数:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ow"><img src="../Images/df3859c16b73eeb0e01a2a4c3508f670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMGr4Pv_bhJaLBSAMQDMVg.png"/></div></div></figure><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/8fdec9d67c99f16ce643aee2a355406d.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*sRoA2tzW4ILDSEAze2qZTg.png"/></div></figure><h2 id="4c31" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">Python中从头开始的逻辑回归</h2><p id="d654" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko ol mc md ks om mf mg kw on mi mj ll ij bi translated">这就是所有的数学。现在，让我们用一个用Python实现的例子来体验一下。</p><p id="2a68" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">让我们看看乳腺癌诊断(良性(B)或恶性(M))的数据集:</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="oy oz l"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source: <a class="ae mk" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/</a></figcaption></figure><blockquote class="ns nt nu"><p id="9620" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">在此下载数据集<a class="ae mk" href="https://www.kaggle.com/shankarat/breast-cancer/download/ZB7v5vIxh1VfTqqnUsqU%2Fversions%2FWXNrAH2EOhtFdfZUTrGE%2Ffiles%2Fbreast%20cancer.csv?datasetVersionNumber=1" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="d71f" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">导入相关库:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><ul class=""><li id="1100" class="lb lc iq ld b le lx lg lz ko pb ks pc kw pd ll pe ln lo lp bi translated"><strong class="ld ir"> scikit-learn </strong> —导入train_test_split方法和LabelEncoder类。train_test_split用于将数据分成训练集和测试集。LabelEncoder用于将分类数据转换为数字(0或1)。</li><li id="e90a" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll pe ln lo lp bi translated"><strong class="ld ir"> numpy </strong> —用于矩阵和向量计算。</li><li id="fda3" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll pe ln lo lp bi translated"><strong class="ld ir">熊猫</strong> —用于从csv文件导入数据。</li><li id="d09a" class="lb lc iq ld b le lq lg lr ko ls ks lt kw lu ll pe ln lo lp bi translated"><strong class="ld ir"> matplotlib </strong> —用于绘制混淆矩阵</li></ul><blockquote class="ns nt nu"><p id="3adb" class="lv lw nv ld b le lx jr ly lg lz ju ma nw mb mc md nx me mf mg ny mh mi mj ll ij bi translated">点击<a class="ae mk" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" rel="noopener ugc nofollow" target="_blank">此处</a>了解混淆矩阵的解释。</p></blockquote><p id="4026" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">乙状结肠功能:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="8551" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">预测概率函数:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="91a0" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">预测功能:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="c1b6" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">梯度下降:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="a04c" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">剧情混乱矩阵功能:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source: <a class="ae mk" href="https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py</a></figcaption></figure><p id="04a2" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">准备数据:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="fd87" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">训练和评估:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="8b86" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">结果:</strong></p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a52dcda4f286649544a007e1ffa87fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*9H3RXqLjEn3VKu5EqhLWag.png"/></div></figure><p id="8789" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从图中，我们可以观察到对数似然值在梯度下降算法的每次迭代中收敛到最大值。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8473d8882be075056ea1ce8267d9a0f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Die6Mlh1rED-X161a2FX_Q.png"/></div></figure><p id="181d" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从上面的混淆矩阵中，我们可以观察到所有71个良性病例和43个恶性病例中的42个被正确分类。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ba4a681c4ece160b4d8e27a298e0fbd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qhNzjk-tW-iD_JrDvP4Dfg.png"/></div></figure><p id="79bf" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">如从标准化混淆矩阵观察到的，预测良性病例的准确度是100%，而预测恶性病例的准确度是98%。</p><h2 id="89c1" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">从Sklearn库实现逻辑回归模型:</strong></h2><p id="11ae" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko ol mc md ks om mf mg kw on mi mj ll ij bi translated"><strong class="ld ir">导入相关库:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="d778" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">准备数据:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="cccb" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">培训和评估:</strong></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="pa oz l"/></div></figure><p id="bb0f" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><strong class="ld ir">结果:</strong></p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/836c06e4573eb70886702599633d8b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*7mH6E1g3sdKd4eMBlt802A.png"/></div></figure><p id="3569" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">从上面的混淆矩阵中，我们可以观察到71个良性病例中的70个和43个恶性病例中的41个被正确分类。</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div class="gh gi na"><img src="../Images/39633eb584eaac62d2e533d49c4ceb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*0uDA6PnZVXh04BO8-ykGFw.png"/></div></figure><p id="709a" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">预测良性病例的准确率为99%，而预测恶性病例的准确率为95%。</p><h2 id="6561" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h2><p id="bc02" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko ol mc md ks om mf mg kw on mi mj ll ij bi translated">在我们刚刚处理的例子中，只有两个类需要分类，这就是所谓的二进制分类。然而，在实际应用中，总会有两个以上的类别需要分类，例如水果分类。涉及到两个以上分类的逻辑回归称为<a class="ae mk" href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir"/></a><strong class="ld ir">【soft max回归】</strong>。我们将在以后的文章中讨论softmax回归。</p><div class="pf pg gp gr ph pi"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd ir gy z fp pn fr fs po fu fw ip bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw ng pi"/></div></div></a></div><p id="dde9" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">感谢您的阅读。希望你喜欢我的文章，并从中获得有用的信息。</p><h2 id="37b1" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参考</h2><p id="7150" class="pw-post-body-paragraph lv lw iq ld b le lf jr ly lg lh ju ma ko ol mc md ks om mf mg kw on mi mj ll ij bi translated">[1]<a class="ae mk" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logistic_regression</a></p><p id="d0ab" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><a class="ae mk" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logit</a></p><p id="5b10" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">[3]<a class="ae mk" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Likelihood_function</a></p><p id="24fc" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated">[4]<a class="ae mk" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" rel="noopener ugc nofollow" target="_blank">https://www . data school . io/simple-guide-to-confusion-matrix-terminals/</a></p><p id="1266" class="pw-post-body-paragraph lv lw iq ld b le lx jr ly lg lz ju ma ko mb mc md ks me mf mg kw mh mi mj ll ij bi translated"><a class="ae mk" href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="px oz l"/></div></figure></div></div>    
</body>
</html>