<html>
<head>
<title>Web Scraping with Python Part 1: A High-Level Explanation of My Experience</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python进行Web抓取第1部分:我的经验的高级解释</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/web-scraping-with-python-part-1-a-high-level-explanation-of-what-ive-learned-5401d46fbec2?source=collection_archive---------13-----------------------#2020-10-05">https://medium.datadriveninvestor.com/web-scraping-with-python-part-1-a-high-level-explanation-of-what-ive-learned-5401d46fbec2?source=collection_archive---------13-----------------------#2020-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="cbca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在是午夜，我已经是第三次轮班回到我原来的服务领域工作了。如果你读过我以前的文章，你就会知道我在三月份被一家酒店的总经理解雇了。长话短说，我现在回来在另一家酒店做夜间审计员，这似乎是一项临时任务。我已经有近两个月没有写任何关于我的数据科学之旅的东西了。这篇文章的主题是网络搜集。</p><p id="63ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我的数据科学沉浸式训练营期间，当我们准备为一个项目开发一个模型来预测Reddit上特定帖子的来源时，我们被介绍到了web抓取。在这些课程中，我们简要介绍了HTML，然后使用Beautiful Soup来执行我们的抓取。对于这个特定的项目，我使用Pushshift API从两个子编辑中抓取信息。遇到的限制包括每次请求只能收集100个帖子，这对于训练模型来说是不够的。为了解决这个问题，我创建了一个函数，其中包括使用“时间”包来实现2秒钟的等待时间；while循环，为函数运行一定次数的迭代；和一个变量，用于在最后一次收集帖子时重新开始下一次迭代。每次循环迭代时，我都用最后一篇文章的UTC(在抓取中收集的)替换变量的值。我使用这个变量来确定从哪里开始下一次迭代。最终，我能够使用下图中的函数为我的特定项目收集所有数据。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/04a7de13494b8a283ce1e35b223e5edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_-cEl1tRI1BU3JP3MKRaw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Python code for using Pushshift API (Chris Johnson)</figcaption></figure><p id="e7ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在接下来的几周里，我唯一的网络抓取经验就是使用API和现有的包。下一次我使用webs craping是为了一个项目，该项目使用twitter数据建立一个模型来检测特定的tweet是否与自然灾害有关。为了这个项目，我和我的时间将测试几种不同的工具，包括<a class="ae lb" href="https://github.com/twintproject/twint" rel="noopener ugc nofollow" target="_blank"> Twint </a>、<a class="ae lb" href="https://www.tweepy.org/" rel="noopener ugc nofollow" target="_blank"> Tweepy </a>和<a class="ae lb" href="https://pypi.org/project/GetOldTweets3/" rel="noopener ugc nofollow" target="_blank"> GetOldTweets3 </a>。这些工具都提供了本质上相同的产品，让我可以从twitter上抓取推文。他们共享相同的功能，允许我们使用特定的关键词、地点、日期等来抓取推文。最终，我们能够使用类似于下面GetOldTweets3的代码，为我们的项目收集三种不同类型的灾难的tweet数据。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lc"><img src="../Images/ca527129a5a9c423c943d052f8af98bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4IT-DC370fl0HMv90etMbA.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Python code for using GetOldTweets3 (Chris Johnson)</figcaption></figure><p id="6601" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你对当我遇到一个web抓取项目时会发生什么感兴趣，我没有像上面那样的API或库，请留意。我将在本周晚些时候发布我从零开始编写网络抓取项目的经历，好家伙，这是一次有趣而令人兴奋的冒险。</p></div></div>    
</body>
</html>