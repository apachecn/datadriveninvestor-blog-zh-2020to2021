<html>
<head>
<title>Machine Learning 101 — The Bias-Variance Conundrum</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习101——偏差-方差难题</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/machine-learning-101-the-bias-variance-conundrum-f4143ba9f179?source=collection_archive---------12-----------------------#2020-08-01">https://medium.datadriveninvestor.com/machine-learning-101-the-bias-variance-conundrum-f4143ba9f179?source=collection_archive---------12-----------------------#2020-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="3346" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">确定我们模型的性能是机器学习过程中最关键的步骤之一。理解偏差-方差权衡是解释我们模型结果的重要一步。尽管本质上很琐碎，但这种权衡背后的概念很容易掌握，并允许我们创建更好、更有用的模型。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b8268309ca12f6f17c6c2c5fe499b8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-LBzAxMOmqXJxLdL"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nathan Dumlao</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ea7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">任何机器学习模型的泛化误差都可以定义为三种不同误差的总和—</p><ol class=""><li id="ae04" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu">不可约误差:</strong>顾名思义，无论我们选择什么算法都是不可约的。它被引入到我们的模型中是因为我们构建问题的方式，并且可能是由影响我们目标变量预测的未知变量引起的。</li><li id="b216" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">偏差误差:</strong>当我们的模型做出错误的假设时，就会出现偏差误差</li><li id="2094" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">方差误差:</strong>由对训练集中微小变化的敏感性引起</li></ol><blockquote class="lt"><p id="8cb1" class="lu lv it bd lw lx ly lz ma mb mc kn dk translated">当我们讨论预测模型时，预测误差可以分解成我们关心的两个主要子部分:由于“偏差”造成的误差和由于“方差”造成的误差。在模型最小化偏差和方差的能力之间有一个权衡。理解这两种类型的误差可以帮助我们诊断模型结果，避免过度拟合或欠拟合的错误。~斯科特·福特曼-罗</p></blockquote><p id="7274" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">在这篇博文中，我们将重点关注<em class="mi">偏差误差、方差误差</em>和<em class="mi">偏差-方差权衡。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1009" class="mq mr it bd ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bi translated">偏移误差</h1><p id="d31c" class="pw-post-body-paragraph jq jr it js b jt no jv jw jx np jz ka kb nq kd ke kf nr kh ki kj ns kl km kn im bi translated">偏差是我们模型的预期预测与实际目标值之间的差异，即我们的预测与实际值之间的差距。本质上，我们的模型的偏差是由它预测我们的目标值的假设决定的。简而言之，高偏差意味着我们的学习算法没有捕捉到潜在的模式。这种模型随后会在训练集和测试集上产生很大的误差。</p><ul class=""><li id="1124" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated"><em class="mi">决策树、k近邻</em>和<em class="mi">支持向量机</em>都是低偏向机器学习算法</li><li id="1df9" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated"><em class="mi">线性回归</em>和<em class="mi">逻辑回归</em>是高偏差机器学习算法</li></ul><h1 id="bf39" class="mq mr it bd ms mt nu mv mw mx nv mz na nb nw nd ne nf nx nh ni nj ny nl nm nn bi translated">方差误差</h1><p id="b43f" class="pw-post-body-paragraph jq jr it js b jt no jv jw jx np jz ka kb nq kd ke kf nr kh ki kj ns kl km kn im bi translated">它被定义为如果我们使用不同的训练集，我们的模型的预测将改变的量。具有高方差的模型往往更关注训练集中存在的数据，并且不能很好地概括，即，它们在测试集上表现不好。换句话说，这种机器学习算法试图尽可能地使自己适应训练数据。通过这样做，他们做出了复杂的假设，这些假设可能只对训练数据成立，因此他们在测试集上的表现要差得多。</p><ul class=""><li id="c801" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated"><em class="mi">线性回归</em>和<em class="mi">逻辑回归</em>是低方差机器学习算法</li><li id="2c5b" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated"><em class="mi">决策树、k近邻</em>和<em class="mi">支持向量机</em>是高方差机器学习算法</li></ul><h1 id="7799" class="mq mr it bd ms mt nu mv mw mx nv mz na nb nw nd ne nf nx nh ni nj ny nl nm nn bi translated">偏差-方差权衡</h1><p id="df44" class="pw-post-body-paragraph jq jr it js b jt no jv jw jx np jz ka kb nq kd ke kf nr kh ki kj ns kl km kn im bi translated">现在，让我们借助牛眼图来尝试理解偏差和方差之间的权衡。我们已经知道的一件事是，偏差和方差彼此成反比，也就是说，如果偏差增加，那么方差减少，反之亦然。</p><div class="nz oa gp gr ob oc"><a href="https://www.datadriveninvestor.com/2020/07/31/using-machine-learning-in-brain-computer-interfaces/" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">在脑机接口中使用机器学习|数据驱动的投资者</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">神经技术是一个刚刚开始大步前进的前沿领域。有了所有的技术…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ky oc"/></div></div></a></div><p id="0d4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们假设图表的中心是一个完美预测目标值的模型，离中心越远，我们的预测就越差。如果我们重复我们的模型构建过程，在这里或那里做一些改变，那么每次我们都会得到多个目标，每个目标代表一个单独模型的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/75d455a514ea292986f545a5a1faa9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*pRlNrZrm98hlQoMr.jpg"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae le" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">Bulls-eye diagram depicting the Bias-Variance Tradeoff</a></figcaption></figure><p id="85dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了了解如何解释我们的结果，让我们来看看我们可能观察到的不同情况:</p><ol class=""><li id="6ae7" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu">低偏差&amp;低方差</strong></li></ol><ul class=""><li id="3ba9" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated">我们机器学习模型的理想情况</li><li id="369d" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">预测的误差尽可能低</li><li id="7c03" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">当我们选择不同的训练集时，预测不会有太大的变化</li></ul><p id="31a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。高偏置&amp;高方差</strong></p><ul class=""><li id="edcf" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated">我们的机器学习模型的最坏情况</li><li id="9a90" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">预测的误差极高</li><li id="a825" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">当我们使用不同的训练集时，预测波动很大</li></ul><p id="9da3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。高偏差&amp;低方差</strong></p><ul class=""><li id="9fc5" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated">通常被称为<em class="mi">欠拟合</em>，这意味着我们的模型无法捕捉数据中存在的潜在模式</li><li id="030c" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">通常是由于存在少量数据而发生的</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/bf5e32aa0738c5c15e509f7313e5b8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5AtkxHk0BuhPXvDw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae le" href="https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html" rel="noopener ugc nofollow" target="_blank">Underfitting vs Overfitting</a></figcaption></figure><p id="9702" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 4。低偏差&amp;高方差</strong></p><ul class=""><li id="11ea" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated">也称为<em class="mi">过度拟合</em>，这意味着我们的模型发现了数据中存在的潜在模式，但也将噪声解释为有用的信息</li><li id="b474" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">当我们在没有被正确清理的数据上训练我们的模型时，就会发生这种情况</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="8d3b" class="mq mr it bd ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bi translated">摘要</h1><p id="2380" class="pw-post-body-paragraph jq jr it js b jt no jv jw jx np jz ka kb nq kd ke kf nr kh ki kj ns kl km kn im bi translated">在本质上，偏差-方差权衡旨在避免欠拟合和过拟合。随着模型复杂性的增加，偏差减少，而方差也增加。换句话说，如果我们不断向模型中添加更多的特征，我们的主要关注点就会从减少偏差转移到减少模型的方差。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6e0149212ff3cc3b43fffbc867726947.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*2MBPZdfQHAKiSOqi.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae le" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">Error Complexity Curve</a></figcaption></figure><p id="f150" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如前所述，我们模型的泛化误差由三种不同的误差组成，可以用数学方法描述如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ff146200e4acc5d737cead9be708c23f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*jcSuv23_xZJzHZSg.png"/></div></figure><p id="d619" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面显示的误差复杂性曲线中的虚线表示最佳模型复杂性，并被视为我们机器学习模型的<em class="mi">最佳点</em>。当偏差的增加等于模型方差的减少时，我们可以说找到了最佳点。数学上我们得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f92c6f7e726958c9571e7d1355f38330.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/0*Z8hO-BXbYA8WqjlJ.png"/></div></figure><p id="97c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们模型的复杂性超过了最佳点，那么我们就过度拟合我们的模型，如果我们没有达到最佳点，那么我们就不足拟合我们的模型。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="b807" class="mq mr it bd ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn bi translated"><strong class="ak">结束… </strong></h1><p id="2854" class="pw-post-body-paragraph jq jr it js b jt no jv jw jx np jz ka kb nq kd ke kf nr kh ki kj ns kl km kn im bi translated">本质上，我们可以将偏差和方差之间的关系定义如下:</p><ul class=""><li id="adbd" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn nt ll lm ln bi translated">增加偏差会减少方差；和</li><li id="ac23" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn nt ll lm ln bi translated">增加方差将减少偏差</li></ul><p id="f4a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然没有确定的方法来获得所谓的最佳点，但我们可以通过使用适当的指标来分析我们模型的性能，或者根据我们的目的选择正确的算法(及其适当的配置)来尽最大努力找到它。因此，我们可以得出结论，偏差-方差权衡是一个重要的考虑因素，我们可以将其用作确定我们的机器学习模型的预测性能的起点。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="f679" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">T3】资源:T5】</strong></p><ol class=""><li id="8a8b" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><a class="ae le" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="mi">机器学习中偏差-方差权衡的温和介绍</em> </a></li><li id="2532" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank"> <em class="mi">了解偏差-方差权衡</em> </a></li><li id="c2d5" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" href="https://www.youtube.com/watch?v=0UTNyTZgEWQ" rel="noopener ugc nofollow" target="_blank"> <em class="mi">偏差-方差权衡— Bhavesh Bhatt </em> </a></li></ol><p id="75ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">访问专家视图— </strong> <a class="ae le" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>