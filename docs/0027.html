<html>
<head>
<title>Drug Design Made Fun Using Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用强化学习使药物设计变得有趣</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/drug-design-made-fun-using-reinforcement-learning-212a4f867f33?source=collection_archive---------1-----------------------#2020-01-03">https://medium.datadriveninvestor.com/drug-design-made-fun-using-reinforcement-learning-212a4f867f33?source=collection_archive---------1-----------------------#2020-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="190a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用强化学习，通过优化生成的分子来增强基于结构的药物设计模型，以展示所需的特性。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/518500b361697f8a94cc84381ac23d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ED5NV8EbzgHb6jLH.jpg"/></div></div></figure><p id="6f23" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="ln">简要描述:</em> </strong> <em class="ln">使用本文中解释的强化学习方法，对生成的分子进行优化，使其表现出对JAK2的抑制活性，以治疗血液癌症，如p </em>白血病<em class="ln"> vera。这证实了该方法对于设计具有用户期望特性的分子是有效的。</em></p><p id="cdab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">将一种药物从研究阶段应用到患者身上需要10年以上的时间，平均花费26亿美元！</p><p id="bdc8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">计算方法，特别是<strong class="kt ir">人工智能</strong>为加速药物发现提供了一种非常有前途的方法。</p><p id="8068" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，目前许多基于机器学习和结构的药物设计方法都有局限性，因为它们<strong class="kt ir"> <em class="ln">无法</em>有效控制生成分子的特性。</strong></p><div class="lo lp gp gr lq lr"><a href="https://www.datadriveninvestor.com/2019/02/08/machine-learning-in-finance/" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab fo"><div class="lt ab lu cl cj lv"><h2 class="bd ir gy z fp lw fr fs lx fu fw ip bi translated">金融中的机器学习|数据驱动的投资者</h2><div class="ly l"><h3 class="bd b gy z fp lw fr fs lx fu fw dk translated">在我们讲述一些机器学习金融应用之前，我们先来了解一下什么是机器学习。机器…</h3></div><div class="lz l"><p class="bd b dl z fp lw fr fs lx fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf kp lr"/></div></div></a></div><p id="504d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过将药物设计建模为“动态游戏”，我们可以使用强化学习来生成具有所需属性的分子。想象一下药物设计，但取而代之的是药物被PacMan取代。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/fa4a0899ac42d7fe3c7cfbb9111a00ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-rAtdR8n2tQwzT-8.png"/></div></div></figure><h1 id="eb7c" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">什么是强化学习？</h1><p id="59a6" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated">强化学习是机器学习的一种。与监督学习和非监督学习不同，强化学习不从数据中学习，而是从实验中学习。</p><p id="6186" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们来看看强化学习是如何帮助机器玩吃豆人的。</p><p id="5e77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习归结起来有两个组成部分:</p><ul class=""><li id="3209" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">代理人=吃豆人</li><li id="2c04" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">批评家=积极的回报和消极的后果</li></ul><p id="de7c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习代理通过尝试不同的动作来学习玩吃豆人。起初，特工“吃豆人”随机移动。</p><p id="f399" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过尝试不同的随机移动，代理人随着时间的推移慢慢地学习与奖励和结果相关的行动。代理人会因为像吃花生豆这样导致奖励的行为而得到奖励，也会因为像撞鬼这样导致后果的行为而受到惩罚。</p><p id="b53d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">吃豆人变得更好的方法是只学习导致奖励的重复动作，而不是导致结果的动作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/73272da70223e7c305567ea2b2d46716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Anc8rzFZ3Kf2TN5B.png"/></div></div></figure><p id="5ce7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">重复的训练过程看起来像这样:</p><ol class=""><li id="e3c8" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nt nk nl nm bi translated">代理人PacMan被分配了一个⁰的州，这是代理人“玩”的环境。</li><li id="e216" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">然后吃豆人采取一个行动，<strong class="kt ir">一个</strong> ⁰并且移动，对于这个具体的例子，它向左移动了7个单位。代理现在已经转移到一个新的状态，<strong class="kt ir"> S </strong>。</li><li id="f38d" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">因为吃豆人吃了7颗豆并且还活着，代理人得到奖励+7。</li></ol><p id="5a0a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">视觉表现:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/239f679c65d3c111aa39de9dc72041c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*iHYXwi5Iy8VhkBf5t3ncpw.jpeg"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Agent in State 0 — <strong class="bd mj">S</strong>⁰</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c97a959a30934a0da32fa4cc80e49e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*3d4xoQHu2pGF7JrMqum53Q.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Agent moves 7 units to the right and is now in state 1<strong class="bd mj"> — S¹</strong>.<strong class="bd mj"> </strong>It gets rewarded 7+.</figcaption></figure><p id="2484" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">总结一下:<strong class="kt ir">通过经验，强化学习优化行动，最大化回报，同时最小化负面后果。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fd7c437f267d188c0c0859db57c532b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*Nt7Sz7PcJi5lOkM7.jpg"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">The agent, like humans learns from its mistakes/experience and optimizes its actions to maximize reward, and minimize punishment.</figcaption></figure><h2 id="b39c" class="ob mi iq bd mj oc od dn mn oe of dp mr la og oh mt le oi oj mv li ok ol mx om bi translated">药物设计的强化学习</h2><p id="5f81" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated">为了将药物设计建模为游戏，以便强化学习可以应用于具有期望特性的分子的<strong class="kt ir">新药物设计</strong>，将两个单独的模型组合在一起— <strong class="kt ir"> [1]负责生成新分子的生成模型</strong>，以及<strong class="kt ir"> [2] </strong> <strong class="kt ir">负责预测用户指定的分子特性的(分子)特性预测模型</strong>。</p><ul class=""><li id="af82" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">在这种情况下，指定的特性是JAK2的pIC-50(最大抑制浓度一半的负对数)值。该值实质上代表了对JAK2 (janus激酶2)的抑制活性的效力。(数值越高=抑制效果越强)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/24b2a051a6ed17c9e1d960be831593cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*80aNJqqkpFpOaI-A.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">JAK2 is a non-receptor tyrosine kinase.</figcaption></figure><p id="e34c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">生成模型[1]充当“强化学习代理”,而属性预测模型[2]充当负责分配奖励或惩罚的“评论家”。评论家根据定义的奖励函数分配一个奖励或惩罚，这个奖励或惩罚是一个数字(正数表示奖励，负数表示惩罚)。</p><p id="feeb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln">设计好奖励函数非常重要，因为这可以决定强化学习模型的训练效果。</em></p><p id="9fce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这种情况下，奖励函数被设计成使得代理(生成模型)学习优化生成的分子，使得它们表现出对JAK2的有效抑制作用。本质上，奖励函数的目标是诱使代理人最大化JAK2的pIC-50值。</p><p id="5109" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">定义的奖励函数为:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/694ac585df8f76e35c8c9a58808c98fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*BSR3eUpIhHSZXqhF8biNtA.png"/></div></figure><p id="d996" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">总结每个变量所代表的含义:</p><ul class=""><li id="2f39" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">s —代理所处的状态</li><li id="b645" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">PP——属性预测模型</li></ul><p id="9e1c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ln">属性预测模型接受输入s，s是智能体所处的当前状态，或者智能体</em><em class="ln"/><strong class="kt ir"><em class="ln">产生的</em> <strong class="kt ir"> <em class="ln">分子为JAK2 </em> </strong> <em class="ln">输出相应的pIC-50值。与较低的pIC-50值相比，如果该试剂产生的分子具有较高的pIC-50值，则该试剂获得更多奖励。</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/bedef815d5328bed2cf47066f4e924c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*7IaflTVzjeX0j1mKCxiKFw.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><em class="oq">The reward function is defined in terms of the property prediction model output given the current state of the agent as input. For more complicated scenarios, the reward function can be defined not only in terms of the current state of the agent, but also take into account previous states, action of the agent, and etc.</em></figcaption></figure><h1 id="7cef" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">培训过程</h1><p id="bc94" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated">生成和属性预测模型最初使用监督学习方法分别训练。</p><ul class=""><li id="40fa" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">生成模型在包含150万个分子的ChEMBL21数据集上进行训练。</li><li id="ca37" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">性质预测模型在JAK2活性数据集上训练，该数据集具有来自ChEMBL数据库的其相应的pIC-50(半最大抑制浓度的负对数)值。</li></ul><p id="fa2a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后，使用一种称为策略梯度(policy gradient)的强化学习方法(一种策略优化方法)将这两种模型结合起来并一起训练。</p><p id="fb51" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">训练迭代看起来像这样:</p><ol class=""><li id="f1c2" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nt nk nl nm bi translated">每当生殖模型产生一个新的分子，它就进入一个新的状态。</li><li id="f5f0" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">性质预测模型评估由发生器产生的分子，并输出其对应的特定于JAK2的pIC-50值。</li><li id="4280" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">基于pIC-50值输出，分配奖励值。</li><li id="6c18" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">生成器使用策略梯度方法基于回报更新其参数。</li></ol><h2 id="6db2" class="ob mi iq bd mj oc od dn mn oe of dp mr la og oh mt le oi oj mv li ok ol mx om bi translated">政策梯度</h2><p id="4417" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated">策略是强化学习代理用来选择其动作的概率分布。这是给定代理所处的状态下，代理可以采取的所有可能动作的可能性。</p><p id="d925" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">更直观地说，<strong class="kt ir">策略是一个代理人用来实现其目标的策略，该目标将被奖励</strong>(或者更正式地说，完成一项任务)。</p><p id="5f3e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们回到吃豆人的例子。让我们来看看两个不同的代理人在吃豆人游戏中获得奖励的策略:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/60775bb07619534bf9ec2375ceb35811.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*GwJFAvQAek2ow6Oc.jpg"/></div></figure><ul class=""><li id="dce8" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">随机代理(无意双关)——随机移动，通过这样做，它真的很幸运，因为它不会碰巧撞上任何鬼魂，并且能够吃掉一堆PacDots，这样做它会得到奖励</li><li id="61f5" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">智能代理——根据奖励高的樱桃或零食的位置，设计它之前想要走的路线</li></ul><p id="9e02" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一些策略比其他策略更好，并且<strong class="kt ir">强化学习的最终目标是为代理学习最优策略以达到预期目标。</strong></p><p id="365e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，政策梯度法大致如下:</p><ol class=""><li id="e3d6" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nt nk nl nm bi translated">代理从任意策略开始，这意味着代理采取随机的动作</li><li id="cb00" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">代理对其环境中的一些动作进行采样</li><li id="85de" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">如果代理人最终采取的行动回报比预期的更好，那么我们增加了再次采取这些行动的可能性。</li><li id="e923" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">如果代理人最终采取的行动回报比预期的更差，那么我们就减少了再次采取这些行动的可能性。</li></ol><ul class=""><li id="83fe" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated">在这种情况下，由于代理是负责生成SMILES字符串格式的分子的生成模型，因此<strong class="kt ir"> <em class="ln">代理可以采取的可能动作A </em>被定义为用于编译SMILES字符串的字母表的集合。</strong></li><li id="3b91" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">构成环境的所有可能状态</strong>被定义为<strong class="kt ir">用于定义SMILES字符串</strong>的字母表的所有可能组合，直到给定的长度，我们称之为T(导致训练结束的终端状态)。</li></ul><p id="6e4c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">策略函数</strong>用于输出所有可能动作的概率分布<strong class="kt ir">使用softmax函数将策略函数logits输出的原始数字转换为0–1</strong>之间的概率。<em class="ln">所有的概率加起来等于1(这就是为什么使用softmax而不是sigmoid激活函数的原因)。</em></p><p id="f04e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">策略梯度损失函数定义为:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/992a40fd22b773423f2e1c4a0f46b946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*L2oA_Ez-14gbFVL5iPUXPg.png"/></div></figure><ul class=""><li id="5cb8" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated"><strong class="kt ir"> R </strong> —采取给定行动后，给定状态下的奖励。</li><li id="b163" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir"> y (gamma) </strong> —折扣因子，是一个介于0和1之间的值。</li><li id="8559" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir"> <em class="ln">伽玛参数</em>代表了<em class="ln">未来可以预期得到多少</em>的回报。</strong> <em class="ln">我们来看2个例子:</em><strong class="kt ir"><em class="ln">A</em></strong><em class="ln">—即时奖励10，长期奖励0，</em><strong class="kt ir"><em class="ln">B</em></strong><em class="ln">—即时奖励0，长期奖励30。</em>选项A将有一个<em class="ln">较小的伽马值</em>，而选项B将有一个<em class="ln">较高的伽马值</em>，因为未来预期会有更多的奖励<em class="ln">。</em></li><li id="466c" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir"> p(si| s0…si-1 θ) </strong> — <strong class="kt ir">给定代理的先前状态，生成模型<em class="ln">(又名代理)</em>输出</strong>(这是代理的下一个状态的可能性)。</li><li id="9119" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">在强化学习中，我们希望找到能带来最大回报的最优策略。因此，<strong class="kt ir">我们希望最大化策略梯度损失，而不是像在梯度下降中那样最小化。</strong> <em class="ln">政策梯度损失函数中的负号很重要，因为它服务于最大化的目的。</em></li><li id="028a" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><strong class="kt ir">使用对数</strong>是因为它们在数值上更加<strong class="kt ir">稳定</strong>，因此性能更好。反向传播也更简单。</li></ul><p id="e2ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简单来说，如果所采取的行动的价值比预期的价值更好，那么特定行动的可能性就会增加。下一次，代理将更有可能再次采取该特定操作。相反，如果所采取行动的价值比期望值更差，那么特定行动的概率就会降低。因此，<strong class="kt ir">我们想要最大化损失函数，因为我们想要最大化报酬。</strong></p><p id="5d86" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们来看看生成模型和属性预测模型的模型架构。</p><h2 id="e747" class="ob mi iq bd mj oc od dn mn oe of dp mr la og oh mt le oi oj mv li ok ol mx om bi translated">生成模型</h2><p id="784d" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated"><strong class="kt ir">生成模型利用变分自动编码器生成SMILES格式的有效分子</strong>(简化的分子输入行输入系统)。</p><p id="7f23" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">变分自动编码器使用编码器、解码器方法和概率方法从潜在空间采样以生成新分子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl ot"><img src="../Images/e04fb16c1aad8545b9da7970df0afe78.png" data-original-src="https://miro.medium.com/v2/format:webp/0*UQH8M9Rm-XkiX5Sl.png"/></div></figure><p id="ed79" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">变分自动编码器(VAE)不同于普通自动编码器，它的潜在空间是连续的。</p><p id="eceb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">潜在空间是连续的，因为VAEs使用了一种概率方法，其中潜在向量z是通过对从输入数据集合的两个向量进行采样来构建的:一个向量表示平均值，另一个向量表示输入数据的标准偏差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl ot"><img src="../Images/219d5fb16a2242ff9fbbb417ec695ec9.png" data-original-src="https://miro.medium.com/v2/format:webp/0*KEl5nP779X3hOcNv.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd mj">μ</strong> represents the mean and <strong class="bd mj">σ </strong>represents the standard deviation</figcaption></figure><p id="a150" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要了解更多关于利用VAEs生成新分子的信息，请查看我写的这篇文章。</p><h2 id="54cc" class="ob mi iq bd mj oc od dn mn oe of dp mr la og oh mt le oi oj mv li ok ol mx om bi translated">房地产预测模型</h2><p id="4240" class="pw-post-body-paragraph kr ks iq kt b ku mz jr kw kx na ju kz la nb lc ld le nc lg lh li nd lk ll lm ij bi translated">属性预测模型由以下层组成:</p><ol class=""><li id="2599" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nt nk nl nm bi translated">输入层-将微笑字符串作为输入</li><li id="caf3" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">嵌入图层-用于为输入单词创建矢量，该图层的输出将成为LSTM图层的输入。</li><li id="4f1a" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">LSTM(长短期记忆)层——一种递归神经网络</li><li id="bb47" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">密集层—规则人工神经网络</li><li id="ddcc" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nt nk nl nm bi translated">输出图层-输出JAK2的相应pIC-50值</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e4f4ee5fe7f62d046c68eb5646f80a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g-HfGJKHiIrMUnxl.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">LSTM cell</figcaption></figure></div><div class="ab cl ov ow hu ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="ij ik il im in"><p id="da37" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习是设计具有期望特性的分子的有效方法。</p><p id="d6bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">深度强化学习为加速人工智能领域提供了希望，因为与监督学习方法相比，它能够掌握对环境的更高层次的理解。以下是最近取得的一些显著成就:</p><ul class=""><li id="270e" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated"><a class="ae ou" href="https://www.technologyreview.com/s/611724/artificial-intelligence-driven-robot-hand-spends-a-hundred-years-teaching-itself-to-rotate/" rel="noopener ugc nofollow" target="_blank"> OpenAI的Dactyl</a>——一个使用强化学习进行虚拟模拟训练的单手机器人学习如何解魔方。</li><li id="58f7" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated"><a class="ae ou" href="https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/" rel="noopener ugc nofollow" target="_blank"> OpenAI的捉迷藏代理</a>——open ai利用强化学习训练代理玩捉迷藏。</li><li id="29de" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">AlphaGo  —智能体使用强化学习进行训练，并能够击败世界上最好的围棋选手。</li></ul><p id="4709" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">参考文献:</strong></p><ul class=""><li id="2892" class="ne nf iq kt b ku kv kx ky la ng le nh li ni lm nj nk nl nm bi translated"><a class="ae ou" href="https://advances.sciencemag.org/content/4/7/eaap7885" rel="noopener ugc nofollow" target="_blank"> <em class="ln">深度强化学习进行de nova药物设计</em> </a> <em class="ln"> (2018) </em></li></ul><h1 id="b463" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">关键要点:</h1><ul class=""><li id="736d" class="ne nf iq kt b ku mz kx na la pc le pd li pe lm nj nk nl nm bi translated">利用强化学习，我们可以合理地设计具有所需性质的新型分子，例如对JAK2的抑制作用。</li><li id="90c7" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">政策梯度是一种强化学习方法。</li><li id="b766" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">生成模型使用变分自动编码器来生成分子，而属性预测模型使用LSTM模型(一种递归神经网络)。</li></ul><h2 id="6e0a" class="ob mi iq bd mj oc od dn mn oe of dp mr la og oh mt le oi oj mv li ok ol mx om bi translated">走之前，别忘了:</h2><ul class=""><li id="0ed7" class="ne nf iq kt b ku mz kx na la pc le pd li pe lm nj nk nl nm bi translated">请访问Synbiolic的网站,了解有关该项目的更多信息！</li><li id="fb90" class="ne nf iq kt b ku nn kx no la np le nq li nr lm nj nk nl nm bi translated">在<a class="ae ou" href="https://www.linkedin.com/in/joey-mach-6293b1175/?originalSubdomain=ca" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系！</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure></div></div>    
</body>
</html>