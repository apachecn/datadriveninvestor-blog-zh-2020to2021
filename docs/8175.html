<html>
<head>
<title>Text Prediction Using Long Short-Term Memory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用长短期记忆的文本预测</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/text-prediction-using-long-short-term-memory-a42fc66e22c3?source=collection_archive---------10-----------------------#2021-01-04">https://medium.datadriveninvestor.com/text-prediction-using-long-short-term-memory-a42fc66e22c3?source=collection_archive---------10-----------------------#2021-01-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dd4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">最近，谷歌和脸书专注于文本预测的幕后机制。除了使用递归神经网络和长短期记忆网络的动机，还有两个word2vec模型生成单词嵌入也进行了讨论。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/48f7cc00c7fd8ad9e8deaa8346ea526c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*ZwpFITgrSf9W2RuDLj6dXw.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">framework for LSTM</figcaption></figure><p id="f735" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在探索深度学习的演变时，我举例说明了一些只在谷歌内部应用的领域，作为操场上的主要神童之一。所以，多亏了深度学习。</p><blockquote class="lg lh li"><p id="dc96" class="jn jo lj jp b jq jr js jt ju jv jw jx lk jz ka kb ll kd ke kf lm kh ki kj kk ij bi translated">word2vec模型有两种开发方式:<strong class="jp ir"> <em class="iq">连续词包</em> </strong> (CBOW)和<strong class="jp ir">跳格</strong>。</p></blockquote><p id="3615" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了更详细地理解，首先，让我们看看CBOW和跳格模型。</p><h1 id="13a8" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">连续跳格和词袋</h1><p id="a86f" class="pw-post-body-paragraph jn jo iq jp b jq ml js jt ju mm jw jx jy mn ka kb kc mo ke kf kg mp ki kj kk ij bi translated">就底层算法而言，这两个模型是相似的，只是在方法上略有不同:CBOW从源上下文单词预测目标单词，而skip-gram则相反，从目标单词预测源上下文单词。简单地说，CBOW模型使用周围的词来预测感兴趣的词，而skip-gram模型使用中心词来定义周围的词。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/50bb87d94d52df3d4a85fe7a4a4d29e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*4_iGy1w1RtIOzXIdrH4GNw.png"/></div></figure><p id="4a6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其他微小的差异包括在skip-gram架构中没有隐藏层，在CBOW中没有严格的词序规则。根据<a class="ae mr" href="https://www.tensorflow.org/tutorials/word2vec" rel="noopener ugc nofollow" target="_blank">这篇TensorFlow教程</a>，CBOW对于较小的数据集更有用，而skip-gram对于较大的数据集更重要。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/9774c93cbb3028c3f366e7852e7fb570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNVKdcQ4GfSu-p5ImwiLOQ.png"/></div></div></figure><h1 id="2963" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">文本生成的实现步骤:</h1><ol class=""><li id="9ebb" class="mx my iq jp b jq ml ju mm jy mz kc na kg nb kk nc nd ne nf bi translated"><strong class="jp ir">清洗输入数据</strong></li></ol><p id="33c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。建筑词汇</strong></p><p id="ab00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。将文本转换为单词向量</strong></p><p id="abb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4。定义模型(编码器-解码器)</strong></p><p id="0850" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。训练模型</strong></p><p id="8cb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6。生成文本</p><p id="2704" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述一步一步的过程将引导我们达到文本预测的最终目标。C <em class="lj">学习数据涉及</em> spans记号化、词条化和词干化。对于<em class="lj">将文本转换为单词向量</em>，我们需要考虑以下步骤:</p><ul class=""><li id="5e87" class="mx my iq jp b jq jr ju jv jy ng kc nh kg ni kk nj nd ne nf bi translated">所有的单词都必须包含在词汇表中。</li><li id="4901" class="mx my iq jp b jq nk ju nl jy nm kc nn kg no kk nj nd ne nf bi translated">设计每个单词到索引的唯一映射</li></ul><p id="44b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，单词映射技术将输入文本转换成单词向量。</p><p id="f198" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当<em class="lj">定义型号</em>时，必须满足以下前提条件:</p><ul class=""><li id="3c7b" class="mx my iq jp b jq jr ju jv jy ng kc nh kg ni kk nj nd ne nf bi translated">使用单词嵌入将输入嵌入到2D数组中</li><li id="6177" class="mx my iq jp b jq nk ju nl jy nm kc nn kg no kk nj nd ne nf bi translated">使用包括rnn和LSTMs的解码器(具有编码器)来进行预测</li><li id="91cc" class="mx my iq jp b jq nk ju nl jy nm kc nn kg no kk nj nd ne nf bi translated">在解码器输出之上应用一些完全连接的层，以生成实际的预测</li></ul><p id="981e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应用LSTM，在<em class="lj">训练中，模型</em>实现输入时间步长，预测下一个单词的类别。</p><blockquote class="lg lh li"><p id="30af" class="jn jo lj jp b jq jr js jt ju jv jw jx lk jz ka kb ll kd ke kf lm kh ki kj kk ij bi translated"><em class="iq">你可以在</em> <a class="ae mr" href="https://github.com/Sangramsingkayte/TextPrediction" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> <em class="iq">中找到每一步的源代码</em></strong></a></p></blockquote><div class="np nq gp gr nr ns"><a href="https://www.datadriveninvestor.com/2020/11/19/how-machine-learning-and-artificial-intelligence-changing-the-face-of-ecommerce/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd ir gy z fp nx fr fs ny fu fw ip bi translated">机器学习和人工智能如何改变电子商务的面貌？|数据驱动…</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">电子商务开发公司，现在，整合先进的客户体验到一个新的水平…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og la ns"/></div></div></a></div><p id="acd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">访问专家视图— </strong> <a class="ae mr" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>