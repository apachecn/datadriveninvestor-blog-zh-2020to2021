<html>
<head>
<title>Top and Best Computer Vision Human-Pose Estimation Projects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最佳计算机视觉人体姿势评估项目</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/top-and-best-computer-vision-human-pose-estimation-projects-186d04204dde?source=collection_archive---------2-----------------------#2020-12-28">https://medium.datadriveninvestor.com/top-and-best-computer-vision-human-pose-estimation-projects-186d04204dde?source=collection_archive---------2-----------------------#2020-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ed2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">人体三维姿态估计是近年来乃至未来最热门的话题之一。<strong class="ak">人体姿态估计</strong>是<strong class="ak">从单个典型的单目图像中估计<strong class="ak">身体</strong> ( <strong class="ak">姿态</strong>)的配置</strong>的过程。背景。<strong class="ak">人体姿态估计</strong>是计算机视觉中的关键问题之一，已经被研究了超过15年。</h2></div><p id="b9ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，你将发现5个最好的开源计算机视觉人体姿势估计项目。我们将回顾五个最好的开源<strong class="kk iu"> <em class="le">计算机视觉人体姿态估计</em> </strong>项目，这些项目可以在<strong class="kk iu"> <em class="le"> Github </em> </strong>上找到，供有兴趣的人使用。</p><h2 id="5dfb" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">额外收获:作为额外收获，我提供了一个关于计算机视觉人体姿态估计的资源库的链接，其中包含论文、代码、文章、教程等等，对这些项目很有帮助。</h2><h2 id="ca0d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">*<a class="ae ly" href="https://github.com/cbsudux/awesome-human-pose-estimation" rel="noopener ugc nofollow" target="_blank">awesome-Human-Pose-estimation/CBS udux</a>—人类姿势估计方面的awesome资源集合。</h2><h2 id="20cb" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">*<a class="ae ly" href="https://github.com/wangzheallen/awesome-human-pose-estimation" rel="noopener ugc nofollow" target="_blank">awesome-Human-Pose-Estimation/wangzhallen</a>——人类姿态估计相关出版物的集合。</h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/29a9149e5895cb85d3dbd47e5ac21f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*VaSY2vBGX8X76agz.png"/></div></figure><blockquote class="mh"><p id="2e64" class="mi mj it bd mk ml mm mn mo mp mq ld dk translated">如果你喜欢读这篇文章，我相信我们有着相似的兴趣，并且现在/将来会从事相似的行业。那么我们就通过<a class="ae ly" href="https://www.linkedin.com/in/mrinal-walia-b0981b158/" rel="noopener ugc nofollow" target="_blank"><strong class="ak"><em class="mr">LinkedIn</em></strong></a><strong class="ak"><em class="mr">和</em></strong><a class="ae ly" href="https://github.com/abhiwalia15" rel="noopener ugc nofollow" target="_blank"><strong class="ak"><em class="mr">Github</em></strong></a><strong class="ak"><em class="mr">来连线吧。请不要犹豫发送联系请求！</em>T47】</strong></p></blockquote><figure class="mt mu mv mw mx me gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/cc40f32ac0c3163f15402c6d17f10381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8KVz1ip4pFflqiDKhlzufQ.jpeg"/></div></div></figure><blockquote class="nc nd ne"><p id="ed6e" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated">拥有良好的理论知识是惊人的，但在实时机器学习项目中用代码实现它们是完全不同的。基于其他问题和数据集，您可能会得到不同的意外结果。</p><p id="24d6" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated">作为奖励，我还添加了各种课程的链接，这些链接在我学习数据科学和ML的过程中帮助了我很多。我个人是 <a class="ae ly" href="https://datacamp.pxf.io/x9nmvv" rel="noopener ugc nofollow" target="_blank"> <em class="it"> DataCamp </em> </a> <em class="it">的粉丝，我就是从它开始的，现在还在通过</em><a class="ae ly" href="https://datacamp.pxf.io/x9nmvv" rel="noopener ugc nofollow" target="_blank"><em class="it">data camp</em></a><em class="it">学习，做新的课程。他们真的有一些令人兴奋的课程。一定要去看看。</em></p><p id="03c8" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/LPDqQZ" rel="noopener ugc nofollow" target="_blank"> <em class="it">数据-科学家-python </em> </a></p><p id="8efb" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/MXQxrJ" rel="noopener ugc nofollow" target="_blank"> <em class="it">数据-科学家-与-r </em> </a></p><p id="e8e2" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/DVLg4j" rel="noopener ugc nofollow" target="_blank"> <em class="it">机器-学习-科学家-与-r </em> </a></p><p id="e16f" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/9WePXW" rel="noopener ugc nofollow" target="_blank"> <em class="it">机器学习-科学家-python </em> </a></p><p id="3da8" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/kjR3mN" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a></p><p id="84c1" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/15bLmd" rel="noopener ugc nofollow" target="_blank"> <em class="it">数据科普</em> </a></p><p id="6971" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/jW13ve" rel="noopener ugc nofollow" target="_blank"> <em class="it">数据工程师与python </em> </a></p><p id="3fa1" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/kjR3mz" rel="noopener ugc nofollow" target="_blank"> <em class="it">数据分析师与python </em> </a></p><p id="fca8" class="ki kj le kk b kl km ju kn ko kp jx kq nf ks kt ku ng kw kx ky nh la lb lc ld im bi translated"><a class="ae ly" href="https://datacamp.pxf.io/e4RM6r" rel="noopener ugc nofollow" target="_blank"> <em class="it">大数据-基础-via-pyspark </em> </a></p></blockquote><h1 id="e60f" class="ni lg it bd lh nj nk nl lk nm nn no ln jz np ka lq kc nq kd lt kf nr kg lw ns bi translated">回到主题:-&gt;</h1></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="f937" class="ni lg it bd lh nj oa nl lk nm ob no ln jz oc ka lq kc od kd lt kf oe kg lw ns bi translated">1.开放姿势</h1><h2 id="397d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> Github链接</a></h2><h2 id="a842" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">星星:19.6K |叉子:6K </a></h2><h2 id="d652" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">论文</a></h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi of"><img src="../Images/a26251b67badc762c6d8acc4fddc482d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ckOGytau4095qUoR.png"/></div></div></figure><p id="7596" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi og translated"><span class="l oh oi oj bm ok ol om on oo di">O</span><a class="ae ly" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="le">pen pose</em></strong></a>代表了第一个<strong class="kk iu"><em class="le"/></strong>实时多人系统联合<strong class="kk iu"> <em class="le">检测单幅图像上的人体、手、面部、脚关键点(共135个关键点)。</em>T71】</strong></p><h2 id="0ee9" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">Openpose库的一些关键特性是:</h2><ul class=""><li id="aafb" class="op oq it kk b kl or ko os kr ot kv ou kz ov ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le"> 2D实时多人关键点检测</em> </strong></li><li id="f69f" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le"> 3D实时单人关键点检测</em> </strong></li><li id="c4eb" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">估计失真、内在和外在摄像机参数。</em>T11】</strong></li><li id="70a2" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">单人追踪，进一步加速或视觉平滑。</em>T15】</strong></li><li id="ae48" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">图像、视频、网络摄像头、Flir/Point Grey、IP摄像头，并支持添加您的自定义输入源(如深度摄像头)。</em>T19】</strong></li><li id="4b2c" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">基本图像+关键点显示/保存(PNG，JPG，AVI，…)，关键点保存(JSON，XML，YML，…)等。</em> </strong></li><li id="7e7a" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">自定义功能的C++ API和Python API</em></strong></li></ul><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi of"><img src="../Images/9f1fb61c6a37d2da29ffb4d0c30b3e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vUuOc0XYBakkV_Gr.gif"/></div></div></figure><div class="ma mb mc md gt ab cb"><figure class="pf me pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/70fe648f44d0c9dbb9623e2fbd06461c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/0*t6Yzbdon7ClKx5th.gif"/></div></figure><figure class="pf me pg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/828629b5157bcc27fe83c17f0532a6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/0*FceOU-CiruNMVz_d.gif"/></div></figure><figure class="pf me pl ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/bf80c7e02fcba9b9e795245f8a407bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/0*E4BfDqaFv_q4Saaj.gif"/></div></figure></div><h1 id="a177" class="ni lg it bd lh nj nk nl lk nm nn no ln jz np ka lq kc nq kd lt kf nr kg lw ns bi translated">2.AlphaPose</h1><h2 id="ac4d" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/MVIG-SJTU/AlphaPose" rel="noopener ugc nofollow" target="_blank"> Github链接</a></h2><h2 id="d0c1" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/MVIG-SJTU/AlphaPose" rel="noopener ugc nofollow" target="_blank">星星:4.7K |叉子:1.3K </a></h2><h2 id="7c38" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://www.mvig.org/research/alphapose.html" rel="noopener ugc nofollow" target="_blank">官方文件</a></h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi pm"><img src="../Images/bbcc49f244611a4571915f929c6948b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FGKehIGp-esY4_e1.jpg"/></div></div></figure><p id="5f9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi og translated"><span class="l oh oi oj bm ok ol om on oo di">一个</span> <a class="ae ly" href="https://www.mvig.org/research/alphapose.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le"> lpha姿态</em> </strong> </a>一个<strong class="kk iu">非常精确的实时</strong> <strong class="kk iu"> <em class="le">多人姿态估计系统</em> </strong>。第一个开源系统可以在<em class="le"> COCO数据集</em>上实现<strong class="kk iu"><em class="le">70+mAP(72.3 mAP)</em></strong>，在<em class="le"> MPII数据集</em>上实现<strong class="kk iu"><em class="le">80+mAP(82.1 mAP)</em></strong>。</p><div class="pn po gp gr pp pq"><a href="https://www.datadriveninvestor.com/2020/07/31/using-machine-learning-in-brain-computer-interfaces/" rel="noopener  ugc nofollow" target="_blank"><div class="pr ab fo"><div class="ps ab pt cl cj pu"><h2 class="bd iu gy z fp pv fr fs pw fu fw is bi translated">在脑机接口中使用机器学习|数据驱动的投资者</h2><div class="px l"><h3 class="bd b gy z fp pv fr fs pw fu fw dk translated">神经技术是一个刚刚开始大步前进的前沿领域。有了所有的技术…</h3></div><div class="py l"><p class="bd b dl z fp pv fr fs pw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="pz l"><div class="qa l qb qc qd pz qe mf pq"/></div></div></a></div><p id="53c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">跨帧关联指示同一个人的姿势还提供了一种高效的<strong class="kk iu"> <em class="le">在线姿势跟踪器，称为姿势流</em> </strong>。也是第一个开源的<strong class="kk iu"> <em class="le">在线姿态跟踪器</em> </strong>满足<strong class="kk iu"> <em class="le"> 60+地图(66.5地图)</em></strong><strong class="kk iu"><em class="le">50+MOTA(58.3 MOTA)</em></strong>在<em class="le"> PoseTrack挑战数据集上。</em></p><p id="cec0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ly" href="https://github.com/MVIG-SJTU/AlphaPose/tree/pytorch" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="le">alpha pose-py torch</em></strong></a>在<em class="le"> COCO </em>验证集<em class="le"/>上运行<strong class="kk iu"> <em class="le"> 20 fps </em> </strong>并达到<strong class="kk iu"> <em class="le"> 71 AP！</em> </strong></p><div class="ma mb mc md gt ab cb"><figure class="pf me qf ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/5934c7b6ae972359f9d0927625c90fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/0*Y-AGKVQZzRvc9JxY.gif"/></div></figure><figure class="pf me qf ph pi pj pk paragraph-image"><img src="../Images/218ad1ec2b3079c9b4421dd598c6e3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*UfQU_2JsdOd68tb5.gif"/></figure><figure class="pf me qf ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/6466a60c5f0bab000d6585dc60ebaf6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/0*ATNh8eSC-cG3vu8e.gif"/></div></figure></div><h2 id="c19e" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">主要特点:</h2><ul class=""><li id="166f" class="op oq it kk b kl or ko os kr ot kv ou kz ov ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">准确实时的多人关键点检测。</em> </strong></li><li id="74af" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">输入:图像、视频、图像列表。</em>T25】</strong></li><li id="ab7d" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">输出:基本图像+关键点显示/保存(PNG，JPG，AVI，…)，关键点保存(JSON)，支持多种格式。</em>T29】</strong></li><li id="7ff6" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">可用:命令行演示、python和Lua程序</em> </strong></li><li id="f0dd" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le"> OS: Ubuntu </em> </strong></li></ul><h1 id="2185" class="ni lg it bd lh nj nk nl lk nm nn no ln jz np ka lq kc nq kd lt kf nr kg lw ns bi translated">3.实时多人姿态估计</h1><h2 id="2c42" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation" rel="noopener ugc nofollow" target="_blank"> Github链接</a></h2><h2 id="bc4a" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation" rel="noopener ugc nofollow" target="_blank">星星:4.5K |叉子:1.3K </a></h2><h2 id="11ae" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">论文</a></h2><div class="ma mb mc md gt ab cb"><figure class="pf me qg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/95c29dcfa0973fd0b7b80e693cdbe46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*HblD73os6CC4ILHK.gif"/></div></figure><figure class="pf me qg ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/b0fcc50f6a7e62d5b7f215b9d0764299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*hImFtYiuhkeCQ7rB.gif"/></div></figure></div><p id="2220" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi og translated"><span class="l oh oi oj bm ok ol om on oo di"> R </span> <a class="ae ly" href="https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le">实时多人姿态估计</em> </strong> </a>提出了一种<strong class="kk iu"> <em class="le">自下而上的方法</em> </strong>用于<strong class="kk iu"> <em class="le">实时多人姿态估计</em> </strong>而不使用任何<em class="le">人物检测器</em>。</p><p id="238d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该方法使用<strong class="kk iu"><em class="le"/></strong>非参数表示，我们称之为<strong class="kk iu"><em class="le">【PAF】</em></strong><em class="le">来将身体部位与图像</em>中的个体相关联。</p><p id="d6e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">他们的方法在首届<strong class="kk iu"> COCO 2016关键点挑战赛中名列第一。它在<em class="le"> MPII多人基准</em>上，无论是<strong class="kk iu"> <em class="le">性能</em> </strong>还是<strong class="kk iu"> <em class="le">效率，都大幅超越了</em></strong> <em class="le">之前的最新成果</em>。T85】</strong></p><h1 id="1e40" class="ni lg it bd lh nj nk nl lk nm nn no ln jz np ka lq kc nq kd lt kf nr kg lw ns bi translated">4.TF姿态估计</h1><h2 id="fdec" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank"> Github链接</a></h2><h2 id="bdf0" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank">星星:3.8K |叉子:1.5K </a></h2><h2 id="833e" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">论文</a></h2><p id="94c5" class="pw-post-body-paragraph ki kj it kk b kl or ju kn ko os jx kq kr qh kt ku kv qi kx ky kz qj lb lc ld im bi og translated"><span class="l oh oi oj bm ok ol om on oo di"> T </span> <a class="ae ly" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="le"> F姿态估计</em> </strong> </a> <strong class="kk iu"> <em class="le"> </em> </strong>是<strong class="kk iu"><em class="le">【open Pose】</em></strong>一种<strong class="kk iu"> <em class="le">人体姿态估计算法</em> </strong>，已经用<strong class="kk iu"> <em class="le"> Tensorflow </em> </strong>实现。它还提供了几种变体，为<strong class="kk iu"> <em class="le"> CPU </em> </strong>或<strong class="kk iu"> <em class="le">低功耗嵌入式设备上的<strong class="kk iu"><em class="le"/></strong>实时处理改变网络结构。</em> </strong></p><div class="ma mb mc md gt ab cb"><figure class="pf me qk ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/7434557365e363a6abd7421c7f78d14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/0*GLcJoYMnalA9Zh1Y.gif"/></div></figure><figure class="pf me ql ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/414cb3bc4a0e052511b10e34dd1ef107.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/0*NgP_8okUSktrqk0s.gif"/></div></figure><figure class="pf me qm ph pi pj pk paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><img src="../Images/09947bf39a10f1daaade6f1a015d49c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/0*ZltXWMqwuS7e5Aao.gif"/></div><figcaption class="qn qo gj gh gi qp qq bd b be z dk qr di qs qt">1. CMU's Original Model<br/>on Macbook Pro 15" | 2. Mobilenet-thin<br/>on Macbook Pro 15" | 3. Mobilenet-thin<br/>on Jetson TX2</figcaption></figure></div><p id="d0d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">要求:</strong></p><ul class=""><li id="f8d0" class="op oq it kk b kl km ko kp kr qu kv qv kz qw ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le">蟒3 </em> </strong></li><li id="e1b3" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"><em class="le">tensor flow 1 . 4 . 1+</em></strong></li><li id="5dc8" class="op oq it kk b kl pa ko pb kr pc kv pd kz pe ld ow ox oy oz bi translated"><strong class="kk iu"> <em class="le"> opencv3，protobuf，python3-tk </em> </strong></li></ul><h1 id="2902" class="ni lg it bd lh nj nk nl lk nm nn no ln jz np ka lq kc nq kd lt kf nr kg lw ns bi translated">5.OpenPifPaf</h1><h2 id="9501" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/vita-epfl/openpifpaf" rel="noopener ugc nofollow" target="_blank"> Github链接</a></h2><h2 id="eab9" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://github.com/vita-epfl/openpifpaf" rel="noopener ugc nofollow" target="_blank">星星:616 |叉子:124 </a></h2><h2 id="e3e2" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated"><a class="ae ly" href="https://arxiv.org/abs/1903.06593" rel="noopener ugc nofollow" target="_blank">论文</a></h2><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi qx"><img src="../Images/91470229bb1ca1c2cfc5b2b1a82e7215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*aTKHJvcJv420TvXG.png"/></div></figure><p id="864c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi og translated"><span class="l oh oi oj bm ok ol om on oo di"> O </span></p><p id="232a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">新方法<strong class="kk iu"> <em class="le"> PifPaf </em> </strong>使用<strong class="kk iu">部位强度场(PIF) </strong> <em class="le">来定位身体部位</em>和<strong class="kk iu">部位关联场(PAF) </strong> <em class="le">来将身体部位</em>彼此关联以形成完整的<em class="le">人体姿态。</em></p><p id="1927" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">论文<a class="ae ly" href="https://arxiv.org/abs/1903.06593" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"><em class="le">“pif PAF:用于人体姿态估计的复合场”</em> </strong> </a>是<em class="le"> PyTorch </em>中报告的正式实现。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><blockquote class="mh"><p id="c7dc" class="mi mj it bd mk ml qy qz ra rb rc ld dk translated">如果你喜欢读这篇文章，我相信我们有着相似的兴趣，并且现在/将来会从事相似的行业。所以让我们通过<a class="ae ly" href="https://www.linkedin.com/in/mrinal-walia-b0981b158/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ly" href="https://github.com/abhiwalia15" rel="noopener ugc nofollow" target="_blank"> Github </a>联系一下。请不要犹豫发送联系请求！</p><p id="1e81" class="mi mj it bd mk ml qy qz ra rb rc ld dk translated">如果你有兴趣阅读更多关于人体姿态估计的内容，我和我的同事发表了一篇关于“从单目2d图像恢复人体3D模型以检测姿态变形”的论文。希望对你有帮助。</p></blockquote></div></div>    
</body>
</html>