<html>
<head>
<title>Evolution of Object Recognition Algorithms I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体识别算法的发展I</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/evolution-of-object-recognition-algorithms-i-5803c7be0691?source=collection_archive---------9-----------------------#2020-04-20">https://medium.datadriveninvestor.com/evolution-of-object-recognition-algorithms-i-5803c7be0691?source=collection_archive---------9-----------------------#2020-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="f364" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">让我们深入了解最先进的自动驾驶汽车背后的技术</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/18e0a9dbbd982a47c4924f2722be1f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8KqIJs2O39hRTC-ADrDxQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Object Recognition using Alexnet [1]</figcaption></figure><p id="aedb" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">对象识别是用于在图像或视频序列中寻找和识别对象的技术。人们可能会想，识别物体有什么迷人之处——“哦，这是一项如此平凡的任务”，但让我告诉你，这项任务对我们来说越简单，机器就越难破译。因此，我们致力于研究使机器更人性化的技术——视觉、语音、语言和运动技能。现在，在这个由两部分组成的系列中，我将带您经历物体识别的进化之旅，这是计算机视觉中一个受欢迎的领域。</p><h2 id="57f4" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">Alexnet</h2><p id="d241" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">简短的对象识别历史始于2012年，当时Krizhevsky等人[1]训练了一个深度卷积神经网络，将ImageNet LSVRC-2010竞赛中的图像分类为1000个不同的类别。他们合并了ReLU、dropouts和GPU实现。目标识别由附加的SVM模型执行。该模型在纯监督学习方面取得了破纪录的结果。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mg"><img src="../Images/5d1e940eb8f2e1329841f87a366413ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZpEcOQKzZH8mO6SqcDpEA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of Alexnet [7]</figcaption></figure><h2 id="d080" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">RCNN</h2><p id="4c8a" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">大约在同一时间，Girshich等人[2]提出了RCNN，声称通过自下而上的区域提议、监督预训练和特定领域微调来定位和分割对象是其性能提升的秘密。</p><div class="mh mi gp gr mj mk"><a href="https://www.datadriveninvestor.com/2020/02/13/on-robot-rights-can-robots-be-enslaved/" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd ir gy z fp mp fr fs mq fu fw ip bi translated">论机器人权利:机器人可以被奴役吗？数据驱动的投资者</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">人们可以随心所欲地对待他们的机器人吗？一个人可以对他们的机器人“暴力”吗？机器人应该是…</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my kz mk"/></div></div></a></div><h2 id="cb55" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">吃得过多</h2><p id="f7c5" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">OverFeat [3]实施了一种多尺度滑动窗口方法，可以有效地执行分类、定位和检测任务。该模型有一些可以弥补的缺陷，首先，定位任务不是通过在整个网络中反向传播来执行的，其次，它利用了l2损耗，而不是IOU标准优化。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/7bc40339168b30bfa708f5c6cf9ec004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CNFxWyKS71UHXWXpzGLeQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of OverFeat [3]</figcaption></figure><h2 id="f49d" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">ZFNet</h2><p id="29c6" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">在2013年，ZFNet [4]赢得了ILSVRC。他们的模型通过为AlexNet模型使用7x7内核来实现信息保留。图像的224×224裁剪(具有3个彩色平面)被呈现为输入。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi na"><img src="../Images/336bc941b39c392ecd70a14508fbb24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oGhVLPHp7cw46OADHzPTQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">The architecture of the 8-layer ZFNet model. [4]</figcaption></figure><p id="8f75" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">该架构与96个不同的第一层滤波器(红色)进行卷积，每个滤波器的大小为7×7，在x和y方向上使用2的步长。然后，得到的特征图:(I)通过校正的线性函数，(ii)合并(在3×3区域内的最大值，使用步长2)以及(iii)跨特征图进行对比度归一化，以给出96个不同的55×55元素特征图。在层2、3、4、5中重复类似的操作。最后两层是完全连接的，以向量形式(6 6 256 = 9216维)将来自顶部卷积层的特征作为输入。最后一层是C-way softmax函数，C是类的数量。所有过滤器和特征地图都是方形的。</p><h2 id="cb50" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">SPPNets</h2><p id="07cb" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">引入空间金字塔池[5]是为了解决固定大小输入图像的问题。SSP-net的优点是它能够生成固定长度的表示，而不管图像的大小/比例。由于任意子图像池，它比具有相似准确度分数的R-CNN相对更快。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c6d97a4c090927c0c49f5547bd2ba120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*RzgXBgG4UUZVnJpmuJoESQ.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">A network structure with a spatial pyramid pooling layer. Here, 256 is the filter number of the conv5 layer, and conv5 is the last convolutional layer [5]</figcaption></figure><h2 id="54d1" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">多框</h2><p id="99c5" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">以前的不可知提议生成方法的缺点是它没有提议排序系统或者提议排序系统非常弱，这对运行时间有不利影响。这导致了MSC-multi box[6]的发展，它提供了基于学习的建议方法，可以与硬设计的方法有效地配对，并提供体面的质量-运行时权衡。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nc"><img src="../Images/cfa44f49b0362b781d0c356e13fc6283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uKnn3daPD_6vl18mPGBeJg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">An illustration of the multi-scale convolutional prediction of the locations and confidences for MultiBox [6]</figcaption></figure><p id="e146" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">这就结束了我们关于物体识别算法发展的第一章。更详细的了解，你可以参考下面参考文献列表中的论文，也可以直接通过<a class="ae nd" href="https://medium.com/@asthanameghna01" rel="noopener"> Medium </a>或<a class="ae nd" href="http://www.twitter.com/meghnaasthana" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。在接下来的一周，我将深入研究下一代算法，如VGGNet、ResNet和YOLO，敬请关注！</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="348e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[1] Krizhevsky，a .，Sutskever，I .和Hinton，G.E .，2012。基于深度卷积神经网络的图像网分类。神经信息处理系统进展(第1097-1105页)。</p><p id="6f66" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[2] Girshick，r .，Donahue，j .，Darrell，t .和Malik，j .，用于精确对象检测和语义分割的丰富特征层次。arXiv 2014。arXiv预印本arXiv:1311.2524。</p><p id="9a89" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[3]塞尔马内，p .，艾根，d .，张，x .，马蒂厄，m .，弗格斯，r .和勒村，y .，2013年。Overfeat:使用卷积网络的综合识别、定位和检测。arXiv预印本arXiv:1312.6229。</p><p id="3ce6" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[4]泽勒医学博士和弗格斯研究中心，2014年9月。可视化和理解卷积网络。在欧洲计算机视觉会议上(第818-833页)。斯普林格，查姆。</p><p id="8c7b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[5]何，王，张，徐，任，孙，2015 .用于视觉识别的深度卷积网络中的空间金字塔池。IEEE模式分析和机器智能汇刊，37(9)，第1904-1916页。</p><p id="8f02" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[6]塞格迪，c .，里德，s .，埃汉，d .，安盖洛夫，d .和约菲，s .，2014年。可扩展的高质量对象检测。arXiv预印本arXiv:1412.1441。</p><p id="0c31" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[7]哈希米，硕士，2019。在输入卷积神经网络之前放大较小的图像:零填充与插值。<em class="js">大数据杂志</em>，<em class="js"> 6 </em> (1)，第98页</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nl nm l"/></div></figure></div></div>    
</body>
</html>