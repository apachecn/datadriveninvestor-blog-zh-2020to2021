<html>
<head>
<title>Building Intuitions for Transformers — Attentions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为变压器建立直觉—注意事项</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/building-intuitions-for-transformers-attentions-5a37700d363e?source=collection_archive---------10-----------------------#2020-02-01">https://medium.datadriveninvestor.com/building-intuitions-for-transformers-attentions-5a37700d363e?source=collection_archive---------10-----------------------#2020-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/333d5c52c65066ecc2e0aff2b7d09ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6CGc5oitVEh2LA7RXquTw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from <a class="ae kc" href="https://www.cio.com/article/3207764/the-powerful-link-between-focus-and-creative-output.html" rel="noopener ugc nofollow" target="_blank">source</a>.</figcaption></figure><h2 id="4a0d" class="kd ke iq bd kf kg kh dn ki kj kk dp kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">用于神经机器翻译的编码器-解码器RNNs】</strong></h2><p id="1c50" class="pw-post-body-paragraph kz la iq lb b lc ld le lf lg lh li lj km lk ll lm kq ln lo lp ku lq lr ls lt ij bi translated">用于NMT的基本<strong class="lb ir">编码器-解码器RNN </strong>的结构是这样的，编码器获取输入句子(每个单词由其相应的嵌入表示)并一次处理一步，然后向解码器输出最终向量。这个输出被称为<strong class="lb ir">上下文向量</strong>，并且被认为是源句子的一个很好的<strong class="lb ir">摘要</strong>。</p><div class="lu lv gp gr lw lx"><a href="https://www.datadriveninvestor.com/2019/02/21/best-coding-languages-to-learn-in-2019/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd ir gy z fp mc fr fs md fu fw ip bi translated">2019年最值得学习的编码语言|数据驱动的投资者</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">在我读大学的那几年，我跳过了很多次夜游去学习Java，希望有一天它能帮助我在…</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jw lx"/></div></div></a></div><p id="1f71" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">解码器用来自编码器的输出上下文向量初始化。当<strong class="lb ir">训练</strong>时，它将向右移动一步的<strong class="lb ir">目标翻译句子</strong>作为输入，当<strong class="lb ir">推理</strong>时，它将被上一步的输出单词所替代。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/3cf3b496af3bd2a08e035ba008409af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFEG9r6qj7Ey7OC-EABYpA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from <a class="ae kc" href="https://6chaoran.wordpress.com/2019/01/15/build-a-machine-translator-using-keras-part-1-seq2seq-with-lstm/" rel="noopener ugc nofollow" target="_blank">source</a>.</figcaption></figure><p id="b134" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">尽管模型中的循环单元是<strong class="lb ir">门控单元</strong>如LSTM或GRU，但性能仍然受到句子长度的限制。对于长句，当完成输入处理时，模型往往会忘记第一部分。</p><h2 id="b9a1" class="kd ke iq bd kf kg kh dn ki kj kk dp kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">注意机制</strong></h2><p id="c824" class="pw-post-body-paragraph kz la iq lb b lc ld le lf lg lh li lj km lk ll lm kq ln lo lp ku lq lr ls lt ij bi translated">具有注意力的编码器-解码器RNN模型的结构类似于没有注意力的编码器-解码器模型的结构，但是它不是仅输出来自编码器的最后隐藏状态的一个上下文向量，而是返回它的<strong class="lb ir">整个序列</strong>。</p><p id="a5d5" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">解码器在每个时间步都可以访问该序列，除此之外，还引入了<strong class="lb ir">注意机制</strong>来决定该序列的哪一部分与当前单词的翻译相关。解码器不再处理整个序列，而是可以在给定的时间步长内将<strong class="lb ir">的注意力集中在</strong>对其重要的事情上。</p><p id="9895" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">这是通过<strong class="lb ir">对编码</strong>进行加权来实现的，每一个时间步长都有一组独特的注意事项。这些关注度加起来是1，其重要部分接近1，其余部分接近0。因此，加权编码将仅具有与当前翻译相关的值，而其剩余部分被乘以非常小的值，因此被忽略。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/db158a3d6dc0e1484936fe0fe7c728d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YlvT6lHhlPWzll2hJHtNDw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Image from <a class="ae kc" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">source</a>.</figcaption></figure><p id="306d" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated"><strong class="lb ir">对准模型</strong>与编码器-解码器模型的其余部分一起被训练以获得关注。它将编码序列和解码器的<strong class="lb ir">先前隐藏状态</strong>作为输入，使用串联或点积来度量它们的<strong class="lb ir">相似度</strong>，然后应用softmax将它们转换成<strong class="lb ir">分布</strong>。</p><p id="4617" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">这可以解释为编码器使用其先前的隐藏状态作为<strong class="lb ir">查询</strong>来寻找编码序列中感兴趣的部分。</p><h2 id="c1f0" class="kd ke iq bd kf kg kh dn ki kj kk dp kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">成比例的点积关注</strong></h2><p id="ebad" class="pw-post-body-paragraph kz la iq lb b lc ld le lf lg lh li lj km lk ll lm kq ln lo lp ku lq lr ls lt ij bi translated">根据上一节对注意力机制的基本直觉，成比例的点积注意力并不神秘。与上述注意机制不同，之前的隐藏状态现在被称为<strong class="lb ir">查询</strong>，编码序列被成对的<strong class="lb ir">键</strong>和<strong class="lb ir">值</strong>所取代。</p><p id="fbeb" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">将成对的键和值想象成一个<strong class="lb ir">字典</strong>，模型使用查询来匹配键。如果一个键被匹配，它相应的值将被获取。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/c6220f68c6d496ec8e42ecf6c5339669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lmd6uqU3amZKDuZz5q6zHA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Scaled Dot-Product Attention</figcaption></figure><p id="d166" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">在softmax中，Q和K的点积用于查找匹配部分，然后对乘积进行缩放，softmax用于获得分布。产生的注意力然后被用于加权值v</p><h2 id="f07b" class="kd ke iq bd kf kg kh dn ki kj kk dp kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">变压器的注意事项</strong></h2><p id="cb46" class="pw-post-body-paragraph kz la iq lb b lc ld le lf lg lh li lj km lk ll lm kq ln lo lp ku lq lr ls lt ij bi translated">变压器由一组链接在一起的编码器和一组链接在一起的解码器组成。要理解的最重要的组件是它的<strong class="lb ir">多头注意力</strong>子层，它只是并行工作的多个缩放的点积注意力。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/959f5960b1edacfb3796db090ea938f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NfoLHSRTNIXvhtEHUNnfMA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Multi-Head Attention. Image from <a class="ae kc" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">source</a>.</figcaption></figure><p id="8061" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">编码器负责生成源句子的编码，这是模型特有的表示，其输出被提供给所有的解码器层。为了产生更好的结果，句子中给定单词的编码必须包含句子中对其重要的所有单词的信息，<strong class="lb ir">包括单词本身</strong>，因此换句话说，<strong class="lb ir">句子正在关注自身</strong>。</p><p id="85e5" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated"><strong class="lb ir">查询</strong>和这里的pair ( <strong class="lb ir">键</strong>，<strong class="lb ir">值</strong>)都来自句子本身。当处理一个给定的单词时，注意力机制负责提取句子中对该单词有意义的所有部分。</p><p id="1df2" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">它实际上是这样工作的:</p><ol class=""><li id="35bc" class="mz na iq lb b lc mm lg mn km nb kq nc ku nd lt ne nf ng nh bi translated">首先，用<strong class="lb ir"> Wq </strong>、<strong class="lb ir"> Wk </strong>、<strong class="lb ir"> Wv </strong>将句子嵌入到三个不同的子空间中，得到句子中每个单词的Q、K、V。</li><li id="31fe" class="mz na iq lb b lc ni lg nj km nk kq nl ku nm lt ne nf ng nh bi translated">计算<strong class="lb ir"> matmul(Q，K) </strong>的<strong class="lb ir"> softmax </strong>，用sqrt(d)缩放以避免<strong class="lb ir"> softmax </strong> <strong class="lb ir">饱和</strong>。以句子“我已经到达”为例，有3个单词，点积的结果softmax将是<strong class="lb ir"> 3x3 </strong>矩阵，每行和每列代表一个单词。并且来自矩阵的每个值将代表<strong class="lb ir">其对应单词</strong>之间的相关性。</li><li id="b3c8" class="mz na iq lb b lc ni lg nj km nk kq nl ku nm lt ne nf ng nh bi translated">用获得的softmax对值<strong class="lb ir"> V </strong>进行加权，以输出句子的正确编码。</li></ol><p id="218e" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">另一方面，解码器做一些类似的事情，但是<strong class="lb ir">对(K，V)来自编码器</strong>。当翻译一个给定的单词时，它使用前一个解码器层传入的Q来查找V中使用k的相关信息。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><div class="ms mt mu mv gt lx"><a href="https://neptune.ai/blog/mlops-pipeline-for-nlp-machine-translation" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd ir gy z fp mc fr fs md fu fw ip bi translated">为NLP构建MLOps管道:机器翻译任务[教程] - neptune.ai</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">机器学习操作通常被称为MLOps，它使我们能够创建一个端到端的机器学习管道…</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">海王星. ai</p></div></div><div class="mg l"><div class="nu l mi mj mk mg ml jw lx"/></div></div></a></div><p id="44b3" class="pw-post-body-paragraph kz la iq lb b lc mm le lf lg mn li lj km mo ll lm kq mp lo lp ku mq lr ls lt ij bi translated">对<strong class="lb ir">变压器</strong>及其<strong class="lb ir">注意机制</strong>的良好理论理解只是一个起点；要加深你对题目的认识，往往需要有一些动手的经验。为此，这篇详细的教程将以简洁明了的方式，一步一步地指导你完成<strong class="lb ir">神经机器翻译流水线</strong>的<strong class="lb ir">开发</strong>和<strong class="lb ir">实现</strong>过程。除了ML之外，你还会在<strong class="lb ir"> MLOps </strong>方面积累一些经验，这是文件中工程师必备的技能。试试看！</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nv nw l"/></div></figure></div></div>    
</body>
</html>