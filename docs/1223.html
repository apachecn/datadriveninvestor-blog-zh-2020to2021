<html>
<head>
<title>Visualizing scikit model performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化scikit模型性能</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/visualizing-scikit-model-performance-fb26ff16f7c6?source=collection_archive---------0-----------------------#2020-03-09">https://medium.datadriveninvestor.com/visualizing-scikit-model-performance-fb26ff16f7c6?source=collection_archive---------0-----------------------#2020-03-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/176f8a7b82faacb77e86b693c80b28d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qX5_ddiWH5SZ7bE3"/></div></div></figure><p id="b737" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">深度学习似乎最近得到了所有的炒作和所有的花哨工具。</p><p id="ad5b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，对于许多应用来说，传统的机器学习方法如提升树、线性回归和朴素贝叶斯是正确的解决方案。你不需要火箭筒，用针就可以了。</p><p id="cfe1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这篇文章中，我将向你展示如何用scikit-learn和<a class="ae kz" href="http://wandb.com/" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差</a>来可视化和比较你的机器学习模型性能。我们还将探索这些图如何帮助我们更好地理解我们的模型，并选择最好的一个。</p><p id="5e05" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将介绍对分析分类、回归和聚类模型有用的图表。</p><h1 id="6c50" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">数据集</h1><p id="f81b" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">我在描述泰坦尼克号乘客的<a class="ae kz" href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>上训练了几个模型。我们的目标是预测乘客是否幸存。</p><p id="4451" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在此  →，尝试生成这些图<a class="ae kz" href="https://colab.research.google.com/drive/1j_4UQTT0Lib8ueAU5zXECxesCj_ofjw7" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">的完整代码示例。<br/>你可以在这里</strong> </a> <strong class="kd iu"> </strong> →，找到详细描述剧情的<a class="ae kz" href="https://docs.wandb.com/library/frameworks/scikit" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">文档。</strong></a></p><h1 id="de77" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">分类图</h1><h2 id="6b99" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">学习曲线</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mp"><img src="../Images/72c1fc9a6246485d028533f793853f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_sr8AXu6ToDP_Kl6q3Hog.png"/></div></div></figure><p id="9358" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">学习曲线是在不同长度的数据集上训练模型，并为训练集和测试集生成交叉验证分数与数据集大小的关系图的结果。</p><p id="7b7b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里我们可以观察到我们的模型过度拟合。虽然它在训练集上表现很好，但测试精度逐渐提高，但从未完全达到与训练精度相当的水平。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="e809" class="md lb it mv b gy mz na l nb nc"># Plot learning curve<br/>wandb.sklearn.plot_learning_curve(model, X, y)</span></pre><h2 id="2289" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">特征重要性</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/176f8a7b82faacb77e86b693c80b28d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qX5_ddiWH5SZ7bE3"/></div></div></figure><p id="57a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为分类任务评估和绘制每个要素的重要性。仅适用于具有“feature_importances_”属性的分类器，如树。</p><p id="8922" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，我们可以看到标题<em class="nd">(小姐、夫人、先生、主人)</em>高度显示了谁幸存了下来。这很有意义，因为“头衔”同时包含了乘客的性别、年龄和社会地位。奇怪的是,“姓名长度”是第二个最具预测性的特征，探究为什么会这样可能会很有趣。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="065f" class="md lb it mv b gy mz na l nb nc"># Plot feature importances<br/>wandb.sklearn.plot_feature_importances(model, [‘width’, ‘height, ‘length’])</span></pre><h2 id="0d38" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">受试者工作特征曲线</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/9f5b8bb330af649a57a4a3e6267745be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OluL84EYxK9NPSG0"/></div></div></figure><p id="d47f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ROC曲线绘制了真阳性率(y轴)对假阳性率(x轴)。理想的分数是TPR = 1，FPR = 0，这是左上角的点。通常我们计算ROC曲线下的面积(AUC-ROC ), AUC-ROC越大越好。</p><p id="bf68" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，我们可以看到我们的模型在预测类存活方面稍好，这由较大的AUC-ROC所证明。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="eb16" class="md lb it mv b gy mz na l nb nc"># Plot ROC curve<br/>wandb.sklearn.plot_roc(y_true, y_probas, labels)</span></pre><h2 id="5d11" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">类别比例</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/4b3242db7d26cc4031424d029e1a2f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o1vNrfG4kvCtQKAU"/></div></div></figure><p id="fb8a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">绘制目标类在训练集和测试集中的分布。用于检测不平衡的类，并确保一个类不会对模型产生过大的影响。</p><div class="ne nf gp gr ng nh"><a href="https://www.datadriveninvestor.com/2019/01/23/which-is-more-promising-data-science-or-software-engineering/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">数据科学和软件工程哪个更有前途？数据驱动的投资者</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">大约一个月前，当我坐在咖啡馆里为一个客户开发网站时，我发现了这个女人…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv jz nh"/></div></div></a></div><p id="6bd0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里我们可以看到，没有生还的乘客比幸存的乘客多。训练集和测试集似乎共享目标类的分布，这对于推广我们的模型输出来说是个好消息。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="668d" class="md lb it mv b gy mz na l nb nc"># Plot class proportions<br/>wandb.sklearn.plot_class_proportions(y_train, y_test, [‘dog’, ‘cat’, ‘owl’])</span></pre><h2 id="fabd" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">精确召回曲线</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/22aa62613337d6171df46e7bc28d8257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s1-jqsod-y5zByNC"/></div></div></figure><p id="ee8b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">针对不同的阈值计算精确度和召回率之间的折衷。曲线下的高区域表示高召回率和高精度，其中高精度与低假阳性率相关，高召回率与低假阴性率相关。</p><p id="d3a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">两者的高分表明分类器正在返回准确的结果(高精度)，以及返回所有肯定结果的大部分(高召回)。当班级非常不平衡时，PR曲线是有用的。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="d0be" class="md lb it mv b gy mz na l nb nc"># Plot precision recall curve<br/>wandb.sklearn.plot_precision_recall(y_true, y_probas, labels)</span></pre><h2 id="ec09" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">校准曲线</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/551e362fa7d43b5542a4bd3ad5ee8c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*087nbAgowsDI2HLO"/></div></div></figure><p id="907a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">绘制分类器预测概率的校准程度以及如何校准未校准的分类器。比较基线逻辑回归模型、作为参数传递的模型及其等渗校正和sigmoid校正的估计预测概率。</p><p id="948f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">校准曲线越接近对角线越好。转置的类sigmoid曲线表示过拟合的分类器，而类sigmoid曲线表示欠拟合的分类器。通过训练模型的等张校准和sigmoid校准并比较它们的曲线，我们可以找出模型是否过度拟合或拟合不足，如果是这样，哪种校准(sigmoid或等张)可能有助于解决这一问题。</p><p id="b802" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">要了解更多细节，请查看scikit-learn的文档。</p><p id="f9f4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这种情况下，我们可以看到，普通AdaBoost遭受过拟合(如转置的sigmoid曲线所示)，这可能是因为冗余特征(如“title”)违反了特征独立性假设。使用sigmoid校准来校准AdaBoost似乎是解决这种过拟合最有效的方法。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="4de4" class="md lb it mv b gy mz na l nb nc"># Plot calibration curve<br/>wandb.sklearn.plot_calibration_curve(clf, X, y, ‘RandomForestClassifier’)</span></pre><h2 id="fd4d" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">混淆矩阵</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1fa2be84a70c7fa0432bf74e9949b62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dD-V9wK86ZNMCHFZ"/></div></div></figure><p id="724b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">计算混淆矩阵以评估分类的准确性。这对于评估模型预测的质量以及在模型出错的预测中寻找模式非常有用。</p><p id="ea33" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对角线表示模型得到的正确预测，即实际标注等于预测标注。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="61cf" class="md lb it mv b gy mz na l nb nc"># Plot confusion matrix<br/>wandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)</span></pre><h2 id="1401" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">汇总指标</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/22914f6c396572ef2b819ed2a34f3c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UxOxgas-3kdNUg6b"/></div></div></figure><p id="e1fb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">计算回归和分类算法的摘要指标(如分类的f1、准确度、精确度和召回率以及均方差、平均绝对误差、回归的r2分数)。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="8a93" class="md lb it mv b gy mz na l nb nc"># Plot summary metrics<br/>wandb.sklearn.plot_summary_metrics(model, X_train, X_test, y_train, y_test)</span></pre><h1 id="bae4" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">聚类图</h1><h2 id="364f" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">轮廓图</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/2c83127d40fc9424ce1cdfeca800f3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VZkbffxEc3_bcz9c"/></div></div></figure><p id="c5f1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">测量和绘制一个簇中的每个点与相邻簇中的点的接近程度。簇的厚度对应于簇的大小。垂直线代表所有点的平均轮廓分数。</p><p id="55d2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">接近+1的轮廓系数表明样本远离相邻聚类。值为0表示样本位于或非常接近两个相邻聚类之间的判定边界，负值表示这些样本可能被分配到错误的聚类。</p><p id="5dce" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">总的来说，我们希望所有的轮廓聚类分数都高于平均值(越过红线)并尽可能接近1。我们也更喜欢反映数据中潜在模式的集群大小。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="03c2" class="md lb it mv b gy mz na l nb nc"># Plot silhouette curve<br/>wandb.sklearn.plot_silhouette(model, X_train, [‘spam’, ‘not spam’])</span></pre><h2 id="1723" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">肘图</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/99e74ae06acc895e9528a33b33a80b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*66tlJn0DaNMOKGkk"/></div></div></figure><p id="449f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">测量并绘制方差百分比，解释为聚类数和训练时间的函数。有助于选择最佳的聚类数。</p><p id="fe0a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，我们可以看到根据肘图的最佳聚类数为3，这反映了数据集(该数据集有3个类-鸢尾、杂色鸢尾和海滨鸢尾)。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="c370" class="md lb it mv b gy mz na l nb nc"># Plot elbow curve<br/>wandb.sklearn.plot_elbow_curve(model, X_train)</span></pre><h1 id="894c" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">回归图</h1><h2 id="067e" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">异常候选图</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/904aead3ba6a26ea52d5014f6d2de7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*975u_JcNfOD_KrYP"/></div></div></figure><p id="0217" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通过库克距离测量数据点对回归模型的影响。具有严重偏差影响的实例可能是异常值。适用于异常值检测。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="c7c8" class="md lb it mv b gy mz na l nb nc"># Plot outlier candidates<br/>wandb.sklearn.plot_outlier_candidates(model, X, y)</span></pre><h2 id="6dbc" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">残差图</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/5554e8e8e89c354a41a16c2d17be8c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F48jIIOoyXa9Kqyh"/></div></div></figure><p id="df72" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">测量并绘制预测目标值(y轴)与实际目标值和预测目标值之间的差值(x轴)以及残差分布。</p><p id="838d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，拟合良好的模型的残差应该是随机分布的，因为除了随机误差之外，好的模型可以解释数据集中的大多数现象。</p><p id="7920" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这里，我们可以看到我们的模型产生的大部分误差在+/-5之间，并且对于训练和测试数据集都是均匀分布的。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="4fa4" class="md lb it mv b gy mz na l nb nc"># Plot residuals<br/>wandb.sklearn.plot_residuals(model, X, y)</span></pre><h2 id="1e46" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">汇总指标</h2><figure class="mq mr ms mt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/22914f6c396572ef2b819ed2a34f3c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UxOxgas-3kdNUg6b"/></div></div></figure><p id="c945" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">计算回归和分类算法的摘要指标(如分类的f1、准确度、精确度和召回率以及均方差、平均绝对误差、回归的r2分数)。</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="1f88" class="md lb it mv b gy mz na l nb nc"># Plot summary metrics<br/>wandb.sklearn.plot_summary_metrics(model, X_train, X_test, y_train, y_test)</span></pre><h1 id="d6fd" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">你自己试试吧</h1><p id="a7c7" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">创建这些情节很简单。<a class="ae kz" href="https://colab.research.google.com/drive/1j_4UQTT0Lib8ueAU5zXECxesCj_ofjw7" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">试举一例</strong> </a> <strong class="kd iu"> → </strong></p><h2 id="955d" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">步骤1:导入wandb并初始化新的运行。</h2><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="f4da" class="md lb it mv b gy mz na l nb nc">import wandb<br/>wandb.init(project=”visualize-sklearn”)</span></pre><h2 id="5254" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">第二步:可视化情节</h2><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="a3ce" class="md lb it mv b gy mz na l nb nc"># Visualize single plot<br/>wandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)</span></pre><p id="2c7b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者一次可视化所有图:</p><pre class="mq mr ms mt gt mu mv mw mx aw my bi"><span id="3fbd" class="md lb it mv b gy mz na l nb nc"># Visualize all classifier plots<br/>wandb.sklearn.plot_classifier(clf, X_train, X_test, y_train, y_test, y_pred, y_probas, labels, model_name=’SVC’, feature_names=None)</span><span id="6fa5" class="md lb it mv b gy ny na l nb nc"># All regression plots<br/>wandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, model_name=’Ridge’)</span><span id="d53b" class="md lb it mv b gy ny na l nb nc"># All clustering plots<br/>wandb.sklearn.plot_clusterer(kmeans, X_train, cluster_labels, labels=None, model_name=’KMeans’)</span></pre><h2 id="9719" class="md lb it bd lc me mf dn lg mg mh dp lk km mi mj lo kq mk ml ls ku mm mn lw mo bi translated">你可以在这里  <strong class="ak"> </strong> →，看到一个带有这些图<a class="ae kz" href="https://app.wandb.ai/lavanyashukla/visualize-sklearn/reports/Visualize-Scikit-Models--Vmlldzo0ODIzNg/" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">的实时仪表盘。<a class="ae kz" href="https://colab.research.google.com/drive/1j_4UQTT0Lib8ueAU5zXECxesCj_ofjw7" rel="noopener ugc nofollow" target="_blank">这里试一个例子</a> →</strong></a></h2><p id="cfc6" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">我很想知道你是否觉得这有用。如果你有改进的建议或者任何你想看的情节，我很乐意听听！</p><figure class="mq mr ms mt gt ju"><div class="bz fp l di"><div class="nz oa l"/></div></figure></div></div>    
</body>
</html>