<html>
<head>
<title>From Word Embeddings to Sentence Embeddings — Part 1/3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从单词嵌入到句子嵌入——1/3</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917?source=collection_archive---------3-----------------------#2020-04-01">https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-1-3-7ba9a715e917?source=collection_archive---------3-----------------------#2020-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a1bf6ba35b2eaa3128fdf6ba14ae42a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rA2aSRj1h70EUqinbDrANg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Designed by <a class="ae kc" href="https://br.freepik.com/fotos-gratis/letras-diferentes_1330225.htm" rel="noopener ugc nofollow" target="_blank">Freepik</a></figcaption></figure><p id="68e9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[关于这篇和更多的帖子，请查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a></p><p id="b017" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，我在工程Talkdesk博客上写了两篇文章，关于<a class="ae kc" href="https://engineering.talkdesk.com/what-are-word-embeddings-and-why-are-they-useful-a45f49edf7ab" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>和<a class="ae kc" href="https://engineering.talkdesk.com/what-are-sentence-embeddings-and-why-are-they-useful-53ed370b3f35" rel="noopener ugc nofollow" target="_blank">句子嵌入</a>。在这一系列的三篇博文中，我将详细解释一些获得句子表示的方法。</p><div class="lb lc gp gr ld le"><a href="https://www.datadriveninvestor.com/2020/03/24/encoder-decoder-sequences-how-long-is-too-long/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">编码器解码器序列:多长是太长？数据驱动的投资者</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">在机器学习中，很多时候我们处理的输入是序列，输出也是序列。我们称这样的一个…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jw le"/></div></div></a></div><p id="ab6c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第一部分中，我将解释如何用数字表示一个单词，以及如何用TF-IDF算法用数字表示一个句子。</p><h1 id="666d" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">获得单词表示</h1><p id="5d44" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们如何表示一个单词？</p><p id="203b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最简单的表示单词的方式是用一个<strong class="kf ir">独热编码向量</strong>。假设我们有一个词汇表大小的向量，其中每个条目对应一个单词(图1)。这样，每个单词的表示就是一个零向量，单词的位置为“1”。然而，这种表示有一些缺点。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/67637fedc1b351d8b5fdbc596ffedd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zqC17k4AbUxc1l_Yvo6c1g.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1 — One-hot encoded representation of the words “Rome,” “Paris,” “Italy” and “France” (Source: <a class="ae kc" href="https://speakerdeck.com/marcobonzanini/word-embeddings-for-natural-language-processing-in-python-at-london-python-meetup?slide=14" rel="noopener ugc nofollow" target="_blank">Marco Bonzanini, Word Embeddings for Natural Language Processing in Python @ London Python meetup</a>)</figcaption></figure><p id="179a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个单词的表示都是<strong class="kf ir">非常高维度的</strong>(一个具有词汇大小的向量)但是稀疏的(只有一个条目的值为‘1’)。</p><p id="4f2e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这并没有提供太多关于词义的信息，也没有揭示单词之间的任何现有关系。单词“Rome”的表示与语料库中任何其他单词“Paris”的表示一样接近，因为它们的表示总是以相同的方式不同。除了单词“罗马”的位置和另一个单词的位置之外，所有其他位置都是相同的。</p><p id="677e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前使用的另一种表示方法是<strong class="kf ir">单词嵌入</strong>(图2)。嵌入是一个低维空间，可以用压缩向量表示高维向量(比如单词的一键编码)。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/b08b27ed55cd41f1625b31dbb4acf1cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJJ-j7GGJ6YCvkeQ-0euHw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2- Word embeddings of the words “Rome,” “Paris,” “Italy,” and “France.” We can see that the words “Rome” and “Paris” have similar embeddings, probably because they are both capital cities. (Source: <a class="ae kc" href="https://speakerdeck.com/marcobonzanini/word-embeddings-for-natural-language-processing-in-python-at-london-python-meetup?slide=22" rel="noopener ugc nofollow" target="_blank">Marco Bonzanini, Word Embeddings for Natural Language Processing in Python @ London Python meetup</a>)</figcaption></figure><p id="dbbc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了那些向量的更高密度之外，嵌入的<strong class="kf ir">优势是相似单词之间的接近度</strong>。这意味着像“罗马”或“巴黎”这样的词可能会有类似的嵌入，例如，不同于“互联网”的嵌入。这对许多其他自然语言处理(NLP)任务非常有用，如单词聚类或主题分析。</p><h1 id="7574" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">使用TF-IDF获取句子表示</h1><p id="09ad" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">为了表示句子，不可能创建一个独一无二的编码模式:句子的数量是无限的。我们必须使用其他种类的句子表达。</p><p id="a3b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个博客系列中，我们将解释四种不同的句子表示算法。对于这个帖子，让我们了解更多关于TF-IDF的信息！</p><p id="d816" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> TF-IDF </strong> </a>(词频-逆文档频)是一种经典的信息检索方法，常用于搜索引擎，其目标是在大型语料库中快速搜索文档。这些文档可以是句子、对话，甚至是长文本。</p><p id="c9fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TF-IDF创建了一个术语-文档矩阵(图3)，其中每个术语都与它出现的所有文档相关联，并且每个术语-文档条目都有一个权重。文档中术语的权重随着该术语在特定文档中出现的次数而增加，并且随着该术语在所有文档中出现的频率而减少。这样，英语语料库中诸如“a”或“the”的术语将具有较低的权重，因为它们出现在许多文档中。</p><p id="672a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图3—TF-IDF创建的矩阵示例，其中的文档是对话。</p><p id="87d7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">必须使用训练语料库(一组文档)来创建TF-IDF矩阵。矩阵的维数将是语料库中不同术语的数量乘以语料库中文档的数量。</p><p id="7853" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过与训练语料库中的文档进行比较来计算文档的表示。</p><p id="f4a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文档表示将是具有语料库中文档数量大小的行向量，其中每个条目<em class="nb"> i </em>将具有表示输入文档与语料库中文档<em class="nb"> i </em>的相似性的值(图4)。</p><p id="7264" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相似性是基于输入文档和语料库中每个文档中提到的术语来计算的。文档表示的条目<em class="nb"> i </em>中的较高权重意味着与语料库中的文档<em class="nb"> i </em>具有较高的相似性。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/b9d9cd8e968d2c76a77d74994f340b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFEPDKRMDhfdGyQ43FPHmw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4 — Calculation of the representation of a document. Using the TF-IDF matrix of Figure 3, is calculated a representation of a document based on the similarity with the dialogues.</figcaption></figure><p id="fd49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于TF-IDF的文档表示有一些优点:</p><ul class=""><li id="a5bb" class="nc nd iq kf b kg kh kk kl ko ne ks nf kw ng la nh ni nj nk bi translated">通过查找TF-IDF矩阵和一些简单的计算，可以非常快速地计算出它们。</li><li id="dd6c" class="nc nd iq kf b kg nl kk nm ko nn ks no kw np la nh ni nj nk bi translated">与其他算法相比，它们在概念上很简单。</li><li id="fe43" class="nc nd iq kf b kg nl kk nm ko nn ks no kw np la nh ni nj nk bi translated">它们的实现是透明的，表示也容易理解。</li></ul><p id="0e17" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它们的缺点如下:</p><ul class=""><li id="eacf" class="nc nd iq kf b kg kh kk kl ko ne ks nf kw ng la nh ni nj nk bi translated">文档之间的相似性没有考虑每个单词在文档中的位置(也称为单词袋模型)。</li><li id="f270" class="nc nd iq kf b kg nl kk nm ko nn ks no kw np la nh ni nj nk bi translated">它没有捕获文档的语义，这意味着它没有考虑相似的单词。</li><li id="94a2" class="nc nd iq kf b kg nl kk nm ko nn ks no kw np la nh ni nj nk bi translated">它会创建稀疏向量，这意味着它会浪费大量零值内存。</li></ul><p id="e9b1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一篇帖子到此为止！阅读<a class="ae kc" href="https://medium.com/@diogoferreira_2387/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1" rel="noopener">第二部分</a>以了解更多创建句子嵌入的高级方法，并查看我的<a class="ae kc" href="https://diogodanielsoaresferreira.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a>上类似的帖子。</p><p id="425f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你的坚持！</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nq nr l"/></div></figure></div></div>    
</body>
</html>