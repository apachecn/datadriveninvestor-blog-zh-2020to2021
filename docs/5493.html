<html>
<head>
<title>Long Short Term Memory Network Maths — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆网络数学—第一部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/long-short-term-memory-maths-part-1-d99b3c3b09d0?source=collection_archive---------2-----------------------#2020-09-22">https://medium.datadriveninvestor.com/long-short-term-memory-maths-part-1-d99b3c3b09d0?source=collection_archive---------2-----------------------#2020-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8631b067a2fa484692a92341a8f06b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7d10zUK4874FayT_AGCc2w.jpeg"/></div></div></figure><p id="619e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我学习神经网络背后的基础数学系列的第四篇文章。你可以在这里查看我以前的文章:</p><ol class=""><li id="6bca" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/neural-network-maths-in-5-minutes-f385eeddf783" rel="noopener">神经网络数学</a></li><li id="e513" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/convolution-neural-network-maths-intuition-6b047cb48e90" rel="noopener">卷积神经网络数学</a></li><li id="3b6b" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv lb lc ld le bi translated"><a class="ae lf" href="https://medium.com/datadriveninvestor/recurrent-neural-network-maths-69214e4d69e1" rel="noopener">递归神经网络数学</a></li></ol><p id="3550" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">来到LSTM，这是最令人生畏的。不相信我！看一看就知道了..</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ll"><img src="../Images/73b6462219bbd1c85aa726bc2fc18cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbzpEg77lJr5r_ZKgXfALw.png"/></div></div></figure><p id="bb7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">第一次看的时候我的心就漏跳了一拍(当然是以不好的方式:P)。但是，事实证明这并不难理解。像往常一样，我会试着用简单的语言来解释它，并且只关注它背后的直觉。</p><h1 id="82d8" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">更长的序列！</h1><p id="c57b" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">因此，从上一篇<a class="ae lf" href="https://medium.com/datadriveninvestor/recurrent-neural-network-maths-69214e4d69e1" rel="noopener">文章</a>中，我们发展出一种直觉，即每当我们有一个序列时，我们就去寻找RNNs。LSTM的工作有点不同，因为它有一个在所有输入中维持的全局状态。所有先前输入的上下文基本上都通过全局状态转移到将来的输入。并且由于这种性质，它不会遭受消失和爆炸梯度问题。</p><div class="mt mu gp gr mv mw"><a href="https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd ir gy z fp nb fr fs nc fu fw ip bi translated">人工智能、深度学习和医疗实践|数据驱动的投资者</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">人工智能和深度神经学习的效用看起来可能是合法和有前途的，特别是…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk jw mw"/></div></div></a></div></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="3224" class="lq lr iq bd ls lt ns lv lw lx nt lz ma mb nu md me mf nv mh mi mj nw ml mm mn bi translated">LSTM解码</h1><p id="36be" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">从表面上看，这真的令人望而生畏，但实际上不是。但实际上只是多个简单片段的顶点。让我们分别讨论每一部分。这是一个更简单的版本，其中X和Y显然是我们的输入和输出。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/f49313d9c3d75c066a28f1c64d4b6513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMdN8aXM-s5R3A07x6Ee0g.png"/></div></div></figure><p id="e104" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="ny">单元格状态:</em> </strong>这一行基本上是包含输入的上下文的记忆层。</p><p id="98df" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="ny">隐藏状态:</em> </strong>这一层类似于我们在RNN将以前输入的输出反馈给新输入。</p><p id="b6a0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="ny">忘门:</em> </strong>决定是否因为新的X，要我们<strong class="ka ir"> <em class="ny">清除以前记忆</em> </strong>的网络。比如——“<em class="ny">拉姆是个男孩。拉玛是个女孩</em>”。当<em class="ny"> Rama </em>将作为输入输入时，应该忘记<em class="ny"> Ram </em>的先前上下文。</p><p id="befd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="ny">输入门:</em> </strong>决定是否因为新的X，要<strong class="ka ir"> <em class="ny">更新网络的先前记忆</em> y </strong>。比如——“<em class="ny">拉姆是个男孩。拉玛是个女孩</em>”。当<em class="ny"> Rama </em>将作为输入被提供时，人的先前上下文应由<em class="ny"> Rama </em>更新。</p><p id="6526" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="ny"> Output Gat </em> e </strong>:每个输入都应根据当前输入、上一层输出和持久存储单元给出输出。这个门也有同样的责任。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="d449" class="lq lr iq bd ls lt ns lv lw lx nt lz ma mb nu md me mf nv mh mi mj nw ml mm mn bi translated">正向传播</h1><p id="8862" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">通常，我倾向于忽略激活功能。但是，这里我将把它们合并在一起，因为不理解它们，就无法帮助我们理解整个网络。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ff73e4a0665c882cfa1cd15c2643a1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjbjBNSh9F5wH0R6K23ABQ.png"/></div></div></figure><p id="e485" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">真的很抱歉上面选择的颜色，但如果没有奇怪的颜色编码，真的很难理解流程。</p><p id="d37e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ny"> X —输入<br/>H’—前一层的输出<br/>输出—给定X的网络输出<br/> H —反馈给下一层的输出<br/>E—Y(预测)和输出(实际)之间的均方误差</em></p><h2 id="92f0" class="nz lr iq bd ls oa ob dn lw oc od dp ma kj oe of me kn og oh mi kr oi oj mm ok bi translated">忘记大门</h2><p id="e586" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">黄色虚线框是我们的遗忘框。</p><ul class=""><li id="7324" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv ol lc ld le bi translated">如您所见，这只是将两个输入加权相加。我们还乘以sigma作为激活函数，以减少(0，1)之间的输出集。</li><li id="7912" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv ol lc ld le bi translated">另外，你可以看到，这与我们的存储单元(C’)相乘。原因是这是一个遗忘之门。如果输入值结果是0，当与存储单元相乘时，我们将能够成功地删除这些值。</li></ul><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi om"><img src="../Images/a71d3c1a43ce7e33c99882c4241cd8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*RdWbOjZqo9nv8AEruHJEww.png"/></div></figure><h2 id="e238" class="nz lr iq bd ls oa ob dn lw oc od dp ma kj oe of me kn og oh mi kr oi oj mm ok bi translated">输入门</h2><p id="9147" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">这是我们的浅蓝色虚线框。</p><ul class=""><li id="90c2" class="kw kx iq ka b kb kc kf kg kj ky kn kz kr la kv ol lc ld le bi translated">如你所见，它是一个a 2运算函数。和谭都是先计算后再相乘的。为什么会这样呢？</li><li id="d5dc" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv ol lc ld le bi translated">所以，答案在它的输出范围内。适马函数的输出范围在(0，1)之间，即总是正的。因此，由于这个门负责设置内存状态中的值，所有的值显然都会被设置。这就是它乘以tan的原因，tan的输出范围为(-1，1)。所以，现在只有那些值会被设置到内存中，内存中的值趋向于1，而趋向于-1的值也可以删除它们。</li><li id="4553" class="kw kx iq ka b kb lg kf lh kj li kn lj kr lk kv ol lc ld le bi translated">JFYI，G也称为候选状态。因为，这是一个临时状态，它将在某个时候反映在主存储器中。</li></ul><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c97a0133c1a54eea3cca09f201d23eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*wc2cDqbwTJSZTDgFDFIDJA.png"/></div></figure><h2 id="74c6" class="nz lr iq bd ls oa ob dn lw oc od dp ma kj oe of me kn og oh mi kr oi oj mm ok bi translated">存储状态</h2><p id="8dd0" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">这是用c标记的红色线。</p><p id="922d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这基本上将来自遗忘门和输入门的值相加，并计算最终的存储值。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/969b765c0b557dcfb9257133d746806b.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*r743od1xWSEbV0ruB_E6KQ.png"/></div></figure><h2 id="a32c" class="nz lr iq bd ls oa ob dn lw oc od dp ma kj oe of me kn og oh mi kr oi oj mm ok bi translated">输出状态</h2><p id="02f8" class="pw-post-body-paragraph jy jz iq ka b kb mo kd ke kf mp kh ki kj mq kl km kn mr kp kq kr ms kt ku kv ij bi translated">这是绿色虚线框。到目前为止，我们只是更新存储单元。现在，这一层基本上负责通过将新输入和最终存储单元相乘来找到最终输出。同样，C乘以tan，因此其输出范围可以在(-1，1)之间，并且在必要时更容易移除值。</p><figure class="lm ln lo lp gt jr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/de31f8cd8fa84cbab8fe56f2adf8318a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*YOBVfaFKcWlYvugGLTJDHg.png"/></div></figure><p id="85ae" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些总结了我们所有的向前传播的公式。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><p id="d377" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">看起来这对于一篇文章来说已经是太多的信息了。将在下一篇文章中讨论反向传播。</p><p id="40d7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">进入专家视角— </strong> <a class="ae lf" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>