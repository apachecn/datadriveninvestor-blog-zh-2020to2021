<html>
<head>
<title>Feature Selection- Selection of the best that matters through Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特性选择-通过Python选择最重要的特性</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/feature-selection-selection-of-the-best-that-matters-through-python-c05018916c94?source=collection_archive---------8-----------------------#2020-10-14">https://medium.datadriveninvestor.com/feature-selection-selection-of-the-best-that-matters-through-python-c05018916c94?source=collection_archive---------8-----------------------#2020-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7c48" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">消除浪费，保留商品是普遍规律。在机器学习中，我们希望我们的模型是优化和快速的，为了做到这一点，并消除不必要的变量，我们采用各种特征选择技术。</p><p id="4e1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用特征选择的主要原因是:</p><ul class=""><li id="6dd8" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">来更快地训练机器学习模型。</li><li id="a4c7" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">以提高模型的准确性，如果选择了优化的子集。</li><li id="cc58" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">降低模型的复杂性。</li><li id="72c7" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">以减少过度拟合并使其更易于解释。</li></ul><p id="93c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">易于使用且效果良好的特征选择技术有:</p><p id="8d9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> A) </strong> <strong class="jp ir">过滤方法</strong></p><ol class=""><li id="63b4" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kz kr ks kt bi translated">删除常量功能</li><li id="0c10" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kz kr ks kt bi translated">单变量选择</li><li id="a31d" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kz kr ks kt bi translated">特征重要性</li><li id="8fc1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kz kr ks kt bi translated">热图相关矩阵</li></ol><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/5dc1c2b28a788eab505d210bb2a9f711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqyOe8WBT175NEtM854z5w.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Filter Methods</figcaption></figure><p id="5f9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">下降常数特性</strong></p><p id="856c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个过滤器中，我们可以移除具有恒定值的特征，这些特征对于解决问题陈述实际上是不重要的。</p><p id="e272" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中，使用sklearn的VarianceThreshhold功能应用此功能的代码是:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="3454" class="lv lw iq lr b gy lx ly l lz ma"><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.feature_selection</strong> <strong class="lr ir">import</strong> VarianceThreshold<br/>var_thres=VarianceThreshold(threshold=0)<br/>var_thres.fit(data)<br/>data.columns[var_thres.get_support()]<br/>constant_columns = [column <strong class="lr ir">for</strong> column <strong class="lr ir">in</strong> data.columns<br/><strong class="lr ir">if</strong> column <strong class="lr ir">not</strong> <strong class="lr ir">in</strong> data.columns[var_thres.get_support()]]<br/>data.drop(constant_columns,axis=1)</span></pre><p id="43a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">单变量选择</strong></p><p id="f476" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种类型的选择中，统计测试用于选择那些与结果/输出变量关系最密切的变量/特征。</p><p id="d6ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">scikit-learn库提供了SelectKBest类，它可以与一组不同的统计测试一起使用，以选择特定数量的特性。</p><ul class=""><li id="3114" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">皮尔逊相关系数:f _回归()</li><li id="63d2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">方差分析:f_classif()</li><li id="303b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">卡方:chi2()</li></ul><p id="15a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下示例使用非负特征的卡方(chi)统计测试从移动价格范围预测数据集中选择10个最佳特征</p><p id="aef1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解Chi2，我们需要理解以下几个术语:</p><p id="4c5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">自由度</strong>指逻辑独立值的最大数量，可以自由变化。简而言之，它可以定义为观测值的总数减去施加在观测值上的独立约束的数量。</p><p id="92da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">卡方检验在统计学中用于检验两个事件的独立性。给定两个变量的数据，我们可以得到观察计数O和期望计数E。卡方测量期望计数E和观察计数O如何相互偏离。</p><p id="71bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Xc2= ∑(Oi — Ei )2 / Ei</p><p id="cc9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中:</p><p id="6508" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">c=自由度，</p><p id="8adc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">O=观察值</p><p id="b650" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">e =预期值</p><p id="7916" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当两个特征独立时，观察计数接近预期计数，因此我们将具有较小的卡方值。所以高卡方值表明独立性假设是不正确的。简而言之，卡方值越高，特征越依赖于响应，并且可以被选择用于模型训练。</p><p id="1c86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中，这种类型的过滤可以通过使用来自sklearn的SelectKBest和chi2库来完成，如以下代码所示:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="6173" class="lv lw iq lr b gy lx ly l lz ma"><strong class="lr ir">import</strong> <strong class="lr ir">pandas</strong> <strong class="lr ir">as</strong> <strong class="lr ir">pd</strong><br/><strong class="lr ir">import</strong> <strong class="lr ir">numpy</strong> <strong class="lr ir">as</strong> <strong class="lr ir">np</strong><br/><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.feature_selection</strong> <strong class="lr ir">import</strong> SelectKBest<br/><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.feature_selection</strong> <strong class="lr ir">import</strong> chi2<br/>data = pd.read_csv("train.csv")<br/>X = data.iloc[:,0:20]  <em class="mb">#independent columns</em><br/>y = data.iloc[:,-1]    <em class="mb">#target column i.e price range</em><br/><br/> <em class="mb">#apply SelectKBest class to extract top 10 best features</em> <br/>bestfeatures = SelectKBest(score_func=chi2, k=10) <br/>fit = bestfeatures.fit(X,y)<br/>dfscores = pd.DataFrame(fit.scores_)<br/>dfcolumns = pd.DataFrame(X.columns)<br/><br/> <em class="mb">#concat two dataframes for better visualization </em><br/>featureScores = pd.concat([dfcolumns,dfscores],axis=1) featureScores.columns = ['Specs','Score'] <em class="mb">#naming the dataframe columns</em><br/>featureScores<br/>print(featureScores.nlargest(10,'Score'))  <em class="mb">#print 10 best features</em></span></pre><p id="38ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">特征重要性</strong></p><p id="71ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征重要性是数据集中每个特征的一种分数，分数越高，特征对于结果或因变量越重要或相关。</p><p id="c1ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过使用模型的要素重要性属性，可以获得数据集每个要素的要素重要性。</p><p id="b410" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征重要性是一个内置的类，带有基于树的分类器，我们将使用额外的树分类器来提取数据集的前10个特征。</p><p id="feb7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，这可以通过以下代码来实现:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="9fda" class="lv lw iq lr b gy lx ly l lz ma"><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.ensemble</strong> <strong class="lr ir">import</strong> ExtraTreesClassifier <br/><strong class="lr ir">import</strong> <strong class="lr ir">matplotlib.pyplot</strong> <strong class="lr ir">as</strong> <strong class="lr ir">plt</strong> <br/>model = ExtraTreesClassifier() <br/>model.fit(X,y)<br/>print(model.feature_importances_) <em class="mb">#use inbuilt class feature_importances of tree based classifiers</em><br/><em class="mb">#plot graph of feature importances for better visualization</em> feat_importances = pd.Series(model.feature_importances_, index=X.columns) <br/>feat_importances.nlargest(10).plot(kind='barh') plt.show()</span></pre><p id="60c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">与热图相关的矩阵</strong></p><p id="c98d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相关性说明变量和输出或目标变量之间的关系。</p><p id="5c5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相关性可以是正的(正比)或负的(反比)</p><p id="f625" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">热点图使得识别哪些特征与目标变量最相关变得容易，我们将使用seaborn库绘制相关特征的热点图。</p><p id="eb7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Python中，这可以通过下面的代码来完成。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="9cad" class="lv lw iq lr b gy lx ly l lz ma"><strong class="lr ir">import</strong> <strong class="lr ir">seaborn</strong> <strong class="lr ir">as</strong> <strong class="lr ir">sns</strong><br/><em class="mb">#get correlations of each features in dataset</em><br/>corrmat = data.corr()<br/>top_corr_features = corrmat.index<br/>plt.figure(figsize=(20,20))<br/><em class="mb">#plot heat map</em><br/>g=sns.heatmap(data[top_corr_features].corr(),annot=<strong class="lr ir">True </strong>, cmap=plt.cm.CMRmap_r)<br/>plt.show()</span></pre><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/7f26d10fea9d180dba49c61c20ec03f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*nbWmSEa8dibT2CF3wb5VeA.png"/></div></figure><p id="d452" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深色表示相关性较高。</p><p id="2bb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> B) </strong> <strong class="jp ir">包装方法</strong></p><p id="0a6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">包装方法的一些常见示例有:</p><p id="c149" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1)正向特征选择</p><p id="4253" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2)反向特征消除</p><p id="0c7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3)递归特征消除。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi md"><img src="../Images/359a3a5e6710472955b46c721bf1f92f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yl9TGFG6ugboU88ogealJg.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Wrapper Methods</figcaption></figure><p id="4757" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1:前向选择</strong>:前向选择是一种优化特征选择的迭代方法，我们从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</p><p id="1aa5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它可以实现为:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="f466" class="lv lw iq lr b gy lx ly l lz ma">1. <em class="mb"># step forward feature selection</em><br/>2.  <br/>3. from sklearn.model_selection import train_test_split<br/>4. from sklearn.ensemble import RandomForestRegressor<br/>5. from sklearn.metrics import r2_score<br/>6. from mlxtend.feature_selection import SequentialFeatureSelector as SFS<br/>7. <em class="mb"># select numerical columns:</em><br/>8.  <br/>9. numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']<br/>10. numerical_vars = list(data.select_dtypes(include=numerics).columns)<br/>11. data = data[numerical_vars]<br/>12. <em class="mb"># separate train and test sets</em><br/>13. X_train, X_test, y_train, y_test = train_test_split(<br/>14.  X,Y, test_size=0.3, random_state=0)<br/>15. <em class="mb"># find and remove correlated features</em><br/>16. def correlation(dataset, threshold):<br/>17.  col_corr = set() <em class="mb"># Set of all the names of correlated columns</em><br/>18.  corr_matrix = dataset.corr()<br/>19.  for i <strong class="lr ir">in</strong> range(len(corr_matrix.columns)):<br/>20.  for j <strong class="lr ir">in</strong> range(i):<br/>21.  if abs(corr_matrix.iloc[i, j]) &gt; threshold: <em class="mb"># we are interested in absolute coeff value</em><br/>22.  colname = corr_matrix.columns[i] <em class="mb"># getting the name of column</em><br/>23.  col_corr.add(colname)<br/>24.  return col_corr<br/>25.  <br/>26. corr_features = correlation(X_train, 0.8)<br/>27. print('correlated features: ', len(set(corr_features)) )<br/>28. <em class="mb"># removed correlated</em> <em class="mb">features</em><br/>29. X_train.drop(labels=corr_features, axis=1, inplace=True)<br/>30. X_test.drop(labels=corr_features, axis=1, inplace=True)<br/>31. X_train.fillna(0, inplace=True)<br/>32.  <br/>33.  <br/><strong class="lr ir">34.</strong> <strong class="lr ir"><em class="mb"># step forward feature selection</em></strong><br/>35.  <br/>36. from mlxtend.feature_selection import SequentialFeatureSelector as SFS<br/>37.  <br/>38. sfs1 = SFS(RandomForestRegressor(), <br/>39.  k_features=10, <br/>40.  forward=True, <br/>41.  floating=False, <br/>42.  verbose=2,<br/>43.  scoring='r2',<br/>44.  cv=3)<br/>45.  <br/>46. sfs1 = sfs1.fit(np.array(X_train), y_train)<br/>47. X_train.columns[list(sfs1.k_feature_idx_)]<br/>48. sfs1.k_feature_idx_</span></pre><p id="9cb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2:向后消除</strong>:在向后消除中，我们从所有的特征开始，并在每次迭代中移除最不重要的特征，从而提高模型的性能。我们重复这一过程，直到在删除特征时没有观察到改进。</p><p id="133f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它可以实现为:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="e6fd" class="lv lw iq lr b gy lx ly l lz ma">49. <em class="mb"># step backward feature elimination</em><br/>50.  <br/>51. sfs1 = SFS(RandomForestRegressor(), <br/>52.  k_features=10, <br/>53.  forward=False, <br/>54.  floating=False, <br/>55.  verbose=2,<br/>56.  scoring='r2',<br/>57.  cv=3)<br/>58.  <br/>59. sfs1 = sfs1.fit(np.array(X_train), y_train)<br/>60. X_train.columns[list(sfs1.k_feature_idx_)]<br/>61. sfs1.k_feature_idx_</span></pre><p id="aa4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3:递归特征消除</strong>:这是一种最贪婪的优化算法，旨在找到性能最佳的特征子集。</p><p id="3cc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它重复地创建模型，并在每次迭代中保留性能最好或最差的特性。它用剩下的特征构造下一个模型，直到所有的特征都处理完。然后，它根据要素被消除的顺序对其进行排序。</p><p id="859d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它可以实现为:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="604d" class="lv lw iq lr b gy lx ly l lz ma"><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.svm</strong> <strong class="lr ir">import</strong> <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" rel="noopener ugc nofollow" target="_blank">SVC</a><br/><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.datasets</strong> <strong class="lr ir">import</strong> <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits" rel="noopener ugc nofollow" target="_blank">load_digits</a><br/><strong class="lr ir">from</strong> <strong class="lr ir">sklearn.feature_selection</strong> <strong class="lr ir">import</strong> <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" rel="noopener ugc nofollow" target="_blank">RFE</a><br/><strong class="lr ir">import</strong> <strong class="lr ir">matplotlib.pyplot</strong> <strong class="lr ir">as</strong> <strong class="lr ir">plt</strong><br/> <br/><em class="mb"># Load the digits dataset</em><br/>digits = <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits" rel="noopener ugc nofollow" target="_blank">load_digits</a>()<br/>X = digits.images.reshape((len(digits.images), -1))<br/>y = digits.target<br/> <br/><em class="mb"># Create the RFE object and rank each pixel</em><br/>svc = <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" rel="noopener ugc nofollow" target="_blank">SVC</a>(kernel="linear", C=1)<br/>rfe = <a class="ae me" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" rel="noopener ugc nofollow" target="_blank">RFE</a>(estimator=svc, n_features_to_select=1, step=1)<br/>rfe.fit(X, y)</span></pre><p id="f41e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">C)</strong>T14】嵌入方法</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/47e4b92e155ea04b2dc17f3cf749b123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*sHJDR2LliN8VOz5u7abwiA.png"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk">Embedded Methods</figcaption></figure><p id="a1e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1:套索回归</strong></p><p id="aeb0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Lasso回归执行L1正则化，这实际上是添加等于系数大小绝对值的惩罚。</p><p id="7cbd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正则化意味着向机器学习模型的不同参数添加惩罚或惩罚，以减少模型的自由度，从而避免过度拟合。在线性模型正则化中，惩罚应用于乘以每个预测值的系数。从不同类型的正则化中，Lasso或l1具有其能够使许多系数值为零的性质。因此，可以从模型中删除该特征。</p><p id="f3a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它被实现为:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="0465" class="lv lw iq lr b gy lx ly l lz ma"><em class="mb">#load libraries</em><br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import Lasso<br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn.preprocessing import StandardScaler<br/><em class="mb"># different scales, so it helps the regression to scale them</em><br/><em class="mb"># separate train and test sets</em><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>X,Y, test_size=0.3,<br/> random_state=0)<br/> <br/>scaler = StandardScaler()<br/>scaler.fit(X_train.fillna(0))<br/><em class="mb"># to force the algorithm to shrink some coefficients</em><br/> <br/>sel_ = SelectFromModel(Lasso(alpha=100))<br/>sel_.fit(scaler.transform(X_train.fillna(0)), y_train)<br/><em class="mb"># make a list with the selected features and print the outputs</em><br/>selected_feat = X_train.columns[(sel_.get_support())]</span></pre><p id="9cf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">很明显，套索正则化可能有助于从数据集中移除不重要的要素。因此，增加惩罚将导致移除的特征数量增加。</p><p id="b99f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">惩罚越高，重要特征被移除的概率越高，这可能导致模型性能下降，然后意识到我们需要降低正则化。</p><p id="9b99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2:随机森林/集合技术</strong></p><p id="0a16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机森林也可以被称为集成决策树，并且对于特征选择和分类器也是有用的。</p><p id="05ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机森林是最流行的机器学习算法之一。因为它们提供了非常好的预测性能、低过拟合和容易解释，因此它们被认为是非常成功的。</p><p id="0679" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在分类中，我们通过基尼系数或信息增益/熵来衡量杂质。在回归分析中，我们用方差来衡量杂质。当我们训练一棵树时，可以计算出每个特征减少杂质的程度。哪个特征减少杂质，它就越重要。在随机森林中，每个特征的杂质减少可以确定该特征的最终重要性。</p><p id="cd5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在随机树的顶部选择的特征通常比在树的末端节点选择的特征更重要，因为通常顶部节点导致更大的信息增益。</p><p id="04b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，它可以实现为:</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="9b3d" class="lv lw iq lr b gy lx ly l lz ma"><em class="mb"># Import libraries</em><br/>from sklearn import preprocessing<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import tree<br/>from sklearn.ensemble import RandomForestClassifier<br/><em class="mb"># Encode categorical variables</em><br/>X = pd.get_dummies(X, prefix_sep='_')<br/>y = LabelEncoder().fit_transform(y)<br/><em class="mb"># Normalize feature vector</em><br/>X2 = StandardScaler().fit_transform(X)<br/><em class="mb"># Split the dataset</em><br/>X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.30, random_state = 0)<br/><em class="mb"># instantiate the classifier with n_estimators = 100</em><br/>clf = RandomForestClassifier(n_estimators=100, random_state=0)<br/><em class="mb"># fit the classifier to the training set</em><br/>clf.fit(X_train, y_train)<br/><em class="mb"># predict on the test set</em><br/>y_pred = clf.predict(X_test)</span></pre><p id="4ba3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">希望看完这篇博客后，通过python选择不同类型的特性会更加容易。</p><p id="8f42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="a7e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mb">原载于2020年10月14日</em><a class="ae me" href="https://www.numpyninja.com/post/feature-selection-selection-of-the-best-that-matters" rel="noopener ugc nofollow" target="_blank"><em class="mb">【https://www.numpyninja.com】</em></a><em class="mb">。</em></p></div></div>    
</body>
</html>