<html>
<head>
<title>Training a Machine Learning model from just a few examples: Few-Shot Learning — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅通过几个例子训练机器学习模型:少量学习—第2部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/training-a-machine-learning-model-from-just-a-few-examples-few-shot-learning-part-2-f8659ece982b?source=collection_archive---------2-----------------------#2020-06-17">https://medium.datadriveninvestor.com/training-a-machine-learning-model-from-just-a-few-examples-few-shot-learning-part-2-f8659ece982b?source=collection_archive---------2-----------------------#2020-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="ee1f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">少数镜头学习<strong class="jz iu"> (FSL) </strong>是机器学习的一个领域，旨在训练场景中的模型，这些场景中很少有例子可以作为监督。利用先前的知识，FSL可以从有限数量的例子中掌握新的任务。本系列文章介绍了各种FSL方法，面向初级到中级机器学习爱好者。这是本系列的第2部分，涵盖了实现FSL的方法。第1部分主要介绍了这个主题。</p><p id="84ba" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">第1部分可从<a class="ae kv" href="https://medium.com/@nvarshney97/training-a-machine-learning-model-from-a-few-examples-few-shot-learning-part-1-50402ab8dfa5" rel="noopener">获得，通过几个例子训练机器学习模型:少量学习-第1部分。</a></p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi kw"><img src="../Images/f316aa8925c5c3fb9981601d1592e56f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhF3Ibprkk9DsMAyQZ20eg.jpeg"/></div></div></figure><h1 id="8ecf" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">概述第2部分:</h1><ol class=""><li id="bb67" class="mg mh it jz b ka mi ke mj ki mk km ml kq mm ku mn mo mp mq bi translated">实现少量学习的方法<br/> —基于数据的方法<br/> —基于模型的方法<br/> —基于算法的方法</li><li id="a69c" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated">结论和参考文献</li></ol></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><h1 id="7efb" class="li lj it bd lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb na md me mf bi translated">实现少投学习的途径</h1><p id="37fb" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">正如本系列第1部分<a class="ae kv" href="https://medium.com/@nvarshney97/training-a-machine-learning-model-from-a-few-examples-few-shot-learning-part-1-50402ab8dfa5" rel="noopener">中提到的，FSL利用先验知识来补偿小的训练数据集。可以通过三种方式利用先前的知识来实现少量的学习。</a></p><ol class=""><li id="b81f" class="mg mh it jz b ka kb ke kf ki ne km nf kq ng ku mn mo mp mq bi translated"><strong class="jz iu">数据</strong> — <em class="nh">利用先验知识增加数据</em>(即<strong class="jz iu">数据扩充</strong>)。</li><li id="6fb1" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated"><strong class="jz iu">模型</strong> — <em class="nh">利用先验知识减少假设空间</em>的大小。</li><li id="2c15" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated"><strong class="jz iu">算法</strong> —使用先验知识改变在给定假设空间中的最佳假设的<em class="nh">搜索。</em></li></ol><p id="f4d7" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在深入探讨上述实现FSL的方法之前，我们先来回顾一些与ML相关的数学。这一部分只对那些对数学感兴趣的人有用。虽然您可以考虑跳过这一部分(阅读突出显示的部分就足够了)，但它将有助于理解本文中要跟进的部分。</p><div class="ni nj gp gr nk nl"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd iu gy z fp nq fr fs nr fu fw is bi translated">认知计算——一套被广泛认为是……</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz lg nl"/></div></div></a></div><p id="3435" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">预期风险与经验风险最小化:</strong></p><p id="3703" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">给定一个<strong class="jz iu"> <em class="nh">假设h </em> </strong>，我们要最小化它的<strong class="jz iu"> <em class="nh">期望风险R </em> </strong>，这是相对于输入x和输出y的地真联合概率分布所度量的损失，<strong class="jz iu"> <em class="nh"> p(x，y) </em> </strong>。具体来说，</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/2aa4cff81bbdb09a71ff5b7bab0352cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrHCYsIfdL6i9i7oSFRqCw.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Expected Risk. Source: Generalizing from a Few Examples: A Survey on Few-Shot Learning</figcaption></figure><p id="1da2" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于<strong class="jz iu"> <em class="nh"> p(x，y) </em> </strong>未知，因此<strong class="jz iu">经验风险</strong>(这是在<em class="nh"> I </em>样本的训练集上样本损失的平均值)通常被用作<strong class="jz iu"> <em class="nh"> R(h) </em> </strong>的代理，导致<strong class="jz iu">经验风险最小化</strong>。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi oa"><img src="../Images/e74e033e9d58f74af2a1675e2aa26729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4u68ArIFkIKwaumbyHTNYA.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Empirical Risk. Source: Generalizing from a Few Examples: A Survey on Few-Shot Learning</figcaption></figure><p id="5a77" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，让我们再了解几个术语</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi of"><img src="../Images/d1dd2485122696f3033ea0fbbc61eab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_EDMAlu1YwXOWDUlDhW_xQ.png"/></div></div></figure><p id="fd2e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">设假设空间为<strong class="jz iu"> H. </strong>由于<strong class="jz iu"> ĥ </strong>未知，算法试图用某个<strong class="jz iu">h∈ht21】来近似它。<strong class="jz iu"> <em class="nh"> h* </em> </strong>是在<strong class="jz iu"> <em class="nh"> H </em> </strong> <em class="nh"> </em>中找不到的最佳近似。<strong class="jz iu"> <em class="nh"> hI </em> </strong>是通过经验风险最小化得到的<strong class="jz iu"><em class="nh"/></strong>中的最佳假设。</strong></p><blockquote class="og oh oi"><p id="299a" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">简单来说，<strong class="jz iu"> h* </strong>是找不到的最优假设(通过最小化期望风险得到的)。因此，我们的目标是在中找到<strong class="jz iu">，这是通过最小化经验风险获得的一个近似值。</strong></p></blockquote><p id="e3f1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">使用这些术语，我们可以强调在<a class="ae kv" href="https://medium.com/@nvarshney97/training-a-machine-learning-model-from-a-few-examples-few-shot-learning-part-1-50402ab8dfa5" rel="noopener">第1部分</a>中讨论的问题，即在小训练数据集的情况下泛化能力差。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi om"><img src="../Images/76b9555996ec3e65ac62e87be2d25df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9LLBLnltBB9mRTVYbcFdw.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Comparison of learning with sufficient and few training samples. Source: Generalizing from a Few Examples: A Survey on Few-Shot Learning</figcaption></figure><p id="1e56" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">左图显示了数据集较大时的假设空间。有足够的具有监督信息的训练数据(即，I很大)，经验风险最小化器<strong class="jz iu"><em class="nh"/></strong>可以提供到最佳可能<strong class="jz iu"> R(h*) </strong>的良好近似<strong class="jz iu"><em class="nh">【R(hI)</em></strong>。换句话说，假设非常接近最优假设。(即<strong class="jz iu"> <em class="nh"> hI </em> </strong>接近<strong class="jz iu"> <em class="nh"> h* </em> </strong>)。另一方面，在小数据集的情况下，经验风险<strong class="jz iu"> <em class="nh"> RI (h) </em> </strong>可能远不是预期风险<strong class="jz iu"> <em class="nh"> R(h) </em> </strong>的良好近似，并且所得的经验风险最小化器<strong class="jz iu"> <em class="nh"> hI </em> </strong>过拟合，即收敛假设显著远离最优假设(即<strong class="jz iu"> <em class="nh"/></strong></p><p id="de6f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">下图描述了不同的FSL方法是如何解决少数几个学习问题的。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi om"><img src="../Images/f72052cdc37d26ce24618363ee394cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GrvEwwYxJU6GDr2M63FHxg.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">How FSL methods solve a few-shot problem. Source: Generalizing from a Few Examples: A Survey on Few-Shot Learning</figcaption></figure><p id="e590" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">接下来我们将尝试理解上图中的每一个情节。</p><h1 id="b919" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">数据— </strong></h1><p id="a3f5" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">这些方法使用先验知识来扩充训练数据集(即增加训练数据集中的样本数量)。先验知识用于扩充数据集。然后，可以将标准的ML算法应用于这个扩展的数据集。为了举例说明这一点，让我们考虑图像，可以通过翻转、旋转、裁剪、镜像、缩放和各种其他变换对图像执行数据扩充。这些转换可以为模型引入不同种类的不变性。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="ab gu cl on"><img src="../Images/0698e956e16b9bc22dc70a2b0b9780cd.png" data-original-src="https://miro.medium.com/v2/format:webp/1*C8hNiOqur4OJyEZmC7OnzQ.png"/></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Data Augmentation by various transformations. Image Source: nanonets.com</figcaption></figure><p id="1618" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">有关图像中数据增强的更多详细信息，请参见<a class="ae kv" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fmedium.com%2F%40thimblot%2Fdata-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec&amp;psig=AOvVaw0SQ4ELnz7C-NvvfRCnouDL&amp;ust=1592175870311000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCLDCsbvz_-kCFQAAAAAdAAAAABBF" rel="noopener ugc nofollow" target="_blank">数据增强:用几行Python代码增强您的图像数据集</a>。</p><p id="1ede" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">属于这一类别的另一种方法是基于通过来自其他数据集的示例来扩展训练数据集。本质上，使用可用的训练数据来训练模型(由于小训练集的可用性，该模型将非常糟糕)，并用于标记其他数据集的示例(这种标记被称为“<strong class="jz iu">伪标记</strong>”)。然后添加这些带有伪标签的示例来扩展实际的训练数据集。伪标签可能不准确，但有助于克服数据量小的问题。</p><blockquote class="og oh oi"><p id="0481" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">这里，我们通过对示例进行一些转换或添加来自其他数据集的数据来扩展训练集，然后使用标准的ML算法来找到最佳假设。</p></blockquote><p id="4bb1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于数据扩充取决于扩展数据集的规则(手工制作或自动学习),因此它存在一些问题。这些规则需要领域专业知识，并且很难创建。此外，不可能列举所有可能的不变性。此外，这些规则可能特定于数据集，可能不适用于其他数据集。</p><p id="c476" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最后，正如您可能已经注意到的，这种方法实际上并没有解决真正的少量学习问题，因为它只是增强了数据集，并依赖于标准的ML算法进行泛化。</p><h1 id="27ae" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">型号— </strong></h1><p id="6879" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">这些方法利用先验知识来约束<strong class="jz iu"> <em class="nh"> H </em> </strong>的复杂度，导致假设空间小得多。如图所示，不考虑优化灰色区域，因为根据现有知识，该区域不太可能包含最佳的<strong class="jz iu"> h* </strong>。对于这个较小的<strong class="jz iu"> H </strong>，训练数据足以学习一个可靠的<strong class="jz iu"> <em class="nh"> hI </em> </strong>并降低过拟合的风险。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/71133caeab2099a500da8ec5d1fbb91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*dVtxWjCrEozOQeN4XpiFJg.png"/></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Reducing the hypothesis space for model-based approaches.</figcaption></figure><blockquote class="og oh oi"><p id="1136" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">这里，我们通过使用先验知识从搜索中消除假设空间的非最优部分来缩小假设空间。</p></blockquote><p id="5a51" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">根据使用什么先验知识来约束假设空间，方法可以分为四种类型。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi op"><img src="../Images/9c8192cdd43169cfa0fdb36a147ed667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8mE_vnS9Oxrf3Lv74okzEA.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">FSL methods pertinent to Model perspective. Source: Generalizing from a Few Examples: A Survey on Few-Shot Learning</figcaption></figure><h2 id="5264" class="oq lj it bd lk or os dn lo ot ou dp ls ki ov ow lw km ox oy ma kq oz pa me pb bi translated"><strong class="ak">多任务学习:</strong></h2><p id="aa7f" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">这种方法学习联合解决多个相关任务，因此一个任务的参数(即假设空间<strong class="jz iu"> H </strong>)受到其他任务的约束。</p><p id="7b61" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">考虑以下设置:</p><blockquote class="og oh oi"><p id="f6ca" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">有与“C”相关的任务T1，T2，…，TC。</p><p id="4682" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">有的例子很少(姑且称之为<strong class="jz iu">目标任务</strong>)有的例子很多(<strong class="jz iu">源任务</strong>)。</p><p id="a9f6" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">多任务学习从“C”个任务的训练集中学习，以获得每个任务Tc的参数θC。如前所述，任务是共同学习的，taks Tc的参数θc受其他任务的约束。</p></blockquote><p id="da7b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">根据约束参数<strong class="jz iu">θc</strong>的方式，方法可以分为两类:</p><ol class=""><li id="862f" class="mg mh it jz b ka kb ke kf ki ne km nf kq ng ku mn mo mp mq bi translated"><strong class="jz iu">参数共享</strong> —有些参数直接在任务间共享。在大多数情况下，前几个组件是跨任务共享的，而最后一个组件是特定于任务的。例如，考虑三个任务，第一个预测图像中汽车的存在，第二个预测动物的存在，第三个预测人的存在。该模型有一些共享层，它们捕捉图像的基本特征。这些共享层之后是特定于任务的层，这些层学习各自任务的参数。<br/>这些共享参数可以从源任务(具有大量训练数据的任务)中学习。对于目标任务(我们需要对其进行FSL)，只需要学习特定于任务的参数，并且可以直接使用学习到的共享参数。因此，使用先验知识(即相关任务的训练数据)来约束假设空间。</li></ol><blockquote class="og oh oi"><p id="bcad" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">这里，我们通过允许跨任务共享参数来约束假设空间。</p></blockquote><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pc"><img src="../Images/4c8a7e67192a16606a1b8f18b94df66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pGWoiHgDLWSvQl1O.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Multitask Learning with parameter sharing.</figcaption></figure><p id="5026" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu"> 2。参数绑定</strong> —这种方法鼓励不同任务(θc)的参数<strong class="jz iu">相似，而</strong>不像参数共享那样直接共享参数。实现这一点的一个方法是调整参数，即迫使不同任务的参数足够接近。例如，假设您有一个在任务1(源任务)上训练的模型，现在我们将使用相同的模型，并在为少数任务训练它时允许参数的最小变化。这样我们就约束了假设空间，实现了FSL。</p><blockquote class="og oh oi"><p id="4e5c" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">这里，我们通过最小化学习参数的变化来约束假设空间。</p></blockquote><p id="e91d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这种方法有一个限制，即需要联合训练，即当需要学习一个新的少量任务时，必须再次训练整个多任务模型，这可能是昂贵和缓慢的。</p><h2 id="388c" class="oq lj it bd lk or os dn lo ot ou dp ls ki ov ow lw km ox oy ma kq oz pa me pb bi translated"><strong class="ak">嵌入学习:</strong></h2><p id="dd4c" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">这种方法将每个样本嵌入到一个低维空间中，使得相似的样本靠得很近，不相似的样本离得很远。在这样的空间中，需要较小的假设空间，因为来自不同类别的样本被很好地分开。因此，假设空间由于嵌入空间的较低维度而受到约束。这意味着现在需要更少的样本来逼近最优假设函数(对FSL来说是完美的)。嵌入函数从先验知识中学习，并且可以另外使用来自FSL任务的训练数据的任务特定信息。基于这种方法有多种方法，例如原型网络、匹配网络、关系网络等。</p><p id="505f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">该方法的组成部分:</p><ul class=""><li id="61d6" class="mg mh it jz b ka kb ke kf ki ne km nf kq ng ku pd mo mp mq bi translated">将训练样本映射到低维空间z的<strong class="jz iu">嵌入函数‘g’</strong></li><li id="1f16" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku pd mo mp mq bi translated">一个<strong class="jz iu">嵌入函数‘f’</strong>，它将一个测试实例映射到一个低维空间z</li><li id="dfca" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku pd mo mp mq bi translated">一个<strong class="jz iu">相似性函数‘s’</strong>,用于查找训练和测试示例嵌入之间的相似性。测试示例被分配给在嵌入空间中与其最相似的类。</li></ul><p id="4c80" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在大多数方法中，这三个函数是使用先验知识(即在有足够监督的情况下其他任务的数据)学习的，并直接用于少量发射任务(即，无需在少量发射任务上重新训练)。</p><p id="77d5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">注意</strong>:有些方法使用相同的‘g’和‘f’函数。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pe"><img src="../Images/06c9ac3e133d9fd8e390e01248c6548d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uU0SVFTXUWqTApbZ0SJHcw.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">FSL using Embedding Learning.</figcaption></figure><blockquote class="og oh oi"><p id="7e39" class="jx jy nh jz b ka kb kc kd ke kf kg kh oj kj kk kl ok kn ko kp ol kr ks kt ku im bi translated">这里，我们通过将数据嵌入到一个低维空间来降低数据的维数，在这个低维空间中，不同类别的样本被很好地分开。由于低维空间，假设空间受到限制。</p></blockquote><p id="254d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这种方法要求少数镜头任务和其他任务有很好的相关性。关于嵌入学习方法的更多细节可以在这篇文章中找到。</p><h2 id="2499" class="oq lj it bd lk or os dn lo ot ou dp ls ki ov ow lw km ox oy ma kq oz pa me pb bi translated"><strong class="ak">使用外部存储器学习:</strong></h2><p id="6621" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">这种方法从训练数据集中提取知识，并将其作为键-值对存储在外部内存中。使用嵌入函数“f”来嵌入一个示例，以形成该示例的密钥。然后，测试集中的每个新样本由从存储器中提取的内容的加权平均值来表示(权重基于测试示例与存储器中的密钥的嵌入的相似性)。这限制了由存储器中的内容表示的新样本，从而实质上减小了假设空间<strong class="jz iu"> H </strong>的大小。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pf"><img src="../Images/141d4d2a8eb5f68a835748c4111ac78e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLrKjFLzoe6mYGnnOwQHwQ.png"/></div></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">FSL with External Memory.</figcaption></figure><p id="b9b8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">来自训练集的数据首先使用函数<strong class="jz iu">‘f’</strong>嵌入，然后以键值格式存储在存储器中。在测试时，首先使用<strong class="jz iu">‘f’</strong>嵌入样本，然后基于相似性函数<strong class="jz iu">‘S’</strong>，选择最相似的关键字。提取并组合对应于所选键的值，以形成测试样本的表示。这然后被传递到分类器(简单地说是softmax函数),该分类器进行分类预测。这种方法使用额外的内存空间。</p><h1 id="cca0" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">算法—</h1><p id="b8e1" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">该策略在假设空间<strong class="jz iu"> H </strong>中搜索最佳假设<strong class="jz iu"> h* </strong>的参数<strong class="jz iu"> θ </strong>。本节中的方法使用先验知识来影响如何获得参数。先验知识通过提供良好的初始化(下图中的灰色三角形)或指导搜索步骤来改变搜索策略。因此，减少了h。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/0022fc570b126429dc9446abbe91115c.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*KOQ0M2a-r6X0IdUGCzWJqw.png"/></div><figcaption class="ob oc gj gh gi od oe bd b be z dk">Search for the best hypothesis using algorithm-based approaches.</figcaption></figure><p id="919d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">实现这一点有三种主要方法:</p><ol class=""><li id="86b6" class="mg mh it jz b ka kb ke kf ki ne km nf kq ng ku mn mo mp mq bi translated"><strong class="jz iu">细化现有参数</strong> —从其他任务学习到的初始参数(θo)(这些参数用作先验知识)使用少发任务的训练数据进行细化。</li><li id="372b" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated"><strong class="jz iu">细化元学习参数</strong> —初始化参数(θo)是从一组任务(即这些元学习参数作为先验知识)中元学习的，这些任务从与少量任务相同的任务分布中提取，然后使用少量任务的训练数据进一步细化</li><li id="00dc" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated"><strong class="jz iu">学习优化器</strong> —它学习一个元学习器作为优化器，直接输出每个学习器的搜索步骤，如改变搜索方向或步长。</li></ol><h1 id="b514" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论和参考:</h1><p id="7d1d" class="pw-post-body-paragraph jx jy it jz b ka mi kc kd ke mj kg kh ki nb kk kl km nc ko kp kq nd ks kt ku im bi translated">本系列文章涵盖了少镜头学习，这是一个相对较少开发的机器学习领域。讨论了各种方法及其利弊的简要概述。如果你打算进一步探索这里提到的算法，你可以访问<a class="ae kv" href="https://towardsdatascience.com/advances-in-few-shot-learning-a-guided-tour-36bc10a68b77" rel="noopener" target="_blank">这个</a>链接。</p><p id="99e5" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">参考文献</strong>:</p><ol class=""><li id="cad4" class="mg mh it jz b ka kb ke kf ki ne km nf kq ng ku mn mo mp mq bi translated">从几个例子中归纳:一个关于少投学习的调查。</li><li id="8167" class="mg mh it jz b ka mr ke ms ki mt km mu kq mv ku mn mo mp mq bi translated"><a class="ae kv" href="https://towardsdatascience.com/advances-in-few-shot-learning-a-guided-tour-36bc10a68b77" rel="noopener" target="_blank">少镜头学习的进步:一次有指导的旅行</a>。</li></ol><p id="b426" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">访问专家视图— </strong> <a class="ae kv" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="jz iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>