# 无标签数据终究不是那么没用吗？看看阿里巴巴在用它做什么

> 原文：<https://medium.datadriveninvestor.com/is-unlabeled-data-not-so-useless-after-all-see-what-alibabas-doing-with-it-34a49ba17bd0?source=collection_archive---------21----------------------->

阿里巴巴昵称为一只的齐翔。

![](img/a15272825cf9333616186258b37dfea7.png)

往往未标注的数据无法得到有效利用。这是行业的一大问题。为了解决这个问题，我们在阿里巴巴提出了一个名为 Auto Risk 的深度学习风险控制算法，以解决我们任何带有未充分标记数据和大量未标记数据的业务场景，这些数据无法得到有效利用。该算法针对行为序列数据。作为这项倡议的一部分，我们还提出了使用代理任务从无标签数据中学习一般特征。

我们的想法在许多方面都遵循预训练模型的相同路线，如自然语言处理领域中领先的来自变压器的双向编码器表示(BERT)模型。然而，行为序列数据和业务与自然语言处理中常见的数据截然不同。因此，我们模型的设计和实现必须是不同的。

我们的模型已经在真实的业务场景中实现，提供了一些真实世界的改进。实验验证表明，该模型的广泛功能可以适用于多种行业场景。此外，与纯监督学习相比，该模型在样本数量较少的情况下也有显著改善。

[](https://www.datadriveninvestor.com/2018/09/22/infographic-journey-to-the-clouds/) [## 信息图:云之旅|数据驱动的投资者

### 聪明的企业领导者了解利用云的价值。随着数据存储需求的增长，他们已经…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2018/09/22/infographic-journey-to-the-clouds/) 

就上下文而言，从淘宝购物者收集的浏览数据和支付宝中的风险控制事件等行为序列数据是阿里巴巴集团常见的数据类型。事实上，这种数据是我们提供的智能服务的重要来源，如产品推荐和风险控制算法。

考虑下面的例子。假设我们得到一个用户的交易序列，并被要求预测用户接下来会购买什么，或者我们得到一个风险控制事件的序列，并被要求预测一个产品是合法还是非法。事实是，对于这两种场景，我们都需要将一系列行为序列表征为向量，并对这些序列进行分类。这就是我们的算法发挥作用的地方。

![](img/e0e9d1729f89eba08f37527ff0a84522.png)

A behavior sequence diagram

传统上，触发器和累积等许多功能都是基于经验设计和手动输入的。然后，基于这些特征训练分类器，例如梯度推进决策树(GBDT)。近年来，一种相对成功的方法是使用神经网络，如递归(RNN)和卷积神经网络(CNN)，以及注意力机制。通过这些，行为序列被直接用作输入，而分类结果或特征向量是输出。这种方法本质上可以总结为一种一切向矢量的思想。它的优点是避免了人工表征的繁琐工作。

因此，正是沿着这些思路，我们的团队提出了算法的详细风险框架。该框架通过多个网络层将用户行为序列的数据转换为分类向量，其中涉及离散字段嵌入、文本卷积、多字段集成、事件卷积和注意力。我们已经在多个场景中成功实现了这个框架。总体而言，该框架大大减少了处理和使用数据所需的人工工作，并且还提高了模型的性能。

![](img/2f72c1d7f6b2f977cd0168a0250a0829.png)

Detail Risk framework diagram

然而，尽管取得了所有这些成功，这些方法中的大多数仍然使用监督学习，并且不能完全避免标记样本不足的问题。也就是说，少量的样本不能充分利用大容量，而大容量是神经网络模型的优势。但是，如果引入多任务标签，就需要仔细评估任务之间的迁移能力，并平衡这两个因素。

此外，我们的业务中不断积累大量未标记的数据。因此，如果我们能够找到一种方法来使用这些未标记的数据来训练模型并学习通用的上层特征，我们将能够将有限的标签留给下游场景来训练一个简单的分类器。这将最终极大地提高我们的整体数据利用率。还有一点，无监督学习中生成的特征向量不同于人工设计的特征，但两者的融合也有助于达到更好的效果。

# 预培训

类似的问题也存在于我们业务的其他部分。去年，基于自然语言处理研究开发了一个解决方案，具体是使用预训练技术的解决方案。预训练技术使用现成的代理任务，这些任务包含知识、大量未标记的数据以及更深的网络。所有这些都允许模型在没有任何人工帮助的情况下学习有效的上层特征。而且，有了这样的特性，整个系统在下游任务中进行微调后，可以达到更好的效果。

沿着这些思路，在 2018 年，预训练模型，如来自语言模型的嵌入(ELMo)、生成性预训练(GPT)、BERT、GPT2 和通过知识集成的增强表示(ERNIE)，不断重新定义基本自然语言处理问题的最先进(SOTA)模型，并推动了该领域的快速发展。在这些新模型中，BERT 一下子创造了 11 项新记录，这引起了算法工程师们的注意。

![](img/e352ae3d59e5f251be78f9e992a3c850.png)

The “AND” training diagram

在计算机视觉领域，用 ImageNet 预先训练大型网络可以追溯到 2014 年，当时深度学习刚刚开始被采用。在自然语言处理中，word2vec 或单词表示的全局向量(Glove)算法通常用于预训练单词向量。然而，直到最近，我们才能够预先训练像 BERT 这样的大型模型。这一变化意味着，即使使用未标记的样本，下游任务的性能也可以得到显著改善。我们认为这项技术之所以成为可能，主要是因为以下条件:

*   代理任务的积累:代理任务不是随机选取的，而是按难度升序由。比如 Cbow、SkipGram 等简单的 agent 任务先选，Masked-LM、下一句预测等难度大的任务留到最后。任务越困难，他们能获得的上层抽象知识就越多。这是前期训练的关键。
*   深度网络:大容量网络越深，网络能获得的上层特征越多，网络能存储的知识也越多。ResNet 和各种规范技术的出现，以及快捷、瓶颈、分支和 SBBB(batch norm)构建模式的出现，简化了深度网络的构建和训练。
*   注意:需要单独介绍注意。注意力为神经网络提供专用内存，并基于内存访问提供各种功能，包括对齐、组合、远程依赖和全局视觉。它可以丰富模型的表现能力。注意力的优势对于我们关注的序列数据尤为显著。
*   CNN 流的兴起:与 RNN 相反，CNN 最初并不用于序列数据。但是，CNN 提供了对并行结构的内在支持，非常适合工业应用，并且易于堆叠，适合构建深度模型。而且，它所缺乏的全球视野可以通过关注来提供。本质上，变形金刚也是一个 CNN 流，是一个宽度为 1 加自关注的卷积核。在过去的两年中，各种使用 CNN 流的工业框架，如 ConvS2S、ByteNet、WaveNet、SliceNet 和 Transformer，已经逐渐取代了 RNN。

![](img/55dc6e840b04ddf23f85e2498073b745.png)

The technology involved in BERT pre-training

# 问题分析

到目前为止，BERT 已经在阿里巴巴内部的一些 NLP 产品中使用，但没有在其他产品中使用。但是，有几个问题阻碍了它的广泛采用:

1.  数据格式不同。风险控制和推荐场景中的数据不是文本，而是行为序列。这意味着不能使用 Google 最初提供的预训练参数，因此我们的模型必须适应以下数据特征:每个时间 T 输入的多个字段，不同模式的字段，没有句子自然分隔的庞大序列长度。
2.  培训开销是巨大的。为 NLP 设计的模型是重量级的:一个变压器在每一层和十几层可以有多达 1600 万个参数，总共至少有 2 亿个参数。当单层的输入序列长度超过 1000 时，自我关注会导致内存不足(OOM)。即使内存足够，收敛速度也很慢。即使使用 8 个最新的图形处理单元(GPU)，训练时间也要几个月。这些模型在自然语言解析和合成方面是有价值的，但是对于行为序列数据来说是浪费的。

因此，为了从预培训中受益，我们必须根据我们的数据和业务的特点来设计和实施我们自己的预培训模型。本文介绍了我们为无监督行为序列设计和实现的预训练框架，并验证了其在实际业务场景中的有效性。我们的业务场景是风险控制，因此我们称这个框架为自动风险模型。

# 模型设计

# 代理任务

预训练模型不需要任何实际任务的标签，而只需要随时可用的代理任务来驱动训练。代理任务的设计决定了模型可以探索的知识。在我们的研究中，我们将行为序列数据与文本进行比较，将每个时间点 T 视为一个单词，将每个连续序列 1:T 视为一个文档，这与 BERT 类似。我们还设计了以下两种类型的代理任务:

![](img/99f334e25eb8bc573737ed1340ca2069.png)

Two types of agent tasks

*   用于单词和事件级代理任务的屏蔽语言模型:我们在时间 T 屏蔽输入序列的值，并要求模型输出覆盖 T 屏蔽值的单词级向量。该任务驱动模型探索序列前后的关联，并将行为置于上下文中进行研究。
*   快速思考句子级和序列级代理任务:我们通过采样将每个序列分成两个子序列，使用孪生网络将两个子序列中的每一个编码成一个向量，然后将子序列批量随机组合。然后，我们使用该模型来预测组合的子序列对是否具有相同的源。这个方法来源于 Skip Gram 的句子级概括:Skip 思想。然后，跳跃思维中的慢速编码器和解码器被快速孪生网络取代，这导致快速思维。快速思考任务驱动网络探索序列的符号特征。

# 网络结构

代理任务提供随时可用的标签，特定模型结构的核心是编码器网络。在上一节中，我们已经说明了直接使用大重量变压器是低效的，例如伯特和 GPT 模型中的变压器。因此，认识到这一点，我们提出了一种基于卷积和注意力的更有效的编码器结构。

*   在嵌入层，输入字段被转换成向量。例如，嵌入了事件类型、时间、金额、支付渠道和项目名称等字段。然后，通过 Add 或 Concat 函数整合这些字段。文本字段显示在列表中。因此，它们必须被嵌入，然后通过卷积或平均运算汇总成单个向量。
*   在卷积层，局部上下文被捕获，我们认为这是风险控制场景中行为序列的主要特征。因此，必须准确有效地捕捉本地上下文。
*   在注意层，捕获全局上下文，我们认为这是风险控制场景中行为序列的次要特征。更重要的是，全球背景提供了额外的视野和能力。
*   一个卷积层和一个关注层形成一个块。以 ResNet 格式堆叠的多个块将形成编码器网络，这类似于由多个堆叠的变压器形成的 BERT 模型。积木可以帮助我们获得抽象和合并的信息。

![](img/e318342279638a2d39b1576776c066d5.png)

Auto Risk model schematic

## 卷积层的改进

当卷积用于序列时，必须堆叠多个卷积层以增加视野。这导致了两个副作用:第一，梯度扩散，优化变得困难。第二，参数和计算显著增加。为了克服这些副作用，我们用两种特殊的卷积代替了一般的卷积:门控 Conv 和深度可分 Conv。首先，一种类似于长短期记忆(LSTM)的门机制被用来抑制梯度的扩散，以便可以堆叠更多的层。然后，卷积分为深度方向和点方向步骤，以减少参数和计算量。例如，如果特征维数 D 是 256 并且卷积核宽度 K 是 5，则参数和计算的数量将从 320，000 减少 80%到 60，000。如果卷积核宽度 K 为 31，则该数字将降至仅为原始数字的 3.6%。改进的卷积层显著提高了模型的收敛速度和最终性能。

![](img/2867155b200a73cfc4d1344c68330d5d.png)

Convolution improvement

## 注意力层的改进

注意力提供了极好的视野和能力，但需要大量的

![](img/4f260c69db49d7565d5eb2f058a06ef8.png)

视频存储器来执行序列之间的比较。在实践中，如果一个序列的长度超过 1000，这对一个行为序列来说并不长，一层的自我关注就会引起 OOM。出于实际原因，我们研究了用固定规模注意力或块注意力来代替自我注意力。这将内存使用优化到 O(2NK)

![](img/84a82bab800fa1cba668e224b134993a.png)

以轻微的性能下降为代价。最后，可以堆叠三个注意力层，允许我们在单个 GPU 上处理长度为 4000 的序列。这使我们能够满足我们的业务需求。

![](img/c53bc5697d9b392550ac02b5eb20e97f.png)

Attention improvement

训练如此庞大的网络需要许多技巧。我们将在后面的文章中讨论这些技巧。

# 培养

经过上述优化后，我们可以只使用一块显卡来:

*   在长度为 4000 的序列上训练一个堆叠了三层编码器的网络。在基于变压器的 BERT 模型中，只能对长度小于 1，000 的序列训练一层网络。
*   实现比 Transformer 快两到三倍的批量训练速度(对于更长的序列，这种优势更大)并且具有更少的收敛步骤，从而可以在一天内完成对数千万条数据记录的训练。

![](img/9dc46b31b9f9e197e7b0e127327a5fc6.png)

下图比较了使用不同编码器结构的训练过程，并显示:

*   同时使用卷积和注意力编码器比只使用卷积或注意力编码器要好。
*   卷积对损失的贡献更大，所起的作用比注意更重要。
*   当堆叠更多的块层时，框架的性能更好。

![](img/827ee8c85eeed3bfcd0b811f92038496.png)

# 申请结果

# 商业利益

首先，让我们评估汽车风险模型的商业利益。我们的风险控制序列包括登录、密码更改和交易等行为中涉及的关键风险控制行为。我们随机选取某些用户作为训练集，训练一个隐值为 128 的三层网络，然后推导出其他用户的向量。最后，我们将这些向量添加到特征池中，以比较曲线下面积(AUC)的改善。我们将比较以下方法:

*   手动功能的 SOTA:SOTA 手动功能包括资产能力和信誉在消费金融、信贷和后支付等场景中经过了良好的测试。
*   自动风险预训练:基于手动特征的 SOTA 添加原始自动风险向量。
*   AutoRisk finetune:根据手动功能的 SOTA，添加了针对特定场景进行微调的自动风险向量。

![](img/bd05c8d587d64b198be40ce437b64bd6.png)

正如您可能看到的，在添加自动风险向量后，AUC 从 3%提高到 6%，这表明无监督自动风险模型可以从行为序列中提取有用的特征。如果针对特定场景对网络参数进行微调，将会获得更好的结果。这与 BERT 等模型中的情况相同。为了便于比较，该图仅显示了用作序列数据的风险控制事件的影响。

# 多场景性能

预训练模型在训练期间不为任何特定场景使用标签。所以模型学到的知识本质上是比较笼统的。我们使用最简单的逻辑回归(LR)分类器测试了不同的场景，包括不相关的性别和年龄预测场景，而没有添加任何手动功能或进行微调。在对场景进行训练和测试之后，我们获得了令人惊讶的结果。在某些情况下，AUC 达到 0.9。一个潜在的商业利益是，我们可以以极低的成本获得各种业务的通用补充功能。

![](img/1c2f35a5320fefbbd95147b8eed4ae20.png)

但是，仅仅使用 LR 怎么能达到这样的效果呢？一种思考方式是，自动风险模型在一个良好的嵌入空间中完全保留了行为序列的信息。这使得我们可以为不同的任务找到合适的线性分类接口。下图显示了消费金融场景中提现的测试集样本和分类平面。由于 Umap 用于将 128 维向量缩减为三维向量，分类性能降低了约 6%的 AUC。然而，我们可以看到:

1.  套现和非套现商户有明确的分类平面。
2.  空间中有明显的流结构。虽然我们没有分析每个聚类的意义，但我们可以肯定，聚类中的点具有相似的行为模式。

![](img/d1ef190c2f8efa61824b229b0305fcdd.png)![](img/c2d54cc73124c256e5ae41a3b7c64a07.png)

# 小样本学习

预训练模型在小样本场景中也有好处。由于深度学习模型中的参数数量很大，当标记样本数量较少时，它们的性能并不好。然而，预训练模型通过无监督的代理任务来学习其大部分知识。因此，对于具有少量标记样本的场景，我们可以获得更好的结果。预培训模式更适合需要冷启动或标签费用昂贵的企业。我们在后支付场景 b 中对两种类型的行为序列进行了实验。结果表明，自动风险模型可以取得比从头训练监督学习神经网络更好的结果。对于行为日志数据，即使我们不执行微调，简单地使用自动风险模型和 LR 分类器也可以获得比监督学习更好的结果。当训练集中的标记样本数量达到 40，000 个，其中有 20，000 个正样本和 20，000 个负样本时，监督学习仍然赶不上自动风险模型和微调。

![](img/7725a7f92822041a2a032afd79ddf568.png)

# 序列类比

类比是单词嵌入的一个有趣特征。在嵌入空间中，*国王—男人* = *王后—女人*和*中国—北京=法国—巴黎*。类似的方程证明了嵌入空间确实可以捕获上层语义。我们的汽车风险空间中的序列有相似的特征吗？同样，我们进行了一个 *A — B* = *C — D* 实验。我们从包含一百万个样本的集合中选择 *A* 、 *B* 和 *D* ，并通过使用与 *A — B + D* 向量的余弦相似度来召回 *C* 。为了便于描述，我们将不同的字段分开显示，即使它们是同时训练的。

*   事件类型: *A* (淘宝上的蚂蚁金服支付)——*B*(阿里巴巴账户余额支付)= *C* (阿里巴巴网站外的蚂蚁金服支付)——*D*(非阿里巴巴账户余额支付)。淘宝支付方式的差异向量由*A-B*获得。当差异向量添加到 *D* 时，支付方式从余额变为蚂蚁信用支付。这表明由模型学习的嵌入空间中的某些方向专门用于记录支付方法。

```
A=[Create transaction - Taobao physical guarantee, Ant Credit Pay payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee, Ant Credit Pay payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee, Ant Credit Pay payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee, Ant Credit Pay payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee]
B=[Create transaction - Taobao physical guarantee, Balance payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee Balance payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee, Balance payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee, Balance payment - Taobao physical guarantee, Create transaction - Taobao physical guarantee]
C=[Ant Credit Pay payment - Instant transfer from non-Alibaba account, Ant Credit Pay payment - Instant transfer from non-Alibaba account, Ant Credit Pay payment - Instant transfer from non-Alibaba account, PC side - Create transaction, Logon_app_other_, Ant Credit Pay payment - Instant transfer from non-Alibaba account, PC side - Create transaction]
D=[App side - Logon, Balance payment - Instant transfer from non-Alibaba account, Balance payment - Instant transfer from non-Alibaba account, Balance payment - Instant transfer from non-Alibaba account, Balance payment - Instant transfer from non-Alibaba account, Balance payment - Instant transfer from non-Alibaba account]
```

*   金额: *A* (大额用户)——*——B*(小额用户)=*——C*(大额用户)——D(小额用户)。 *C* 的召回值为 8000 和 10000，与 *A* 的召回值相同。这证明该模型对数值分级具有良好的记忆性。

```
A=[\\N,\\N,10000.0,10000.0,10000.0,8000.0,8000.0,8000.0,\\N,8000.0,\\N,\\N,8000.0,\\N,8000.0]
B=[\\N,\\N,10.0,10.0,10.0,10.0,10.0,10.0,\\N,10.0,\\N,\\N,10.0]
C=[\\N,\\N,\\N,8000.0,\\N,\\N,8000.0,\\N,\\N,\\N,\\N,\\N,\\N,10000.0,\\N,10000.0]
D=[\\N,1.0,\\N,1.0,\\N,1.0,1.0,\\N,1.0,\\N,1.0,1.0,\\N,1.0,\\N,\\N]
```

*   物品名称: *A* (经常打车的用户)——*B*(经常用硬币充值的用户)= *C* (经常打车的用户)——*D*(经常用硬币充值的用户)。注意，物品名称字段不同于其他字段，必须通过 CNN 或平均子网转换成向量。此外，“公交车票”和“出行时间”等功能也被召回。这些字段与“滴滴出行快递”相似但不相同。这证明网络能够概括文本描述。

```
A=["DiDi Express-Driver Zhou","DiDi Express-Driver Zhou",...,"DiDi Express-Driver Shao","DiDi Express-Driver Shao",...]
B=["Tencent QQ coins recharged by 100 RMB","Tencent QQ coins recharged by 100 RMB",...,"Tencent QQ coins recharged by 100 RMB","Tencent QQ coins recharged by 100 RMB",...]
C=["DiDi Express-Driver Feng","DiDi Express-Driver Feng","Bus ticket**Bus terminal (south district","","Bus ticket**Bus terminal (south district)","","Quick Unicom recharge of 10 RMB","","","DiDi Express-Driver Qi","DiDi Express-Driver Qi","","","0000****-Plate no. [00*****] Travel time: 2019-04-1914:53:02"]
D=["1000 Tencent QQ coins 1","Tencent QQ coins/QQ coin card/",...,"Tencent QQ coins/QQ coin card/","Tencent QQ coins/QQ coin card/",...]
```

# 摘要

在本文中，我们讨论了我们用于行为序列深度学习的自动风险算法。这种算法不需要特定的标签进行训练，而是基于类似于 BERT 的 agent 任务预训练的概念。它探索大量未标记数据中的上下文关联和符号特征，以生成有用的上层特征。这解决了标记样本不足和未标记样本难以使用的问题。

我们根据数据和业务特征设计了模型结构，以便于快速训练和部署。我们已经在实际业务操作中实现了该算法，并取得了显著的性能改进。培训期间不需要贴标签。因此，模型结果可以应用于许多其他场景，并可以在样本数量较少的场景中显著提高性能。序列类比实验证明，在自动风险向量空间中可以捕获上层语义。

未来，我们将继续当前的工作，扩展模型以适应更多类型的数据源和应用场景，并验证其他代理任务，包括使用之前积累的多场景标签的代理任务。

# 原始来源:

[](https://www.alibabacloud.com/blog/is-unlabeled-data-not-so-useless-after-all-see-what-alibabas-doing-with-it_595801?spm=a2c41.14001948.0.0) [## 无标签数据终究不是那么没用吗？看看阿里巴巴在用它做什么

### qixiang.qx 年 2 月 6 日 767 作者齐翔，在阿里巴巴的昵称是一只。通常未标记的数据不能有效地…

www.alibabacloud.com](https://www.alibabacloud.com/blog/is-unlabeled-data-not-so-useless-after-all-see-what-alibabas-doing-with-it_595801?spm=a2c41.14001948.0.0)