# BANDIT ç®—æ³•â€”ä»å¤´å¼€å§‹å®æ–½

> åŸæ–‡ï¼š<https://medium.datadriveninvestor.com/bandit-algorithm-implemented-from-scratch-38542cedafb3?source=collection_archive---------3----------------------->

## çªƒå–æœ€å¤šçš„å¥–åŠ±ğŸ‘€ğŸ†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ 

![](img/a7283aafdf98dac47ab7df7e73bd265e.png)

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å›é¡¾äº†å¼ºåŒ–å­¦ä¹ å’Œ BANDIT ç®—æ³•çš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶è§£é‡Šäº†ä»£ç å®ç°ã€‚å¦‚æœæ‚¨æƒ³ç›´æ¥è·³åˆ°ä»£ç å®ç°ï¼Œè¯·è·³è¿‡ç¬¬ä¸€éƒ¨åˆ†ã€‚

# èƒŒæ™¯

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨èµŒåœºç©æ¸¸æˆï¼Œä½ æœ‰å¤šå°è€è™æœºï¼Œä½ å¯ä»¥å¯¹å†²ä½ çš„èµŒæ³¨ï¼Œæ¯”å¦‚è¯´ 10 å°æœºå™¨ã€‚

ä½ å¦‚ä½•çŸ¥é“ä»å“ªäº›æœºå™¨å¼€å§‹ï¼Ÿå¦‚æœä½ çš„ç›®æ ‡æ˜¯å›æŠ¥æœ€å¤§åŒ–ï¼Œä½ æ€ä¹ˆçŸ¥é“é€‰æ‹©å“ªå°æœºå™¨å‘¢ï¼Ÿ

è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è¯´çš„å¤šè‡‚åœŸåŒªé—®é¢˜ã€‚

å¼ºåŒ–å­¦ä¹ æœ¬èº«é€šå¸¸æ˜¯å…³äºæ‹¥æœ‰ä¸€ä¸ª**ä»£ç†**ï¼Œä¸å®ƒçš„**ç¯å¢ƒ**äº’åŠ¨ã€‚é€šè¿‡ä¸ç¯å¢ƒäº’åŠ¨ï¼Œå®ƒæ¥æ”¶åé¦ˆï¼Œç„¶åå¯ä»¥åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥éª¤ä¸­ä½¿ç”¨è¿™äº›åé¦ˆæ¥æ›´å¥½åœ°åšå‡ºå†³ç­–ã€‚

åœ¨ BANDIT é—®é¢˜ä¸­ï¼Œæ¯å°åƒè§’å­è€è™æœºéƒ½æœ‰ä¸€ä¸ªå…·æœ‰ç‰¹å®šæ–¹å·®(æ ‡å‡†åå·®)çš„å¹³å‡å¥–åŠ±ï¼Œå› æ­¤æ¯æ¬¡å¥–åŠ±éƒ½ä¸ä¼šç›¸åŒï¼Œä½†ç®—æ³•åº”è¯¥çŸ¥é“å“ªäº›æœºå™¨å¹³å‡æ¯”å…¶ä»–æœºå™¨å¥½ï¼Œå®ƒå¯ä»¥ä½¿ç”¨è¿™ä¸ªå¹³å‡å€¼ä½œä¸ºä»ç‰¹å®šåƒè§’å­è€è™æœºè·å¾—çš„æœªæ¥å¥–åŠ±çš„æŒ‡æ ‡ã€‚

[](https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/) [## AI å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿæ•°æ®é©±åŠ¨çš„æŠ•èµ„è€…

### è¿™ä¸¤ä¸ªä¸»é¢˜èƒŒåæœ‰å¾ˆå¤šä»¤äººå…´å¥‹çš„ä¸œè¥¿ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¿«é€ŸæŒ‡å—ï¼Œä»‹ç»äº†å®ƒä»¬æ˜¯ä»€ä¹ˆä»¥åŠå®ƒä»¬æœ‰ä»€ä¹ˆâ€¦

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/) 

ç„¶è€Œï¼Œå¦ä¸€ä¸ªé—®é¢˜å‡ºç°äº†â€”â€”å‡è®¾å¼ºç›—ä¸€æ¬¡åˆä¸€æ¬¡åœ°åšæŒåŒä¸€ä¸ªè€è™æœºï¼Œåªæ˜¯å› ä¸ºå®ƒçŸ¥é“è€è™æœºåœ¨è¿„ä»Šä¸ºæ­¢æ¢ç´¢çš„æ‰€æœ‰è€è™æœºä¸­ç»™å‡ºäº†æœ€é«˜çš„ä»·å€¼ã€‚å¦‚æœæœ‰å¦ä¸€ä¸ªåƒè§’å­è€è™æœºå®é™…ä¸Šç»™å‡ºäº†æ›´é«˜çš„å¥–åŠ±ï¼Œä½†ç®—æ³•æ²¡æœ‰ç ”ç©¶å®ƒï¼Œå› ä¸ºå®ƒè®¤ä¸ºåšæŒä½¿ç”¨å®ƒå·²ç»ç†Ÿæ‚‰çš„åƒè§’å­è€è™æœºä¼šäº§ç”Ÿæ›´é«˜çš„å¥–åŠ±ï¼Ÿ

è¿™å°±æ˜¯æ‰€è°“çš„**æ¢ç´¢ä¸åˆ©ç”¨**é—®é¢˜â€”â€”æ¢å¥è¯è¯´ï¼Œæ˜¯åšæŒç†Ÿæ‚‰çš„è¡Œä¸ºå¥½ï¼Œè¿˜æ˜¯æŠ±ç€æ›´é«˜å›æŠ¥çš„å¸Œæœ›å»æ¢ç´¢æ–°çš„è¡Œä¸ºå¥½ï¼Ÿ

ä½¿ç”¨åœŸåŒªé‡‡å–çš„è¡ŒåŠ¨çš„æ•°å€¼ä»¥åŠä¼°è®¡çš„å›æŠ¥å’Œå‚æ•°æ¥ç®¡ç†æ¢ç´¢å’Œå¼€å‘ä¹‹é—´çš„æƒè¡¡ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åœ¨ä»£ç ä¸­æ¨¡æ‹ŸåœŸåŒªé—®é¢˜ã€‚

# ä»£ç å®ç°

è®©æˆ‘ä»¬ä»‹ç»ä¸€äº›æœ¯è¯­ã€æ•°å­¦ç¬¦å·å’Œä»£ç ï¼Œä½¿æ‰€æœ‰è¿™äº›æ›´å…·ä½“ä¸€äº›ã€‚

*Qt(a)* è¡¨ç¤ºåœ¨æ—¶é—´æ­¥é•¿ *t* ç»™å®šè¡ŒåŠ¨çš„é¢„æµ‹å¥–åŠ±ï¼Œè€Œ q*(a)è¡¨ç¤ºè¡ŒåŠ¨ a çš„å®é™…å¹³å‡å¥–åŠ±ã€‚Rt è¡¨ç¤ºåœ¨æ—¶é—´æ­¥é•¿ t çš„å¥–åŠ±

æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è·å¾—ç²¾ç¡®æµ‹é‡ Qt(a)çš„æœ€ä½³æ–¹æ³•ã€‚è¿™æ˜¯é€šè¿‡é‡‡å–è¡ŒåŠ¨æ—¶è·å¾—çš„æ‰€æœ‰å…ˆå‰å¥–åŠ±çš„å¹³å‡å€¼æ¥å®Œæˆçš„ã€‚ä¸ºäº†ä¼˜åŒ–å†…å­˜ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯é€šè¿‡å¯¹æ—¶é—´æ­¥é•¿ t-1 ä¸­çš„åŠ¨ä½œè¿›è¡Œä¼°è®¡æ¥è®¡ç®—æ—¶é—´æ­¥é•¿ t çš„å¹³å‡å€¼ï¼Œç„¶åå°†ä¼°è®¡å€¼å’Œå®é™…å¥–åŠ±ä¹‹é—´çš„å·®å€¼ç›¸åŠ ï¼Œå†é™¤ä»¥å·²ç»é€šè¿‡çš„è¯•éªŒæ¬¡æ•°ï¼Œè¿™ä¸è®¡ç®—å¹³å‡å€¼åŸºæœ¬ç›¸åŒã€‚

å€¼ epsilon å†³å®šäº†æ¨¡å‹æ¢ç´¢çš„æ—¶é—´æ¯”ä¾‹(å®ƒè¿½æ±‚éšæœºåŠ¨ä½œ)å’Œå®ƒåˆ©ç”¨çš„æ—¶é—´é‡(å®ƒé€‰æ‹©å…·æœ‰æœ€é«˜ä¼°è®¡å›æŠ¥çš„åŠ¨ä½œ)ã€‚

è®©æˆ‘ä»¬ç”¨ä»£ç æ¥çœ‹çœ‹è¿™ä¸ªã€‚

ä¸‹é¢æ˜¯æˆ‘ä¸ºä¸€ä¸ªå¼ºç›—ä»å¤´å¼€å§‹æ„å»ºçš„ä¸€äº›ä»£ç ã€‚è¯·åœ¨è¿™é‡Œéšæ„æŸ¥çœ‹æˆ‘çš„[å…¬å…± github åº“](https://github.com/MukundhMurthy/BartoSutton-RL-Resources)ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ª Jupiter ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¯•éªŒä¸åŒçš„å‚æ•°å¹¶å¯è§†åŒ–æ—¶é—´æ­¥é•¿â€”â€”å¥–åŠ±å›¾ã€‚

è¯·éšæ„æµè§ˆè¿™æ®µä»£ç ï¼Œå¹¶å°è¯•åº”ç”¨æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢åœ¨æœ¬æ–‡ä¸­è®¨è®ºçš„å†…å®¹â€”â€”ä½†æ˜¯å¦‚æœä¸€åˆ‡éƒ½æ²¡æœ‰æ„ä¹‰ï¼Œä¹Ÿä¸è¦æ‹…å¿ƒã€‚æˆ‘ä¼šä¸€æ¬¡è¿‡å‡ è¡Œã€‚

```
def bandit(stationary, num_steps, num_runs, k=10, epsilon=0.05, Q_init_value=0, std_dev = 0.01, act_val_method = 'Sample-average', alpha = 0.1, UCB = False, c = 2):
    reward_runs_list = []
    optimal_runs_list = []
    for j in range(num_runs):
        Q_a = empty_tensor.new_full([k], Q_init_value)
        rewards = torch.normal(torch.zeros(k), torch.ones(k))
        N_a = torch.zeros(k)
        reward_val = []
        opt_val = []
        max_action_val = torch.argmax(rewards).item()
        assert act_val_method in ['Sample-average', 'Constant-step-size', None]
        print ("Run: {0} Reward List: {1}".format(j, rewards))
        for i in range(num_steps):
            if not stationary:
                rewards += torch.normal(torch.zeros(k), empty_tensor.new_full([k], std_dev))
            if UCB:
                quotient = torch.sqrt(torch.from_numpy(np.array((np.log(j+1))/((N_a+1).numpy()))))
                max_action = torch.argmax(Q_a + (c * quotient))
                non_changed = (Q_a==Q_init_value).nonzero()
                if len(non_changed)>1:
                    max_action = np.random.choice((Q_a==Q_init_value).nonzero().squeeze().numpy())
            elif decision(epsilon)==False:
                max_action = torch.argmax(Q_a).item()
                non_changed = (Q_a==Q_init_value).nonzero()
                if len(non_changed)>1:
                    max_action = np.random.choice((Q_a==Q_init_value).nonzero().squeeze().numpy())
            else:
                max_action = np.random.randint(0, k)
            reward = torch.normal(rewards[max_action], 1)
            times_seen = N_a[max_action] 
            estimate = Q_a[max_action]
            N_a[max_action] += 1
            if (act_val_method == 'Constant-step-size'):
                step_param = alpha
            else:
                step_param = (1/(times_seen+1))
            Q_a[max_action] += (reward - estimate)*(step_param)
            reward_val.append(reward.item())
            opt_val.append(max_action == max_action_val)
            print ("Step {0}: Action chosen:{1}   Reward:{2:.2f}    Estimate: {3:.2f}".format(i, max_action, reward.item(), estimate))
        reward_runs_list.append(reward_val)
        optimal_runs_list.append(opt_val)
    reward_runs_array = np.array(reward_runs_list)
    avg_reward_per_step = np.mean(reward_runs_array, axis=0)
    opt_arr = np.array(optimal_runs_list, dtype="bool")
    percent_opt_arr = np.sum(opt_arr, axis=0)/num_runs
    return (avg_reward_per_step, percent_opt_arr)
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å‡½æ•°å¤´ï¼Œå¯¹æ‰€æœ‰æ¶‰åŠçš„å‚æ•°æœ‰ä¸€ä¸ªæ¸…æ¥šçš„äº†è§£ã€‚

```
def bandit(stationary, num_steps, num_runs, k=10, epsilon=0.05, Q_init_value=0, std_dev = 0.01, act_val_method = 'Sample-average', alpha = 0.1, UCB = False, c = 2):
```

æ¶‰åŠçš„å‚æ•°æœ‰

*   *å›ºå®š* â€”è¿™ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå®ƒå†³å®šäº†å¥–åŠ±æ˜¯ä¸å˜çš„ï¼Œè¿˜æ˜¯åœ¨å¼ºç›—æœç´¢æ—¶ä¼šæ”¹å˜ã€‚ä½ å¯èƒ½ä¼šæƒ³è±¡ä¸ç¨³å®šçš„å€¼ä¼šä½¿å¼ºç›—æ›´éš¾ä¼˜åŒ–å¥–åŠ±ã€‚
*   *k â€”* åŒªå¾’æ‹¥æœ‰çš„æ­¦å™¨æ•°é‡(æœ¬è´¨ä¸Šæ˜¯å¯èƒ½è¡ŒåŠ¨çš„æ•°é‡)
*   *num_steps* â€”å¼ºç›—é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œçš„æ¬¡æ•°
*   *num_runs* â€”åˆå§‹åŒ–å¹¶è¿è¡Œäº† *num_steps* çš„ä¸åŒ k-armed åœŸåŒªçš„æ•°é‡
*   *epsilon* â€”å†³å®šå¼ºç›—æ¢ç´¢çš„å°éƒ¨åˆ†æ—¶é—´çš„å‚æ•°
*   *Q_init_value* â€”æ¯ä¸ªåŠ¨ä½œçš„å¥–åŠ±çš„åˆå§‹åŒ–ä¼°è®¡å€¼ã€‚å°†åˆå§‹å€¼è®¾ç½®å¾—æ›´é«˜å°†å¯¼è‡´æ›´å¤šçš„æ¢ç´¢ã€‚åœ¨ç¬¬ä¸€æ¬¡å¯¹æ¯ä¸ªç‚¹è¿›è¡Œé‡‡æ ·åï¼Œä¼°è®¡å€¼ä¼šæ€¥å‰§ä¸‹é™â€”â€”ä½ å¯ä»¥è®¤ä¸ºè¿™æ˜¯å¼ºç›—å¤±æœ›äº†ã€‚å®ƒä¼šåœ¨é€‰æ‹©å‡ ä¸ªç‚¹ä¹‹å‰ï¼Œå¯¹å‡ ä¹æ‰€æœ‰çš„ç‚¹è¿›è¡Œé‡‡æ ·ã€‚

å¦‚æœå›æŠ¥æ˜¯ä¸ç¨³å®šçš„

*   *std_dev* â€”å¥–åŠ±åœ¨æ—¶é—´æ­¥é•¿ä¹‹é—´å˜åŒ–çš„æ•°é‡

Act-val æ–¹æ³•â€”è¿™äº›æ–¹æ³•åœ¨æ”¶åˆ°ç»™å®šè¡Œä¸ºçš„å¥–åŠ±åæä¾›è¯„ä¼°æ›´æ–°ã€‚æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•â€”â€”ä¸€ç§æ˜¯æ ·æœ¬å¹³å‡æ³•ï¼Œå¦ä¸€ç§æ¶‰åŠæ’å®šæ­¥é•¿Î±ã€‚

*   æ ·æœ¬å¹³å‡å€¼â€”â€”å°†å¥–åŠ±ç›¸åŠ ï¼Œç„¶åé™¤ä»¥å¼ºç›—çœ‹åˆ°å®ƒä»¬çš„æ¬¡æ•°(ç”¨ N_a è¡¨ç¤º)
*   æ’å®šæ­¥é•¿â€”â€”ä¸æ˜¯é™¤ä»¥ N_aï¼Œè€Œæ˜¯å°†*(ä¼°è®¡-å›æŠ¥)*å€¼ä¹˜ä»¥æ’å®šæ­¥é•¿Î±ã€‚è¿™å…è®¸è¾ƒæ—©çš„å¥–åŠ±æ¯”æœ€è¿‘çš„å¥–åŠ±å°‘è®¡ç®—(åœ¨ä¸ç¨³å®šçš„ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰ç”¨)

# ä¸€ä¸ªç¤ºä¾‹å®éªŒ

è¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹å®éªŒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ BANDIT å‡½æ•°æ¥è¿›è¡Œâ€”â€”æ‚¨å¯ä»¥çœ‹åˆ°åˆå§‹ Q å€¼å¯¹æœ€ç»ˆå¥–åŠ±å¯è§†åŒ–çš„å½±å“ã€‚

![](img/5df340c379a850f17c37325fbce33e83.png)

åœ¨è¿™é‡Œï¼Œæˆ‘æµ‹è¯•äº†ä¸¤ä¸ªä¸åŒ Q å€¼çš„å¹³å‡å›æŠ¥å›¾å’Œæœ€ä¼˜é€‰æ‹©å›¾ä¹‹é—´çš„å·®å¼‚ã€‚æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼ŒQ=0 çš„æ›²çº¿å˜å¾—æ›´å¿«ï¼Œä½†è¾¾åˆ°äº†æ›´ä½çš„æœ€ä½³é€‰æ‹©ç™¾åˆ†æ¯”æœ€å¤§å€¼(Q=0 æ—¶ä¸º 70%ï¼ŒQ=5 æ—¶ä¸º 90%)ã€‚

ä»æ¦‚å¿µä¸Šè®²ï¼Œè¿™æ˜¯å› ä¸ºè¾ƒå¤§çš„ Q å…è®¸æ›´å¤šçš„æ¢ç´¢ã€‚

è¯·éšæ„ç”¨è¿™ä¸ªå·¥å…·åšè‡ªå·±çš„å®éªŒï¼Œè·å¾—å¼ºåŒ–å­¦ä¹ åŸºç¡€çš„ç›´è§‰ï¼

*å˜¿ï¼æˆ‘æ˜¯ Mukundh Murthyï¼Œ16 å²ï¼Œå¯¹æœºå™¨å­¦ä¹ å’Œè¯ç‰©å‘ç°çš„äº¤å‰é¢†åŸŸå……æ»¡çƒ­æƒ…ã€‚æ„Ÿè°¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼å¸Œæœ›ä½ è§‰å¾—æœ‰å¸®åŠ©:)*

*éšæ—¶æŸ¥çœ‹æˆ‘åœ¨ Medium ä¸Šçš„* [*å…¶ä»–æ–‡ç« *](https://medium.com/@mukundh.murthy) *å’Œæˆ‘åœ¨*[*LinkedIn*](https://www.linkedin.com/in/mukundhmurthy/)*ä¸Šè”ç³»ï¼*

å¦‚æœä½ æƒ³è®¨è®ºä»¥ä¸Šä»»ä½•è¯é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä¸ä½ è”ç³»ï¼å®é™…ä¸Šï¼Œæˆ‘ç›®å‰æ­£åœ¨å°è¯•ä¼˜åŒ– ML æ¨¡å‹ï¼Œä»¥é¢„æµ‹å† çŠ¶ç—…æ¯’è›‹ç™½è´¨é¶ç‚¹(N è›‹ç™½)çš„é«˜æ•ˆé€‚ä½“è¯ç‰©ã€‚(åœ¨ mukundh.murthy@icloud.com ç»™æˆ‘å‘é‚®ä»¶æˆ–è€…åœ¨[*LinkedIn*](https://www.linkedin.com/in/mukundhmurthy/)*ä¸Šç»™æˆ‘å‘ä¿¡æ¯)å¦å¤–ï¼Œè¯·éšæ—¶æŸ¥çœ‹æˆ‘çš„ç½‘ç«™ï¼Œç½‘å€æ˜¯*[*ã€mukundhmurthy.com*](http://mukundhmurthy.com/)*ã€‚*

*å¦‚æœä½ æœ‰å…´è¶£å…³æ³¨æˆ‘çš„è¿›å±•ï¼Œè¯·åœ¨è¿™é‡Œ* *æ³¨å†Œæˆ‘çš„æ¯æœˆç®€è®¯*[](http://eepurl.com/gImYNX)

***è®¿é—®ä¸“å®¶è§†å›¾â€”** [**è®¢é˜… DDI è‹±ç‰¹å°”**](https://datadriveninvestor.com/ddi-intel)*