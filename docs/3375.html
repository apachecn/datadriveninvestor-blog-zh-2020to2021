<html>
<head>
<title>Trump tweets: Topic Modeling using Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特朗普推文:使用潜在狄利克雷分配的主题建模</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/trump-tweets-topic-modeling-using-latent-dirichlet-allocation-e4f93b90b6fe?source=collection_archive---------0-----------------------#2020-06-15">https://medium.datadriveninvestor.com/trump-tweets-topic-modeling-using-latent-dirichlet-allocation-e4f93b90b6fe?source=collection_archive---------0-----------------------#2020-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bfabd49fe6860970376330b2fc5d3569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KceHm-C9n1mCAviuLqwDow.png"/></div></div></figure><div class=""/><p id="7bde" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我将关注主题建模，这是自然语言处理的一个广为人知的应用。阅读完本主题后，您将能够使用Python进行主题建模。</p><p id="57f8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">什么是话题建模(TM): </strong></p><p id="64cd" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一种无监督的ML技术，因为文本数据没有附加任何标签。听起来，TM旨在分析和发现文本数据集合(文本、推文、电子邮件、书籍等)中的见解和主题。</p><p id="8a1b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">几种常用的方法主要可用于TM:</p><ul class=""><li id="e3f5" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">潜在狄利克雷分配</li><li id="d87e" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">潜在语义分析(LSA)</li><li id="fdaa" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">非负矩阵分解</li></ul><p id="0b53" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我的例子中，我将在这篇文章中只关注潜在的狄利克雷分配(LDA)。</p><p id="7a2d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">潜在狄利克雷分配:</strong></p><p id="8305" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc"> <em class="lk">潜在狄利克雷分配</em> </strong>试图找到输入数据中隐藏分布的概率，因为文本数据可以混合主题和见解。</p><p id="8b8c" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从功能的角度来看，LDA是一个“单词包”,这意味着单词顺序并不重要，但它假定:</p><ul class=""><li id="7897" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">具有相似单词的文档通常具有相同的主题</li><li id="4c04" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">包含频繁出现的单词组的文档通常具有相同的主题。</li></ul><p id="fe26" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在LDA中，观察被称为文档(在我的例子中是tweets内容)，特征集被称为词汇/单词，结果类别被称为主题。</p><p id="2493" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">特朗普推文处理:</strong></p><p id="6ca0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这一节中，我将展示如何使用Python来实现主题建模的LDA。数据集可以从<a class="ae ll" href="https://www.kaggle.com/austinreese/trump-tweets" rel="noopener ugc nofollow" target="_blank"> Kaggle下载。</a></p><p id="1987" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">数据集包含tweets内容，这些内容将通过LDA用于将tweets分组为10个主题(类别)。</p><p id="7c6e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我首先接收数据并检查数据帧，以检测任何异常情况:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="89dc" class="lv lw jb lr b gy lx ly l lz ma">df = pd.read_csv('realdonaldtrump.csv', encoding='UTF-8')<br/>print(len(df), 'tweets')</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/774546204349905c2226cf1829caf8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*wjCsntrC9ZQMsFd97vMxdw.png"/></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="9abd" class="lv lw jb lr b gy lx ly l lz ma">df.head()</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mc"><img src="../Images/b7fe5316cf34f85c8f7be16a8259d5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOv28p9IUsDkt1mIk7RVqA.png"/></div></div></figure><p id="b6ab" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如上所述，在处理、清理和LDA应用期间，我将重点放在内容列上，其余的列将被忽略。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="1461" class="lv lw jb lr b gy lx ly l lz ma">tweets_df=df.loc[:,['content']]<br/>tweets_df.info()</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi md"><img src="../Images/9ee4c332040300e1cb61f4dda2f7b331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ver-SVFOzR5df2qYASUjUg.png"/></div></div></figure><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="f4ac" class="lv lw jb lr b gy lx ly l lz ma">a = 42270<br/>for i in range(a,a+10):<br/>    print(tweets_df.content[i])<br/>    print()</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/3eaf981081ab620c2683d8270a27edc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mMypWH8s3CKehHE01CKh9Q.png"/></div></div></figure><p id="2b6e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">数据清理:</strong></p><p id="14c2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在任何机器学习任务中，清理数据与建模一样重要，甚至更重要。当涉及到像文本这样的非结构化数据时，这个过程就更加重要了。</p><p id="23c1" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，我将执行一些清理活动，作为建模前的预处理步骤:</p><ul class=""><li id="721a" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated">小写文本</li><li id="cc3e" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">使用正则表达式删除括号</li><li id="9c0e" class="kw kx jb ka b kb lf kf lg kj lh kn li kr lj kv lb lc ld le bi translated">使用正则表达式删除标点和数字</li></ul><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a33f" class="lv lw jb lr b gy lx ly l lz ma">def clean_text(text):<br/>''', , and '''<br/>#Make text lowercase   <br/> text = text.lower()<br/>#remove text in square brackets<br/> text = re.sub(r'\[.*?\]', '', text)<br/>#remove punctuation   <br/> text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) <br/>#remove words containing numbers<br/> text = re.sub(r'\w*\d\w*', '', text)<br/> return text</span><span id="3203" class="lv lw jb lr b gy mf ly l lz ma">tweets_df_clean = pd.DataFrame(tweets_df.content.apply(lambda x: clean_text(x)))</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mg"><img src="../Images/579cb5a1f6e707487f1f71ca076d8960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDZ-vBkFE8iMb1QbSddOsw.png"/></div></div></figure><p id="a663" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一旦我的数据不包含任何不想要的字符，我将执行词汇化，以减少词根词的词形变化。</p><p id="5241" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我选择使用spaCy进行词汇化</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="ac23" class="lv lw jb lr b gy lx ly l lz ma">nlp = spacy.load("en_core_web_sm")<br/>def lemmatizer(text):        <br/>    sent = []<br/>    doc = nlp(text)<br/>    for word in doc:<br/>        sent.append(word.lemma_)<br/>    return " ".join(sent)<br/>tweets_df_clean = pd.DataFrame(tweets_df_clean.content.apply(lambda x: lemmatizer(x)))<br/>tweets_df_clean['content'] = tweets_df_clean['content'].str.replace('-PRON-', '')</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mh"><img src="../Images/15a34bb062a8fd67f858068e7ab61873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcafc2piHqrnDSSGsEhnrw.png"/></div></div></figure><h1 id="c0ff" class="mi lw jb bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated"><strong class="ak">探索性数据分析:</strong></h1><ul class=""><li id="502c" class="kw kx jb ka b kb nf kf ng kj nh kn ni kr nj kv lb lc ld le bi translated"><strong class="ka jc">推文长度:</strong></li></ul><p id="db43" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我现在试着去探究推特一般有多长。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="2ef0" class="lv lw jb lr b gy lx ly l lz ma">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns<br/>plt.figure(figsize=(10,6))<br/>doc_lens = [len(d) for d in tweets_df_clean.content]<br/>plt.hist(doc_lens, bins = 100)<br/>plt.title('Distribution of Tweets character length')<br/>plt.ylabel('Number of Tweets')<br/>plt.xlabel('Tweets character length')<br/>sns.despine();</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/2e52a0b45fcb3648c6180e6dcfc63935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHiSjUJLRjMw6z-EiHdQVQ.png"/></div></div></figure><ul class=""><li id="d6b1" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><strong class="ka jc">文字云:</strong></li></ul><p id="da41" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我使用wordcloud来简单地可视化数据，并根据特朗普推文中出现的频率，以不同的大小显示哪些词。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="61ac" class="lv lw jb lr b gy lx ly l lz ma">import matplotlib as mpl<br/>from subprocess import check_output<br/>from wordcloud import WordCloud, STOPWORDS</span><span id="4b16" class="lv lw jb lr b gy mf ly l lz ma">mpl.rcParams['figure.figsize']=(12.0,12.0)  <br/>mpl.rcParams['font.size']=12            <br/>mpl.rcParams['savefig.dpi']=100             <br/>mpl.rcParams['figure.subplot.bottom']=.1 <br/>stopwords = set(STOPWORDS)</span><span id="debd" class="lv lw jb lr b gy mf ly l lz ma">wordcloud = WordCloud(<br/>                          background_color='white',<br/>                          stopwords=stopwords,<br/>                          max_words=500,<br/>                          max_font_size=40, <br/>                          random_state=100<br/>                         ).generate(str(tweets_df_clean.content))</span><span id="87fa" class="lv lw jb lr b gy mf ly l lz ma">print(wordcloud)<br/>fig = plt.figure(1)<br/>plt.imshow(wordcloud)<br/>plt.axis('off')<br/>plt.show();</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/b996b78b392548fb1441a52348aa8b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WG1k1v9FewZ5k5ZYEsj4pA.png"/></div></div></figure><ul class=""><li id="96bc" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><strong class="ka jc">单字:</strong></li></ul><p id="2a3d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一元语法，也称为一元语法，是通过N元语法语言模型预测的一个单词序列，该模型计算给定N元语法在单词序列中的概率。</p><p id="fdbf" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了检测有意义的单字，我删除了所有的停用词。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="7969" class="lv lw jb lr b gy lx ly l lz ma">from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer<br/>def get_top_n_words(corpus, n=None):<br/>vec = CountVectorizer(stop_words='english').fit(corpus)<br/>bag_of_words = vec.transform(corpus)<br/>sum_words = bag_of_words.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]<br/>words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>return words_freq[:n]<br/>common_words = get_top_n_words(tweets_df_clean.content, 10)<br/>unigram = pd.DataFrame(common_words, columns = ['unigram' , 'count'])</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f7197d938717a45058a1cb894805735c.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*tlO17lzyCkE1IvbP_-ZH_g.png"/></div></figure><ul class=""><li id="0151" class="kw kx jb ka b kb kc kf kg kj ky kn kz kr la kv lb lc ld le bi translated"><strong class="ka jc">三元模型:</strong></li></ul><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="df94" class="lv lw jb lr b gy lx ly l lz ma">def get_top_n_trigram(corpus, n=None):<br/>vec = CountVectorizer(ngram_range=3,3),stop_words='english').fit(corpus)<br/>bag_of_words = vec.transform(corpus)<br/>sum_words = bag_of_words.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in      vec.vocabulary_.items()]<br/>words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>return words_freq[:n]<br/>common_words = get_top_n_trigram(tweets_df_clean.content, 10)<br/>trigram = pd.DataFrame(common_words, columns = ['trigram' , 'count'])</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/c971192c7fb80d8f7a93867412178552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCiSqz6rCopxJIxo8OJMZA.png"/></div></div></figure><p id="987e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在领奖台的顶部，你可以看到“让美国再次伟大”(MAGA)，这是唐纳德·特朗普在他成功的2016年总统竞选中使用的竞选口号。</p><h1 id="5516" class="mi lw jb bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">主题建模:</h1><p id="47ff" class="pw-post-body-paragraph jy jz jb ka b kb nf kd ke kf ng kh ki kj no kl km kn np kp kq kr nq kt ku kv ij bi translated">我开始应用LDA所需要的就是来自<code class="fe nr ns nt lr b">sklearn.feature_extraction.text</code>模块的<code class="fe nr ns nt lr b">CountVectorizer</code>类来创建一个文档术语矩阵，以创建我的tweets数据中所有单词的词汇表。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="1943" class="lv lw jb lr b gy lx ly l lz ma">from sklearn.decomposition import LatentDirichletAllocation<br/>vectorizer = CountVectorizer(<br/>analyzer='word',       <br/>min_df=3,# minimum required occurences of a word <br/>stop_words='english',# remove stop words<br/>lowercase=True,# convert all words to lowercase<br/>token_pattern='[a-zA-Z0-9]{3,}',# num chars &gt; 3<br/>max_features=5000,# max number of unique words<br/>                            )</span><span id="fa4a" class="lv lw jb lr b gy mf ly l lz ma">data_matrix = vectorizer.fit_transform(tweets_df_clean.content</span><span id="5a39" class="lv lw jb lr b gy mf ly l lz ma">data_matrix</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/a80a13af34e11a80777de382cfdef5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXOSqljkHbEcq9S5WltDUA.png"/></div></div></figure><p id="7b2f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">42295个文档中的每一个都被表示为5000维向量，这意味着我们的词汇表有5000个单词。</p><p id="3a88" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我将使用<a class="ae ll" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank"> LDA </a>来创建主题以及每个主题的词汇表中每个单词的概率分布。</p><p id="6bf4" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将使用<code class="fe nr ns nt lr b">sklearn.decomposition</code>库中的<code class="fe nr ns nt lr b">LatentDirichletAllocation</code>类对我的文档术语矩阵执行LDA。参数<code class="fe nr ns nt lr b">n_components</code>指定了我希望我的文本被分成的主题的数量，参数<code class="fe nr ns nt lr b">random_state</code>被设置为20，以允许跨多个函数调用可再现的结果，参数<code class="fe nr ns nt lr b">n_jobs</code>被设置为-1，以允许使用我所有可用的处理器。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="f498" class="lv lw jb lr b gy lx ly l lz ma">lda_model = LatentDirichletAllocation(<br/>n_components=10, # Number of topics<br/>learning_method='online',<br/>random_state=20,       <br/>n_jobs = -1  # Use all available CPUs<br/>                                     )<br/>lda_output = lda_model.fit_transform(data_matrix)</span></pre><p id="25ce" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我将使用<code class="fe nr ns nt lr b">pyLDAvis</code>来解释适合tweets数据语料库的主题。该软件包从拟合的LDA主题模型中提取信息，以通知基于web的交互式可视化。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="0d9e" class="lv lw jb lr b gy lx ly l lz ma">!pip install pyLDAvis<br/>import pyLDAvis<br/>import pyLDAvis.sklearn<br/>pyLDAvis.enable_notebook()<br/>pyLDAvis.sklearn.prepare(lda_model, data_matrix, vectorizer, mds='tsne')</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/b71542da80e6609068c3e4b909d7e008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sCitRtwhqdLNEHyGxw_J8Q.png"/></div></div></figure><p id="f09f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我保留了LDA发现的每个主题中出现频率最高的10个单词:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="4664" class="lv lw jb lr b gy lx ly l lz ma">for i,topic in enumerate(lda_model.components_):<br/>print(f'Top 10 words for topic #{i}:')<br/>print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])<br/>print('\n')</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/62cf35d17ca9c2590c6332c6f5f0822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPPEOz-g7xYSoL4jB2s9sQ.png"/></div></div></figure><p id="30b9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">输出显示，最后一个主题可能包含关于2016年总统选举的推文，等等。你可以看到所有类别中都有一些常用词。这是因为几乎所有的主题都使用很少的词。</p><div class="ip iq gp gr ir nx"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd jc gy z fp oc fr fs od fu fw ja bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ix nx"/></div></div></a></div><p id="136d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在最后一步中，我将向输入数据帧添加一个新创建的主题列，并根据概率值为每一行分配合适的主题。为了找到具有最大值的主题索引，我们可以调用<code class="fe nr ns nt lr b">argmax()</code>方法并传递1作为轴参数的值。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a3c2" class="lv lw jb lr b gy lx ly l lz ma">topic_values = lda_model.transform(data_matrix)<br/>tweets_df['Topic'] = topic_values.argmax(axis=1)</span></pre><p id="8534" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们检查一下初始数据的合并，正如你所看到的，每条推文都有一个主题。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="a35a" class="lv lw jb lr b gy lx ly l lz ma">tweets_df.head()</span></pre><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/dbb642ca5fc0d4ff0553bb82018b75b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFG_8uDrdxDpEIm4f8-Kdg.png"/></div></div></figure><h1 id="528b" class="mi lw jb bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">结论:</h1><p id="2f33" class="pw-post-body-paragraph jy jz jb ka b kb nf kd ke kf ng kh ki kj no kl km kn np kp kq kr nq kt ku kv ij bi translated">这项任务到此结束。希望对理解主题建模问题有所帮助。</p><p id="d086" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢阅读！如果你有任何问题，请让我知道，我会尽力回答你。</p><p id="20fb" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka jc">进入专家视图— </strong> <a class="ae ll" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>