<html>
<head>
<title>Paper Weekly — Why Neural Network Can Learn Natural Images Much Better than Noise</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">纸周刊—为什么神经网络可以比噪声更好地学习自然图像</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/paper-weekly-why-neural-network-can-learn-natural-images-much-better-than-noise-36f8d0abfafa?source=collection_archive---------3-----------------------#2020-02-24">https://medium.datadriveninvestor.com/paper-weekly-why-neural-network-can-learn-natural-images-much-better-than-noise-36f8d0abfafa?source=collection_archive---------3-----------------------#2020-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="01f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每周二，我都会强调我在研究或工作中遇到的一篇有趣的论文。希望我的评论能帮助你在2分钟内获得论文中最多汁的部分！</p><h1 id="8b76" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">基本思想</h1><p id="b10f" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">上周我发表了一篇名为<a class="ae lo" href="https://medium.com/p/c2acdba1aa53/edit" rel="noopener"> Deep Image Prior </a>的论文，这是一种新颖的图像重建方法，可以在没有任何训练数据的情况下恢复图像。基本思想很简单，如果我们在一幅图像上拟合一个神经网络(学习一个身份映射函数，使得输入和输出是相同的)，那么噪声成分将最后被学习。因此，通过提前停止训练，图像的“自然”成分将被保留，但是像噪声或水印这样的伪像将被“过滤”。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/46042398a0dc6c2b1c4bb1d37a8328aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ze5RPQmMdbLa5EQ-CJ3MNg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from <a class="ae lo" href="https://sites.skoltech.ru/app/data/uploads/sites/25/2018/04/deep_image_prior.pdf" rel="noopener ugc nofollow" target="_blank">Deep Image Prior</a></figcaption></figure><p id="851b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是个不错的地方，不是吗？但是为什么CNN更抗噪呢？Heckel和Soltanolkotabi在他们的论文<em class="mf">中提出了一些可靠的数学证明:通过利用卷积生成器的结构偏差进行去噪和正则化。</em>下面是链接:<em class="mf"/><a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf</a></p><p id="bc00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">他们发现，只有具有固定卷积核的神经网络(在大多数CNN卷积滤波器中学习)才能很好地对图像去噪，因为自然成分的学习速度比白噪声快得多</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mg"><img src="../Images/885ca7475c935503ed73772ea91bf880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pY1p2XDQque_1EVZO0o-nQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from the <a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="d535" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自然图像和噪声的关键区别在于它们的频谱(它们的离散傅立叶变换是不同的，如果你有电气工程背景的话)。对于那些不太熟悉信号处理的人，请记住:任何信号都可以由一系列基本频率分量的总和来表示。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/47bfacca87cdbf9953f0cf539c38b663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*XpTbBq82KoAlgP2_OYCmMg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">picture credit: <a class="ae lo" href="https://phys.org/news/2012-01-faster-than-fast-fourier.html" rel="noopener ugc nofollow" target="_blank">https://phys.org/news/2012-01-faster-than-fast-fourier.html</a></figcaption></figure><p id="bf1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">研究人员已经表明，具有某些卷积滤波器(如深度解码器中的上采样核)的CNN的神经正切核的主要奇异向量是三角函数，可以将其视为图像的基本成分。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mi"><img src="../Images/d07178fd3126a750993094c777ac8e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypV3IqYwbm3Q_3dtJtneJQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from the <a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="31d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在梯度下降中，低频分量与梯度更一致，并且拟合得更快。下图显示了雅可比矩阵(输出(非损耗)w.r.t .对权重的导数)和图像/噪声的内积分布。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mj"><img src="../Images/3dd67fc2d4e408ecbcb8532049adba46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzy1lRMJtxECaBzqPHMwZA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from the <a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><h1 id="ad79" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">结果</strong></h1><p id="fb17" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">研究人员不仅解决了深度图像先验和深度解码器背后的神话(类似于深度图像先验，事实上，Heckel是深度解码器的作者)，他们还提出了一些公式。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi mk"><img src="../Images/91c880c13280a6f5d8286bdea19171da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiwJrVqs4RIT0z-5mxAxFw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from the <a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ml"><img src="../Images/32100a450aefd88a27dc75fce5398982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bl5F2AqyFeQuMIvKyDig_w.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">from the <a class="ae lo" href="https://viterbi-web.usc.edu/~soltanol/DeepPrior.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><h1 id="9964" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">一些想法</h1><p id="9739" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这篇论文非常、非常、非常数学化，即使在一位作者的直接指导下，我也花了一个学期才理解它。然而，我确实相信，每个人都可以在不经过三十页证明的情况下欣赏这篇论文。请记住这一点:CNN的架构以某种方式强制实施了一种类似过滤的特性，这种特性会降低高频成分的学习速度。</p><div class="mm mn gp gr mo mp"><a href="https://www.datadriveninvestor.com/2019/03/22/fixing-photography/" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd ir gy z fp mu fr fs mv fu fw ip bi translated">修复摄影|数据驱动的投资者</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">汤姆·津伯洛夫在转向摄影之前曾在南加州大学学习音乐。作为一个…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd lz mp"/></div></div></a></div><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="ne nf l"/></div></figure></div></div>    
</body>
</html>