# 人工智能的可解释性、可解释性和透明性

> 原文：<https://medium.datadriveninvestor.com/ai-explainability-interpretability-transparency-in-finance-95e61b5cd6df?source=collection_archive---------12----------------------->

## 人工智能中的伦理

![](img/c1bf2ff2f02c81bada412533a347cc69.png)

> 技术需要合乎道德，或者更明确地说，它需要合乎道德地使用。道德可能意味着许多不同的事情，它可能意味着如何使用它来解决客户挑战，能够解释和诠释该技术的结果和决策，在这些决策中保持公平和公正，并对该技术的使用负责。

# 为什么**对于大多数组织来说，法规遵从性是一个大问题**

总的来说，这取决于**的背景**——你试图解决的问题，以及你如何解决这个问题。

人工智能将被用于何处(背景/行业/问题)，以及它将如何被使用(解决方案/用例)对于确定你的产品/服务需要何种监管合规性和审查非常重要。

在所有情况下，您仍应遵循透明度和数据同意方面的良好治理实践，但是法规遵从性的级别会根据您试图解决的挑战的影响强度而递增，因此，金融和医疗保健行业往往会因其客户挑战的本质而受到严格监管。

监管机构希望了解您的—

**数据基础设施&系统架构—** 内部系统在处理数据卫生、集成和稳健性方面有多稳健；数据将存储在哪里——内部还是云上，人工智能模型将在哪里运行——内部还是第三方供应商 API 集成等等，遗留系统如何相互交互..

**数据收集&分发实践—** 数据的来源是什么，数据集的有效性，正在提取或请求哪些数据点，将如何使用它们，将在哪里使用它们..

**用户隐私和同意** —是否进行了同意检查，是否明确通知用户其数据的使用情况，这些同意/条款和条件是否易于理解；基本上，你是否有意遵守**隐私？**

**机器学习模型&算法** —我们如何验证这些 AI 模型所做决策的有效性和准确性？我们能够准确地解释和诠释所做的决定吗？有没有人交叉检查这些结果？领域专家是否参与了数据的管理和标记以及人工智能模型的设计？算法或输入给人工智能的数据是否公平、公正？数据集是否具有扩展性和包容性？使用了什么建模技术？

**风险管理** —在结果不准确或不良的情况下降低风险；由于数据和结果的不准确，会有什么后果和相应的行动计划？

组织必须通过严格的监管障碍(特别是在金融和医疗保健行业)，才能创建安全可靠的基于人工智能的系统，因此，这些制衡往往看起来像是创新的障碍。

> 然而，我们不应该将监管视为商业成功的障碍，而应该将其视为一种让我们的商业行为逐渐变得更加道德和人性化的方式。

# **阻碍企业围绕人工智能可解释性交付能力的障碍(内部、外部)**

人工智能仍然是一项新生的技术；这些算法和我们提供给它们用来训练它们的数据以及人工智能模型的设计一样好。这些模型仍然是一项正在进行的工作，从每次失败和不一致的结果中学习。因此，人工智能是一个黑匣子，但我相信，随着我们在完善和精炼这些模型方面取得进展，它将慢慢开始解开。

从我的经验来看，可解释性的主要障碍是在组织环境中缺乏意识**。我们甚至需要向我们的监管者和客户解释人工智能模型所做的决定，这种想法在项目开始时并没有经过深思熟虑。因此，围绕每个功能或产品的可解释性，投入最少的努力来创建实践。

其次，人工智能算法的**魅力在于它能够自我学习，从数据集中找到人类思维无法轻易提取的推论和隐藏模式。因此，可解释性是一个有效的挑战，并将在相当长的一段时间内继续是一个挑战，直到我们进入人工智能生态系统的高级阶段。

也就是说，世界上有一些公司正在开发能够对人工智能算法的结果进行逆向工程的算法，并获得有关如何和为什么做出某个决定的细节，做出了哪些假设，哪些数据组合导致了上述结果。****

# 企业如何应对算法中的偏差？

*   **从提问开始** —我的模型是否有一个可以用来训练它的全包数据集？它是否代表了不同的年龄组、性别、种族、背景、环境和挑战？
*   **拥有包容的&多样化的员工群体** —当我设计这些模型时，作为一名数据科学家，我的判断是否存在偏差？确保您的数据科学团队是多元化和包容性的，因为模型中的偏见源于设计这些模型的人。设计模型的人和他们输入模型的数据是决定人工智能模型结果/输出的两个最重要的因素。
*   **扩展边缘案例**——确保模型不断地从不同的用例及挑战中学习，甚至可能是最微小的边缘案例。
*   **治理模板** —将人工智能和数据治理作为组织中的实践/节奏，而不管项目或业务。
*   **让监管者参与创新**——不要让监管成为一个疯狂的紧急项目，从一开始就让监管者参与你的创新设计过程，这样他们就能理解你试图解决的挑战和你的最终目标。
*   **设计的隐私/公平/可解释性** —从项目一开始就创建一个基础设施，将“正确的”设计到产品的蓝图中。
*   **模型构建过程中的宏观透明性** —与上面类似，通过文档和对话将透明性嵌入到您的 AI 模型中。
*   **衡量** —尝试确定您用来衡量模型和输出中 ***公平性*** 或(不)偏差的指标。这是一个很难解决的问题，然而并不是所有的产品都偏向偏见；事实上，有些产品可能[偏爱某个细分市场](https://www.theverge.com/2020/2/7/21128236/gender-app-giggle-women-ai-screen-trans-social),因为这是他们商业模式的本质。
*   **与领域利益相关者&学术机构**合作——这是一种很好的方式，可以将专家的集体智慧运用到你的产品/服务中，以识别数据或人工智能模型中的偏差。
*   **模型管理和治理** —在您的组织中将其规范化，以便人工智能治理成为您的“就绪定义”清单中的必备项目。

> 信任与透明度成正比

如果组织能够在其业务和技术实践中做到透明、公平、有道德和负责任，这将提高最终客户的信任度，进而为等式两边的增值机会铺平道路。