<html>
<head>
<title>Stepping towards Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迈向强化学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/stepping-in-to-reinforcement-learning-c6f751bee6df?source=collection_archive---------5-----------------------#2020-01-28">https://medium.datadriveninvestor.com/stepping-in-to-reinforcement-learning-c6f751bee6df?source=collection_archive---------5-----------------------#2020-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="8954" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RL的基本介绍</p><p id="f8c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated">机器学习，是的，它是监督学习、非监督学习和第三强化学习的组合。好了，今天我只是保留我的第一步，去挖掘和探索这个所谓奇妙的强化学习领域。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ku"><img src="../Images/6eb107d7b2a8ed0224f159541d3826fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tax3Kk7aLW4MIzdmR8Ke1Q.jpeg"/></div></div></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="ee79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">强化学习是什么意思？</strong></p><p id="f7be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">强化学习(RL)，也称为机器学习中的半监督学习模型，是一种允许智能体采取行动并与环境交互以最大化总回报的技术。这就像试错学习。代理人将从其环境经验中发现一个好的政策，同时不会失去太多的回报。</p><div class="ln lo gp gr lp lq"><a href="https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd ir gy z fp lv fr fs lw fu fw ip bi translated">DDI编辑推荐:5本让你从新手变成专家的机器学习书籍|数据驱动…</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">机器学习行业的蓬勃发展重新引起了人们对人工智能的兴趣</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me le lq"/></div></div></a></div><p id="4271" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是RL中的一些关键点</p><p id="2923" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输入:</strong>输入应该是一个初始状态，模型将从这个状态开始</p><p id="09cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输出:</strong>由于特定问题有多种解决方案，因此可能会有多种输出</p><p id="9593" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练:</strong>训练基于输入，模型将返回一个状态，用户将根据其输出决定奖励或惩罚模型。</p><p id="e77b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">模型保持持续学习。</strong></p><p id="d534" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">最佳解决方案是根据最大奖励决定的。</strong></p><p id="1863" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说，RL是一种使用奖惩系统训练算法的动态编程。RL算法或RL代理通过与其环境交互来学习。强化学习与监督学习的不同之处在于，它不需要呈现带标签的输入/输出对，也不需要显式纠正次优动作。相反，重点是在探索(未知领域)和开发(现有知识)之间找到平衡。我在这篇文章的后面讨论了这两个术语。</p><p id="f251" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数强化学习算法使用动态规划技术。因此，环境通常以马尔可夫决策过程(MDP)的形式来描述。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="ec36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">强化学习与深度学习、机器学习有什么区别？</strong></p><p id="b434" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是deepsense.ai对差异的解释。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi mf"><img src="../Images/e13bfe46a4ccc82349b3fc1fdf51d3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBWbR58btYPxeRiP2L2O6w.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Comparison with Machine Leaning</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="e8ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL与其他机器学习范式有何不同？</strong></p><p id="34bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下是答案。</p><ul class=""><li id="2fab" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">没有监督人，</li><li id="d5da" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">只有奖励信号反馈被延迟，而不是瞬时的</li><li id="cf12" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">时间真的很重要(顺序)</li><li id="b897" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">代理的操作会影响它接收的后续数据</li></ul><p id="d1be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">示例:</p><ul class=""><li id="92b7" class="mk ml iq jp b jq jr ju jv jy mm kc mn kg mo kk mp mq mr ms bi translated">直升机飞行特技表演</li><li id="6365" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">在双陆棋中击败世界冠军</li><li id="dcd1" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">管理投资组合</li><li id="8959" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">控制发电站</li><li id="8ebb" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">让人形机器人行走</li><li id="cbba" class="mk ml iq jp b jq mt ju mu jy mv kc mw kg mx kk mp mq mr ms bi translated">比人类更好地玩许多不同的雅达利游戏</li></ul></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="cd46" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">强化的种类有哪些？</strong></p><p id="ee6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种类型的强化:</p><p id="7c55" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.<strong class="jp ir">—</strong><br/>正强化的定义是当一个事件，因某一特定行为而发生时，增加了该行为的强度和频率。换句话说，它对行为有积极的影响。</p><h2 id="fa87" class="my mz iq bd na nb nc dn nd ne nf dp ng jy nh ni nj kc nk nl nm kg nn no np nq bi translated"><em class="nr">正向强化学习的优势:</em></h2><p id="c094" class="pw-post-body-paragraph jn jo iq jp b jq ns js jt ju nt jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk ij bi translated">最大化性能</p><p id="cda7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在很长一段时间内保持变化</p><h2 id="d106" class="my mz iq bd na nb nc dn nd ne nf dp ng jy nh ni nj kc nk nl nm kg nn no np nq bi translated">正向强化学习的缺点:</h2><p id="891b" class="pw-post-body-paragraph jn jo iq jp b jq ns js jt ju nt jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk ij bi translated">过多的强化会导致状态过载，从而削弱结果</p><p id="3380" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir">负面–</strong><br/>负面强化是指因为负面情况被阻止或避免而强化一种行为。</p><h2 id="406c" class="my mz iq bd na nb nc dn nd ne nf dp ng jy nh ni nj kc nk nl nm kg nn no np nq bi translated">负强化学习的优势:</h2><p id="1d10" class="pw-post-body-paragraph jn jo iq jp b jq ns js jt ju nt jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk ij bi translated">增加行为</p><p id="e5b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无视最低性能标准</p><h2 id="b63e" class="my mz iq bd na nb nc dn nd ne nf dp ng jy nh ni nj kc nk nl nm kg nn no np nq bi translated">负强化学习的缺点:</h2><p id="7288" class="pw-post-body-paragraph jn jo iq jp b jq ns js jt ju nt jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk ij bi translated">它仅提供足够满足最低行为的能力</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="9005" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL中的奖励是什么？</strong></p><p id="ec4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">奖励Rt是一个标量反馈信号，它指示代理在步骤t做得有多好。代理的工作是最大化累积奖励。RL是基于报酬假设的。</p><p id="b14d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">报酬假设</strong> :-所有目标都可以用期望累积报酬最大化来描述</p><p id="0408" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">奖励示例:</em></p><p id="cfcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">制作一个人形机器人——向前行走奖励——摔倒奖励</p><p id="95a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">驾驶直升机进行特技飞行动作，按照预期轨迹飞行有奖励，坠毁有奖励</p><p id="ea96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">顺序决策</strong></p><p id="3de7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目标:选择行动以最大化未来总回报</p><p id="705a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">行动可能会产生长期后果</p><p id="3c06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">奖励可能会延迟，牺牲眼前的奖励来获得更多的长期奖励可能更好</p><p id="defe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">顺序决策的例子:</em></p><p id="b683" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">金融投资(可能需要几个月才能成熟)</p><p id="9b1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给直升机加油(可能会在几个小时内防止坠机)</p><p id="4f72" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">阻挡对手的移动(可能有助于从现在开始赢得许多移动的机会)</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/857609386447a689496002fc227d86aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*iNrWVrEahie26RBt1uZnTw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Sequential Decision Making process in RL</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="9422" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL中的历史和状态是什么？</strong></p><p id="36dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">历史是观察、行动和回报的序列。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0a84f48115fbc11932887db22058eb4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*BYx941R2bHZttHX0_fmbPQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">History concept in RL</figcaption></figure><p id="40cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来会发生什么取决于历史。因为行动者选择行动，环境选择观察或奖励。</p><p id="1787" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">状态是用于确定接下来会发生什么的信息。所以国家是历史的函数。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b60322a4d3975aa93646c8a057c093c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*H4pL6CsqcWLoh1X1vRKJtA.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">State is a function of history</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="d336" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL中的环境状态是什么？</strong></p><p id="fd1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">环境状态是环境的私有表示。</p><p id="751f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着数据环境用来选择下一个观察或奖励。</p><p id="156a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常环境状态对代理是不可见的。即使它是可见的，也可能包含不相关的信息。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8d2802d9a7b59eab99ab49fe1c944f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*3Vh-4eNEW1tO0CMnzqWTOw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Environment State in RL</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="665a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL中的代理状态是什么？</strong></p><p id="7779" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代理状态是代理的内部表示</p><p id="8d56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着代理用来选择下一个动作的信息。(强化学习算法使用的信息)</p><p id="309b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是历史的作用。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6fc79beb4cd843020bb55cb504f691f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*n1XHclrX7Fx0gRLtRH4Eaw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Agent state is a function of the history</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c680210fa9e7e286ad6c68c92b98049b.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*HVA0xr8cyYCESWKn5A3tXQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Agent State in RL</figcaption></figure><p id="2e44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL中有哪些完全可观测的&amp;部分可观测的环境？</strong></p><p id="6533" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">完全可观测性</strong>:智能体直接观测环境状态</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7cc282ec197b96adc9634a5ed4c3e647.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*X8aVemNq0L3O5nF9cR3Phw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Full Observability in RL</figcaption></figure><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6fe6afdd468fb0a912d45c52a046a90c.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/1*9Qj3qZu0mhW0DHonqHYd3Q.png"/></div></figure><p id="e917" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代理状态=环境</p><p id="28b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">状态=信息状态</p><p id="1e40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">形式上，这是一个马尔可夫决策过程(MDP)</p><p id="a4f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">部分可观察性</strong>:智能体间接观察环境。</p><p id="0209" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">部分可观察性的例子:</em></p><p id="0a5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个有摄像头的机器人不会被告知它的绝对位置</p><p id="93b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">交易代理只观察当前价格</p><p id="ae39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">玩扑克的代理人只观察公共牌</p><p id="d63c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代理州！=环境状态</p><p id="d3f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">形式上，这是一个部分可观测的马尔可夫决策过程(POMDP)</p><p id="ac23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代理必须构造自己状态表示，</p><p id="bc0a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.完整历史:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi og"><img src="../Images/dde47fcc1bec5afee1dbdad0e860fb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:164/format:webp/1*KlE100CdDe_jpK4bR81PGw.png"/></div></figure><p id="0bdd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.环境状态的信念:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0c44658e877221c843e89e6ae4265e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*xDKmv-Oy_PSvuPeSQmdOog.png"/></div></figure><p id="d135" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.递归神经网络:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/967a8e57ab8b4af463fe8036ec207c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*EGV3yTnWSqSA2JGiwBzIkQ.png"/></div></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="d358" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL代理的主要组成部分是什么？</strong></p><p id="fd40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它们是政策、价值函数和模型。</p><p id="e831" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nx">策略</em> </strong>:代理的行为功能</p><p id="ed71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nx">价值函数</em> </strong>:每个状态和/或动作有多好</p><p id="93df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="nx">模型</em> </strong>:代理人对环境的再现</p><p id="feb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">政策</strong></p><p id="83d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">策略是代理的行为</p><p id="ce85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是从国家到行动的地图，</p><p id="ef62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，确定性策略:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b4ac39942ea5f8c227cdd0caba063be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*v3Cdv3EukP60ICqIpRXDzw.png"/></div></figure><p id="b9d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随机政策:</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a173f021f4c34b7c64fbec800d86fb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*Ln90gHQI3SxHWaWJ9Q4JDA.png"/></div></figure><p id="9975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">策略是学习代理在给定时间的行为方式。策略是从感知的环境状态到处于这些状态时要采取的行动的映射。策略可能是一个简单的函数或查找表，而在其他情况下，它可能涉及大量的计算，如搜索过程。策略是强化学习代理的核心。</p><p id="f3e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">价值函数</strong></p><p id="3f08" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">价值函数是对未来报酬的预测</p><p id="f168" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于评估状态的好/坏</p><p id="75d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，为了在动作之间进行选择，</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/d76be4257e65b68dac8deb6d274b99dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*sfltMg7zc1qNhNU_vKannQ.png"/></div></figure><p id="8998" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">型号</strong></p><p id="d06a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个模型预测环境下一步会做什么</p><p id="044e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">p预测下一个状态</p><p id="b893" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">r预测下一个(直接)奖励，</p><p id="0f83" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1c7effeff1a9d62cb9a540da455aaebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*tCmh0O1kdfzqIKTfXCzoRA.png"/></div></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="4685" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> RL代理</strong></p><p id="df5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于值的</strong></p><p id="cda0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无策略(隐含)</p><p id="459b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">价值函数</p><p id="363e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于策略</strong></p><p id="dc88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">政策</p><p id="f7b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无价值函数</p><p id="b52d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">演员评论家</strong></p><p id="8ffa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">政策</p><p id="f289" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">价值函数</p><p id="e510" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">型号自由</strong></p><p id="b344" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">政策和/或价值功能</p><p id="1723" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有模型</p><p id="5468" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">基于模型的</strong></p><p id="8c90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">政策和/或价值功能</p><p id="37eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="7a15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> RL代理分类</strong></p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ad8c5d8ba5a7067f2d5b654c2b158aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*CT3lL1ZJdSEjAvPiR9oN5w.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">RL Agent Taxonomy</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="5742" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">什么是RL中的探索与剥削？</strong></p><p id="cea7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">探索发现更多关于环境的信息</p><p id="9e97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">利用已知信息来获取最大回报</p><p id="a617" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">探索和利用通常都很重要。</p><p id="3976" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">勘探开发实例:</em></p><p id="4851" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">餐厅选择</strong></p><p id="fb4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">剥削:去你最喜欢的餐馆</p><p id="d8cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">探索:尝试一家新餐馆</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="d68d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">什么是RL中的预测和控制？</strong></p><p id="acde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预测:评估给定政策的未来</p><p id="b500" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">控制:通过寻找最佳策略来优化未来</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="936c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">什么是RL算法？</strong></p><p id="213b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一些流行的RL算法有:</p><p id="5371" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">蒙特卡洛</p><p id="1fa2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">q学习</p><p id="7572" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">萨尔萨</p><p id="4d56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">q学习—λ</p><p id="4dd6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">萨尔萨—拉姆达</p><p id="a652" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DQN</p><p id="551c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DDPG</p><p id="dc1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">A3C00</p><p id="2293" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NAF</p><p id="fb87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">TRPO</p><p id="add5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">聚苯醚（Polyphenylene Oxide的缩写）</p><p id="f20f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">TD3</p><p id="0b95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">袋</p><p id="ae29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是对维基百科中所述RL算法的一个很好的比较。</p><figure class="kv kw kx ky gt kz gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/cbb19317f5a747b830b02e0a42008d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Ha2Hh4wFnlPJG17J0egKaQ.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Comparison of RL algorithms</figcaption></figure></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="e57f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL有哪些实际应用？</strong></p><p id="7a16" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RL可用于工业自动化的机器人技术。</p><p id="cf42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RL可用于机器学习和数据处理</p><p id="846e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RL可用于创建培训系统，根据学生的要求提供定制的指导和材料。</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="4121" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">RL的成功案例</strong></p><p id="d293" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Deepmind的DQN突围</p><p id="9fd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Deepmind的AlphaGo</p><p id="4092" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Deepmind使用RL在Mujoco的模拟模型上模拟运动行为</p></div><div class="ab cl lg lh hu li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="ij ik il im in"><p id="0141" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从这篇基础文章中，我希望你迈向RL的第一步会成功。强化学习，无疑是一项尖端技术，有潜力改变我们的世界。它将使机器在寻找新的、创新的方法来执行任务时具有创造性。毫无疑问，这个领域将在未来的人工智能范式中取得巨大的里程碑。然而，RL将在机器学习、深度学习的基础上，在大规模的人工智能父性下成长。法国著名科学家、脸书研究负责人Yann LeCun开玩笑说，强化学习是人工智能蛋糕上的樱桃，机器学习是蛋糕本身，深度学习是糖衣。如果没有前面的迭代，cherry将一无所获。</p><p id="fc05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nx">让我们深入挖掘，寻找AI之家的强化学习舞者。</em></p><p id="3a2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献:</strong></p><p id="0d69" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae op" href="https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14" rel="noopener" target="_blank">https://towards data science . com/reinforcement-learning-demystified-36 c 39 c 11 EC 14</a></p><p id="f927" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae op" href="https://www.geeksforgeeks.org/what-is-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/what-is-reinforcement-learning/</a></p><p id="939b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae op" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p><p id="6efe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Reinforcement_learning</a></p><div class="ln lo gp gr lp lq"><a href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd ir gy z fp lv fr fs lw fu fw ip bi translated">什么是强化学习？完全指南- deepsense.ai</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">人工智能预计市场规模为73.5亿美元，正在突飞猛进地发展…</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">deepsense.ai</p></div></div><div class="lz l"><div class="oq l mb mc md lz me le lq"/></div></div></a></div><figure class="kv kw kx ky gt kz"><div class="bz fp l di"><div class="or os l"/></div></figure></div></div>    
</body>
</html>