<html>
<head>
<title>The Easiest and Most Intuitive Explanation of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络最简单直观的解释</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/the-easiest-and-most-intuitive-explanation-of-neural-networks-53162e5ce104?source=collection_archive---------14-----------------------#2020-09-08">https://medium.datadriveninvestor.com/the-easiest-and-most-intuitive-explanation-of-neural-networks-53162e5ce104?source=collection_archive---------14-----------------------#2020-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7f8c9ced8bc5949f12ebdd85a7dc8155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRCfO-p3MNT3RibNrGGZJw.png"/></div></div></figure><p id="5eb4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你会用一个<strong class="kd iu">人工大脑</strong>做什么？做作业？战胜股市？接管世界？可能性是无限的。如果我告诉你，人类已经造出了人造大脑，会怎么样？</p><p id="2d39" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">事实上，<strong class="kd iu">我们有！</strong> —嗯，有点。它不像我们的大脑那么复杂，但它们是<strong class="kd iu">极其强大的工具，有能力改变世界。它们被称为神经网络。</strong></p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="c94b" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">什么是神经网络？</h1><p id="e44d" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">神经网络(或简称NN)是一组连接的节点<strong class="kd iu">能够<strong class="kd iu">接收信息，处理信息，然后产生输出</strong>。</strong></p><p id="f213" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">神经网络的一个例子是我们的大脑！它由许许多多<strong class="kd iu">相连的生物神经元</strong>组成，<strong class="kd iu">接收信息</strong>(屏幕上的文字)，<strong class="kd iu">处理</strong>，然后<strong class="kd iu">产生输出</strong>(理解文字并存储信息)。</p><h2 id="00ed" class="mj lh it bd li mk ml dn lm mm mn dp lq km mo mp lu kq mq mr ly ku ms mt mc mu bi translated">生物神经网络:</h2><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/1070bda883e0bf232ec169f54a218144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wa78kJmYCboZyetJCrDN5A.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><a class="ae ne" href="https://www.brainfacts.org/for-educators/for-the-classroom/2018/squishy-neurons-activity-111918" rel="noopener ugc nofollow" target="_blank">Simplified Diagram of a Biological Neuron</a></figcaption></figure><p id="6011" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">理解生物神经元如何工作是</strong> <strong class="kd iu">对</strong> <strong class="kd iu">人工神经元直观理解的关键。首先让我们看一下生物神经元的物理部分，稍后将与人工神经元的部分进行比较。</strong></p><p id="badf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">生物神经元的组成部分:</p><ul class=""><li id="6ee1" class="nf ng it kd b ke kf ki kj km nh kq ni ku nj ky nk nl nm nn bi translated"><strong class="kd iu">树突</strong> : <strong class="kd iu">接收来自其他神经元的输入</strong></li><li id="9a39" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">细胞体</strong> : <strong class="kd iu">结合树突收集的所有信息</strong>，并进行一些<strong class="kd iu">计算</strong>。它可以发送信号(根据强度不同而不同)或不发送信号，这取决于计算</li><li id="3f9a" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">轴突</strong>:接受细胞体的计算，<strong class="kd iu">传递信号</strong></li><li id="9dc8" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">轴突末梢</strong> : <strong class="kd iu">向其他神经元发出信号</strong></li></ul><p id="d004" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意:在继续之前，绝对有必要熟悉ML和深度学习的关键定义，以便获得ANN背后的直观感受。关于这些关键定义，请阅读我的短文。</p><div class="nt nu gp gr nv nw"><a href="https://www.datadriveninvestor.com/2020/01/13/the-future-of-humanity-is-genetic-engineering-and-neural-implants/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">人类的未来是基因工程和神经移植|数据驱动投资者</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">领先的技术、音乐和电影节将于2020年3月13日至22日举行。它将以前沿的谈话为特色…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok jz nw"/></div></div></a></div><h2 id="784d" class="mj lh it bd li mk ml dn lm mm mn dp lq km mo mp lu kq mq mr ly ku ms mt mc mu bi translated">人工神经网络:</h2><p id="a210" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">现在进入文章的主题:<strong class="kd iu">人工神经网络</strong>！<strong class="kd iu">ANN’s不像我们的大脑<a class="ae ne" href="https://en.wikipedia.org/wiki/Physical_neural_network#:~:text=A%20physical%20neural%20network%20is,function%20of%20a%20neural%20synapse." rel="noopener ugc nofollow" target="_blank">那样是物理NN的</a></strong>，但它们可以是！)，它们其实只是运行在电脑上的软件！</p><p id="1200" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们先看看人工神经网络的组成部分，然后深入了解它们是如何相互作用的。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/c8432e56ecf1a31d0ce4378117a8e8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0TUtOl9cFJGXpD78oOJ8_Q.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><a class="ae ne" href="https://www.kdnuggets.com/2017/10/neural-network-foundations-explained-gradient-descent.html" rel="noopener ugc nofollow" target="_blank">A Simple Neural Network</a></figcaption></figure><p id="ce1a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">神经网络的组成部分:</p><ul class=""><li id="0477" class="nf ng it kd b ke kf ki kj km nh kq ni ku nj ky nk nl nm nn bi translated"><strong class="kd iu">节点</strong> : <strong class="kd iu">接收前一个节点的输入</strong>，进行一些<strong class="kd iu">计算，</strong>和<strong class="kd iu">输出</strong>它。节点类似于生物神经网络中的细胞体、树突和轴突终末</li><li id="3342" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">突触</strong> : <strong class="kd iu">将一个节点输出的信号</strong>传递到下一个节点的输入。就像生物神经网络中的轴突</li><li id="302b" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">输入层</strong> : <strong class="kd iu">其中输入数据</strong>(数字形式)被<strong class="kd iu">插入</strong></li><li id="bfb8" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">隐藏层</strong>:即<strong class="kd iu">不是输入或输出的层</strong>(奖励:如果你有超过1层，你的神经网络从简单神经网络升级到深层神经网络)</li><li id="3f49" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky nk nl nm nn bi translated"><strong class="kd iu">输出层</strong>:NN最终输出<strong class="kd iu">的层</strong></li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="61b8" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">神经网络是如何工作的？</strong></h1><p id="d1cd" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">神经网络的实际工作原理:</p><ol class=""><li id="5e9e" class="nf ng it kd b ke kf ki kj km nh kq ni ku nj ky om nl nm nn bi translated">首先会有一些<strong class="kd iu">输入数据</strong>，我们将<strong class="kd iu">将这些数据</strong>放入<strong class="kd iu">输入层</strong>。输入数据是数字的<strong class="kd iu">形式</strong></li><li id="56e0" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">这将是通过突触发送的<strong class="kd iu">。当来自前一个节点的数据通过synapse发送时，它将被<strong class="kd iu">乘以参数并加上一个偏差</strong>。这个<strong class="kd iu">放大或衰减信号</strong></strong></li><li id="3bed" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">该信号将被输入到下一个节点，该节点连同所有的和其他<strong class="kd iu">信号将被加在一起</strong>，然后传递到突触</li><li id="89d3" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated"><strong class="kd iu">重复</strong>的“计算+发送”步骤</li><li id="e3a4" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated"><strong class="kd iu">最终输出将是一些数字</strong>，我们可以将其解释为我们的结果</li></ol><p id="1bfc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">这5个步骤</strong> (+稍后解释的另一个步骤)正是我们的大脑处理信息所经历的<strong class="kd iu">相同的步骤</strong>！安的大脑是模仿我们的大脑的！</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/187b8e4fe89b2124b9feef55fdeae173.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*U764N4AFvP3GorpGsDU5ng.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><a class="ae ne" href="https://math.stackexchange.com/questions/2048722/a-name-for-layered-directed-graph-as-in-a-fully-connected-neural-network" rel="noopener ugc nofollow" target="_blank">A bit more complex NN</a></figcaption></figure><p id="f1a5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上面的图像看起来有点复杂，层中有许多节点，有大量的突触连接它们，但它仍然不能反映真实的神经网络。在现实生活中，<strong class="kd iu"> NN的每一层都有数千个节点，有几十层</strong>。虽然<strong class="kd iu">的过程都是一样的</strong>。但问题依然存在:计算机将如何计算所有这些？</p><h2 id="cc62" class="mj lh it bd li mk ml dn lm mm mn dp lq km mo mp lu kq mq mr ly ku ms mt mc mu bi translated">神经网络编程:</h2><p id="1806" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated"><strong class="kd iu">NN中的一切都是数字，</strong>是用矢量(数字列表)表示的<strong class="kd iu">，矢量<strong class="kd iu">会通过<a class="ae ne" href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products/v/vector-dot-product-and-vector-length" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">点积</strong> </a>与</strong>相互作用。我们这样做是因为计算机处理矢量和点积的速度快得惊人。</strong></p><p id="32fc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们再来一遍这个过程，但这次是用矩阵:</p><ol class=""><li id="a7ac" class="nf ng it kd b ke kf ki kj km nh kq ni ku nj ky om nl nm nn bi translated"><strong class="kd iu">输入层</strong>将是输入数据的一个<strong class="kd iu">矢量</strong>(例如:单个图像或单个属性列表)</li><li id="bddd" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">这个向量将在<strong class="kd iu">上<strong class="kd iu">传递</strong>到<strong class="kd iu">下一层</strong>中的每个单个节点</strong></li><li id="8772" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">在每个<strong class="kd iu">节点</strong>中，将有一个参数的<strong class="kd iu">向量，每一个都代替突触。这两个向量将彼此点积产生一个数。一个<strong class="kd iu">偏差将被加到这个数字上</strong>以产生一个输出</strong></li><li id="7783" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">现在，整个层是一组输出，我们<strong class="kd iu">将它们组合成一个矢量</strong></li><li id="9d70" class="nf ng it kd b ke no ki np km nq kq nr ku ns ky om nl nm nn bi translated">这个<strong class="kd iu">向量将沿着</strong>传递。过程<strong class="kd iu">重复</strong>直到它到达最后一层</li></ol><p id="eef7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注意</strong>如何在<strong class="kd iu">步骤3中，</strong>我们<strong class="kd iu">将我们的输入乘以某个常数</strong>，然后<strong class="kd iu">加入另一个常数</strong>？这听起来就像9年级的线性函数。是的，你必须记住的那个方程，<strong class="kd iu"> y = mx + b </strong>，终于可以在现实生活中应用了！<strong class="kd iu"> y </strong>为<strong class="kd iu">输出</strong>，<strong class="kd iu"> m </strong>为<strong class="kd iu">参数</strong>，<strong class="kd iu"> x </strong>为<strong class="kd iu">输入</strong>，<strong class="kd iu"> b </strong>为<strong class="kd iu">偏置</strong>！我们称第三步为线性层。</p><p id="bcca" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你特别注意的话，可能会突然出现一些东西:NN中的一堆线性层可能会被<strong class="kd iu">组合</strong> <strong class="kd iu">成一个层，那么</strong> <strong class="kd iu">为什么是所有这些层</strong>？这是一个很好的观点，这将通过<strong class="kd iu">键缺失步骤</strong>—<strong class="kd iu">非线性</strong>来解决。</p><h2 id="1c02" class="mj lh it bd li mk ml dn lm mm mn dp lq km mo mp lu kq mq mr ly ku ms mt mc mu bi translated"><strong class="ak">非线性功能:</strong></h2><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/4f58377e6a5e0d92ba64fbcdc22c6969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkZ5OYHJvCnYPigMJ2SIyg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><a class="ae ne" href="https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7" rel="noopener">ReLU or Rectified Linear Unit, is the most popular and simple Non-Linearity Function</a></figcaption></figure><p id="12ee" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通过在每个线性层之间添加非线性，我们能够将它们分开！我们所要做的就是<strong class="kd iu">将<strong class="kd iu">非线性函数</strong>应用于<strong class="kd iu">线性层</strong>的所有<strong class="kd iu">输出</strong>。</strong></p><p id="3350" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有无数的非线性函数在那里。最流行最简单的非线性叫做<strong class="kd iu"> ReLU </strong>。坚持用那个，因为简单，快速，容易理解！</p><p id="3c25" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注:</strong>非线性功能的层称为<strong class="kd iu">激活层</strong></p><p id="a15e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">等等，为什么我们要有这么多层？我就不能做吗？答案是:通用逼近定理</p><p id="d1fc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae ne" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">通用逼近定理</strong> </a>陈述了如果你有足够多的线性层和非线性层，你可以以任意精度逼近任意函数。换句话说，<strong class="kd iu"> NN的什么都能做</strong>(理论上<strong class="kd iu">至少</strong>)！</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="cdfa" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢阅读！我是Dickson，17岁的技术爱好者，很高兴能加速自己的步伐，影响数十亿人🌎</p><p id="2db1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你想跟随我的旅程，你可以加入<a class="ae ne" href="https://bit.ly/DicksonNewsletter" rel="noopener ugc nofollow" target="_blank">我的每月简讯</a>，查看<a class="ae ne" href="https://bit.ly/DicksonWebsite" rel="noopener ugc nofollow" target="_blank">我的网站</a>，连接<a class="ae ne" href="https://bit.ly/DicksonLinkedin" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae ne" href="https://bit.ly/DicksonTwitter" rel="noopener ugc nofollow" target="_blank"> Twitter </a>😃</p><p id="0dd7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">访问专家视图— </strong> <a class="ae ne" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>