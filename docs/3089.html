<html>
<head>
<title>The problem of Overfitting in Regression and how to avoid it?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归中的过拟合问题，如何避免？</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/the-problem-of-overfitting-in-regression-and-how-to-avoid-it-dac4d49d836f?source=collection_archive---------1-----------------------#2020-05-31">https://medium.datadriveninvestor.com/the-problem-of-overfitting-in-regression-and-how-to-avoid-it-dac4d49d836f?source=collection_archive---------1-----------------------#2020-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f5c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi kl translated"><span class="l km kn ko bm kp kq kr ks kt di"> H </span> ello世界！我的数据科学社区博客。上一篇文章我们已经讨论了线性回归中的端到端管道，这里我们将讨论数据科学家和从业者面临的最常见的问题。如果你的模型是<strong class="jp ir">过度拟合</strong>并且你试图解决它，那么这篇文章是给你的。</p><h1 id="6532" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">什么是过度拟合？</h1><p id="ad29" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated"><strong class="jp ir">过拟合</strong>是一种建模错误，当一个函数或模型过于接近训练集，并在测试集中获得拟合的巨大差异时，就会出现这种错误。<strong class="jp ir">过度拟合</strong>模型通常采用制作过于复杂的模型的形式来解释研究数据中的模型行为。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/c341916c84aa1036a23dce8b07bfaefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atOGj6hy7F1ZfSwcILzDNg.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Overfitted Data [‘Image Created By Dheeraj Kumar K’]</figcaption></figure><h1 id="7d38" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">过度拟合的例子</h1><p id="cbac" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">让我们用例子来说明，</p><p id="6d37" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们需要根据一个学生的简历来预测他是否会得到一份工作面试。现在假设我们从20，000份简历及其结果的数据集训练一个模型。</p><p id="2041" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后我们在原始数据集上尝试一个模型，它预测的结果有98%的准确率…哇！很神奇，但现实中没有。</p><div class="mn mo gp gr mp mq"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne mh mq"/></div></div></a></div><p id="774f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是现在坏消息来了。当我们在新的简历数据集上运行一个模型时，我们只有50%的准确率。</p><p id="9c30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的模型没有从我们的训练数据中很好地一般化，以查看看不见的数据。这被称为<strong class="jp ir">过拟合</strong>，是数据科学中的常见问题。</p><p id="2745" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">事实上，<strong class="jp ir">过拟合</strong>在现实世界中无时无刻不在发生。我们需要处理它来推广这个模型。</p><h1 id="2bdf" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">如何发现过度拟合？</h1><p id="aaca" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">机器学习和数据科学中的主要挑战是，在测试之前，我们无法评估模型的性能。因此，找到过度拟合的第一步是将数据分成训练集和测试集。</p><p id="140c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如果我们的模型在训练集上比在测试集上表现得更好，那么我们很可能过度拟合了。</strong></p><p id="a8c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以使用在两个数据集中观察到的准确度百分比来测量性能，以推断是否存在<strong class="jp ir">过度拟合</strong>。如果模型在训练集上比在测试集上表现得更好，这意味着模型很可能<strong class="jp ir">过度拟合</strong>。例如，如果我们的模型在训练集上看到99%的准确性，但在测试集上只有50%的准确性，这将是一个很大的警报。</p><h1 id="4ef2" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">如何防止过度拟合？</h1><ol class=""><li id="6275" class="nf ng iq jp b jq ls ju lt jy nh kc ni kg nj kk nk nl nm nn bi translated">使用更多数据进行培训</li><li id="8148" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">数据扩充</li><li id="9722" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">交叉验证</li><li id="037f" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">特征选择</li><li id="7f64" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">正规化</li></ol><p id="da3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们深入一点，</p><h1 id="caee" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">1.使用更多数据进行培训</h1><p id="0e92" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">防止<strong class="jp ir">过度拟合</strong>的方法之一是借助更多数据进行训练。这使得算法能够更好地检测信号，从而将误差降至最低。用户应该不断收集更多的数据，作为提高模型准确性的一种方式。然而，这种方法被认为是昂贵的，因此，用户应该确保所使用的数据是相关的和干净的。</p><h1 id="1fe0" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">2.数据扩充</h1><p id="fab1" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">使用更多数据进行训练的一种替代方法是数据扩充，与前者相比，它的成本更低。如果您无法持续收集更多的数据，您可以使可用的数据集看起来多样化。每次模型处理样本数据时，数据扩充都会使样本数据看起来略有不同。该过程使得每个数据集对模型来说看起来是唯一的，并且防止模型学习数据集的特征。</p><h1 id="3829" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">3.交叉验证</h1><p id="35c7" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">交叉验证是防止<strong class="jp ir">过度拟合</strong>的有力预防措施。</p><p id="a748" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个想法很聪明:使用你的初始训练数据来生成多个迷你训练测试分割。使用这些分割来调整您的模型。</p><p id="df93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在标准的k折叠交叉验证中，我们将数据分成k个子集，称为折叠。然后，我们在k-1个折叠上迭代地训练算法，同时使用剩余的折叠作为测试集。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nt"><img src="../Images/eb9d19350d45d1c8039569e1731d483d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6duhDxKPpU7A-BW95U1zQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Cross-Validation [‘Image Created By Dheeraj Kumar K’]</figcaption></figure><p id="cda6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">交叉验证允许您仅使用原始训练集来调整超参数。这允许您将您的测试集作为一个真正不可见的数据集来选择您的最终模型。</p><pre class="ly lz ma mb gt nu nv nw nx aw ny bi"><span id="0007" class="nz kv iq nv b gy oa ob l oc od">from sklearn.model_selection import cross_val_score<strong class="nv ir"><br/></strong>from sklearn import model_selection<strong class="nv ir"><br/>def</strong> kfold(models, train_X, train_y, seed=7, scoring='accuracy', n_splits=10):<br/>    results = []<br/>    names = []<br/>    <strong class="nv ir">for</strong> name, model <strong class="nv ir">in</strong> models:<br/>        kfold = model_selection.KFold(n_splits=n_splits, random_state=seed)<br/>        cv_results = model_selection.cross_val_score(model, train_X, train_y, cv=kfold, scoring=scoring)<br/>        results.append(cv_results)<br/>        names.append(name)<br/>        msg = "<strong class="nv ir">%s</strong>: <strong class="nv ir">%f</strong> (<strong class="nv ir">%f</strong>)" % (name, cv_results.mean(), cv_results.std())<br/>        print(msg)</span></pre><h1 id="c7b6" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">4.特征选择</h1><p id="b0b6" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">一些算法具有内置的特征选择。</p><p id="c7a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于没有概化的要素，您可以通过移除不相关的输入要素来手动提高它们的概化能力。</p><p id="25bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个有趣的方法是讲述一个关于每个特性如何适应模型的故事。如果任何事情都没有意义，或者如果很难证明某些特征，这是识别它们的好方法。</p><pre class="ly lz ma mb gt nu nv nw nx aw ny bi"><span id="a527" class="nz kv iq nv b gy oa ob l oc od">from sklearn.feature_selection import VarianceThreshold<br/>varModel=VarianceThreshold(threshold=0)<br/>varModel.fit(x_train)</span><span id="12eb" class="nz kv iq nv b gy oe ob l oc od">constArr=varModel.get_support()<br/>constArr</span><span id="8728" class="nz kv iq nv b gy oe ob l oc od">import collections<br/>collections.Counter(constArr)</span></pre><h1 id="a211" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">5.正规化</h1><h2 id="f875" class="nz kv iq bd kw of og dn la oh oi dp le jy oj ok li kc ol om lm kg on oo lq op bi translated">什么是正规化？</h2><p id="f8aa" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">正则化是一种降低模型复杂性的技术。它通过惩罚损失函数来做到这一点。这有助于解决<strong class="jp ir">过拟合</strong>问题。</p><h2 id="5402" class="nz kv iq bd kw of og dn la oh oi dp le jy oj ok li kc ol om lm kg on oo lq op bi translated">为什么我们需要正规化？</h2><p id="d97c" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">让我们看一些例子，</p><p id="78ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们想预测一个学生的学生成绩。对于预测，我们使用学生的GPA分数。该模型无法预测一系列学生的分数，因为该模型过于简单，因此具有较高的偏差。</p><p id="ae38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在开始添加更多可能会影响学生成绩的功能。我们向我们的模型添加了更多的输入特征，出勤率、中学生的平均成绩、初中生的身体质量指数、平均睡眠时间。我们看到随着更多的输入特征，模型开始变得过于复杂。</p><p id="4909" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的模型还学习了数据模式以及训练数据中的噪声。当一个模型试图拟合数据模式以及噪声时，那么该模型具有高方差ad，这将<strong class="jp ir">过度拟合</strong>。</p><p id="805e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<strong class="jp ir">过度拟合的</strong>模型在训练数据上表现很好，但是不能泛化。</p><p id="6966" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正规化有三种类型</p><ol class=""><li id="b218" class="nf ng iq jp b jq jr ju jv jy oq kc or kg os kk nk nl nm nn bi translated">L 1或套索</li><li id="40c0" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">L 2或山脊</li><li id="7d3f" class="nf ng iq jp b jq no ju np jy nq kc nr kg ns kk nk nl nm nn bi translated">L 3或弹性网</li></ol><h1 id="6511" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">L 1正则化或套索</h1><p id="98a2" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated"><strong class="jp ir">套索回归</strong>非常<strong class="jp ir">相似就像脊回归</strong>和<strong class="jp ir">有</strong>非常<strong class="jp ir">的区别。类似于Ridge </strong> lets，我们从一群老鼠的<strong class="jp ir">重量</strong>和<strong class="jp ir">尺寸</strong>测量开始。我们将数据分成两组<strong class="jp ir">红点</strong>是<strong class="jp ir">训练数据</strong>而<strong class="jp ir">绿点</strong>是<strong class="jp ir">测试数据。</strong></p><p id="dc58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们<strong class="jp ir">使用最小二乘法将直线</strong>拟合到<strong class="jp ir">训练数据</strong>，这意味着<strong class="jp ir">最小化残差平方和。</strong></p><p id="209b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们这样做时，即使<strong class="jp ir">线与<strong class="jp ir">训练数据</strong>非常吻合，它<strong class="jp ir">具有低偏差</strong>，如果它<strong class="jp ir">与</strong>测试数据</strong>不吻合，它<strong class="jp ir">具有高方差。</strong>在Lasso回归中<strong class="jp ir">残差平方和+λ*(|斜率|) </strong>这里我们采用绝对值代替斜率平方。</p><p id="439b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像岭回归<strong class="jp ir"> λ </strong>值可以是从<strong class="jp ir"> 0到正无穷大</strong>的任何值，并且使用交叉验证<strong class="jp ir">来确定。</strong>岭回归和套索回归的<strong class="jp ir">大区别</strong>在于<strong class="jp ir">岭</strong>只能<strong class="jp ir">将变量</strong> <strong class="jp ir">渐近于0 </strong>而<strong class="jp ir">套索</strong>可以<strong class="jp ir">将变量一直收缩到0 </strong></p><pre class="ly lz ma mb gt nu nv nw nx aw ny bi"><span id="0507" class="nz kv iq nv b gy oa ob l oc od">from sklearn.datasets import load_boston<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.linear_model import Ridge<br/>from sklearn.cross_validation import train_test_split</span><span id="b835" class="nz kv iq nv b gy oe ob l oc od">boston=load_boston()<br/>boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)</span><span id="2c4d" class="nz kv iq nv b gy oe ob l oc od">newX=boston_df.drop('Price',axis=1)<br/>print newX[0:3] # check <br/>newY=boston_df['Price']</span><span id="fb7d" class="nz kv iq nv b gy oe ob l oc od">X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3)</span><span id="08c2" class="nz kv iq nv b gy oe ob l oc od">lasso = Lasso()<br/>lasso.fit(X_train,y_train)</span><span id="2d9b" class="nz kv iq nv b gy oe ob l oc od">lasso100 = lasso(alpha=100)<br/>lasso100.fit(X_train, y_train)</span><span id="1853" class="nz kv iq nv b gy oe ob l oc od">train_score=lasso.score(X_train,y_train)<br/>test_score=lasso.score(X_test,y_test)</span><span id="aae1" class="nz kv iq nv b gy oe ob l oc od">lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)<br/>lasso00001.fit(X_train,y_train)</span><span id="5b29" class="nz kv iq nv b gy oe ob l oc od">train_score00001=lasso00001.score(X_train,y_train)<br/>test_score00001=lasso00001.score(X_test,y_test)<br/>coeff_used00001 = np.sum(lasso00001.coef_!=0)</span></pre><h1 id="c810" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">L 2正则化或岭</h1><p id="f0f2" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated"><strong class="jp ir">岭回归</strong>。<strong class="jp ir">岭回归</strong>是线性<strong class="jp ir">回归</strong>的延伸。它基本上是一个正则化的线性回归模型。</p><p id="461d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们开始收集一群老鼠的体重和尺寸。由于数据看起来相对线性，我们使用线性回归，最小二乘法，来模拟体重和体型之间的关系。我们应该找到残差平方和最小的线结果。</p><p id="7389" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直线方程</p><p id="1544" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尺寸= 0.9+0.75 * <strong class="jp ir">重量</strong></p><p id="8aad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尺寸= 0.9 +0.75* 2.5</p><p id="18ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尺寸= 0.9 +1.88</p><p id="0a5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尺寸= 2.8</p><p id="fe12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们进行大量测量时，我们可以相当确信<strong class="jp ir">最小二乘法直线</strong>准确地反映了<strong class="jp ir">尺寸</strong>和<strong class="jp ir">重量之间的关系。</strong></p><p id="533e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们只有两个测量值，那么<strong class="jp ir">新线</strong>用最小二乘法拟合。由于<strong class="jp ir">新线</strong>与数据点重叠，<strong class="jp ir">最小残差平方和= 0。</strong>第二张图显示了原始数据和用于比较的原始线。</p><p id="3da2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">两个<strong class="jp ir">红点</strong>为<strong class="jp ir"> </strong>训练<strong class="jp ir"> </strong>数据<strong class="jp ir"> </strong>其余<strong class="jp ir">绿点</strong>为测试数据。两个<strong class="jp ir">红点</strong><strong class="jp ir">训练数据</strong>的残差平方和较小(本例中为<strong class="jp ir"> 0 </strong>，而<strong class="jp ir">绿点、</strong><strong class="jp ir">测试数据</strong>的残差平方和较大。</p><p id="9d6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们从<strong class="jp ir">插入</strong>对应于<strong class="jp ir">最小二乘拟合的数字开始。</strong></p><p id="6288" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">体积= </strong> 0.4 + 0.3 * <strong class="jp ir">重量</strong></p><p id="2fd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">残差平方和+ λ *(斜率)^ 2 </strong></p><p id="15a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(因为线重叠点)0 <strong class="jp ir"> + </strong> 1 * (1.3) ^2 = 1.69</p><p id="262e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让<strong class="jp ir">将</strong>数字插入<strong class="jp ir">脊线回归线</strong></p><p id="c0a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">尺寸= </strong> 0.9 + 0.8 * <strong class="jp ir">重量</strong></p><p id="c9ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">残差平方和+ λ *(斜率)^ 2 </strong></p><p id="8346" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">(0.3) ^ 2 + (0.1) ^ 2 + 1 * (0.8) ^2 = 0.74</p><p id="738a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，<strong class="jp ir">λ</strong>值增加，然后<strong class="jp ir">对尺寸</strong>的预测变得<strong class="jp ir">不敏感</strong>，因此我们必须进行<strong class="jp ir">交叉验证</strong>来决定<strong class="jp ir"> λ </strong>值<strong class="jp ir">。</strong></p><pre class="ly lz ma mb gt nu nv nw nx aw ny bi"><span id="6bea" class="nz kv iq nv b gy oa ob l oc od">from sklearn.datasets import load_boston<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.linear_model import Ridge<br/>from sklearn.cross_validation import train_test_split</span><span id="94d7" class="nz kv iq nv b gy oe ob l oc od">boston=load_boston()<br/>boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)</span><span id="706f" class="nz kv iq nv b gy oe ob l oc od">newX=boston_df.drop('Price',axis=1)<br/>print newX[0:3] # check <br/>newY=boston_df['Price']</span><span id="eb42" class="nz kv iq nv b gy oe ob l oc od">X_train,X_test,y_train,y_test=train_test_split(newX,newY,test_size=0.3,random_state=3)</span><span id="fcb8" class="nz kv iq nv b gy oe ob l oc od">rr = Ridge(alpha=0.01)<br/>rr.fit(X_train, y_train)</span><span id="9085" class="nz kv iq nv b gy oe ob l oc od">rr100 = Ridge(alpha=100)<br/>rr100.fit(X_train, y_train)</span><span id="5ab4" class="nz kv iq nv b gy oe ob l oc od">train_score=lr.score(X_train, y_train)<br/>test_score=lr.score(X_test, y_test)Ridge_train_score = rr.score(X_train,y_train)<br/>Ridge_test_score = rr.score(X_test, y_test)Ridge_train_score100 = rr100.score(X_train,y_train)<br/>Ridge_test_score100 = rr100.score(X_test, y_test)</span></pre><h2 id="f889" class="nz kv iq bd kw of og dn la oh oi dp le jy oj ok li kc ol om lm kg on oo lq op bi translated">L 3正则化或弹性网</h2><p id="6c98" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">如果我们很了解我们模型中的所有参数，就很容易选择我们是想使用<strong class="jp ir">套索回归</strong>还是<strong class="jp ir">岭回归。</strong></p><p id="eb4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们的模型有几个变量，我们能做什么？</p><p id="cdb4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要知道太多的变量是很困难的，那么我们当然需要用一些正则化的方法来估计它们。</p><p id="9eab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而我们的变量可能有用或无用，我们事先并不知道。</p><p id="bb17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在应该选择<strong class="jp ir">套索回归</strong>还是<strong class="jp ir">岭回归？</strong></p><p id="dda0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们什么都不用选，反而可以去<strong class="jp ir">弹网回归。</strong></p><p id="914b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像Lasso和Ridge <strong class="jp ir">弹性回归从最小二乘</strong>，<strong class="jp ir">开始，它结合了Lasso回归罚函数和Ridge回归罚函数</strong></p><p id="2b9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">λ1 * |变量1| + … + |变量x |+λ2 *[|变量1|] + … + [|变量x|] </strong></p><p id="0bc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">套索回归+ </strong>我们不用选择任何东西，相反，我们可以去<strong class="jp ir">弹性网回归。</strong></p><p id="4176" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就像套索和岭<strong class="jp ir">弹性回归从最小二乘</strong>，<strong class="jp ir">开始，它结合了套索回归罚函数和岭回归罚函数</strong></p><p id="e240" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">λ1 * |变量1| + … + |变量x |+λ2 *[|变量1|] + … + [|变量x|] </strong></p><p id="9e47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">套索回归+岭回归</strong></p><p id="3303" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对λ1和λ2的不同组合进行交叉验证，以找到最佳值</p><p id="9dde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对λ1和λ2的不同组合进行交叉验证，以找到最佳值</p><h1 id="4eff" class="ku kv iq bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">结论</h1><p id="d987" class="pw-post-body-paragraph jn jo iq jp b jq ls js jt ju lt jw jx jy lu ka kb kc lv ke kf kg lw ki kj kk ij bi translated">在这篇博客中，我们讨论了过度拟合、过度拟合的预防以及正则化技术的类型，正如我们所看到的，Lasso帮助我们在偏差-方差之间进行权衡，并帮助我们选择重要的特征。然而，Ridge只能收缩接近于零的系数。</p><p id="0f85" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望这有助于探索其他测试和方法。</p><p id="9788" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，让我知道我是否错过了<strong class="jp ir">过度配合</strong>概念中的任何内容。</p><p id="5a25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与我联系:- <a class="ae ot" href="https://www.linkedin.com/in/dheerajkumar1997/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><p id="1b33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与我联系:- <a class="ae ot" href="https://github.com/DheerajKumar97" rel="noopener ugc nofollow" target="_blank"> Github </a></p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ou ov l"/></div></figure></div></div>    
</body>
</html>