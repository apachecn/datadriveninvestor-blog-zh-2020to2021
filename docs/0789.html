<html>
<head>
<title>RL Explained- Reinforcing the Intuition and Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RL解释-加强直觉和数学</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/rl-explained-reinforcing-the-intuition-and-math-fd1185369186?source=collection_archive---------4-----------------------#2020-02-17">https://medium.datadriveninvestor.com/rl-explained-reinforcing-the-intuition-and-math-fd1185369186?source=collection_archive---------4-----------------------#2020-02-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="e2cc" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">吃豆人的困境</h1><p id="f1e5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">理解强化学习类似于我童年最喜欢的游戏，吃豆人！考虑以下环境，其中吃豆人肯定知道以下是正确的。</p><ul class=""><li id="a9a3" class="lj lk iq kn b ko ll ks lm kw ln la lo le lp li lq lr ls lt bi translated">可能的动作有向前、向后、向左和向右</li><li id="9606" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">一些动作/物体奖励吃豆人</li><li id="a6ca" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">每步移动1格(适用于所有方向)</li><li id="6c30" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">小圆圈-&gt; 10分</li><li id="c728" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">较大的圆-&gt; 100点</li><li id="c814" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">浆果-&gt; 1000点(头奖！)</li><li id="2b79" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">墙壁-&gt; -100分</li><li id="1d26" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li lq lr ls lt bi translated">鬼-&gt; -500点</li></ul><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/2e18ce0b42f9560587885acf17c6d488.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/0*w68wdqDm1PjFlBFi"/></div></figure><p id="3456" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">通过试验和磨难，吃豆人将每个对象的相对满意度联系起来，并了解哪些具体的<strong class="kn ir">行动</strong>和<strong class="kn ir">状态</strong>最有利于获得更高的<strong class="kn ir">奖励</strong>。</p><p id="fedd" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">从那里开始，Pac-Man将重复他的方法，即<em class="mk">最大化奖励</em>和<em class="mk">最小化惩罚</em>。这个简单的奖励反馈让吃豆人学会了自己的行为；这可以描述为<strong class="kn ir">加强信号</strong>。</p><p id="d2c3" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">吃豆人的思维过程和重新配置其方法的能力可以通过<strong class="kn ir">强化学习</strong>和<strong class="kn ir">深度Q学习</strong>来理解。</p><div class="ml mm gp gr mn mo"><a href="https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd ir gy z fp mt fr fs mu fu fw ip bi translated">深度学习用7个步骤解释-更新|数据驱动的投资者</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc mf mo"/></div></div></a></div><h1 id="02c2" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">强化学习解释</h1><p id="f2a8" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">强化学习探索了<em class="mk">机器学习</em>的基本范式，其中软件代理在一个环境中采取行动，目的是最大化某种累积回报的概念。</p><p id="8fdd" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">本质上，机器学习算法(<em class="mk">代理</em>)评估环境(<em class="mk">状态</em>)，采取<em class="mk">行动</em>，并从环境接收<em class="mk">反馈</em>，反馈可以是<strong class="kn ir">正面</strong>(奖励)或<strong class="kn ir">负面</strong>(惩罚)。奖励是标量<em class="mk">反馈信号</em>，其指示代理在步骤时间<em class="mk"> t. </em>的表现</p><p id="1cfc" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir">环境</strong>接收代理动作并产生新的观察值<em class="mk"> O(t) </em>和标量奖励信号<em class="mk"> R(t) </em>。产生的下一个动作依赖于<strong class="kn ir">历史</strong>，历史是指<em class="mk">序列</em>的观察、动作和奖励信号<em class="mk"> t </em>。</p><p id="e149" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">这些环境可以是<strong class="kn ir">完全可观测环境</strong>，其中智能体直接观测环境状态(<em class="mk"> Oₜ=Sₜᵃ=Sₜᵉ </em>)或<strong class="kn ir">部分可观测环境</strong>，其中智能体间接观测环境(<em class="mk"> Sₜᵃ≠Sₜᵉ </em>)。</p><p id="b152" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir">状态</strong>是用于确定接下来发生什么的信息。有三种主要类型的状态——环境状态<em class="mk"> (Sₜᵉ) </em>、代理状态<em class="mk"> (Sₜᵃ) </em>，以及信息/马尔可夫状态(Sₜ).</p><p id="13d3" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">这种现象可以这样建模。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/51ecc06c7e9a78104c8d560422822e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*199tfFlKitpmxZhV"/></div></div></figure><p id="4179" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">建模的<strong class="kn ir">反馈回路</strong>表示具有t和t + 1的时间步长，其中每个时刻的状态是不同的。与无监督和有监督学习相反，强化学习围绕着<strong class="kn ir">状态-动作对</strong>的<em class="mk">顺序出现</em>。代理人通常希望最大化目标函数，并根据之前每个行为的结果和反馈评估未来的行为。</p><p id="282a" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">例如，大多数视频游戏都是以获得最多分数为目标的——代理人在游戏中获得的每一个额外分数都有助于其随后的行为。通过这些互动和反馈，代理变得更加了解哪些对象和行动与<em class="mk">正面</em>和<em class="mk">负面</em>反馈相关联，以及采取哪些行动可以最大化得分。这些通常可以表示为<strong class="kn ir">奖励计数</strong>。当奖励次数增加时，代理人知道他们在做正确的事情。</p><p id="1834" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">目标函数可以这样建模。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b1201d580e3aac780af4737dfa345d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/0*Z-bmsyF_8m-Aa0DQ"/></div></figure><blockquote class="nj nk nl"><p id="feed" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">奖励函数</em> r <em class="iq">在</em> t <em class="iq">(时间步长)上求和，这意味着目标函数计算通过游戏可以获得的所有潜在奖励。</em> x <em class="iq">代表任意给定时间步的状态(用</em> t <em class="iq">表示)，而</em> r <em class="iq">代表</em> x <em class="iq">和</em> a <em class="iq">的奖励函数。</em></p></blockquote><h1 id="fbda" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">基于价值的强化学习</h1><p id="e153" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">基于价值的强化学习最大化价值函数V(s ),并且代理期望在策略下当前状态的长期回报。它们本质上代表了每个代理存在的状态有多“好”，这等于来自状态(<em class="mk"> s </em>)的回报假设。</p><p id="fbb3" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">价值函数取决于代理执行动作的策略，这意味着当代理使用给定策略来决定动作时，相应的价值函数表示为:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi np"><img src="../Images/8620ef84f7e81475ddb83eecaeecc931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cJb3ebzjL5MxDARQ"/></div></div></figure><p id="9de4" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir">值迭代</strong>实质上是通过改进<em class="mk">值状态估计</em>来计算<strong class="kn ir">最优状态值</strong>函数。在算法将V(s)初始化为随机值之后，它不断更新Q(s，a)和V(s)值，直到它们<strong class="kn ir">收敛</strong>。这个值迭代负责收敛到最优值。通过<em class="mk">动态编程</em>，这个值可以用下面的等式求出。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nq"><img src="../Images/439b65341beea421c312ddad92856461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wWVhutR6dZa95X65"/></div></div></figure><blockquote class="nj nk nl"><p id="4ef5" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated">让我们用下面的例子来说明这个定理。</p></blockquote><p id="8bd6" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">你进入了一个迷宫，出口在左上方。每种状态都提供了四种可能的动作——上、下、左、右。与任何界限的接触都会使你回到原来的位置，每一步的移动都被归因于负一奖励。从<strong class="kn ir">终端状态</strong>开始，所述公式可用于传播<em class="mk"> V* </em>的值</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nr"><img src="../Images/15890a2d47a2e2c4cd2fc97d68fe0081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eoUBq8sRGbJLeQC9"/></div></div></figure><blockquote class="nj nk nl"><p id="13be" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated">考虑到前7次迭代的可视化表示，代理将定位最高的相邻V*值，并对每个状态执行。</p></blockquote><h1 id="afd7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">策略迭代</h1><p id="6797" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">尽管价值迭代结构继续改进V(s)直到函数收敛，但是<strong class="kn ir">最优策略可能在价值函数</strong>之前收敛，因为代理的唯一优先级是找到最优策略。</p><p id="b9c6" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">因此，另一个算法- <strong class="kn ir">策略迭代</strong>在<em class="mk">每个步骤</em>之后重新定义策略，并根据新策略计算值，直到达到收敛值。该方法将总是在最优策略处达到收敛，并且通常比先前的框架需要更少的迭代。</p><blockquote class="nj nk nl"><p id="4916" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">下面是我们应用Q-learning时策略迭代的样子。</em></p></blockquote><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi np"><img src="../Images/c4191284c77d75f28e0e6af97ae77450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5xyXOLzMC1-YMedg"/></div></div></figure><blockquote class="nj nk nl"><p id="3a0a" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">这是4次迭代后的样子。</em></p></blockquote><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi np"><img src="../Images/4573d2429c4fef72743371ab38710d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*__M1fRlE_P07RXvM"/></div></div></figure><h1 id="32d7" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">深度Q学习</h1><p id="aa57" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们再看一遍Pac-Mac的例子——除了这一次，Pac-Man只意识到他的功能运动，而不是<em class="mk">奖励和惩罚对他所做的每个物体和动作都有影响。</em></p><p id="4590" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">因为吃豆人不知道先验的状态转换和奖励模式，他不知道他的行为对环境的影响。相反，他可以简单地观察任何给定状态下的环境并采取行动。</p><p id="5f3c" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir"> Q-learning </strong>描述了吃豆人从<em class="mk">每次与环境</em>的互动中学习的过程；<strong class="kn ir"> Q函数</strong>将代理的状态和动作作为输入，并将它们导向与未来可能的动作相关的可能的回报；本质上<em class="mk">将状态-动作对</em>映射到期望它们产生的值。</p><p id="ef4e" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">本质上，强化学习概述了代理通过状态-动作对序列导航的过程。他们观察来自每个结果的反馈，并使他们的预测适应那些奖励的Q函数，直到他们能够准确地预测最佳路径以最大化奖励；这个预测被称为<strong class="kn ir">策略</strong>。</p><blockquote class="nj nk nl"><p id="078b" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">这些学习算法主要有两类。</em></p></blockquote><h2 id="4c2f" class="ns jo iq bd jp nt nu dn jt nv nw dp jx kw nx ny kb la nz oa kf le ob oc kj od bi translated">基于模型的学习</h2><p id="c9c4" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">代理利用他们与环境交互的历史来近似环境-状态转换和奖励模型。然后，代理使用值迭代或策略迭代来传播最优策略。</p><h2 id="a5c1" class="ns jo iq bd jp nt nu dn jt nv nw dp jx kw nx ny kb la nz oa kf le ob oc kj od bi translated">无模型学习</h2><p id="b6d0" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">不是代理学习特定的奖励函数和环境状态转换，而是代理<em class="mk">直接从与环境的交互中推导出最优策略</em></p><p id="2461" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir"> Q-learning </strong>是无模型学习算法的扩展，其中状态-动作对是从Q(s，a)的样本中近似得到的，这些样本是从与环境的交互中观察到的——这种方法被称为<strong class="kn ir">时差学习</strong>。</p><h1 id="264c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">勘探和开发</h1><p id="0bf7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">探索与利用的两难困境描述了代理在学习过程中如何选择采取哪些行动。<strong class="kn ir">ε-贪婪方法</strong>经常在代理决定一个<em class="mk">随机动作</em>(因此，探索)或概率(1-ε)的每一步使用，代理根据Q函数值的当前估计选择动作。随着时间的推移，代理对Q值的估计变得更有信心，而ε值下降。</p><h1 id="d998" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">神经网络和深度Q学习</h1><p id="0e0b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">由于神经网络基本上是函数逼近器，因此可以通过学习构建Q值的<strong class="kn ir">状态-动作对</strong>来利用它们预测值或政策函数。神经网络不是使用Q表，而是简单地在来自状态或动作空间的数据集上训练，以预测与每个连续运动相关联的值。</p><blockquote class="nj nk nl"><p id="1320" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">通过反向传播和梯度下降的迭代过程，这些权重和偏差最终被优化以最小化误差函数。</em></p></blockquote><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi np"><img src="../Images/21f4c8b37e5712f1b4dff9483fc5ede3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MgaY7J9XcMhezm8h"/></div></div></figure><p id="218b" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir">卷积网络</strong>，负责图像识别和分类，在环境可视化时常用于识别智能体的当前状态。应用强化学习架构不同于监督学习架构，因为监督学习将标签与图像匹配。</p><p id="d8f9" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">下面的示例显示了神经网络将如何计算每个类名为真的概率。下面的卷积层可以预测图像是95%的猫和5%的狗。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/66a1cba0aed310c09765a6b060f004c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/0*Rd-dJ-3bthreF1sl"/></div></figure><p id="d49e" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">然而，在强化学习中——输入的图像反映了智能体的当前状态，卷积神经网络用于对智能体接下来可能执行的动作进行排序。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c7983ced2718f37d3eb5243a1488b156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/0*LoGcdY3L0nUEowrr"/></div></figure><p id="7e16" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">看这个例子，神经网络可能预测跑步将获得5分，而跳跃将获得10分。这说明了策略代理的功能及其在将状态映射到最佳动作中的作用，最佳动作可以描述为<em class="mk"> a= π(s)。</em></p><p id="e212" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">我们的Q值方程看起来像这样。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi og"><img src="../Images/1fbbe35307732fbed44c7b4a60b21998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X5Pz3oqjwk4Z3vE_"/></div></div></figure><p id="36bd" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated">然而，由于神经网络被用来代替Q表的使用，原始方程可以进一步简化。由于反向传播优化器<em class="mk">已经具有学习速率</em>，等式中的这一部分可以被移除。在移除学习率之后，两个Q(s，a)项都被取消，并且也可以被移除。</p><h1 id="a5be" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">培训过程</h1><p id="2fb7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然强化学习模型不需要任何特定的训练数据集，但它们需要在探索环境时收集的数据点。</p><p id="1d50" class="pw-post-body-paragraph kl km iq kn b ko ll kq kr ks lm ku kv kw mh ky kz la mi lc ld le mj lg lh li ij bi translated"><strong class="kn ir">每次经历=(先前状态，动作，奖励功能，新状态)</strong></p><blockquote class="nj nk nl"><p id="c390" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">用每个经验训练模型基本上遵循相同的框架</em></p></blockquote><ol class=""><li id="1ab7" class="lj lk iq kn b ko ll ks lm kw ln la lo le lp li oh lr ls lt bi translated">从先前状态估计Q值</li><li id="1729" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">估计新状态的Q值</li><li id="d8d7" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">使用已知的相关奖励计算每个行动的目标Q值</li><li id="e0b0" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">用输入=(旧状态)和输出=(目标Q值)训练模型</li></ol><h1 id="b4eb" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">强化学习——人工智能的未来</h1><p id="7c76" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">强化学习的含义和应用绝对是疯狂的——在自动驾驶汽车的RL架构、网络系统配置、个性化医疗和视频游戏开发之间，这种机器学习的范式几乎可以用于任何事情。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h1 id="169b" class="jn jo iq bd jp jq op js jt ju oq jw jx jy or ka kb kc os ke kf kg ot ki kj kk bi translated">最后一件事</h1><blockquote class="nj nk nl"><p id="6c58" class="kl km mk kn b ko ll kq kr ks lm ku kv nm mh ky kz nn mi lc ld no mj lg lh li ij bi translated"><em class="iq">感谢你阅读我的文章，我希望你能学到更多关于</em> <strong class="kn ir"> <em class="iq">强化学习背后的数学和直觉。如果你能做以下事情来支持我这个作者，那对我来说就意味着一切！</em></strong></p></blockquote><ol class=""><li id="8bea" class="lj lk iq kn b ko ll ks lm kw ln la lo le lp li oh lr ls lt bi translated">注意👏图标？给我的文章发些掌声</li><li id="8ecb" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated"><strong class="kn ir">在<a class="ae ou" href="https://www.linkedin.com/in/suraj-bansal/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</strong>和我👈</li><li id="d0d7" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">跟着我去✍️</li><li id="1633" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">查看我的作品集，了解我的最新作品💪</li><li id="4091" class="lj lk iq kn b ko lu ks lv kw lw la lx le ly li oh lr ls lt bi translated">关注我的旅程，订阅我的每月时事通讯🦄</li></ol><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ov ow l"/></div></figure></div></div>    
</body>
</html>