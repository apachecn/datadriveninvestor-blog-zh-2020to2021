<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/data-science-support-vector-machine-svm-fa74fb12d349?source=collection_archive---------1-----------------------#2020-05-03">https://medium.datadriveninvestor.com/data-science-support-vector-machine-svm-fa74fb12d349?source=collection_archive---------1-----------------------#2020-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/a91eb36a555fab996b8d6f3daeb580c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*HSpI9sDZ7gjiFHcshcc2Gg.png"/></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Source:CannyPic</figcaption></figure></div><div class="ab cl iz ja hu jb" role="separator"><span class="jc bw bk jd je jf"/><span class="jc bw bk jd je jf"/><span class="jc bw bk jd je"/></div><div class="ij ik il im in"><h1 id="6f4d" class="jg jh ji bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated"><strong class="ak">数据科学:支持向量机(SVM) </strong></h1><p id="e868" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ld">简介:</strong></p><p id="48a9" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">机器学习包括预测和分类数据。有各种算法来完成这项任务。被广泛称为SVM的支持向量机就是其中之一。SVM是用于回归和分类问题的监督机器学习算法。</p><p id="8ad5" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">直觉:</strong></p><p id="a133" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">SVM的主要目的是在N维(N个特征)空间中找到一条线或超平面，该线或超平面对数据点进行最佳分类。</p><p id="8671" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">让我们看看下面的线性数据集的例子。</p><p id="635d" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">我们在2维空间中有下面的数据点，我们的目标是画一条线，用最大宽度将下面的两个类分开。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi lj"><img src="../Images/4ec3b0fbb5b38b7a4cabbdc4c981eb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ru6GpCg-5y5mZouHn1ECpA.jpeg"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Pic1: Dataset with two class</figcaption></figure><p id="647f" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">我们的期望是画一条类似下图的线，最大宽度在两个数据点(蓝色和绿色)下面分开。</p><div class="ip iq gp gr ir ls"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="lt ab fo"><div class="lu ab lv cl cj lw"><h2 class="bd ld gy z fp lx fr fs ly fu fw lz bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="ma l"><h3 class="bd b gy z fp lx fr fs ly fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="mb l"><p class="bd b dl z fp lx fr fs ly fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh it ls"/></div></div></a></div><p id="222e" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">更大的宽度将导致更一般化的模型，其可以准确地预测外部世界的未知/新数据。宽度越小，就不能很好地概括未知数据，我们的预测可能会出错。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mi"><img src="../Images/a3c69d096bc5e1c35c2ec00e44e40906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Fjw9iN6e2yUkA60aNtIew.jpeg"/></div></div></figure><h2 id="d232" class="mj jh ji bd jj mk ml dn jn mm mn dp jr kq mo mp jv ku mq mr jz ky ms mt kd mu bi translated">少数缩写:</h2><p id="d352" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated"><strong class="kh ld">蓝色和绿色</strong>:这些是数据点</p><p id="57cf" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">外线(---)</strong>:这些被称为边界，由两类(绿色和蓝色)的最近向量支持。通过最近支持向量计算分类边界。</p><p id="be92" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">中线(— ) </strong>:中线在二维空间称为直线，在三维和四维空间称为平面和超平面。维度将根据数据点的复杂性不断增加。</p><h2 id="8994" class="mj jh ji bd jj mk ml dn jn mm mn dp jr kq mo mp jv ku mq mr jz ky ms mt kd mu bi translated"><strong class="ak">如何处理非线性数据集？</strong></h2><p id="a9ca" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ij bi translated">可用直线线性分离的数据集称为线性数据集，不用直线分离的数据集称为非线性数据集。</p><p id="52dc" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">在SVM有一个概念<strong class="kh ld">内核技巧</strong>用于分离非线性数据集。根据内核窍门:</p><p id="6cf6" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><em class="mv">这里的想法是将非线性可分离数据集映射到一个更高维的空间，在那里我们可以找到一个能够最好地分离数据集的超平面。</em></p><p id="e29b" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">对每个数据实例应用核函数，以将原始非线性观察值映射到高维空间中，在该空间中它们变得可分离。基本低于转化</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/1951219062fb5460ac1a4a2555a08e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*ow5itF4So_DMYXanCm3juA.jpeg"/></div></figure><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mi"><img src="../Images/8b68b7074dabeb93157bd45ffe5c3e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SqugGh2MLZNa0FM2mx3_8Q.jpeg"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">Transforming data from 2-D to 3-D</figcaption></figure><p id="ba58" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">注</strong>:内核函数不断增加维度，直到数据线性可分。</p><p id="57ee" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">有三种类型的内核:</strong></p><ol class=""><li id="2c22" class="mx my ji kh b ki le km lf kq mz ku na ky nb lc nc nd ne nf bi translated">线性核:用于线性数据分类</li><li id="536f" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">多项式核:用于非线性数据分类</li><li id="cbf3" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">径向或径向基核:用于非线性数据分类</li></ol><p id="c25c" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">还有另外两个非常重要的参数有助于N维中SVM线或超平面的生成。</p><ol class=""><li id="3d53" class="mx my ji kh b ki le km lf kq mz ku na ky nb lc nc nd ne nf bi translated"><strong class="kh ld"> C </strong></li><li id="50f2" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated"><strong class="kh ld">伽玛</strong></li></ol><p id="a6f4" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld"> C : </strong>这是一个正则化参数，控制实现低训练误差和低测试误差之间的权衡，即针对未知数据推广您的分类器的能力。</p><p id="a1dc" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">它基本上是说，我的模型可以考虑多少错误，或者你可以说你想要惩罚多少错误分类的点。</p><p id="1eef" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld"> 1。当C太大</strong>时，优化算法试图尽可能地减少错误分类，从而产生试图正确分类每个训练数据的超平面。</p><p id="9642" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">这将导致分类器泛化属性的损失。所以基本上决策边界会很小。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mi"><img src="../Images/25c275a42a2342dd57e79903a3e0ffde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVnfjvfRYKQe5QQVi8c36w.jpeg"/></div></div></figure><p id="d7ef" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld"> 2。当C太小</strong>时，由于决策边界较宽，会出现数据点的误分类。较宽的决策边界可以很好地概括训练和测试数据，但它可能会错误地分类少数记录。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nl"><img src="../Images/22345d272bac67c25cda27190ec8700c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rt_N_S9hdYFH4EKqaC6rdw.jpeg"/></div></div></figure><p id="e191" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">因此，建议找出c的最佳值。</p><p id="8120" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">伽玛:</strong></p><p id="d358" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">定义单个训练示例的影响范围，低值表示“远”，高值表示“近”。</p><p id="44ee" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">高伽马值:</strong></p><p id="de7f" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">对于高伽玛值，决策边界的确切细节将取决于非常接近它的点。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nm"><img src="../Images/5958e91b8376c431e80c9ef2e32c3326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4G3QBWPfb80v97pWDClygQ.jpeg"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">With High Gamma Value</figcaption></figure><p id="fb3a" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">低伽马值:</strong></p><p id="4cba" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">低gamma值表明，当我们实际决定在哪里绘制决策边界时，即使是很远的点也会被考虑在内。</p><figure class="lk ll lm ln gt is gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nn"><img src="../Images/3f4da6bdbdc2fe3885baa86fd1cbd7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4QoRoW3sDR1jKYIiPrbtA.jpeg"/></div></div><figcaption class="iv iw gj gh gi ix iy bd b be z dk">With Low Gamma Value</figcaption></figure><p id="4937" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">这里再次应该选择γ的最佳值。</p><p id="b594" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">请参考下面的SVM代码</strong>:</p><figure class="lk ll lm ln gt is"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="36dd" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">支持向量机优势:</strong></p><ol class=""><li id="441a" class="mx my ji kh b ki le km lf kq mz ku na ky nb lc nc nd ne nf bi translated">当阶级之间有明确的界限时，SVM相对来说运作良好。</li><li id="ed03" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">SVM在高维空间(如文本数据)中更有效</li><li id="1694" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">它适用于线性和非线性数据集。</li><li id="574c" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">支持向量机目前是从文本到基因组数据的许多分类任务中表现最好的。</li></ol><p id="1abc" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated"><strong class="kh ld">缺点:</strong></p><ol class=""><li id="6dad" class="mx my ji kh b ki le km lf kq mz ku na ky nb lc nc nd ne nf bi translated">SVM算法不适合大数据集。</li><li id="3b0e" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">支持向量机不擅长处理有噪声、意义不大或损坏的数据。</li><li id="5e0b" class="mx my ji kh b ki ng km nh kq ni ku nj ky nk lc nc nd ne nf bi translated">它被认为是黑匣子。选择内核、C和Gamma的最佳值是通过试凑法完成的。GridSearchCV就是为了解决这个问题而引入的，它可以为任何模型提供最佳参数。</li></ol><p id="3a16" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">结论:我已经根据我对这个主题的了解和参考了这么多地方来解释SVM。希望你会喜欢这篇文章。如果你想激励我写更多，请点击拍手(最多50次)。</p><p id="ee39" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">想要连接:</p><p id="dd7b" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">链接进来:<a class="ae nq" href="https://www.linkedin.com/in/anjani-kumar-9b969a39/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/anjani-kumar-9b969a39/</a></p><p id="850e" class="pw-post-body-paragraph kf kg ji kh b ki le kk kl km lf ko kp kq lg ks kt ku lh kw kx ky li la lb lc ij bi translated">如果你喜欢我在Medium上的帖子，并希望我继续做这项工作，请考虑在<a class="ae nq" href="https://www.patreon.com/anjanikumar" rel="noopener ugc nofollow" target="_blank"><strong class="kh ld"/></a>上支持我</p><figure class="lk ll lm ln gt is"><div class="bz fp l di"><div class="nr np l"/></div></figure></div></div>    
</body>
</html>