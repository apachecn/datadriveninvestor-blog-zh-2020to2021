<html>
<head>
<title>Weight Initialization Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">权重初始化技术</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/weight-initialization-techniques-5df62e9b41a1?source=collection_archive---------7-----------------------#2020-12-11">https://medium.datadriveninvestor.com/weight-initialization-techniques-5df62e9b41a1?source=collection_archive---------7-----------------------#2020-12-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d099" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么最适合你</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cbe4a9673bb688bdea2f33c811cf3a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EALhcuyGSSSiFuzkkdxxeg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image <a class="ae kv" href="https://unsplash.com/photos/uc5Use-klm0" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d890" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重负责单元之间的连接，在神经网络中，这些可以随机初始化，然后在<a class="ae kv" href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8" rel="noopener" target="_blank">反向传播</a>中更新，以减少损失。</p><p id="431f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">初始化权重之前，需要记住几件重要的事情:</p><p id="035d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1)权重应该很小</strong>，但不能太小，因为它会产生像<a class="ae kv" href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8" rel="noopener" target="_blank">消失渐变问题</a>(消失为0)这样的问题。也就是说，它将永远收敛到全局最小值。</p><p id="c380" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重不能太高，因为这会导致像爆炸梯度问题(模型的权重爆炸到无穷大)这样的问题，这意味着有很大的空间可用于搜索全局最小值，因此收敛变得很慢。</p><p id="f5a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了防止网络激活的梯度消失或爆炸，我们需要以下规则:</p><ol class=""><li id="31bd" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">激活的<em class="mb">意味着</em>应该为零。</li><li id="460e" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">激活的<em class="mb">方差</em>应该在每一层保持相同。</li></ol><p id="a2f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2) </strong>此外，如果这些将被初始化为零，他们将不会学到任何东西。</p><p id="69e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3) </strong> <strong class="ky ir">权重应该有方差</strong>，应该有一些均值，应该有一个标准差。</p><p id="95c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">几种权重初始化技术有:</strong></p><p id="bcb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1)正态或天真初始化</strong> -在正态分布中，权重可以是正态或高斯分布的一部分，平均值为零，标准差为一个单位。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c7a60675882ad6847c60af14064af8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/0*JIW3r-1_vUv1Rfn-.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="4f0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">进行随机初始化，以使收敛不会达到错误的最小值。</p><p id="dc0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Python中，它是这样实现的</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="9945" class="mn mo iq mj b gy mp mq l mr ms">np.random.normal(loc=0.0, scale=1.0) * 0.01 #i.e a small number</span></pre><p id="9021" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Keras中，它可以简单地写成超参数</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="0a32" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer='random_normal'<br/>#or<br/>kernel_initializer=kernel_initializers.RandomNormal(mean=0.,stddev=1.)</span></pre><p id="9a93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2)均匀初始化:</strong>在权重的均匀初始化中，权重属于范围a，b中的均匀分布，a和b的值如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/deff5c796b6791abd26ac91039698ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*ugiqyqgW-3nCpVq05GTtEw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="e357" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无论何时使用<a class="ae kv" href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8" rel="noopener" target="_blank">s形</a>激活功能，Uniform都能正常工作。</p><p id="c7c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在喀拉斯，可以这样做</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="135a" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer=kernel_initializers.RandomUniform(minval=-0.05,maxval=0.05)</span></pre><p id="cb4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3) <strong class="ky ir"> Xavier/ Glorot权重初始化:</strong></p><p id="08e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当正态分布应用于深度神经网络时，深度神经网络很难收敛到全局最小值，这意味着零均值和固定标准偏差。</p><p id="5432" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当进行反向传播时，没有考虑正态分布情况下的权重变化，这导致太大或太小的激活值，这又分别导致爆炸梯度和消失梯度问题。</p><p id="b753" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个问题只会在更深层次的神经网络中增加。</p><p id="01ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了克服这个问题，Xavier初始化被引入。它保持每一层的方差不变。我们将假设我们的层的激活正态分布在零附近。</p><p id="00b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Glorot和Xavier相信，如果他们在所有层中保持激活的变化，那么与使用gap更大的标准初始化相比，向前和向后收敛将更快。</p><p id="8cb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与<a class="ae kv" href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8" rel="noopener" target="_blank"> tanh、sigmoid </a>激活功能配合良好。</p><div class="mu mv gp gr mw mx"><a href="https://www.datadriveninvestor.com/2020/11/19/how-machine-learning-and-artificial-intelligence-changing-the-face-of-ecommerce/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd ir gy z fp nc fr fs nd fu fw ip bi translated">机器学习和人工智能如何改变电子商务的面貌？|数据驱动…</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">电子商务开发公司，现在，整合先进的客户体验到一个新的水平…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl kp mx"/></div></div></a></div><p id="eae4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> a)泽维尔正态分布</strong></p><p id="5bd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Xavier正态分布中，权重属于均值为零且标准差如下的正态分布:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/cc3a9874eda91feac61f154b60af6cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*-KzmvEhsaI4XrxF7rIvlyA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="d8fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在喀拉斯是这样做的</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="dff6" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer=kernel_initializers.GlorotNormal(seed=None)</span></pre><p id="ea17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> b)泽维尔均匀分布</strong></p><p id="185f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Xavier均匀分布中，权重属于a和b范围内的均匀分布，定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8814cce2ae79f7e7245ca27da5261510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*T1ZTcCTrSmlucldEZ_z-Lw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="1ece" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">据说它与<a class="ae kv" href="https://towardsdatascience.com/neural-network-its-internal-functioning-and-uses-7adc4d37f3d8" rel="noopener" target="_blank"> sigmoid和tanh activation </a>功能配合得很好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/25ece265111b1942ed7333688a06aff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eu6VJSWARBY7fVcQiBecjQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd np">Sigmoid </strong>[Image by Author]</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/4c4db98d8a9c131788d267cf0290a82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bto7tqqSteKMTo_nxpdgFw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd np">tanh </strong>[Image by Author]</figcaption></figure><p id="4b5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在喀拉斯</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="d651" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer=kernel_initializers.GlorotUniform(seed=None)</span></pre><p id="ed37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 4) He-Initialization- </strong>当使用以零为中心且输出范围在-1，1之间的激活函数(如tanh和softsign)时，激活输出的平均值为0，平均标准偏差约为1。</p><p id="2919" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果使用ReLu而不是tanh，可以观察到，平均而言，它的标准偏差非常接近2的平方根除以输入连接。</p><p id="d8a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">明凯提出，当使用Relu激活时，权重以这种方式初始化，因为这种方法将较深神经网络的标准偏差保持在1左右。</p><p id="ae29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种权重初始化方法与Relu激活功能配合良好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/eacfe48231c83dfc7b1f492816d1017e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tC2NpmIlbIOEs9O4bocsg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd np">Relu </strong>[Image by Author]</figcaption></figure><p id="f749" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> a)他正常初始化</strong></p><p id="d834" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在He-Normal初始化方法中，权重属于正态分布，其中均值为零，标准差如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ea14336a692a9d8278ffd7c35d2782c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*siCVhoxCBQUCvW58SOXR6Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="51f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Keras中，这可以作为超参数来完成</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="1138" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer=kernel_initializers.HeNormal(seed=None)</span></pre><p id="3e11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2与Relu一起工作很好，即使一些神经网络被停用或死亡，权重仍然保持良好。</p><p id="0351" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在tanh中使用1，因为它足以表示重量。</p><p id="38a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> b)统一初始化:</strong></p><p id="1179" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在均匀初始化中，权重属于均匀分布范围，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/dd28e29e96da49f2cee36e96d01588bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*Jceej2LU-xrMMyFlAJ1Sjg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="1ea0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Keras中，它被称为内核初始化器</p><pre class="kg kh ki kj gt mi mj mk ml aw mm bi"><span id="43a0" class="mn mo iq mj b gy mp mq l mr ms">kernel_initializer=kernel_initializers.HeUniform(seed=None)</span></pre><p id="7266" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="1f98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望看完这篇博客后，你一定已经知道tanh，Sigmoid和ReLu哪种体重初始化更好了。虽然受到破坏，但这项任务对于神经网络的良好运行非常重要。通过对它的理解和新的研究，你可以选择最适合你的技术。在keras中，这个任务非常简单，只需要一个参数就可以调用。</p><p id="5e55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="bd14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb">原载于2020年12月11日</em><a class="ae kv" href="https://www.numpyninja.com/post/weight-initialization-techniques" rel="noopener ugc nofollow" target="_blank"><em class="mb">【https://www.numpyninja.com】</em></a><em class="mb">。</em></p><h2 id="32bb" class="mn mo iq bd np oa ob dn oc od oe dp of lf og oh oi lj oj ok ol ln om on oo op bi translated">访问专家视图— <a class="ae kv" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank">订阅DDI英特尔</a></h2></div></div>    
</body>
</html>