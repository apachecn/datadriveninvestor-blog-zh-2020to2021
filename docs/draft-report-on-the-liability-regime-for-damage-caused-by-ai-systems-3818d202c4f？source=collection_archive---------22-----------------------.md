# 关于人工智能系统造成损害的责任制度的报告草稿

> 原文：<https://medium.datadriveninvestor.com/draft-report-on-the-liability-regime-for-damage-caused-by-ai-systems-3818d202c4f?source=collection_archive---------22----------------------->

![](img/aa14c66a265aa2bc05545523315f6ada.png)

去年 4 月 27 日，欧洲议会公布了一份报告草案，其中就人工智能系统造成的损害的责任制度向委员会提出了建议。法律事务委员会应在 2020 年 6 月底或 7 月初讨论报告草稿的修正案。计划于 2020 年 9 月 28 日就该报告进行表决，随后于 2020 年 10 月在全体会议上进行表决。

*前言*

导言中已经确定,“赔偿责任”的概念在我们的日常生活中发挥着双重作用，一方面确保遭受损害的人有权向被证明应对损害负责的人索赔，另一方面为自然人和法人避免造成损害提供经济奖励；任何前瞻性的责任框架都必须在有效保护潜在的损害或伤害受害者与提供足够的余地使新技术的发展成为可能之间达成平衡，同时考虑到任何责任框架的目标都应该是为所有各方提供法律确定性，无论是生产者、经销商、受影响者还是任何其他第三方。

鉴于对损害赔偿责任制度的监管概述，还澄清了人工智能(AI)系统对现有责任框架提出了重大的法律挑战，并可能导致其不透明性(“黑盒”)可能导致极其昂贵甚至不可能确定谁在控制与 AI 系统相关的风险，或者最终导致有害操作的代码或输入！

因此，本文认为将人工智能系统引入社会和经济的挑战是当前政治议程上最重要的问题之一，因为基于人工智能的技术可以在几乎所有领域改善我们的生活，从个人领域到全球挑战。因此，在这一领域提供一个全面和一致的监管框架成为一项至关重要的要求，因为数字单一市场的特点是快速的跨境动态和国际数据流动，因此需要充分协调。

没有必要对运作良好的责任制度进行彻底改革，但信息披露系统的复杂性、连通性、不透明性、脆弱性和自主性对避免遭受损害或财产受损的人最终得不到赔偿是一个重大挑战。

[](https://www.datadriveninvestor.com/2020/09/15/a-college-student-used-a-language-generating-ai-tool-to-create-a-viral-blog-post/) [## 一名大学生使用语言生成人工智能工具创建了一个病毒式博客帖子|数据驱动…

### 作为作家，我们喜欢告诉自己，我们处在一个无法自动化的职业中，至少短期内不会。但是…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/09/15/a-college-student-used-a-language-generating-ai-tool-to-create-a-viral-blog-post/) 

因此，结论是，真正的数字单一市场需要通过监管实现全面协调。

必须解决人工智能(AI)的发展带来的新的法律挑战，为制造商、制造商、相关人员和任何其他第三方建立最大限度的法律确定性。

产品责任指令(PLD) 85/374/EEC 已被证明是获得缺陷产品造成的损害赔偿的有效手段，但为了整个欧盟的法律确定性,“后端运营商”应遵守与创作者、生产者和开发者相同的责任规则。

本报告侧重于"部署者"这一数字，这是一个取自"部署"一词的英法术语，在计算机科学中，这意味着向客户交付或发布一个应用程序或软件系统，并对其进行安装、配置和试运行或操作，以开发和测试结束，并开始维护。

从现在起，为了方便起见，我们将它称为人工智能系统的“部署者”。

*操作者的责任*

涉及客户的责任规则原则上应涵盖人工智能系统的所有操作，无论操作发生在哪里，也无论是实际操作还是虚拟操作；然而，公共场所的操作使第三方面临风险，这是需要进一步审查的案件，因为损害或伤害的潜在受害者往往不知道该操作，并且没有针对可能的操作者的常规合同责任索赔，这使得他们很难证明人工智能系统生产商的直接责任，如果不是不可能的话。

也可能存在不止一个运营商的情况；在这种情况下，所有负责“管理”该系统的人都应承担连带责任，同时有权根据不同风险的不同责任规则按比例相互索赔！因此，为高风险人工智能系统建立严格的责任制度似乎是合理的(在这种情况下，自主操作涉及以随机方式对一个或多个人造成损害的重大潜在原因，并且不可能事先预测，考虑到可能损害的严重性、风险成为现实的概率和人工智能系统的使用方式之间的相互作用)。

建议在本工作的附录中列出这些高风险活动，随后发布一项法规，并责成委员会每六个月审查一次附录，如有必要，通过授权法案进行修订。由人工智能系统驱动的所有活动、装置或过程，造成损害，但未列入拟议条例的附件，仍应受到基于过失的赔偿责任的约束，前提是有关人员可以受益于对项目负责人过失的推定。

如果部署者目前正在从事一项新技术、产品或服务，该新技术、产品或服务属于附件中规定的现有关键领域之一，并且以后可能符合高风险人工智能系统的条件，则部署者必须通知委员会。

*保险和人工智能系统*

充足的风险保障对于确保公众能够信任新技术也是至关重要的，尽管新技术可能会受到损害:拟议条例附件中所列的高风险 IA 系统的所有安装者都应投保责任保险；这一强制性保险计划应涵盖建议书本身所列的赔偿金额和数额。

严重程度应根据作业造成的潜在损害量、受影响人数、潜在损害的总价值和对整个社会的损害来确定。概率应根据算法计算在决策过程中的作用、决策的复杂性和影响的可逆性来确定。

在第 16 段中，对"部署者"的尽职要求应与以下方面相称:

一、内部审计制度的性质，

二。受法律保护的潜在当事人的权利，

三。IA 系统可能造成的潜在损害以及

四。损坏的可能性。

应该考虑到程序员可能对人工智能系统中使用的算法和数据了解有限。应该假设“部署者”已经在选择合适的、经过认证的、受监控的(操作可靠性)、更新的人工智能系统时给予了应有的关注，并且已经通知了制造商操作过程中的任何违规行为。

应该始终明确的是，任何创建、维护、控制或干扰人工智能系统的人都必须对活动、设备或过程造成的损害负责。这源于普遍接受的司法概念，根据这一概念，如果风险成为现实，给公众造成风险的人应承担责任。

因此，在生产者和管理者之间以及生产者和管理者与欧洲和非欧洲生产者在欧洲联盟内任命的 AI 代表之间提供持续的信息交流是适当的。

*关键点*

考虑到上述所有适当的前提，第 2 条规定:“国际保险系统的“经营者”与因国际保险系统而遭受损害的自然人或法人之间的任何协议，凡倾向于规避或限制本条例规定的权利和义务的，无论是在损害发生之前还是之后订立的，均应被视为无效。

第 3 条给出了以下定义:

-人工智能系统":"通过分析某些输入并以一定程度的自主性进行干预以实现特定目标而显示出智能行为的系统。人工智能系统可以完全基于软件，在虚拟世界中运行，也可以嵌入硬件设备中”；

-"自主":"一个人工智能系统，它通过感知某些输入而工作，无需遵循一系列预先确定的指令，尽管其行为受到给予它的目标和其开发者作出的其他相关设计选择的限制"；

-“高风险”:“在独立运行的人工智能系统中，随机对一人或多人造成伤害或损害且无法提前预测的重大可能性；潜在风险的重要性取决于可能的伤害或损害的严重性、风险成为现实的可能性以及人工智能系统的使用方式之间的相互作用”；

- (d)“部署者”系指决定使用内部审计系统、控制相关风险并从其运作中获益的人；

--(e)‘受影响或受影响的人’是指受到人工智能系统驱动的物理或虚拟活动、装置或过程造成的伤害或损害的任何人，并且不是其操作者；

- (f)`伤害或损害'是指影响自然人或法人的生命、健康、人身完整或财产的负面影响，但非物质损害除外；

- (g)“生产商”是指 IA 方案的发起人或经营者，或理事会指令 85/374/EEC 第 3 条中定义的生产商。

第 5 条涉及赔偿金额(如果有的话),最高可达 1，000 万欧元，如果因同一高风险 IA 计划的相同操作而造成一人或多人死亡或健康或身体完整性受损；或对财产造成损害，包括对一人或多人拥有的几件物品造成损害，赔偿总额最高可达 200 万欧元。

第 7 条涉及时效期限，人身伤害的时效期限为自损害发生之日起 30 年，财产损害的时效期限为 10 或 30 年。

重要的是第 10 条，该条涉及所谓的疏忽，即当损害是由于有关人员的行为而发生时，该行为造成了损害，触发了人工智能系统的“恶意”程序，在这种情况下，通过证明人工智能系统本身生成的数据，可以免除“操作者”的责任。

在附录中，我们发现，最后，该计划的部门被认为是“高风险”。

版权所有

***拉斐尔·阿格莫，律师***

**访问专家视图—** [**订阅 DDI 英特尔**](https://datadriveninvestor.com/ddi-intel)