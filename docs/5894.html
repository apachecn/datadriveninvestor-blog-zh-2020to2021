<html>
<head>
<title>Taking BatchNorm For Granted</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将BatchNorm视为理所当然</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/taking-batchnorm-for-granted-a672effbc2ad?source=collection_archive---------8-----------------------#2020-10-04">https://medium.datadriveninvestor.com/taking-batchnorm-for-granted-a672effbc2ad?source=collection_archive---------8-----------------------#2020-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="493f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为什么我们会忽略神经网络中某些层的魔力，以及如何更好地理解它们+来自一篇极其有趣的论文的9个要点。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/91f719cfee97e53b63efa0de9a27d5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2SRjEdy7CfX2Jz_xZhYLXQ.jpeg"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">A batch of flowers (<a class="ae ll" href="https://www.pinterest.com/pin/834503005944030902/?nic_v2=1aaBt7TUy" rel="noopener ugc nofollow" target="_blank">https://www.pinterest.com/pin/834503005944030902/?nic_v2=1aaBt7TUy</a>)</figcaption></figure><p id="b3bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">批范数是神经网络中使用最广泛的层之一。自从它问世以来，训练更快、更准确、更抗变化的神经网络成为可能。听起来很神奇，不是吗？你可能会想，对于如此神奇的东西，实现起来一定非常困难。(你可能错了)</p><p id="e440" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实是，由于神经网络的黑盒性质，我们开始认为这种魔力是理所当然的。当然，关于它如何以及为什么会有这样的效果，有许多假设，但是我最近发现了一篇<a class="ae ll" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank">论文</a>，它做出了<strong class="js iu">认真</strong>的尝试去理解它。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="f81c" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">什么是批量定额</h1><p id="e6b6" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">在我们开始做任何事情之前，先快速了解一下批处理规范。<br/>我们为什么需要它？标准化网络输入。这将允许网络“聚焦”并从数字上了解什么更重要。<br/>现在，它看起来如何(注意，这些是组件，而不是整个实现)</p><p id="cbe2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我们首先生成一个随机数组。输入γ、β是可学习的参数，我们将在后面研究。ϵ是一个很小的数字，它会阻止我们的值变成0。我们首先取平均值，然后取方差，然后用它们来标准化数据。最后，我们取一个乘积，用参数求和。</p><p id="d764" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，考虑随机数组是一批数据，我们将它应用于整批数据，而不是单个平均值，我们取一个移动平均值。(也称为基于流数据的连续变化值)。大概就是这样。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="83ea" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">火车！！</h1><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/63e1069351adfaef350bdae5c601db08.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*iOtVpuDYgS3OOfAygVrr-g.jpeg"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><a class="ae ll" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.merriam-webster.com%2Fwords-at-play%2Fthe-history-of-the-word-train&amp;psig=AOvVaw38n5cRpT5J_NpyiHo4QAmM&amp;ust=1601922053977000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCJCz3-vGm-wCFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">Train</a> xD</figcaption></figure><p id="b3ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">好的，这一段会在Pytorch。我会试着解释我所做的一切，但我不能在这里粘贴完整的代码，因为这会变得太大。因此，我将在这里展示与标准培训的不同之处。</p><p id="8ae3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完整的代码(带注释)可以在<a class="ae ll" href="https://github.com/SubhadityaMukherjee/pytorchTutorialRepo/tree/master/BatchnormOnlyBatchnorm" rel="noopener ugc nofollow" target="_blank">我的资源库</a>中找到</p><p id="f454" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以我们的主要工作流程和其他深度学习项目一样。</p><ol class=""><li id="c940" class="mq mr it js b jt ju jx jy kb ms kf mt kj mu kn mv mw mx my bi translated">加载数据(检查main.py)</li><li id="5558" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">对其进行预处理(检查main.py)</li><li id="815d" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">枚举批次</li><li id="2aef" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">优化损失函数</li><li id="27c3" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">测试一下</li></ol><div class="ne nf gp gr ng nh"><a href="https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">人工智能、深度学习和医疗实践|数据驱动的投资者</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">人工智能和深度神经学习的效用看起来可能是合法和有前途的，特别是…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv lf nh"/></div></div></a></div></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="3b03" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">密码</h1><p id="bdb2" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">让我们首先得到我们需要的图书馆。又名PyTorch和tqdm(这是一个非常小的进度条助手，我非常喜欢)</p><pre class="kw kx ky kz gt nw nx ny nz aw oa bi"><span id="648d" class="ob ln it nx b gy oc od l oe of">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from tqdm import tqdm</span></pre><p id="eec2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们考虑下一步之前，让我们先试着了解我们想要什么。我们需要像往常一样训练网络，但是要确保<strong class="js iu">只训练</strong>batch norm层。只是想看看我们能把它拉长到什么程度。</p><p id="fff7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要做到这一点，让我们创建一个函数，通过我们的整个模型，如果它发现任何层不是batchnorm，它会告诉pytorch忘记该层的梯度。在此过程中，确保只对Batchnorm层进行培训。为此，我们冻结了权重和偏见。</p><pre class="kw kx ky kz gt nw nx ny nz aw oa bi"><span id="eff1" class="ob ln it nx b gy oc od l oe of">def freezeOthers(m):<br/>    for param in m.parameters():<br/>        if not isinstance(m, torch.nn.modules.batchnorm._BatchNorm):<br/>            if hasattr(m, 'weight') and m.weight is not None:<br/>                m.weight.requires_grad_(False)<br/>            if hasattr(m, 'bias') and m.bias is not None:<br/>                m.bias.requires_grad_(False)</span></pre><p id="9c3c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在是主要的训练循环。我们首先使模型可训练，并将其发送到GPU。然后我们迭代数据。按照PyTorch标准循环，我们重置梯度，并通过我们的模型传递批次。我们执行反向传播并逐步通过我们的优化器。然后我们应用之前的函数。在此之后，我们的模型将忘记每隔一层的梯度。我们还添加了一个小选项来打印当前的损失。(这有助于你观察列车)</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="19b8" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">那是怎么做到的？</h1><p id="36bb" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">嗯，我在CIFAR 10数据集上测试了ResNet110，对于普通网络，我得到了大约92%的测试准确率。在只训练BatchNorm的时候，我达到了大约68%的测试准确率。现在你可能会想，这太遥远了。是的，它是..但是请注意，我们只使用了大约0.48%的数据<a class="ae ll" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank">城市1 </a></p><p id="1bbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">哇…</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="09fa" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">不要把BatchNorm视为理所当然</h1><p id="bdee" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">现在我们对这些层的表达能力有了更多的了解。让我分享一些我从<a class="ae ll" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank">论文</a>中发现的非常有趣的观点。</p><ol class=""><li id="c3aa" class="mq mr it js b jt ju jx jy kb ms kf mt kj mu kn mv mw mx my bi translated">BatchNorm学习禁用网络中的特征，这允许它很好地学习并对特征施加稀疏度</li><li id="996a" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">图层中的仿射参数(执行某种变换的参数)起着非常重要的作用</li><li id="f709" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">该层帮助网络学习更好的表示</li><li id="3c72" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">随机特征在神经网络中起着重要的作用，到了我们还没有完全理解的程度。</li><li id="5763" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">将BatchNorm放在激活之前可以获得更好的性能</li><li id="d4f9" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">如果我们只训练层，增加深度会比增加宽度得到更好的结果</li><li id="44f3" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">可以移除网络中的许多要素，而不会对值产生太大影响</li><li id="31da" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn mv mw mx my bi translated">ResNets中的快捷连接实际上可能会降低性能，因为它们无法正确使用BatchNorm</li></ol></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="7409" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">停业清理</h1><p id="fd1d" class="pw-post-body-paragraph jq jr it js b jt mk jv jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn im bi translated">到现在为止，我想我们开始意识到，也许我们不应该把我们认为我们知道的视为理所当然。有时候，要理解一篇论文是很困难的。BatchNorm在网络中确实扮演着重要的角色。这在某种程度上证明了我们需要能够挖掘神经网络架构的结构和黑盒。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="61e2" class="lm ln it bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj bi translated">参考</h1><ul class=""><li id="6573" class="mq mr it js b jt mk jx ml kb og kf oh kj oi kn oj mw mx my bi translated"><a class="ae ll" href="https://arxiv.org/abs/2003.00152" rel="noopener ugc nofollow" target="_blank">引用1:原始论文</a>。Jonathan Frankle等人(是的，我在文章中多次引用这一点，因为这样更容易识别)</li><li id="1921" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn oj mw mx my bi translated"><a class="ae ll" href="https://medium.com/deeplearningmadeeasy/everything-you-wish-to-know-about-batchnorm-6055e07fdce2" rel="noopener">阿尔瓦罗·杜兰</a></li><li id="3c8b" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn oj mw mx my bi translated"><a class="ae ll" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">蝙蝠纸</a></li><li id="cac7" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn oj mw mx my bi translated">Sayak Paul的博客文章如前所述</li><li id="8711" class="mq mr it js b jt mz jx na kb nb kf nc kj nd kn oj mw mx my bi translated">【pytorch论坛的链接</li></ul><h2 id="9b03" class="ob ln it bd lo ok ol dn ls om on dp lw kb oo op ma kf oq or me kj os ot mi ou bi translated">访问专家视图— <a class="ae ll" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank">订阅DDI英特尔</a></h2></div></div>    
</body>
</html>