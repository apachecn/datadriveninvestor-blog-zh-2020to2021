<html>
<head>
<title>Reinforcement Learning — MDP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——MDP</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/reinforcement-learning-mdp-639aecec6da4?source=collection_archive---------2-----------------------#2020-03-27">https://medium.datadriveninvestor.com/reinforcement-learning-mdp-639aecec6da4?source=collection_archive---------2-----------------------#2020-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="43cd" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</h1><p id="adfb" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">强化学习基于<strong class="kn ir">奖励假设</strong>。</p><blockquote class="lj lk ll"><p id="cce6" class="kl km lm kn b ko ln kq kr ks lo ku kv lp lq ky kz lr ls lc ld lt lu lg lh li ij bi translated">所有的好都可以用预期<strong class="kn ir">累积</strong>回报的最大化来描述</p></blockquote><p id="8903" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">单词<strong class="kn ir">累积</strong>在这里很重要，因为它允许代理采取产生低回报的行动，但从长远来看可能会导致更高的回报。</p><h2 id="31fe" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">例如:</h2><p id="3a89" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">让我们把一个未经训练的人想象成我们想要减肥的代理人。这个人，让我们称他为威利先生，每天可以选择去健身房或者吃一整块蛋糕。</p><p id="fb6e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">现在，去健身房的直接回报对威利先生来说真的很低，因为这是一项艰苦的工作，并不真正令人愉快…而且他有点懒。</p><p id="c9a9" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">另一方面，吃一个超级美味的甜巧克力蛋糕会有很高的即时回报。所以，如果威利先生只关心短期利益，他会吃掉整个蛋糕。然而，正如你和我可能知道的，每周去三次健身房，吃一份沙拉可能会导致六块腹肌(或者至少不会因糖尿病而过早死亡)，这意味着从长远来看更高的回报。耶！</p><p id="03ec" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">嗯，是的，强化学习，在这种情况下，就像你妈妈告诉你吃蔬菜，因为从长远来看更健康。没有及时行乐。不#yolo。</p><p id="7710" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">去健身房锻炼一年并健康饮食的累积回报高于每天只吃一块蛋糕。</p><p id="3ada" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">当然，正如我们将在后面看到的，有一个参数可以调整行为，使人们更喜欢即时奖励而不是长期累积奖励。</p><p id="64fe" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">让我们定义系统的重要组件:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/b346fb0ebd22dbcb82a4531438175c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTxgYLjeu2R8de8qzKQtjw.png"/></div></div></figure><p id="9d0e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">阿根是在动作之间做出选择的组件。在上面的例子中，这是威利先生的大脑。</p><p id="8084" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">环境</strong>是代理之外的一切(威利先生的身体、健身房等。)</p><p id="034c" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">决定采取<strong class="kn ir">一项行动<em class="lm"> A_t </em> </strong>(吃蛋糕、去健身房等。)在时间步长t，环境的状态被这个动作改变。</p><p id="d4eb" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">现在，环境在时间步<em class="lm"> t+1 </em>发出奖励<em class="lm"> R </em>。(吃含糖蛋糕的多巴胺奖励高，在健身房无聊锻炼的奖励低)。威利先生在时间步<em class="lm"> t+1 </em>收到奖励和新的更新状态<em class="lm"> S </em>(体重增加或减少)。</p><p id="46c6" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">现在，我们只是假设代理的状态与环境的状态相同。绝对不总是这样。例如，中世纪的托马斯爵士二世认为地球是平的(代理人的状态)，但事实是世界是一个球体。(环境状况)。</p><p id="047e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这意味着代理人的选择改变了环境。根据所做的选择，会发出或高或低的奖励，代理需要根据更新后的状态采取新的行动。</p><h1 id="41ca" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">MDP-马尔可夫决策过程</h1><p id="36e7" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">马尔可夫决策过程是一个为决策建模的数学框架。</p><p id="a4c6" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">如上所述，环境的状态与代理的状态相同。对于所谓的完全可观测MDP来说，情况总是如此。</p><p id="2a23" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">在我们的MDP中，我们有以下组件:</p><ul class=""><li id="ea3b" class="mt mu iq kn b ko ln ks lo kw mv la mw le mx li my mz na nb bi translated"><strong class="kn ir">答:</strong>代理可以采取的一组有限的<strong class="kn ir">动作</strong></li><li id="06da" class="mt mu iq kn b ko nc ks nd kw ne la nf le ng li my mz na nb bi translated"><strong class="kn ir"> S: </strong>状态<strong class="kn ir">的有限集</strong></li><li id="22a7" class="mt mu iq kn b ko nc ks nd kw ne la nf le ng li my mz na nb bi translated">如果采取特定动作<em class="lm">和</em>，从状态s转换到s’的概率:</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b721ca83e52e207ca59613fea61c508b.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/0*5aBZFC_j3LFrTl8X.png"/></div></figure><ul class=""><li id="e556" class="mt mu iq kn b ko ln ks lo kw mv la mw le mx li my mz na nb bi translated">采取特定动作<em class="lm"> a </em>并从状态s转换到状态s’后的即时奖励:</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1f9656de1449d7e618c316d42ab5de91.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/0*kAaVVVrb3vcpinQU.png"/></div></figure><p id="3f62" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">让我们用上面的健身房例子来说明一个MDP:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ni"><img src="../Images/295de6c4b197bbc551a2dc688b3808b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7kHGxuDl2NI4KKRPspglg.png"/></div></div></figure><h2 id="b69a" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">轨迹和返回</h2><p id="db2b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们定义了状态<em class="lm"> A、B、C、D </em>和结束状态，这里显示为一个正方形。在导致从状态<em class="lm"> s </em>转换到状态<em class="lm">s’</em>的每一个所采取的行动下，都显示了直接奖励<em class="lm"> R </em>。</p><p id="4a3f" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">让我们从状态<em class="lm"> A </em>开始。如果代理处于状态<em class="lm"> A </em>并选择动作<em class="lm">蛋糕，</em>那么环境会立即奖励他<em class="lm"> R=-1 </em>，这仍然高于动作<em class="lm">健身房</em>和<em class="lm"> R=-2 </em>的奖励。然后，他将从状态<em class="lm"> A </em>转换到状态<em class="lm"> B </em>。从那里他可以选择吃更多的蛋糕或者放弃，这将把他带回到<em class="lm"> A </em>。</p><p id="594e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">假设威利先生决定辞职，而不是吃更多的蛋糕。</p><p id="40ba" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">最终，他决定选择动作<em class="lm">健身房(R=-2) </em>，然后<em class="lm">有氧</em> <em class="lm"> (R=-1) </em>和<em class="lm">睡眠(R=+10) </em>，这将导致<em class="lm">结束状态</em>。</p><p id="33eb" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">链条将会是</p><p id="f73d" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir"> <em class="lm"> A、</em> </strong> <em class="lm">蛋糕，R=-1 → </em> <strong class="kn ir"> <em class="lm"> B、</em> </strong> <em class="lm">退出，R=0 → </em> <strong class="kn ir"> <em class="lm"> A、</em> </strong> <em class="lm">健身房，R=-2→</em><em class="lm">C、</em>  <em class="lm">有氧，R =-2</em></p><p id="a92f" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">当然，有更多的方法可以达到最终状态。那些不同的链也被称为<strong class="kn ir">轨迹。</strong></p><p id="7e81" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">每一个轨迹都有一个<strong class="kn ir">回报</strong>，这个回报是在到达终点状态的途中所获得的所有奖励的总和。上述轨迹的回报将是-1 + 0 + -2 + -2 + 10 = +5。</p><p id="d99b" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">上面MDP的另一个重要部分是黑点。当代理决定将动作方从状态<em class="lm"> C </em>带走时，有0.2的概率从那里转移到状态<em class="lm"> B </em>，有0.4的概率转移回状态<em class="lm"> C </em>，有0.2的概率转移到结束状态。这里的不同之处在于，在采取行动之后，并不能100%确定我们最终会处于哪种状态。采取这一行动后，我们无法完全控制环境会把我们抛向何方。嗯，在聚会上喝了几杯后，威利先生可能会跳过有氧运动，去睡觉，或者在他想吃蛋糕的状态中结束。然而，对于所有其他标有行动的箭头来说，他最终会处于哪种状态是很清楚的。从<em class="lm"> s </em>过渡到<em class="lm">s’</em>的概率就是我们前面提到的</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7cf8d4457c12e4366bd8446a84c40c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/0*OVGl9ZkgJk3DGsQW.png"/></div></figure><p id="1f3d" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">让我们分析另一个轨迹。</p><p id="f331" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir"> <em class="lm"> A、</em> </strong> <em class="lm">健身房，R=-2 → </em> <strong class="kn ir"> <em class="lm"> C、</em> </strong> <em class="lm">聚会，R=-2 → </em> <strong class="kn ir"> <em class="lm">结束</em> </strong></p><p id="f510" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这条轨迹的回报是:-4</p><p id="aea2" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">让我们将返回定义为<strong class="kn ir"> <em class="lm"> G_t </em> </strong>，它是从时间步长t开始的返回<em class="lm"> G </em></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d413dc47d3156b045ac0a5e316910a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/0*w_4ZeXvzMX9Z9J_x.png"/></div></figure><h2 id="52a9" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">折扣</h2><p id="015b" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们的回报中又增加了一样东西。<em class="lm"> γ </em>就是所谓的折扣。<em class="lm"> γ </em>是介于0和1之间的数字。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7792146558703349dee6bec166510bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/0*rL6G7OFW8t0dtTaw.png"/></div></figure><p id="3830" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">如果我们选择更接近于零的<em class="lm">γ</em><strong class="kn ir">，</strong>那么眼前的奖励比后来的奖励更有价值。现在吃蛋糕变得比未来六块腹肌的奖励更重要。另一方面，对于更接近1的值，后来的奖励变得更相关。</p><p id="1ba7" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">折扣值用于以下一些原因</p><ul class=""><li id="5ccd" class="mt mu iq kn b ko ln ks lo kw mv la mw le mx li my mz na nb bi translated">避免了循环MDP中的无限回报。通过在某个时候使用折扣，未来的回报将几乎为零。这将使链变得有限。</li><li id="0722" class="mt mu iq kn b ko nc ks nd kw ne la nf le ng li my mz na nb bi translated">不确定在遥远的将来会发生什么。如果我们可能永远也到不了那里，那么重视那些不太相关的回报是有意义的。</li><li id="a7bf" class="mt mu iq kn b ko nc ks nd kw ne la nf le ng li my mz na nb bi translated">在某些情况下，支持即时奖励更有意义，比如在金融业，现在的现金比以后的现金更好。</li></ul><h2 id="300b" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">政策</h2><p id="49e5" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">正如我们之前说过的，我们现在感兴趣的是获得尽可能高的累积奖励。这意味着我们在寻找回报最大的轨迹。为了实现这一点，我们的代理人需要在每个状态下做出最佳选择，使累积回报最大化。</p><p id="906f" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这就是政策发挥作用的地方:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0363af0cda117ba20800d40c6caea3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*QhBgJKFYvnK6N5AR.png"/></div></figure><p id="5880" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">一个策略基本上描述了代理选择行动<em class="lm"> a i </em>的概率，如果他处于状态<em class="lm"> s </em>，</p><p id="b0e3" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这可能是确定性的，就像我们上面的健身房例子中的状态<em class="lm"> D </em>一样，代理人唯一可以选择的是<em class="lm">睡眠</em>，在那里他将过渡到最终状态。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5197241b123c7c7718c55f8f06e95d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/0*o3qYZJRxCYWFxafO.png"/></div></figure><p id="0aec" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">在确定性策略函数<strong class="kn ir"> <em class="lm"> π </em> </strong>的情况下，它将状态<em class="lm"> s </em>映射到动作<em class="lm"> a </em>。这告诉我们“如果在状态<em class="lm"> s </em>中，代理将以100%选择动作<em class="lm"> a </em>”。</p><p id="7f57" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">另一方面，随机策略是代理人选择一个行动<em class="lm"> a，</em>给定在某些状态<em class="lm"> s. </em>的替代行动的概率。例如，在状态<em class="lm"> A </em>中，策略可以告诉我们代理人可能选择行动<em class="lm">蛋糕</em>的概率为20%，选择<em class="lm">健身房</em>的概率为80%。</p><p id="69fa" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这意味着最大化累积回报和创建具有最高回报的轨迹的方法是找到最优策略函数π。从长远来看，最好的政策会导致我们采取最大化回报的行动。</p><h2 id="7674" class="lv jo iq bd jp lw lx dn jt ly lz dp jx kw ma mb kb la mc md kf le me mf kj mg bi translated">最优策略</h2><p id="7d79" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，MDP的目标是找到<strong class="kn ir">最优政策函数π </strong>以获得最高回报<em class="lm"> G_t </em>。</p><p id="366e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">现在你问:“沃尔特。但是，我们如何得到最优的政策，如何！？."</p><p id="f053" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">放松点孩子。我们需要先谈谈价值函数:</p><p id="d896" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">状态值功能</strong></p><blockquote class="lj lk ll"><p id="8314" class="kl km lm kn b ko ln kq kr ks lo ku kv lp lq ky kz lr ls lc ld lt lu lg lh li ij bi translated">MDP的状态值函数v_π(s)是从状态s开始，然后遵循策略π的期望收益G_t。</p></blockquote><p id="0887" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">它描述了每个状态<em class="lm"> s </em>的期望收益<em class="lm"> G_t </em>。基本上，处于一个状态s有多有价值，因此在遵循π策略的同时进入另一个状态也有多有价值。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c5d02f4d008739a84124599eb56347ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/0*BAD9qa898trofDL2.png"/></div></figure><p id="a65e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们可以将<strong class="kn ir"> v_π(s) </strong>分解如下:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b553faef5354448753ccdc181c4d6583.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*GQ_jGyeshxYv9NFf.png"/></div></figure><p id="5b46" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">这是预期的即时回报加上下一个状态的贴现值。</p><p id="6dc8" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">动作值功能</strong></p><blockquote class="lj lk ll"><p id="d0bd" class="kl km lm kn b ko ln kq kr ks lo ku kv lp lq ky kz lr ls lc ld lt lu lg lh li ij bi translated">行动值函数q_π(s)是从状态s开始，采取行动a，然后遵循策略π的期望收益G_t。</p></blockquote><p id="5e54" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><em class="lm"> q_π(s，a) </em>是状态<em class="lm"> s </em>和动作<em class="lm"> a </em>对的期望值。与状态值函数的主要区别在于动作值函数告诉我们一个动作<em class="lm"> a </em>在 之后<strong class="kn ir"> <em class="lm">有多有价值，它是在遵循策略π时从状态<em class="lm"> s </em>中取出的。</em></strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d45f50eae885f8d42b723de6c1ee58a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/0*1OWIcJvj9ZAcYOhn.png"/></div></figure><p id="2ae6" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们也可以改写<strong class="kn ir"> <em class="lm"> q_π(s，a): </em> </strong></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0612d8c83b2fb35b427a0123794913b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/0*uD_jOWH7y_kjWd0I.png"/></div></figure><p id="e053" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">同样我们可以用期望的即时回报和下一步S_t+1和A_t+1 <em class="lm"> </em>的状态-动作<em class="lm">对S和A来表达<strong class="kn ir"> <em class="lm"> q_π(s，a) </em> </strong> <em class="lm">。</em></em></p><p id="5e9e" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">在MDP图中，我们将把<strong class="kn ir"> <em class="lm"> q_π(s，a) </em> </strong>写在黑点上。在上面的例子中，我们只有一个。对于那些没有点的箭头，以及在承诺一个特定的动作后，我们清楚地转换到哪个状态的箭头，我们可以想象那里有一个黑点，只有一个可能的状态结束。</p><p id="5d94" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">寻找最优策略</strong></p><p id="befd" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">由于<em class="lm"> v_π(s) </em>是遵循策略<em class="lm"> π </em>时给定状态<em class="lm"> s </em>的期望收益，我们需要选择策略<em class="lm"> π </em>，其中<em class="lm"> v_π(s) </em>是最大值状态函数。这意味着对于所有状态，如果v_π(s) ≥ v_π'(s) ，则一个<em class="lm">策略π </em>优于另一个<em class="lm">π’:</em></p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c4940e963eb732db5b4656e156bde7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*1XwLxOIHux1740a9.png"/></div></figure><p id="ae19" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">最佳状态值函数</strong> <strong class="kn ir"> <em class="lm"> v*_π(s) </em> </strong>是所有值状态函数中的最大值状态函数，每个值状态函数遵循不同的策略。</p><p id="7485" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">换句话说，当我们找到了v*_π(s) 时，我们也找到了最佳策略<em class="lm"> π*。</em></p><p id="d380" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><em class="lm"> v*_π(s) </em>是我们通过遵循策略<em class="lm"> π*可以从所有状态的MDP中得到的最大值。</em></p><p id="5154" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><em class="lm"> v*_π(s) </em>是遵循特定策略<em class="lm"> π </em>的结果。所以这个策略只能是最优策略<em class="lm"> π*。</em></p><blockquote class="lj lk ll"><p id="929b" class="kl km lm kn b ko ln kq kr ks lo ku kv lp lq ky kz lr ls lc ld lt lu lg lh li ij bi translated"><strong class="kn ir"> <em class="iq">定理:</em> </strong> <em class="iq">对于任意有限的MDP，至少存在一个最优策略π*，使得它优于或等于其他所有可能的策略π。</em></p><p id="c643" class="kl km lm kn b ko ln kq kr ks lo ku kv lp lq ky kz lr ls lc ld lt lu lg lh li ij bi translated">π∫≥π，对于所有策略π</p></blockquote><p id="4b06" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated"><strong class="kn ir">贝尔曼最优性方程</strong></p><p id="7bf5" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">为了实际获得最佳状态值函数<em class="lm"> v*_π(s) </em>代理需要从状态s的所有可能动作a中挑选状态-动作对q*(s，a)的下一个最大值:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2b218b8677b4faf57f355d9580dacedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/0*0p9kdEGa3pZ9C9V2.png"/></div></figure><p id="0adb" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">对于代理来说，这只是意味着:如果您处于状态<em class="lm"> s </em>中，只需选择动作<em class="lm"> a </em>，该动作具有<em class="lm"> q </em>的最高值。</p><p id="de08" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">现在要实际计算<em class="lm"> q* </em>，我们需要做以下工作:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ce416b4cd45781182669234e9a750ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*m2pZ2_LanwjfqZn8.png"/></div></figure><p id="db5a" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们考虑行动<em class="lm"> a </em>的即时回报R和代理人可能结束的所有价值状态的乘积的贴现和，以及在采取行动<em class="lm"> a </em>后从<em class="lm"> s </em>转换到<em class="lm">s’</em>的概率P。</p><p id="7451" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们现在可以通过用最后一个表达式替换<em class="lm"> q*(s，a) </em>来表示<em class="lm"> v*(s) </em>:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/eab45aa7acd9fd8f43678aa307e2b5f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/0*-zRgfGeZmkrx64uf.png"/></div></figure><p id="99e3" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">最后一个方程称为状态值函数的贝尔曼最优方程，它为我们提供了一种将当前状态的值表示为下一个状态的值的方法。因此，我们可以递归计算所有状态的所有<em class="lm"> v*(s) </em>。</p><p id="09e8" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们还可以将q*(s，a)的贝尔曼最优方程描述如下:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/50d7815734647c354fcb7e37efc990b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*2BAd3994MZUAZoc6.png"/></div></figure><p id="fa4a" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">类似于状态值函数的贝尔曼最优方程，我们可以递归地计算所有状态-动作对的所有q*(x)。</p><p id="8fd3" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们通过在q*(s，a)上最大化来获得最大化策略</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/33458aa8162c327547ab19782cbf1b7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/0*LCJ9qJlPt9Omyf2M.png"/></div></figure><p id="4790" class="pw-post-body-paragraph kl km iq kn b ko ln kq kr ks lo ku kv kw lq ky kz la ls lc ld le lu lg lh li ij bi translated">我们遍历一个状态的所有<em class="lm"> q*(s，a) </em>，并决定采取获得最大<em class="lm"> q*(s，a) </em>值的动作。因此，我们将状态<em class="lm"> s </em>中的动作<em class="lm"> a </em>的策略设置为1，而将所有其他动作的策略设置为0。我们现在有一个确定性的MDP政策。对于遵循最优策略的代理，100%确定采取哪个动作来获得每个状态的最大值。</p><h1 id="673c" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">感谢阅读！</h1><p id="9c7a" class="pw-post-body-paragraph kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在下一篇文章中，我将描述我们如何利用这些知识通过迭代过程来计算策略。留下一些掌声来支付威利先生的健身房费用。</p></div></div>    
</body>
</html>