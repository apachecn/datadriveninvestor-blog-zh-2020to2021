<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/linear-regression-35f8a65d83b9?source=collection_archive---------1-----------------------#2020-01-28">https://medium.datadriveninvestor.com/linear-regression-35f8a65d83b9?source=collection_archive---------1-----------------------#2020-01-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="24d5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">简单和多元线性回归基础</h2></div><ol class=""><li id="34f4" class="kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="kh ir">简介</strong></li></ol><p id="7a14" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">线性回归是监督学习方法。特别是，线性回归是预测连续值(目标值)的有用方法，它试图对目标值和一个或多个预测值之间的线性关系进行建模。</p><p id="ff42" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2。简单线性回归(一个预测值)</strong></p><p id="000a" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">简单线性回归名副其实:简单地说，就是找到一个预测值(𝑥)和目标值(y)之间的关系。数学上，我们可以把这种线性关系写成</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/af6e2dbf58904a53cb7e734a7d3766b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*o4JK2-TExfLsJH_gmvfzZQ.png"/></div></figure><p id="7116" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">在等式1中，ŷ是目标，𝑥是预测，𝜷₀和𝜷₁(系数)是两个未知常数，表示线性模型中的截距和斜率项。简单的线性回归试图产生模型系数的估计𝜷₀和𝜷₁。𝜷₀和𝜷₁获得了，我们可以预测目标。</p><p id="fa09" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2.1剩余</strong></p><p id="ab4e" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">线性回归实际响应</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/236d46538280ed6e48040425d48a621c.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*vSAk8N6e3Z9r6DwHNLuc6w.png"/></div></figure><p id="f685" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">e是剩余误差。从数学上讲，我们可以将线性回归中的残差写为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/1b0cecb1222f0531eecc9d2aa983e306.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*1hWX57LEbmFswaY1wv7IYA.png"/></div></figure><p id="93c8" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">这是第I个实际值和第I个预测值之差。我们将<em class="lu"> RSS </em>(残差平方和)定义为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/9454bddfb983d68dd38948f13dcc330c.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*xL02wFWOBqvlYoqHsfuLHQ.png"/></div></figure><p id="c371" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">或者等同于</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/c7510479d3b9689595e779f2147ffa7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*YCHrBqYgrOmsKsLQqQ1Uiw.png"/></div></figure><p id="8da6" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2.2估计系数</strong></p><p id="0cb1" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">𝜷₀和𝜷₁are未知。因此，在使用等式1.1进行预测之前，我们必须使用数据来估计系数。如等式4.2所述，最小二乘法选择𝜷₀和𝜷₁来最小化<em class="lu"> RSS </em>。使用一些微积分𝜷₀和𝜷₁我们可以写为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/30f49e5fb75d4164bcb6d1c47b87822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*kowc7gLobfPrs21oT_0I6w.png"/></div></figure><p id="128d" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">其中y_hat是y(目标)的平均值，x_hat是𝑥(预测)的平均值。在实践中，我们使用电视、广播、报纸中的广告案例作为预测，以销售额作为目标。为了实现简单的线性回归，在实践中将使用唯一的TV预测器。销售和电视图表如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/3c169e2099ea4bbdf824c122a79c9592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/0*F3PppQFJuV3bDO9u"/></div></figure><p id="ba29" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2.3实施</strong></p><p id="1974" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">如上所述，我们仅使用电视作为𝑥(预测值)，销售额作为y(目标值)，数据数量为200行(<a class="ae lz" href="https://github.com/arifromadhan19/linear_regression" rel="noopener ugc nofollow" target="_blank"> github </a>)</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/de3922be250d172fbf2c1c3f31105f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*YOuhmp4I679K-vom"/></div></div></figure><p id="e50f" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">首先，我们用<strong class="kh ir">方程5.0 </strong>求解系数𝜷₁(斜率)，y_hat = 14.0225，x_hat = 147.0425</p><p id="1023" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">𝜷₁ = 69727.65 / 1466818.94</p><p id="c15d" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">𝜷₁ = 0.047536644161412324</p><p id="a566" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">𝜷₁ = 0.048</p><p id="8314" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">一旦我们知道了系数𝜷₁的值，我们就可以用<strong class="kh ir">等式6.0 </strong>求解系数𝜷₀(截距):</p><p id="f241" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">𝜷₀ = 14.022 - (𝜷₁ * 147.04)</p><p id="fd14" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">𝜷₀ = 7.033</p><p id="1364" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">因此，回归方程为</p><p id="916c" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ = 7.033 + (0.048 * 𝑥)</p><p id="8a3b" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">销售和电视的回归图如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/6251723e7257c01634fa2832e3d4307a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*VrHa2Yemv9oBXzcWD2kJvQ.png"/></div></figure><p id="9014" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2.4如何使用回归方程</strong></p><p id="49f3" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">由于我们有回归方程，我们可以预测新数据(电视)的销售。在我们的例子中，电视的新数据是100，100台电视的预测销售额如下</p><p id="5970" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ= 𝜷₀ + 𝜷₁𝑥</p><p id="bd46" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">ŷ = 7.033 + (0.048 *新数据)</p><p id="84bb" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ = 7.033 + (0.048 * 100)</p><p id="2444" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ = 11.833</p><p id="eadd" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">销售额= 11.833</p><p id="321c" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 2.5用Python sklearn实现</strong></p><p id="b5c3" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><a class="ae lz" href="https://github.com/arifromadhan19/linear_regression" rel="noopener ugc nofollow" target="_blank"> github库</a></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mf"><img src="../Images/e414b53335e6313b1a11cbb8fb6e3189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ntAQzJxmSMEv2qD4mDm-A.png"/></div></div></figure><p id="1561" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3。多元线性回归(预测值&gt; 1) </strong></p><p id="900a" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">处理如此大量的值和运算时，计算机往往会高效地执行矩阵运算，为了用矩阵形式表示回归方程，我们需要定义三个矩阵:𝑦、𝜷和𝑥.</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/373317e894505121e90ccebe3b28e054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*CA6hS540V7-8g2BNqu_-zQ.png"/></div></figure><p id="66b1" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">通用参数方程:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/a0ee77cd32d4fb0d4829718bc52af889.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*3Ij4q4Q2oCs8-qeDgAnpNg.png"/></div></figure><p id="a51e" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">以矩阵形式表示<strong class="kh ir">方程7.0 </strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/2afedd1220c4aedbd6b1a3ef28068928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*S576ToOZQAD80ZXC5XkUYw.png"/></div></figure><p id="5972" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3.1剩余</strong></p><p id="9200" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">与简单线性回归相似，多元线性回归也计算以矩阵形式表示的残差</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/05082bed2df5e89c60e174623a686108.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*flFFGm0dg1ROlqIGbpKtXA.png"/></div></figure><p id="6c51" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><em class="lu">等式4.0中的RSS </em>可改写为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/ca3a41849df64a6ccd7d9c590c069fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*w4zBsSr9NYCiIrS0gfEs1w.png"/></div></figure><p id="310f" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3.2估计系数</strong></p><p id="cc5e" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">计算多元回归中的系数与简单线性回归中的相同，即最小化<em class="lu"> RSS </em>(残差平方和)。利用一些微积分计算系数可以写成</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/12ef293065950c66b4dbdbb5f195cc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*qHy7cylPgDp4dVG9ptQSBQ.png"/></div></figure><p id="edaf" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3.3实施</strong></p><p id="800a" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">与简单的线性回归一样，我们也使用电视和广播中的广告案例作为预测因素，销售额作为目标。电视、广播和销售图表如下:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mm"><img src="../Images/8d78f528f4c2dbadb03bae4906d4f18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1BdTNUrLvBBk3GVS"/></div></div></figure><p id="61b0" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">接下来呢？我们定义𝑥，𝑥 <em class="lu">转置</em>，计算𝑥 <em class="lu">转置</em> * 𝑥，求逆𝑥 <em class="lu">转置</em> * 𝑥，定义𝑦from这个表其中电视为𝑥1，广播为2，销售为</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/cb2a915cabc80b6c0dab737b35c91ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*9_6Uq9R66v-NcRlFNVqljw.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/a9fbcfc41c0ddbca47c095876cb23e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*afeytRC-cgsMIliRDzwjsg.png"/></div></figure><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/10a7f0d030c64c49b8b09b26c5288907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*U6zFo5DiwDE-IWS12OJJ8g.png"/></div></figure><p id="ee82" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">继𝑥之后，𝑥 <em class="lu">转置</em>，𝑥 <em class="lu">转置* </em> 𝑥，逆𝑥 <em class="lu">转置* </em> 𝑥，还有𝑦.我们可以用<strong class="kh ir">式9.0 </strong></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/f10d0675859477bc3f759c2978520995.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*PPLwAe18bZ4r9hLIWyut8w.png"/></div></figure><p id="ebfa" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3.4如何使用回归方程</strong></p><p id="1088" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">由于我们有回归<strong class="kh ir">方程7.0，</strong>我们可以预测新数据(电视和广播)的销售。在我们的示例中，电视的新数据为100，收音机为200，100台电视的预测销售额如下</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/7d802eb63937648ecb9cb9a1bb1f0e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*sE1f5fzY2B4iQhM6v8g7og.png"/></div></figure><p id="46f7" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">ŷ = -1.898 + (0.068*新数据电视)+ (0.228*新数据广播)</p><p id="fa50" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ = -1.898 + (0.068 * 100) + (0.228 * 200)</p><p id="7443" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi">ŷ = 50.502</p><p id="f844" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">销售额= 50.502</p><p id="26e2" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir"> 3.5用python sklearn实现</strong></p><p id="20fd" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><a class="ae lz" href="https://github.com/arifromadhan19/linear_regression" rel="noopener ugc nofollow" target="_blank"> github库</a></p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ms"><img src="../Images/2ff426d053c8aeed6471e54d715b5f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzwhsLi1bY9ULlCyODqnFw.png"/></div></div></figure><p id="6b62" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir">结果与第3.3点不同，因为在本节课中使用了所有数据集</strong></p><p id="4d10" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir">关于我</strong></p><p id="153f" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated">我是一名数据科学家，专注于机器学习和深度学习。你可以通过<a class="ae lz" href="https://medium.com/@arifromadhan19" rel="noopener">媒介</a>、<a class="ae lz" href="https://www.linkedin.com/in/arif-romadhan-292116138/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或<a class="ae lz" href="https://github.com/arifromadhan19/linear_regression" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><p id="290f" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir">我的网站:</strong><a class="ae lz" href="https://komuternak.com/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">【https://komuternak.com/】</strong></a></p><p id="b55c" class="pw-post-body-paragraph kx ky iq kh b ki kj jr kz kk kl ju la km lb lc ld ko le lf lg kq lh li lj ks ij bi translated"><strong class="kh ir">参考</strong></p><ol class=""><li id="6926" class="kf kg iq kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><a class="ae lz" href="http://faculty.marshall.usc.edu/gareth-james/ISL/" rel="noopener ugc nofollow" target="_blank">统计学习简介</a></li><li id="9fa0" class="kf kg iq kh b ki mt kk mu km mv ko mw kq mx ks kt ku kv kw bi translated"><a class="ae lz" href="https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86" rel="noopener" target="_blank">https://towards data science . com/linear-regression-detailed-view-ea 73175 F6 e 86</a></li><li id="c09c" class="kf kg iq kh b ki mt kk mu km mv ko mw kq mx ks kt ku kv kw bi translated"><a class="ae lz" href="https://stattrek.com/multiple-regression/regression-coefficients.aspx" rel="noopener ugc nofollow" target="_blank">https://stat trek . com/multiple-regression/regression-coefficients . aspx</a></li><li id="b606" class="kf kg iq kh b ki mt kk mu km mv ko mw kq mx ks kt ku kv kw bi translated"><a class="ae lz" href="https://www.youtube.com/watch?v=K_EH2abOp00" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=K_EH2abOp00</a></li></ol></div></div>    
</body>
</html>