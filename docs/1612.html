<html>
<head>
<title>Deep-Q-Networks (1 of 2): How Computers Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度Q网络(1/2):计算机如何学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/deep-q-networks-1-of-2-how-computers-learn-a86ad2074df?source=collection_archive---------6-----------------------#2020-03-25">https://medium.datadriveninvestor.com/deep-q-networks-1-of-2-how-computers-learn-a86ad2074df?source=collection_archive---------6-----------------------#2020-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b07a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入了解经典Q-Learning及其工作原理。</h2></div><p id="c03b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">人类是如何学习的？让我们想象一个孩子(这个例子摘自<a class="ae le" href="https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/" rel="noopener ugc nofollow" target="_blank">托马斯·西蒙尼尼</a>的一篇优秀文章)。</p><p id="6de7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">孩子走近壁炉。他们感到温暖舒适。<code class="fe lf lg lh li b">Reward += 1</code></p><p id="c853" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">孩子想要更多的那种感觉——他们触摸壁炉。哎哟！<code class="fe lf lg lh li b">reward-= 1</code>。</p><p id="bf5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在孩子知道火是积极的(温暖舒适)，但不是用来触摸的(烧伤)。这个想法正是强化学习的工作方式，它正在做一些疯狂的事情。</p><div class="lj lk gp gr ll lm"><a href="https://www.datadriveninvestor.com/2019/02/19/artificial-intelligence-trends-to-watch-this-year/" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd iu gy z fp lr fr fs ls fu fw is bi translated">今年值得关注的5大人工智能趋势|数据驱动的投资者</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">预计2019年人工智能将取得广泛的重大进展。从谷歌搜索到处理复杂的工作，如…</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma mb lm"/></div></div></a></div><p id="cc60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AlphaGo的继任者DeepMind的<a class="ae le" href="https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>，在没有任何人工输入的情况下，仅用了4个小时就超越了Stockfish。这是<strong class="kk iu">无模型</strong>强化学习浪潮的一部分，就像人类的孩子一样，代理开始时没有任何人类输入，完全靠自己学习。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/cf698ced62af0e3c380051fe9aaee0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iVCYyoUINIKMcWXv8f_0wA.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">“Shedding new light on chess, Go and shogi”</figcaption></figure><p id="f14e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RL在已建立的有监督和无监督机器学习之间找到了一个绝对独特的领域。监督ML(分类，支持向量机)是指给计算机提供带有特征及其各自标签的训练样本来进行训练。无监督ML(聚类)本质上是当计算机没有被给定标记样本时。强化学习是更接近这个还是那个，还是它自己的领域？一旦我们完成了基础工作，我将回头再来讨论这个问题。</p><h1 id="f40e" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">我被卖了！这到底是怎么回事？</h1><p id="081c" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在深入Q-learning之前，最好先了解一下什么是经典Q-Learning。</p><p id="e97a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们举一个简单的例子:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/8fae0d932b3c1f119258d73c3068c3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HK8ZszKWcw6RRUQchKTtw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">This is the environment MountainCar-v0 from OpenAI’s gym module for Python</figcaption></figure><p id="2f1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个“游戏”的目标是让汽车到达旗子那里，问题是汽车的引擎不够强劲，无法一次到达那里。首先让我们定义一些术语:</p><ul class=""><li id="b4ab" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">代理</strong>——“玩家”。它可以采取行动改变环境。</li><li id="7544" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><strong class="kk iu">操作</strong> —代理可以与其环境交互的方式。</li><li id="dd0c" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><strong class="kk iu">状态</strong> —关于环境当前状态的信息。</li><li id="d153" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><strong class="kk iu">奖励</strong> —代理如何跟踪是否完成了目标。</li></ul><p id="2af9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们更深入地看看这个游戏。</p><h1 id="4217" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">山地汽车2 —电动布加洛</h1><p id="4909" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在任何RL环境中，<strong class="kk iu">代理人</strong>根据环境的<strong class="kk iu">状态</strong>采取<strong class="kk iu">行动</strong>，这将试图最大化<strong class="kk iu">奖励</strong>(或最小化惩罚——我们很快会谈到这一点)。所以在这个例子中:</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5257" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">凭直觉，我们可以理解这个环境的目标——尽可能高效地到达旗帜。电脑是怎么知道的？每个时间步长产生-1的奖励，达到目标产生0的奖励。代理有200个时间步到达旗子，然后游戏结束。这是最小化惩罚— <strong class="kk iu">代理人试图获得最高可能的奖励值，</strong>因此可以得出<strong class="kk iu">最优行动将在最少的时间内将代理人带到标记处。</strong></p><blockquote class="of og oh"><p id="6a8e" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">好的，我明白了——我们到底要怎么做？</p></blockquote><p id="491e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这通过使用<strong class="kk iu">Q-表</strong>来完成。Q-table是一个演练或指南，它告诉代理在给定的状态下应该做什么。它包含环境中每一种可能的状态，并根据动作的数量分配n个q值。在这种情况下，对于环境中的每个状态，我们有3个q值(1表示左，1表示无，1表示右)。代理参考这个q表，并且给定状态，它选择具有最高q值的动作。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="8487" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Q-table被随机初始化，并且每当它接收到奖励时，它所采取的所有动作的Q值使用一个公式相应地更新:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi om"><img src="../Images/61073b025134b44da5da6f728728a365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*LJIwmm7NHOq7HUoIF5333w.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk"><a class="ae le" href="https://en.wikipedia.org/wiki/Q-learning" rel="noopener ugc nofollow" target="_blank">This is… quite the formula</a></figcaption></figure><p id="f4d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来分解一下。</p><ul class=""><li id="64ec" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated"><strong class="kk iu">学习率:</strong>衡量算法放弃旧算法支持新算法的难易程度(值在0到1之间)。</li><li id="941b" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">奖励:代理现在得到的奖励。</li><li id="1a46" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><strong class="kk iu">折扣系数:</strong>衡量代理人对未来奖励相对于当前奖励的重视程度(值在0到1之间)。长期回报不确定，要打折扣；他们不应该打折扣太多，否则代理人会优先考虑短期回报，而不是长期的总体目标。</li><li id="4bb8" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated"><strong class="kk iu"> Max Q: </strong>所有可能动作<em class="oi"> a </em>的新状态(动作<em class="oi"> aₜ </em>后)的最大q值。</li></ul><p id="e9fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">函数Q接受某个状态-动作对，并从本质上给出该动作的“质量”。由此可见，我们的Q表只是我们的Q函数的所有输出的集合！在采取每个行动之后，这个Q表被更新。代理玩得越多，代理得到的奖励(负的或正的)就越多，我们的Q表就越优化。</p><h1 id="d13f" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">用Python编写Q学习算法</h1><p id="fffe" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">对于这个项目，我使用了<a class="ae le" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI的gym模块</a>，这是许多不同RL环境的开源集合(我还用它做了Deep-Q-Network，敬请关注)。我用了我们已经用过的例子，山地车。这就是我们想要做的:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/f18fe9911256b43e3619a804565de2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*O7qW6UAwBtReP-lpaY7eCg.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">A great flowchart from <a class="ae le" href="https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/" rel="noopener ugc nofollow" target="_blank">Thomas Simonini</a> explaining the structure of Q-Learning</figcaption></figure><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Setting up imports and constants</figcaption></figure><blockquote class="of og oh"><p id="f0c4" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">那个<code class="fe lf lg lh li b">epsilon</code>值在那里做什么？为什么我们希望代理随机行动？</p></blockquote><p id="58b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，代理在开始时会随机行动，因为Q表是随机初始化的。即使在Q表开始改善之后，我们仍然希望代理人去探索。<strong class="kk iu">代理不应该停留在迭代和优化次优策略上</strong>——它应该尝试探索和发现潜在的更好的策略。</p><p id="e38f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，我们还是希望代理在某一点上进行优化；如果代理从不使用最优策略，那么找到最优策略又有什么意义呢？这就是为什么<code class="fe lf lg lh li b">epsilon</code>被设置为随时间衰减，在这种情况下<code class="fe lf lg lh li b">epsilon</code>达到0并在中间点停止衰减。</p><p id="a742" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，我线性衰减<code class="fe lf lg lh li b">epsilon</code>，但是也有其他方式衰减<code class="fe lf lg lh li b">epsilon</code>！许多程序指数衰减它，但也有一些研究表明基于奖励的<code class="fe lf lg lh li b">epsilon</code>衰减更优越。基于奖励的衰减就像它听起来的那样:只有当代理得到足够好的奖励时才会衰减。这可以被认为是“责任”，当代理表现得更好时，我们允许它对自己的行为承担更多的责任。</p><blockquote class="of og oh"><p id="0f50" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">现在我们必须初始化Q表，对吗？</p></blockquote><p id="7abb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不完全是。我们的桌子有多大？回想一下，状态有两个值:位置和速度。有多少种可能的状态？<strong class="kk iu">无限多</strong>。我们可以有速度0，0.01，0.001，0.0001等等。由于我们的状态信息是<strong class="kk iu">连续的</strong>(相对于离散的)，我们必须量化这些信息。这意味着我们必须将所有可能的状态分类到一定数量的桶中。这给了我们有限数量的可能状态。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Setting up our discrete observation space and defining a helper function to access it</figcaption></figure><p id="648c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们初始化q表:</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="e427" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在代理准备开始学习了！</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="f38c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这虽然乍一看很复杂，但实际上并没有那么复杂。循环是所有奇迹发生的地方。这是代理采取所有步骤并更新其q值的地方；每次环境结束时，我们都返回到<code class="fe lf lg lh li b">for</code>循环的顶部。这是代理采取行动、评估奖励并更新其Q表的地方。如果您需要更详细的了解，这里有文档！</p><blockquote class="of og oh"><p id="f800" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">早先那个庞大而笨拙的方程式在哪里？</p></blockquote><p id="cef9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python使其可读性显著提高:</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="5599" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不知道你怎么样，不过那个看起来比以前优雅多了:)</p><h1 id="a303" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结果和总结</h1><blockquote class="of og oh"><p id="fce8" class="ki kj oi kk b kl km ju kn ko kp jx kq oj ks kt ku ok kw kx ky ol la lb lc ld im bi translated">代理做的怎么样？</p></blockquote><p id="e2e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是经纪人报酬的图表(我决定拍4万集):</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/759c8c3f5e0136b5c08ea5f8f0bdf1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*JDJSvhc35Ui8EENus8DZ0g.png"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">It looks like the max levels out at around 20000 episodes</figcaption></figure><p id="b34e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一张展示最佳代理的gif图:</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="op oe l"/></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Cool, isn’t it?</figcaption></figure><p id="b42b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Q-learning是一个强大的工具，但它完全是经典的。它没有使用任何强大的新工具(PyTorch、TensorFlow等)。).它也不能有效地用于更复杂的任务，因为它们变得太复杂而不能以如此简单的方式建模。在我的下一篇文章中，我将谈论<strong class="kk iu">深度Q网络</strong>。</p><p id="813d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一些事情需要思考:我们如何使用神经网络，并将其与Q-learning结合起来(提示:“苹果和橘子”)？</p><p id="80ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们回到引言中的问题:我们如何对RL进行分类？RL初看起来与无监督的ML有一些相似之处:计算机没有任何带标签的例子，也没有大量带标签的训练数据来得出像神经网络一样的绝对函数逼近。</p><p id="dff9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，它确实有相当多的训练数据，而且——这是关键——它有一个基于它获得的回报的表现指标。我没有为代理人提供许多不同情况下的最佳行动，但我提供了总体奖励，表明代理人的成功/失败。事实上，强化学习是独一无二的，不能轻易放置，在我看来，这是一个完全不同的领域。</p><h1 id="10fe" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">关键要点:</h1><ul class=""><li id="6a3f" class="np nq it kk b kl nf ko ng kr oq kv or kz os ld nu nv nw nx bi translated">强化学习是最大似然学中的一个新兴领域</li><li id="23bc" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">RL代理通过动作与它们的环境交互</li><li id="d02c" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">奖励激励代理人采取某些行动</li><li id="bae3" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">q-表代表了代理在给定状态下选择行动的“指南”</li><li id="bce1" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">Q表用Q函数的输出值填充</li><li id="cfa2" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">Q函数看起来很复杂！但不是真的。</li><li id="dcbc" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">代理应该在早期采取随机行动(探索)，并在以后采取它发现的最佳行动(利用)</li><li id="d97b" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">Q-learning很酷，但是深度Q-Learning更酷。敬请关注。</li></ul><h1 id="a9de" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">在你走之前</h1><p id="8994" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated"><em class="oi">感谢阅读！如果你想看更多，你可以看看我的其他文章。我希望在接下来的几天里会有这个系列的第二部分。请随时到dron.h.to@gmail.com找我。注意安全！</em></p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="ot oe l"/></div></figure></div></div>    
</body>
</html>