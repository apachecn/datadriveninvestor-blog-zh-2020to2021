<html>
<head>
<title>Evolution of Object Recognition Algorithms II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体识别算法的发展2</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/evolution-of-object-recognition-algorithms-ii-bd35a6e0d329?source=collection_archive---------10-----------------------#2020-05-12">https://medium.datadriveninvestor.com/evolution-of-object-recognition-algorithms-ii-bd35a6e0d329?source=collection_archive---------10-----------------------#2020-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="ca86" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">让我们深入了解最先进的自动驾驶汽车和安全监控背后的技术。</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/595c35467989c1ed16117d7a616b8325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3jFiDCv4z-_E_BulaPIIw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Object Recognition Algorithm results [4]</figcaption></figure><p id="905a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">在对象识别算法发展的上一期中，我们深入研究了一些初步技术，如Alexnet、RCNN等。虽然这些算法是该领域的先驱，但它们有许多缺点。这些已经在到目前为止仍然相关的后期架构中进行了处理。在我们演进之旅的第二阶段，我们将关注这些在2014年后发布的增强架构。</p><h2 id="e518" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">VGGNet</h2><p id="2a92" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">2015年，VGGNet [1]提供了一个非常深的网络架构，具有小型(3 x 3)卷积滤波器和高达19层的层。与他们先前的定位和分类任务相比，这些被证明表现得更好。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mg"><img src="../Images/57cea174a0d61b4c777d2dfdef6cdada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5m-Pbc7_CZc3IQcawOVUHQ.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of VGG16 [9]</figcaption></figure><h2 id="646e" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">概念网</h2><p id="dc19" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">基于赫比原理和多尺度处理直觉的精心设计导致了InceptionNet [2]的建立，它在保持计算开销不变的同时，对网络的深度和宽度进行了实验。最有效的架构由初始模块栈组成，而不是多个并行卷积和最大池层。</p><h2 id="f419" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">快速RCNN</h2><p id="2e2e" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">鉴于R-CNN和SPPNet培训费用高、速度慢、由多级管道组成的缺点，需要一种补救办法。快速R-CNN [3]是建立在以前的作品R-CNN的对象分类。它结合了非常深的VGG16网络，提供的结果分别比R-CNN和SPPnet快9倍和3倍。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mh"><img src="../Images/29e7e309b864e246fcf9d16f3b3a78b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Rg5dJKQi_V1GEZqlHuvSA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of Fast RCNN architecture [10]</figcaption></figure><h2 id="3d61" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">YOLO</h2><p id="046e" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">YOLO [4](你只看一次)的方法是基于多盒区域建议，它被认为是一个回归问题，空间分离的边界盒与相关的类概率。该架构速度很快，实时处理速度为每秒45帧。该模型唯一的缺点是定位错误的发生率较高，然而，它优于R-CNN。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mi"><img src="../Images/e1550b8ede7663a7849cc9913a899e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhAxYZgLeWd8ilDkH2S3pA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of YOLO architecture [4]</figcaption></figure><h2 id="8dd0" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">雷斯内特</h2><p id="3bc2" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">深度残差学习，ResNet [5]提供了一个重新设计层的框架，包括参考输入层的残差函数。与先前未引用的函数相比，该方法被证明更容易优化。ResNet ensemble在2015年在ILSVRC上取得了3.57%的误差，击败了人类的平均表现(5-10%)。</p><h2 id="6449" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">更快的RCNN</h2><p id="eab4" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">与SPPNet和利用启发式区域建议的快速R-CNN [3]相比，更快的R-CNN [6]为其架构引入了区域建议网络(RPN)。此外，它还结合了“注意力”机制，专门将网络导向正确的区域以找到对象。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mj"><img src="../Images/18538c2a0b2317349bb8bd08a785ee4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_FoJhKioY93I-65pGpwIA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Constituents of Faster RCNN [11]</figcaption></figure><h2 id="2fa4" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">（同solid-statedisk）固态（磁）盘</h2><p id="b382" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">单次多盒检测器SSD [7]通过完全消除额外的物体提议步骤，将相对简单的方法应用于先前的模型，从而使其速度更快。这是通过将输出空间离散化为默认框来实现的，网络为这些默认框生成每个对象类别的预测分数。这种方法的主要好处是对不同大小、比例和长宽比的图像获得更快、更准确的结果。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mk"><img src="../Images/c3a86a8f033de8ac8b43779241fde5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPaWm6Ufuvj8eUSxtdv4Xg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Outline of SSD architecture [7]</figcaption></figure><h2 id="a14d" class="li lj iq bd lk ll lm dn ln lo lp dp lq lf lr ls lt lg lu lv lw lh lx ly lz ma bi translated">屏蔽RCNN</h2><p id="2d39" class="pw-post-body-paragraph jq jr iq jt b ju mb jw jx jy mc ka kb lf md ke kf lg me ki kj lh mf km kn ko ij bi translated">2018年开发的Mask R-CNN [8]通过扩展更快的R-CNN [6]的工作，以最小的小开销实现加法对象掩蔽分支，以简单灵活的框架完成了多个对象识别任务。它已经被证明对于像估计人的姿态这样的任务是有用的。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ml"><img src="../Images/d12aa5380d0bbc5aa3fef184652b170a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyEkh9MpzsuTZY1ikY7qsw.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Image Segmentation from Mask RCNN algorithm [8]</figcaption></figure><p id="baac" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">这就结束了我们关于物体识别算法发展的系列文章。更详细的了解，你可以参考下面参考文献列表中的论文，也可以直接通过<a class="ae mm" href="https://medium.com/@asthanameghna01" rel="noopener"> Medium </a>或<a class="ae mm" href="http://www.twitter.com/meghnaasthana" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。如果你已经到达这里，谢谢你支持我的工作。我很想听到你的建议，所以请不要犹豫，通过我的两个平台联系我。</p><div class="mn mo gp gr mp mq"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd ir gy z fp mv fr fs mw fu fw ip bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne kz mq"/></div></div></a></div></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="ee0c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[1] Simonyan，k .和Zisserman，a .，2014年。用于大规模图像识别的非常深的卷积网络。arXiv预印本arXiv:1409.1556</p><p id="5d0c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[2] Szegedy，c .，Liu，w .，Jia，y .，Sermanet，p .，Reed，s .，Anguelov，d .，Erhan，d .，Vanhoucke，v .和Rabinovich，a .，2015年。用回旋越走越深。IEEE计算机视觉和模式识别会议论文集(第1-9页)。</p><p id="6dcb" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[3]吉希克，河，2015年。快速r-cnn。IEEE计算机视觉国际会议论文集(第1440-1448页)。</p><p id="4263" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[4]雷德蒙，j .，迪夫瓦拉，s .，吉尔希克和法尔哈迪，a .，2016年。你只看一次:统一的，实时的物体检测。IEEE计算机视觉和模式识别会议论文集(第779-788页)。</p><p id="e821" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[5]何，王，张，徐，任，孙，2016 .用于图像识别的深度残差学习。IEEE计算机视觉和模式识别会议论文集(第770-778页)。</p><p id="1118" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[6]任，s，何，k，吉希克，r和孙，j，2015年。更快的r-cnn:用区域建议网络实现实时目标检测。神经信息处理系统进展(第91-99页)。</p><p id="d500" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[7]刘，w，安盖洛夫，d，尔汗，d，塞格迪，c，里德，s，傅，C.Y和伯格，A.C，2016，10月。Ssd:单次多盒探测器。在欧洲计算机视觉会议上(第21-37页)。斯普林格，查姆。</p><p id="f50d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[8]何，k .，格基奥萨里，g .，多拉尔，p .和吉尔希克，r .，2017年。屏蔽r-cnn。IEEE计算机视觉国际会议论文集(第2961–2969页)。</p><p id="0686" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[9]Kaggle.com。(2020).<em class="js"> CNN架构:VGG，ResNet，Inception + TL </em>。[在线]可在:<a class="ae mm" href="https://www.kaggle.com/shivamb/cnn-architectures-vgg-resnet-inception-tl/notebook" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Shiva MB/CNN-architectures-vgg-resnet-inception-TL/notebook</a>[2020年3月6日访问]。</p><p id="bfd4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[10]中等。(2020).<em class="js">快速R-CNN进行物体检测</em>。[在线]可从以下网址获取:<a class="ae mm" href="https://towardsdatascience.com/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022" rel="noopener" target="_blank">https://towards data science . com/fast-r-CNN-for-object-detection-a-technical-summary-a0ff 94 fa a022</a>【2020年3月6日获取】。</p><p id="60d1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb lf kd ke kf lg kh ki kj lh kl km kn ko ij bi translated">[11]中等。(2020).<em class="js">更快的RCNN物体检测</em>。[在线]可从以下网址获取:<a class="ae mm" href="https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4" rel="noopener" target="_blank">https://towards data science . com/faster-rcnn-object-detection-f 865 e 5 ed 7 fc 4</a>【2020年3月6日获取】。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div></div>    
</body>
</html>