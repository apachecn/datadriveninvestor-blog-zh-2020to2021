# 不。安全和透明的人工智能并不矛盾。

> 原文：<https://medium.datadriveninvestor.com/no-secure-and-transparent-ai-is-not-an-oxymoron-1036cde97af5?source=collection_archive---------17----------------------->

在当前不信任和监管压力的环境下，安全性正日益成为讨论的焦点。近年来，硅谷已经见证了它的丑闻([脸书/剑桥分析公司](https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html)、[优步](https://www.theguardian.com/technology/2017/nov/21/uber-data-hack-cyber-attack)和[更多](https://www.csoonline.com/article/2130877/the-biggest-data-breaches-of-the-21st-century.html))，表明如果管理不当，围绕科技的机会主义会引发问题。不祥之兆是——各地的组织在选择创新合作伙伴时都采取了更加谨慎的方法。

监管格局也在发生变化。你肯定听说过 GDPR(通用数据保护条例)——旨在保护公民个人数据隐私的欧盟指令。[第 22 条](https://advisera.com/eugdpracademy/gdpr/automated-individual-decision-making-including-profiling/)特别具有挑战性:*“数据主体有权不受制于仅基于自动处理的决定，包括特征分析，这种决定对他或她产生法律效力或对他或她产生类似的重大影响”。*说白了——每个用户都有权控制自己的个人数据将如何被使用。加州紧随其后，CCPA 和许多其他司法管辖区也跟进或即将跟进。

![](img/c96e147df5cfb8a4ba329f1fdd40e560.png)

这些转变对于人工智能来说特别有意义。“自动化处理”是人工智能的核心，如果没有企业人工智能如何做出决策的清晰“地图”(例如，所有决策点的完整日志)，企业将无法提供关于用户数据的适当可追溯性。尤其是像深度学习这样本质上依赖于黑盒的算法，陷入了深深的困境。

[](https://www.datadriveninvestor.com/2020/08/30/ai-and-medical-imaging-startups-6-key-trends/) [## AI 和医学影像创业公司？6 大趋势|数据驱动的投资者

### “IBM Watson 健康成像”是医疗保健的未来吗？谷歌详细说明了人工智能对胸部 x 光的分类…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/08/30/ai-and-medical-imaging-startups-6-key-trends/) 

这不全是厄运和黑暗！对于那些希望降低重复性、易出错工作流程的成本，并利用迷失在“大数据”海洋中的宝贵见解的公司来说，人工智能似乎是圣杯。但是这样一个组织应该如何审查潜在的合作伙伴呢？数据安全处于前所未有的高度优先地位；他们必须牺牲部署时间或准确性吗？企业应该在人工智能提供者中寻找什么样的安全特性？什么是真正可用的？

冒着被规定的风险，我已经列出了一些当我和我的客户一起工作时我所遵循的事情。

# 1.针对每个问题部署定制的解决方案，确保不会有数据泄露。

人工智能解决方案必须部署在私有云或本地服务器上。虽然这在硅谷并不流行，但请理解数据隐私对我们的客户非常重要。还有另一个优势——这种架构将允许您根据自己的独特需求进行配置，并实现高精度。

# 2.记录您的引擎所做的每个决策的证据，确保客户对流程有一个完整的了解。

尽可能多地使用可解释的人工智能，例如决策树或[校准的量子网格](https://medium.com/datadriveninvestor/introduction-to-calibrated-quantum-mesh-an-ai-for-nlp-9a5d1be31b1e?source=friends_link&sk=6769960595f52cfc63d15a74d9085f48)。即使使用黑盒人工智能，在应用人工智能时，也要区分[学习规则和业务规则](https://medium.com/@prafulla.krishna/how-can-business-leaders-build-an-ai-in-a-day-42e28f785406?source=friends_link&sk=3860e385e1981832d822cc049fc68f23)。记录所有用于做出任何决定的证据。这些证据可以根据监管机构、审计机构的要求提供，或者仅仅是为了让您的内部团队更好地理解数据中的独特见解。

# 3.放弃外部模型的迁移学习，这样你就可以确保你的数据不会出现在不该出现的地方。

你的数据和见解是你的，也是你一个人的。重用机器从一个应用程序中学到的任何东西，以确保组织的另一个应用程序是成功的。然而，即使您的解决方案托管在云上，您的非公开数据的任何一个字节都不应该用于训练其他人的解决方案。这确实是有代价的——迁移学习就像是在小数据集上训练人工智能引擎的催化剂，通过将一个数据集的“经验教训”重新用于另一个数据集，我们的引擎“一次学习一个”。通过忽视使用这种技术，你可能会失去的时间，你会得到安全和内心的平静。

[](https://www.darkreading.com/threat-intelligence/the-infosec-barrier-to-ai/a/d-id/1338401) [## 人工智能的信息安全壁垒

### 信息安全挑战被证明是人工智能生态系统的巨大障碍。相反地…

www.darkreading.com](https://www.darkreading.com/threat-intelligence/the-infosec-barrier-to-ai/a/d-id/1338401) 

# 4.为您的数据和员工使用所有最新的加密和安全标准。

您的 IT 部门很可能已经了解了这一点—无论信息是存储在您的云中还是内部，同样的理念也适用于对 IT 基础架构或应用程序的访问。一切都必须加密、密码保护、双重认证，或者简单地说，安全。

安全的最后一个组成部分是您的员工。每一个接近你的敏感数据的员工都应该在美国和国际上进行彻底的背景调查。虽然这些事情都是例行公事，但有时定期提醒它们是有用的。

在变化的环境中，很容易被动。需要克服巨大组织动力的大公司仍然面临着同样的竞争威胁。每个人都在寻找优势——但是层层的复杂性和相互冲突的目标看起来让人不知所措。人工智能是来帮忙的，但它需要一些额外的照顾。

## 访问专家视图— [订阅 DDI 英特尔](https://datadriveninvestor.com/ddi-intel)