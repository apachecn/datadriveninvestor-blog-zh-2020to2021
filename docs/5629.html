<html>
<head>
<title>Model Overfitting? Use Dropout!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模型过度拟合？使用辍学！</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/model-overfitting-use-dropout-a32010f0afd0?source=collection_archive---------8-----------------------#2020-09-26">https://medium.datadriveninvestor.com/model-overfitting-use-dropout-a32010f0afd0?source=collection_archive---------8-----------------------#2020-09-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e2eb" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="/topic/machine-learning" rel="noopener ugc nofollow" target="_blank">深度学习</a></h2><div class=""/><div class=""><h2 id="768e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">最好的正规化技术。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/6f6c5414fa88df8c2985e03a31933be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CdU9SlUAxbKD_1pd"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alina Grubnyak</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c4ff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">被困在付费墙后面？点击<a class="ae le" href="https://medium.com/@D3nii/model-overfitting-use-dropout-a32010f0afd0?source=friends_link&amp;sk=933812164d01ba9ccd5e238faf4bfb4a" rel="noopener">这里</a>阅读完整故事与我的朋友链接！</p><p id="d40d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们都知道<strong class="lh ja"> <em class="mb">的数据是有限的</em> </strong>。是的，今天的数据非常丰富，比以往任何时候都多，但仍然有一个因素，那就是它并不是任何事情都有无限的数据。</p><p id="e2cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">而且，这常常会导致机器学习实践者出现一些问题，如<a class="ae le" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjgk7_5yfrrAhVJqxoKHfxmAYoQFjAXegQIBRAB&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FOverfitting&amp;usg=AOvVaw15Bea2nVsUGiMjDU4IUG8D" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"><em class="mb"/></strong></a>&amp;<a class="ae le" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjMzMSQyvrrAhUyyoUKHUgdB2cQFjAHegQIERAG&amp;url=https%3A%2F%2Fwww.investopedia.com%2Fterms%2Fo%2Foverfitting.asp&amp;usg=AOvVaw1i0SFcaDex4kvH5SXrnowr" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"><em class="mb">过拟合</em> </strong> </a>。我们将讨论<em class="mb">欠配</em>以及如何处理，但今天让我们讨论一种处理<em class="mb">过配</em>的方法。</p><h1 id="27fa" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">什么是退学？</h1><p id="1a6a" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">ML从业者使用许多技术来避免他们的模型的过度拟合行为。其中应用最广泛的是<strong class="lh ja"> <em class="mb">辍学</em> </strong>。</p><p id="a9c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">辍学</em>的定义很简单！</p><blockquote class="mz na nb"><p id="626e" class="lf lg mb lh b li lj ka lk ll lm kd ln nc lp lq lr nd lt lu lv ne lx ly lz ma ij bi translated"><strong class="lh ja"> Dropout </strong>是一种随机选择的神经元在训练时被忽略的技术。[1]</p></blockquote><p id="4505" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">随机选择要关闭的神经元。通过“关闭”，我的意思是它们在向前运行和向后传播中起0%的作用，因此，模型必须使用其他神经元来继续学习。</p><blockquote class="nf"><p id="ed26" class="ng nh iq bd ni nj nk nl nm nn no ma dk translated">Dropout做了一件看似疯狂的事情，在你的网络中随机淘汰一些单位。</p></blockquote><p id="32ae" class="pw-post-body-paragraph lf lg iq lh b li np ka lk ll nq kd ln lo nr lq lr ls ns lu lv lw nt ly lz ma ij bi translated">直觉I:  <em class="mb">这样更好，因为你在用一个更小的神经网络工作。因此，使用一个较小的神经网络似乎应该有一个正则化的效果。</em></p><p id="bc13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">直觉二:</strong> <em class="mb">一个神经元不会依赖于一个特征，会分散权重。这样，它类似于L2正则化。它缩小了重量。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/1fcb9289d5db12c8540d835e3f3ac40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvQPcRchcbQwXTLKDvnYjw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Image from this <a class="ae le" href="https://towardsdatascience.com/an-intuitive-explanation-to-dropout-749c7fb5395c" rel="noopener" target="_blank">website</a></figcaption></figure><h1 id="d747" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">为什么退学？</h1><p id="0cd0" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">让我们提醒一下这些神经元实际上在做什么。神经元在开始时被设置为随机值，随着时间的推移，它们会犯错误并相应地改变它们的权重。</p><p id="146d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">过度拟合是什么样子的？</em>T3】</strong></p><p id="b39e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我给你举个例子。假设，你在一个5人小组，A1，A2，A3，A4，A5。现在，你们的导师要求你们准备一份演讲稿，并在全校面前发表。现在你，A1，是小组的领头人，A3是你最好的朋友。你会偏向于把大部分发言给A3，对吧？因此，与A2、A4和A5相比，A3将具有更高的影响语音结果的比率。<strong class="lh ja"> <em class="mb">这是超配！</em> </strong> <em class="mb">如何？因此，如果A3在演示当天感觉不舒服，那么每个人都要面对后果，尽管其他成员都很健康。<em class="mb">你看到了吗？</em>一个成员的行动对整体结果起着重要作用。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/6545d2fde8237cd0b325799b56514c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mCGw6Ioj1xUN-51r"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@iamchang?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chang Duong</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ee53" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">现在，就神经网络而言？</em>T15】</strong></p><p id="eab6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我们提到<em class="mb">模型过度拟合</em>时，我们的意思是特定层中的一些神经元比它们的相邻神经元产生更大的影响，这使得预测<em class="mb">有偏差</em>，因此<em class="mb">错误</em>。</p><h2 id="3b4f" class="nw md iq bd me nx ny dn mi nz oa dp mm lo ob oc mo ls od oe mq lw of og ms iw bi translated">如何避免？</h2><p id="9ea1" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">我们用<strong class="lh ja"> <em class="mb">辍</em> </strong>。现在，你应该对退学能有什么帮助有一些直觉了。我们基本上做的是在每个时期随机关闭一些神经元，让其他神经元学习和适应。这样，我们:</p><ul class=""><li id="4d35" class="oh oi iq lh b li lj ll lm lo oj ls ok lw ol ma om on oo op bi translated">更好地概括模型。</li><li id="ae23" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated">避免过度拟合。</li><li id="3aaa" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated">获得独立的神经元表示。</li><li id="cecc" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated">获得更好的效果。(<em class="mb">大部分时候。</em>😅)</li></ul><h1 id="166f" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">效果？</h1><p id="fb78" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">你可以想象，如果一个层中的一些神经元被删除，那么其他神经元将不得不努力学习丢失神经元的表示。</p><p id="bb5b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">想想看，如果我们取出一些神经元，让其他神经元学习更多的东西。这难道不会使神经元对神经元的个体权重不那么敏感吗？因为没有哪个神经元会自己做出决定。唯一的缺点是<em class="mb">成本函数</em>没有被很好地定义。</p><h1 id="1621" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">在Keras实施</h1><p id="1dd2" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在<a class="ae le" href="https://keras.io" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja"><em class="mb">Keras</em></strong></a>中实现<strong class="lh ja"> <em class="mb">辍学</em> </strong>是相当容易和直接的。</p><p id="b575" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在制作<em class="mb">模型的架构</em>时，我们只是在<a class="ae le" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiSn9qvy_rrAhVGyxoKHb4BCrQQFjAFegQIERAF&amp;url=https%3A%2F%2Fiq.opengenus.org%2Ffully-connected-layer%2F&amp;usg=AOvVaw2ryRE9NTUgepA0LOAnsA70" rel="noopener ugc nofollow" target="_blank">全连接</a>层或<a class="ae le" href="https://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">卷积</a>层之间添加了脱落层。</p><pre class="kp kq kr ks gt ov ow ox oy aw oz bi"><span id="2470" class="nw md iq ow b gy pa pb l pc pd">from keras.layers import Dropout, ..., ...</span><span id="3bb7" class="nw md iq ow b gy pe pb l pc pd">model = Sequential()</span><span id="3507" class="nw md iq ow b gy pe pb l pc pd">model.add(Dense(.......))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(.......))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(.......))</span></pre><p id="5080" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">传入的0.25是什么函数？</em>T9】</strong></p><p id="7a32" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们传递一个神经元被丢弃的概率。在这种情况下，每个神经元都有大约<em class="mb"> 25% </em>被忽略的概率。</p><h1 id="e6e1" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">利用辍学？接受这些建议。</h1><ul class=""><li id="cdb0" class="oh oi iq lh b li mu ll mv lo pf ls pg lw ph ma om on oo op bi translated">辍学概率的正确选择很难做到完美，但20%通常是一个很好的起点。然后，将其作为超参数进行调整。</li><li id="5fa0" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated">对于在哪个层上使用辍学没有限制。可以用在多层感知器，卷积层，递归层等。</li><li id="769d" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated">使用Dropout时，尝试使用更大的网络。因为更大的网络更容易超载，所以使用Dropout会有更多的好处。</li></ul></div><div class="ab cl pi pj hu pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="ij ik il im in"><p id="6c9b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">好吧，我希望这篇文章能帮助你。我们上<a class="ae le" href="https://www.linkedin.com/in/d3ni/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> Linkedin </em> </a>连线吧！</p><h1 id="528b" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">参考</h1><p id="a10f" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">[1]深度学习模型中的辍学正则化与Keras，<a class="ae le" href="https://machinelearningmastery.com/author/jasonb/" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">杰森·布朗利</strong> </a> <strong class="lh ja">，</strong>2016年6月20日，<a class="ae le" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">https://machine Learning mastery . com/Dropout-regulation-Deep-Learning-Models-Keras/</a></p></div><div class="ab cl pi pj hu pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="ij ik il im in"><h1 id="ca5f" class="mc md iq bd me mf pp mh mi mj pq ml mm kf pr kg mo ki ps kj mq kl pt km ms mt bi translated">进一步阅读</h1><div class="pu pv gp gr pw px"><a href="https://medium.com/@D3nii/deep-learning-for-cats-vs-dogs-classification-309463f3fc46" rel="noopener follow" target="_blank"><div class="py ab fo"><div class="pz ab qa cl cj qb"><h2 class="bd ja gy z fp qc fr fs qd fu fw iz bi translated">针对猫vs狗分类的深度学习！</h2><div class="qe l"><h3 class="bd b gy z fp qc fr fs qd fu fw dk translated">区分猫狗的循序渐进指南！</h3></div><div class="qf l"><p class="bd b dl z fp qc fr fs qd fu fw dk translated">medium.com</p></div></div><div class="qg l"><div class="qh l qi qj qk qg ql ky px"/></div></div></a></div><div class="pu pv gp gr pw px"><a href="https://medium.com/dev-genius/deep-learning-for-weather-classification-fe877cdc721c" rel="noopener follow" target="_blank"><div class="py ab fo"><div class="pz ab qa cl cj qb"><h2 class="bd ja gy z fp qc fr fs qd fu fw iz bi translated">用于天气分类的深度学习</h2><div class="qe l"><h3 class="bd b gy z fp qc fr fs qd fu fw dk translated">在不同的天气状态之间分类！</h3></div><div class="qf l"><p class="bd b dl z fp qc fr fs qd fu fw dk translated">medium.com</p></div></div><div class="qg l"><div class="qm l qi qj qk qg ql ky px"/></div></div></a></div><div class="pu pv gp gr pw px"><a href="https://medium.com/@D3nii/tackling-common-neural-network-problems-e08b30775100" rel="noopener follow" target="_blank"><div class="py ab fo"><div class="pz ab qa cl cj qb"><h2 class="bd ja gy z fp qc fr fs qd fu fw iz bi translated">解决常见的神经网络问题</h2><div class="qe l"><h3 class="bd b gy z fp qc fr fs qd fu fw dk translated">大家好！这是我开始攻读纳米学位并成功完成第一个学位后的第四篇文章</h3></div><div class="qf l"><p class="bd b dl z fp qc fr fs qd fu fw dk translated">medium.com</p></div></div><div class="qg l"><div class="qn l qi qj qk qg ql ky px"/></div></div></a></div></div><div class="ab cl pi pj hu pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="ij ik il im in"><h1 id="189b" class="mc md iq bd me mf pp mh mi mj pq ml mm kf pr kg mo ki ps kj mq kl pt km ms mt bi translated">联系人</h1><p id="bbf7" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">如果你想了解我最新的文章和项目<a class="ae le" href="/@D3nii" rel="noopener ugc nofollow" target="_blank">，请关注我的媒体</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="a532" class="oh oi iq lh b li lj ll lm lo oj ls ok lw ol ma om on oo op bi translated"><a class="ae le" href="https://www.linkedin.com/in/d3ni/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="df7a" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated"><a class="ae le" href="https://github.com/D3nii?tab=repositories" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="021d" class="oh oi iq lh b li oq ll or lo os ls ot lw ou ma om on oo op bi translated"><a class="ae le" href="https://twitter.com/danyal0_o" rel="noopener ugc nofollow" target="_blank">推特</a></li></ul><blockquote class="nf"><p id="6ce9" class="ng nh iq bd ni nj qo qp qq qr qs ma dk translated"><em class="qt">快乐学习。:)</em></p></blockquote></div></div>    
</body>
</html>