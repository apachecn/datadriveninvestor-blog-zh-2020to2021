<html>
<head>
<title>Saving data from a slow API to a Pickle File</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将数据从慢速API保存到Pickle文件</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/saving-data-from-a-slow-api-to-a-pickle-file-9da3dcca9954?source=collection_archive---------1-----------------------#2020-01-06">https://medium.datadriveninvestor.com/saving-data-from-a-slow-api-to-a-pickle-file-9da3dcca9954?source=collection_archive---------1-----------------------#2020-01-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6d72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个创建的函数，用于在遍历for循环时保存API数据</h2></div><p id="d51a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从缓慢的API中绘制数据时的一个常见问题是，您可能会在截止日期前进行操作，并且您会发现从API中获取整个数据集需要几天时间。</p><p id="1f03" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初，您可能会发现自己只是运行一个函数来提取数据，一旦您将数据放入Jupyter笔记本，您就可以对其进行处理。但是出现的另一个常见问题是，当您第二天早上回来工作时，在数据提取过程中突然出现一个错误。该错误将会导致您丢失从API中提取的任何数据。这可能是因为您从JSON文件中保存的字典的键或值中存储了数据。</p><div class="le lf gp gr lg lh"><a href="https://www.datadriveninvestor.com/2019/02/25/6-alternatives-to-the-yahoo-finance-api/" rel="noopener  ugc nofollow" target="_blank"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">雅虎财经API |数据驱动投资者的6种替代方案</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">长期以来，雅虎金融API一直是许多数据驱动型投资者的可靠工具。许多人依赖于他们的…</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv lw lh"/></div></div></a></div><p id="f5af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在快节奏的数据科学环境中，您必须在截止日期前提供结果，这些问题阻碍了进展。对于数据科学家来说，学会解决这些问题非常重要，这样他们才能为利益相关者提供有意义的产品。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/3ebf5f0dc52f38d87cb9ad3e9fc7afb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAQOGezCkD5TfOujgZW3Kw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Source: <a class="ae mm" href="https://www.smartfile.com/blog/python-pickle-security-problems-and-solutions/" rel="noopener ugc nofollow" target="_blank">https://www.smartfile.com/blog/python-pickle-security-problems-and-solutions/</a></figcaption></figure><p id="c3b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我在熨斗学校进行我的数据科学期末项目时，我遇到了这个问题。所以经过一番努力，我想到了一个解决方案，创建一个pickle函数，它将打开一个预先保存的pickle文件，遍历一个API，并将从API提取的数据转储到pickle文件中。</p><p id="da50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将解释我的项目和我遵循的逻辑，并提供注释代码。</p><p id="5dc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我正在完成的项目要求我使用<a class="ae mm" href="https://pushshift.io/" rel="noopener ugc nofollow" target="_blank"> pushshift.io </a>工具从subreddit社区线程中获取reddit文本数据。我正在处理的数据的结构是{post_id : [comment_ids…]}.</p><p id="5614" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是在运行下面的功能之前，我必须将我的post _ ids和评论id加载到我的Jupyter笔记本中。你会在下面看到包含这个数据的变量的名字是<strong class="kk iu"> comment_dict </strong>。此外，我需要创建一个pickle文件，其中保存了来自<strong class="kk iu"> comment_dict </strong>的至少一个post_id/comment_id的文本数据。所以当我在我的函数中加载pickle文件时，它实际上是有内容可加载的。预先保存的pickle文件被标记为<strong class="kk iu"> data_denier2.pickle </strong>，因为这个标签在我的项目中使用过。一旦所有这些都在您的笔记本中完成，pickle文件存在，您就可以执行我提供的功能了。</p><p id="6f19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从导入必要的库开始。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="beff" class="ms mt it mo b gy mu mv l mw mx">import pickle<br/>import requests<br/>import pandas as pd<br/>import numpy as np<br/>import time<br/>import random<br/>from tqdm import tqdm_notebook</span></pre><p id="5bae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在您已经导入了所需的库，您可以跳转到数据选择器函数。</p><pre class="ly lz ma mb gt mn mo mp mq aw mr bi"><span id="daa8" class="ms mt it mo b gy mu mv l mw mx">def comments_pickler(comment_dict):<br/>    <br/>    #load the existing pickle<br/>    comment_data = pickle.load(open('data_denier2.pickle','rb')) <br/>    #keys to keep<br/>    keys = ['author', 'body', 'created_utc', 'id', 'parent_id', 'score']<br/>    <br/>    #browser bug fix added, may be removed if deemed unnecessary<br/>    headers = requests.utils.default_headers()<br/>    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'<br/>    <br/>    #tqdm_notebook tracks progress of the data extraction<br/>    #comment_dict contains a post_id (key) and a list of comment_ids (value)  <br/>    #we are grabbing the comments from the comment_ids<br/>    for key in tqdm_notebook(comment_dict):<br/>        #in case code breaks, this if statement will 'continue' until the last dictionary key<br/>        if key in comment_data.keys():<br/>            continue<br/>        #container for future list of dictionaries<br/>        x = []<br/>        #pause this list over here<br/>        sleep_counter = 0 # set counter back to zero once the key changes<br/>        <br/>        for value in comment_dict[key]:<br/>            try:<br/>                response = requests.get(f'<a class="ae mm" href="https://api.pushshift.io/reddit/comment/search?ids={value}'" rel="noopener ugc nofollow" target="_blank">https://api.pushshift.io/reddit/comment/search?ids={value}'</a>, headers=headers) <br/>                #performing randomized sleep every 20 iterations to avoid maxing out api calls<br/>                sleep_counter += 1<br/>                if sleep_counter == 20:<br/>                    sleep_counter = 0<br/>                    seq = [i/10 for i in range(5,15)]<br/>                    time.sleep(random.choice(seq))        <br/>            except Exception as e:<br/>                print(e)<br/>                print('key:    ' + str(key))<br/>                print('value:  ' + str(value))<br/>                continue<br/>    <br/>            try:<br/>                data = response.json()          <br/>            except Exception as e:<br/>                print(e)<br/>                print('key:      ' + str(key))<br/>                print('value:    ' + str(value))<br/>                continue<br/>                <br/>            #dictionary comprehension extending the 'data' value from dictionary to list of dictionaries, x<br/>            x.extend([dict((k, data['data'][0][k]) for k in keys if k in data['data'][0])])<br/>        comment_data[key] = x<br/>        #save the data obtained from this key, and jump to the next key<br/>        pickle.dump(comment_data,open('data_denier2.pickle','wb'))</span></pre><p id="4a69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个函数的逻辑是，我将遍历一个列表字典(<strong class="kk iu"> comment_dict </strong>)，最终<em class="my">扩展</em>一个字典列表(<strong class="kk iu"> data_denier2.pickle </strong>)。本词典(<strong class="kk iu"> comment_dict </strong>)包含id。这些id一旦转换成<strong class="kk iu"> data_denier2.pickle </strong>就会变成文本数据。</p><p id="2f11" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我加入了一些特性，比如一个随机化的计时器以避免api耗尽，try和except语句读取和运行任何错误，tqdm_notebook跟踪数据提取的进度，以及一些字典理解以将api中的数据整理成一种可用的形式，用于包含所有文本数据的最终pickle文件。</p><p id="8811" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最终，我使用的步骤流程和逻辑应该作为数据科学家的指南，以便他们可以高效、及时地保存他们的数据。希望这有所帮助！</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="mz na l"/></div></figure></div></div>    
</body>
</html>