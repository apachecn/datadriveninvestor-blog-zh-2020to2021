<html>
<head>
<title>Easily Scrape Stock Market News Headlines from Twitter for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为NLP从Twitter上轻松抓取股市新闻标题</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/easily-scrape-stock-market-news-headlines-from-twitter-for-nlp-223ece5761cd?source=collection_archive---------2-----------------------#2020-05-30">https://medium.datadriveninvestor.com/easily-scrape-stock-market-news-headlines-from-twitter-for-nlp-223ece5761cd?source=collection_archive---------2-----------------------#2020-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1989" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用现有的python包快速抓取各种股票的带日期标签的市场新闻标题。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/be1e12dfe6eddd83aebc4cea42a27e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*XYmiUqwDttOf5HSKUfFjDg.jpeg"/></div></figure><p id="af21" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">今天的股票市场波动很大；卡在历史上最长的牛市和永无止境的厄运和绝望的新闻循环之间。这种市场环境产生了相当多的对头条价格高度敏感的股票。</p><div class="lj lk gp gr ll lm"><a href="https://www.datadriveninvestor.com/2020/02/02/the-best-way-to-use-stock-market-correlations/" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab fo"><div class="lo ab lp cl cj lq"><h2 class="bd ir gy z fp lr fr fs ls fu fw ip bi translated">利用股市相关性的最佳方式|数据驱动的投资者</h2><div class="lt l"><h3 class="bd b gy z fp lr fr fs ls fu fw dk translated">当阿尔弗雷德·温斯洛·琼斯开创了世界上第一个“对冲”基金(后来“d”被去掉了)时，他让其他投资者大吃一惊…</h3></div><div class="lu l"><p class="bd b dl z fp lr fr fs ls fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kl lm"/></div></div></a></div><p id="6ed7" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在这篇文章中，我将介绍我从主要的市场新闻twitter账户中建立一个熊猫数据框架的步骤。这些步骤旨在帮助您执行NLP技术，如主题建模和情感分析，并最终与股票价格数据相结合，制定基于新闻/事件的交易策略。</p><h1 id="3fd4" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">该过程</h1><ol class=""><li id="11db" class="mt mu iq kp b kq mv kt mw kw mx la my le mz li na nb nc nd bi translated">在Twitter上搜索包含股票代码(TSLA)和公司名称(特斯拉)的新闻标题。</li><li id="841a" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated">将抓取的推文转换成熊猫数据帧。</li><li id="efea" class="mt mu iq kp b kq ne kt nf kw ng la nh le ni li na nb nc nd bi translated">清理文本，删除“停用字词”，并转换为ngrams。</li></ol><h1 id="d133" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">第一步:清理推特</h1><p id="aee0" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw nj ky kz la nk lc ld le nl lg lh li ij bi translated">使用<a class="ae nm" href="https://pypi.org/project/twitterscraper/" rel="noopener ugc nofollow" target="_blank">这个包</a> ( <a class="ae nm" href="https://github.com/taspinar/twitterscraper" rel="noopener ugc nofollow" target="_blank">文档</a>)获取历史推文比使用Twitter API要容易得多。首先，我们将从终端安装它，然后运行高级twitter搜索，并将结果输出到一个json文件中:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="688c" class="ns mc iq no b gy nt nu l nv nw">pip install twitterscraper</span><span id="3151" class="ns mc iq no b gy nx nu l nv nw">twitterscraper "$TSLA OR Tesla from:wsj OR from:reuters OR from:business OR from:cnbc OR from:RANsquawk OR from:wsjmarkets" -o tsla_tweets.json -l 10000</span></pre><p id="115b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先在<a class="ae nm" href="https://twitter.com/search-advanced?lang=en" rel="noopener ugc nofollow" target="_blank"> Twitter高级搜索</a>上构建查询，然后复制并粘贴到上面的代码中会有所帮助。作为参考，-o命名您的输出文件，而-l限制一个会话中抓取的tweets的数量。</p><p id="b81c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果一切顺利，您应该有一个json文件，其中包含几千条多年前的tweets。如果愿意，也可以在查询中指定日期范围。</p><h1 id="d7a6" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">步骤2:转换为数据帧</h1><p id="4a94" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw nj ky kz la nk lc ld le nl lg lh li ij bi translated">一旦我们有了输出json文件，我们就可以打开Jupyter Notebook并开始研究和清理文本。</p><p id="b13f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在下面的代码中，我们将转换为dataframe，将索引转换为datetime，删除重复的tweet，只保留每条tweet的文本和来源:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="69ab" class="ns mc iq no b gy nt nu l nv nw">import pandas as pd</span><span id="1152" class="ns mc iq no b gy nx nu l nv nw">df_orig = pd.read_json(‘tesla_tweets.json’, encoding=’utf-8')<br/>df = df_orig.copy()</span><span id="9c34" class="ns mc iq no b gy nx nu l nv nw">df = df[['timestamp', 'text', 'username']]</span><span id="c920" class="ns mc iq no b gy nx nu l nv nw">df.set_index(pd.to_datetime(df['timestamp']), inplace=True)<br/>df.drop(columns='timestamp', inplace=True)</span><span id="57c9" class="ns mc iq no b gy nx nu l nv nw">df.drop_duplicates(subset=['text'], keep=False, inplace=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2b9c00ed9c9fca4ec3b5458418ca2108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*ZVG2P6kRND_Rza8wR-ekIQ.png"/></div></figure><h1 id="256d" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">步骤3:清理并转换为“ngrams”</h1><p id="7a24" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw nj ky kz la nk lc ld le nl lg lh li ij bi translated">现在我们已经将每条tweet放在一个数据帧中，我们可以构建一个函数来清理文本并将其转换为“ngrams”；一个NLP术语，仅指数量为“n”的有序字的元组。比如“特斯拉做电动汽车”这句话，有3个bi-gram(n = 2)；“特斯拉制造”、“制造电动”、“电动汽车”。与简单地将句子分成单个单词相比，使用双单词元组可以让我们保留更多的上下文。</p><p id="0591" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们需要为下面的函数安装和导入一些NLP包，以及regex。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="dc37" class="ns mc iq no b gy nt nu l nv nw">import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>from nltk.util import ngrams<br/>from stop_words import get_stop_words<br/>import re</span><span id="9f0e" class="ns mc iq no b gy nx nu l nv nw">def generate_clean_ngrams(tweet_text, n):<br/>    <br/>    # create list of stop words from two different libraries<br/>    stop_words = list(stopwords.words('english')) +\<br/>                 list(get_stop_words('en'))<br/>    <br/>    # identify numbers to later remove words with numbers<br/>    pattern = '[0-9]'<br/>    <br/>    # convert words to lower case<br/>    tweet_text = tweet_text.lower()<br/>    <br/>    # replace special characters with a space<br/>    tweet_text = re.sub(r'[^a-zA-Z0-9\s]', ' ', tweet_text)<br/>    <br/>    # replace numbers with string 'http' to later remove both links <br/>    # and words with numbers in one line<br/>    tweet_text = re.sub(pattern, 'http', tweet_text)<br/>    <br/>    tokens = [token for token in tweet_text.split(' ') <br/>              if token != '' <br/>              and token not in stop_words <br/>              and 'http' not in token]<br/>    <br/>    output = list(ngrams(tokens, n))<br/>    <br/>    return output</span></pre><p id="ce8b" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">上面的函数代表了对令牌化的探索性尝试，在部署之前可能需要进一步的定制。抽查你的输出，确保你得到了你需要的结果。这里有一个例子:</p><h2 id="635a" class="ns mc iq bd md nz oa dn mh ob oc dp ml kw od oe mn la of og mp le oh oi mr oj bi translated">之前:</h2><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="8205" class="ns mc iq no b gy nt nu l nv nw">'Tesla plans up to $1.15 billion capital raise to cut risk http://bloom.bg/2noEGIM\xa0pic.twitter.com/64AHh3i5fm'</span></pre><h2 id="1e96" class="ns mc iq bd md nz oa dn mh ob oc dp ml kw od oe mn la of og mp le oh oi mr oj bi translated">之后:</h2><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="b4a1" class="ns mc iq no b gy nt nu l nv nw">[('tesla', 'plans'),<br/> ('plans', 'billion'),<br/> ('billion', 'capital'),<br/> ('capital', 'raise'),<br/> ('raise', 'cut'),<br/> ('cut', 'risk')]</span></pre><p id="4c6c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对我们的第一次尝试来说还不错。当NLP试图完美地“修剪”你的文本时，它会变得很棘手，并且它会花很长时间来完成许多步骤。</p><h1 id="f9fb" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">后续步骤</h1><p id="acad" class="pw-post-body-paragraph kn ko iq kp b kq mv jr ks kt mw ju kv kw nj ky kz la nk lc ld le nl lg lh li ij bi translated">在投入多个小时来改进我的NLP预处理结果之前，我决定在我的tweets语料库中的大约14，000个二元模型上创建虚拟变量，以实现一些探索性的数据分析。如果有有希望的结果，我可以回去改进在帖子前面展示的功能。</p><p id="2b37" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">请继续关注第2部分！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ol l"/></div></figure></div></div>    
</body>
</html>