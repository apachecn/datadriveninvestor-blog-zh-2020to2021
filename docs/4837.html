<html>
<head>
<title>Top 5 Machine Learning Algorithms used by Data Scientists with Python: Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家使用Python的5大机器学习算法:第1部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/top-5-machine-learning-algorithms-used-by-data-scientists-with-python-part-1-51bae4e1b21e?source=collection_archive---------8-----------------------#2020-08-26">https://medium.datadriveninvestor.com/top-5-machine-learning-algorithms-used-by-data-scientists-with-python-part-1-51bae4e1b21e?source=collection_archive---------8-----------------------#2020-08-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/fb4e33d9ced19e0f2f7e30fdc0714a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BlytPgqW44eK3EEmPpORrA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@altumcode?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">AltumCode</a> on <a class="ae kc" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8111" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习是一种重要的人工智能技术，它可以通过经验学习来有效地执行任务。据<a class="ae kc" href="https://www.forbes.com/#745b86852254" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">福布斯</em> </strong> </a>报道，未来10年内，机器学习将取代25%的工作岗位。</p><p id="c342" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习在现实世界中最受欢迎的应用之一是分类。它对应于日常生活中常见的任务。例如，一家医院可能希望将患者划分为患某种疾病的高、中、低风险人群，一家民意调查公司可能希望将受访者划分为可能会投票给几个政党中的每一个或尚未决定的人群，或者我们可能希望将学生项目划分为优秀、优秀、通过或失败。其他应用包括聚类、语言翻译、推荐、语音和图像识别等。</p><blockquote class="lc ld le"><p id="79cb" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">选择相关的机器学习技术是主要任务之一，因为有各种算法可用于不同的用例，并且它们都有其优点和效用。</p></blockquote><p id="19c1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将讨论数据科学家最常用的5种机器学习算法。</p><p id="5aa7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于这是博客的第一部分，因此这包括两个常用的机器学习算法，即逻辑回归和聚类。</p><h1 id="9d48" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">目录:</h1><ul class=""><li id="c78b" class="mg mh iq kf b kg mi kk mj ko mk ks ml kw mm la mn mo mp mq bi translated"><strong class="kf ir">逻辑回归</strong></li><li id="7550" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir"> Logit功能</strong></li><li id="867b" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir">实际用例:使用Python的逻辑回归</strong></li><li id="a03c" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir">聚类</strong></li><li id="ead9" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir">如何衡量集群的性能？</strong></li><li id="17b9" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir">聚类的类型</strong></li><li id="c5cf" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kf ir">实际用例:用Python实现K-Means聚类</strong></li></ul></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="2c17" class="li lj iq bd lk ll nd ln lo lp ne lr ls lt nf lv lw lx ng lz ma mb nh md me mf bi translated">1.逻辑回归</h1><p id="18e4" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">逻辑回归(也称为Logit回归)是一种用于分类(二元和多类分类)的回归技术。它是一个概率统计模型，因变量是一个分类值。(要了解更多关于因变量的信息，请点击这个<a class="ae kc" href="https://www.theaisorcery.com/post/linear-regression-for-beginners-a-mathematical-introduction" rel="noopener ugc nofollow" target="_blank">链接</a>，我在这里简要解释了因变量和自变量之间的区别)</p><h1 id="b2b1" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">分类问题为什么不直接用线性回归？</h1><p id="f34f" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">你可能会感到奇怪，因为逻辑回归是一种回归算法，但它仍然用于分类，而不是<a class="ae kc" href="https://en.wikipedia.org/wiki/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,is%20called%20simple%20linear%20regression" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">线性回归</em> </strong> </a>。为了理解这种困惑，让我们考虑下面的例子:</p><p id="2e4d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑这样一种情况，我们必须根据一个人当前的体重来预测他/她是“肥胖”还是“不肥胖”。下图是基于数据绘制的特定样本点。y轴表示分类目标值，其中1表示人肥胖，0表示人不肥胖。</p><p id="add7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<strong class="kf ir"> f(x) </strong>为绘制的数据点的线性回归线(或最佳拟合线)。现在，如果我们使用线性回归，我们需要设置一个阈值，在此基础上我们可以执行分类。如果估计的概率(P)位于内部<strong class="kf ir"> 0.5 &lt; P &lt; 1 </strong>中，模型将预测值为1，表示此人肥胖，如果概率(P)给定为<strong class="kf ir"> 0 &lt; P &lt; 0.5 </strong>，则模型将预测此人不肥胖(目标值为0)。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/8e513491b873ec71669b2f05c4123531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNvEzVf430tXTWcB9uVxCg.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 1: Graph depicting best fit lines</figcaption></figure><p id="1391" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图1中，回归线<strong class="kf ir"> f(x) </strong>由公式<strong class="kf ir"> y= mx+c </strong>给出，其中<strong class="kf ir"> y </strong>为<strong class="kf ir"> f(x) </strong> , <strong class="kf ir"> m </strong>为斜率，<strong class="kf ir"> x </strong>为因变量，<strong class="kf ir"> c </strong>为常数。但是这种配方存在某些问题。</p><p id="b1f3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们向我们的训练数据集中添加了一些“非常积极”的点。回归线将向这些例子倾斜(由<strong class="kf ir">f’(x)</strong>给出)，将更多边缘情况的正确分类置于危险之中。这可能导致估计的概率(P)大于1。这是不幸的，因为无论如何，我们已经正确地分类了这些非常积极的点。我们希望在职业之间划一条线，作为一个边界，而不是作为一个计分员穿过这些职业。</p><p id="4df8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑分类的正确方式是将特征空间划分为多个区域，使得任何给定区域内的所有点都注定被分配相同的标签。区域是由它们的边界定义的，所以我们希望回归找到分隔线而不是拟合。</p><h1 id="2f29" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">1.1 Logit函数</h1><p id="82ba" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">与回归模型类似，逻辑回归模型通过添加偏差项来计算相关特征的加权和，但估计概率由以下等式给出:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/75913c95ea0531ef82d3915d681807a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMYvtSdWKIumOhzc4fl3Tw.jpeg"/></div></div></figure><p id="e328" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，<strong class="kf ir"> y </strong>是预定义的类，<strong class="kf ir"> Wᵗx </strong>是预测，<strong class="kf ir"> σ </strong>是sigmoid函数。logit函数或sigmoid函数如下所示:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/f8c743e6c4eb0b5e74a2463ef108e682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5B6ihbRnTPraMZScvQueA.jpeg"/></div></div></figure><p id="2635" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该函数将一个真实值<strong class="kf ir">∞≤x≤∞</strong>作为输入，并产生一个范围在<strong class="kf ir"> [0，1] </strong>内的值，即一个概率。使用logit函数的优势在于，它有助于处理异常值并最大化成本函数。</p><h1 id="453b" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">1.2实际用例:使用Python的逻辑回归</h1><p id="5b9f" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">对于使用Python的实际实现，我们将使用<a class="ae kc" href="https://www.kaggle.com/giripujar/hr-analytics" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">HR Analytics</em></strong></a><strong class="kf ir"><em class="lb"/></strong>数据集，该数据集在<a class="ae kc" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">ka ggle</em></strong></a>上可用。在该数据集中，为预测给出了3个分类值，即“低”、“中”、“高”。我们将仅使用“低”和“高”类别来演示使用逻辑回归的二元分类。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="45e2" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">import pandas as pd<br/>import numpy as np</strong></span><span id="2063" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">#import the dataset<br/>df=pd.read_csv('HR_comma_sep.csv')</strong></span><span id="b55b" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">#remove the 'medium' target category<br/>df=df.loc[(df['salary'] == 'low') | (df['salary']=='high')]<br/>df=df.drop(columns="Department")</strong></span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/beac21e8e2518eaae8d00d7d4737d2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFIIlS1T4oumiM35ifeQ3Q.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 2: Dataframe</figcaption></figure><p id="6a56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图2所示，我们有8个自变量和1个因变量，由“薪金”列表示。现在，在应用任何ML算法之前，我们需要将目标变量转换成数值。为此，我们将使用<a class="ae kc" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">标签编码</em> </strong>。</a></p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="a2e4" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">from sklearn.preprocessing import LabelEncoder </strong></span><span id="9d8f" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">le = LabelEncoder() <br/>df['salary']= le.fit_transform(df['salary']) <br/>X=df.iloc[: ,0:-1]<br/>y=df.iloc[: ,-1]</strong></span></pre><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/7c9189586dedb7c43147bf455a978957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yjkDyfwXnH8uTeHyHvl-6w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 3: Dataframe after label encoding</figcaption></figure><p id="5f72" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在“薪金”一栏中可以清楚地看到，变量已被转换为数值。我们将使用<a class="ae kc" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">sk learn</em></strong></a>中的“<a class="ae kc" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">train _ test _ split</em></strong></a>”模块来拆分我们在训练和测试数据集中的数据，然后使用逻辑回归进行分类。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="cc83" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">from sklearn.model_selection import train_test_split</strong></span><span id="7331" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">X_train_Logistic, X_test_Logistic, y_train_Logistic, y_test_Logistic = train_test_split(X,y,train_size=0.25)</strong></span><span id="4d78" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">#logistic regression model</strong></span><span id="80a6" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">from sklearn.linear_model import LogisticRegression</strong></span><span id="c4e1" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">model_Logistic = LogisticRegression()<br/>model_Logistic.fit(X_train_Logistic, y_train_Logistic)</strong></span></pre><p id="4a32" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练与测试数据的比率是75:25。“model_Logistic”变量由为逻辑回归创建的实例组成。对数据进行训练后，我们将对测试数据进行预测。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="1ac8" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">model_Logistic.predict(X_test_Logistic)</strong></span></pre><p id="56da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成以下输出，展示了对输出变量的预测。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5187b1b760c595f285edca0476eca6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*U5Uwn5owkcF9nlnkP0E8dA.png"/></div></figure><p id="a525" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出是一个带有预测的二进制数组。现在进一步，我们可以在此基础上计算精度和混淆矩阵。</p><p id="c3d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此时，分析师可能会做一些模型选择；找出足以解释它们对目标变量的联合效应的变量子集。一种方法是去掉最不重要的系数，重新调整模型。这一过程会重复进行，直到模型中不再有其他项被删除。</p><p id="bb44" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个更好但更耗时的策略是移除一个变量，重新调整每个模型，然后进行偏差分析，以决定排除哪个变量。拟合模型的残差偏差是其对数似然的负两倍，两个模型之间的偏差是它们各自残差偏差的差(类似于平方和)。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="c731" class="li lj iq bd lk ll nd ln lo lp ne lr ls lt nf lv lw lx ng lz ma mb nh md me mf bi translated">2.使聚集</h1><blockquote class="lc ld le"><p id="1de0" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><em class="iq">机器学习的典型商业应用，如预测建模和聚类，比以往任何时候都更少依赖原始代码的生成——David Amoux</em></p></blockquote><p id="8acd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">聚类是一种基于将相似对象分组在一起的无监督机器学习技术。集群有多种使用案例，下面给出了其中一些:</p><ul class=""><li id="70fa" class="mg mh iq kf b kg kh kk kl ko og ks oh kw oi la mn mo mp mq bi translated">在财务应用程序中，查找具有相似财务业绩的公司集群。</li><li id="fadc" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">在营销应用程序中，查找具有相似购买行为的客户群。</li><li id="a628" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">在医疗应用程序中，查找具有相似症状的患者群。</li><li id="0dbf" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated">在犯罪分析应用程序中，我们可能会寻找大量犯罪(如盗窃)的群集，或者尝试将非常罕见(但可能相关)的犯罪(如谋杀)聚集在一起。</li></ul><h1 id="3f90" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">如何衡量聚类分析模型的性能？</h1><p id="df42" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">由于聚类是一种无监督的机器学习技术，因此没有任何措施可以让我们测量模型的性能，如准确性、精确度等。因此，问题出现了，我们如何测量我们的聚类模型的性能？</p><blockquote class="lc ld le"><p id="d886" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">主要有两种技术可以帮助我们衡量性能，一种是允许我们比较不同的聚类方法，另一种是检查聚类的特定属性，如紧密性等。</p></blockquote><p id="2ffb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于不同聚类算法之间的比较，使用了<a class="ae kc" href="https://en.wikipedia.org/wiki/Rand_index#:~:text=The%20Rand%20index%20or%20Rand,is%20the%20adjusted%20Rand%20index" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">Rand measure</em></strong></a>。通过Rand度量，我们比较了不同方法得到的不同聚类的一致性。通俗地说，这种方法检查数据聚类结果之间的相似性，另一方面，为了检查具体属性，如紧密性，使用<a class="ae kc" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)#:~:text=The%20silhouette%20value%20is%20a,poorly%20matched%20to%20neighboring%20clusters" rel="noopener ugc nofollow" target="_blank">轮廓分析</a>。</p><p id="3efc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">轮廓分析基于轮廓分数，该轮廓分数指示数据点属于特定聚类的程度。例如，如果我们有三个群集C1、C2、C3，并且我们从群集C1中随机选取一个点x，轮廓分数将告诉我们点x属于群集C1的程度。第2.1.1 (b)节讨论了轮廓分析。</p><h1 id="092a" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">2.1聚类算法的类型</h1><p id="20bb" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">有许多类型的聚类算法可用于不同的用例及数据。一些聚类算法包括k-均值聚类、层次聚类、DBSCAN、模糊c-均值聚类等。在本文中，我们将讨论用Python实现的最常用的聚类算法(k-means聚类)。</p><h1 id="164e" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">2.1.1实际用例:使用Python代码的K-Means聚类</h1><p id="b89b" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb"> K-means算法</em> </strong> </a>是一种硬划分算法，目标是将每个数据点分配到单个聚类中。K-means算法将一组<strong class="kf ir"> n </strong>个样本分成<strong class="kf ir"> k </strong>个不相交的簇<strong class="kf ir"> ci，i = 1，…，k </strong>，每个簇由簇中样本的均值<strong class="kf ir"> i </strong>来描述。这些平均值通常被称为簇形心。K-means算法假设所有的<strong class="kf ir"> k </strong>组具有相等的方差。</p><p id="dc0b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一种快速、易于理解且通常有效的聚类方法。它首先猜测聚类中心可能在哪里，评估这些中心的质量，然后对它们进行优化，以做出更好的中心估计。k均值聚类算法的算法如下所示:</p><blockquote class="lc ld le"><p id="651e" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">1.初始化k个聚类中心<br/> 2。使用循环，将所有数据点分配给它们最近的质心(聚类中心)。<br/> 3。重新计算形成的k个簇的质心<br/> 4。重复以上两步，直到质心不再移动</p></blockquote><p id="4f04" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于实际实现，让我们考虑一下<a class="ae kc" href="https://www.cs.cmu.edu/~./enron/" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">安然邮件数据集</em> </strong> </a>。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="704d" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">emails = pd.read_csv('enron.csv')</strong></span></pre><p id="c8b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加载数据后，将获得以下数据帧:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/29eb9ce6140f3046c20cbe141bd920bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*WC6MuQG8ciX-1XLQH2qgLQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 4: Dataframe for the Enron Email Dataset</figcaption></figure><p id="ef11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们对数据集进行预处理，得到Tf-Idf特征</p><blockquote class="ok"><p id="428a" class="ol om iq bd on oo op oq or os ot la dk translated">K <!-- --> indly参考我的<a class="ae kc" href="https://github.com/abhishek-924/Cohesion-Analysis-in-Email-Clustering-" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> <em class="ou"> Github库</em> </strong> </a>供参考。</p></blockquote><p id="96e0" class="pw-post-body-paragraph kd ke iq kf b kg ov ki kj kk ow km kn ko ox kq kr ks oy ku kv kw oz ky kz la ij bi translated">在应用k-means聚类之前，需要找到最佳聚类的数量。这可以通过两种最常见的方法得到:<a class="ae kc" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">肘法</em> </strong> </a>和<a class="ae kc" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)#:~:text=The%20silhouette%20value%20is%20a,poorly%20matched%20to%20neighboring%20clusters" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> <em class="lb">剪影评分</em> </strong> </a>。</p><h1 id="0796" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">a)弯头法</h1><p id="928d" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">为了求出形心的数目，使用了肘形法。在这种方法中，通过对每个k值后的数据集进行聚类来计算不同k值(即聚类数)的误差平方和(SSE)值。elbow方法的工作原理是最小化类内平方和(WCSS ),公式如下:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pa"><img src="../Images/4967b795970fd22fdafc85b4b3189a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBE3cLZInwCxWqVnX-aU4w.jpeg"/></div></div></figure><p id="f762" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个等式中，<strong class="kf ir"> Si </strong>给出了点的平均值，<strong class="kf ir"> x </strong>包含d维向量中的观察值，<strong class="kf ir"> k </strong>是聚类中心的数量。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="4555" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">from sklearn.cluster import KMeans<br/>import matplotlib.pyplot as plt</strong></span><span id="33a5" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">wcss = []<br/>for i in range(1, 6):<br/>    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)<br/>    kmeans.fit(X.todense())<br/>    wcss.append(kmeans.inertia_)<br/>plt.plot(range(1, 6), wcss)<br/>plt.title('The Elbow Method')<br/>plt.xlabel('Number of clusters')<br/>plt.ylabel('WCSS')<br/>plt.show()</strong></span></pre><p id="1b6d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们将循环的范围选择在1到6之间，这代表了潜在的最佳集群的数量。由于<strong class="kf ir"> X </strong>是一个稀疏矩阵，我们使用了<strong class="kf ir"> dense() </strong>方法将稀疏矩阵转化为密集矩阵。</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/e6f4f2caa79890e392dff468aa268ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*abzzoD3WV21vE6d17IBKQg.png"/></div></figure><p id="ac50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图上出现“铰链”的点被认为是<strong class="kf ir"> k </strong>的最佳值。从图中我们可以清楚的看到有两个x坐标，<strong class="kf ir"> 2 </strong>和<strong class="kf ir"> 3 </strong>，图中给出了铰链。因此，聚类的最佳数量可以是2或3。为了进一步给出精确的聚类数，使用了轮廓分数。</p><h1 id="fe4f" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">b)轮廓分数</h1><p id="55c1" class="pw-post-body-paragraph kd ke iq kf b kg mi ki kj kk mj km kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">轮廓值总是位于-1和1之间，其中-1表示我们考虑的数据点更靠近其相邻聚类，而远离指定的聚类中心，而+1表示数据点靠近指定的聚类中心，而远离相邻聚类。轮廓分数由以下公式给出:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pc"><img src="../Images/37f276a2f69eba9b8144baa6ff35799d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoFSWJuVvFGwpl_d34tuAg.jpeg"/></div></div></figure><p id="465c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上式中，<strong class="kf ir"> M(x) </strong>是点<strong class="kf ir"> x </strong>与指定聚类内所有点的平均距离，<strong class="kf ir"> N(x) </strong>是点<strong class="kf ir"> x </strong>与相邻聚类内所有点的平均距离。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="93e8" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">from sklearn.metrics import silhouette_score</strong></span><span id="fd75" class="nx lj iq nt b gy oc nz l oa ob"><strong class="nt ir">range_n_clusters = list (range(2,6))<br/>for n_clusters in range_n_clusters:<br/>    clusterer = KMeans (n_clusters=n_clusters)<br/>    preds = clusterer.fit_predict(X)<br/>    centers = clusterer.cluster_centers_<br/>    score = silhouette_score (X, preds, metric='euclidean')</strong></span></pre><p id="7bea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也采用相同的质心范围来计算轮廓分数。度量被选择为“欧几里得”,这意味着为了计算平均距离，我们使用了欧几里得距离。这段代码生成了以下输出:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/85f12752fbf4ce6a79976c4e0ab314ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*wFU3jPLzO5ul6CP4D_pzXQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 5: Average Silhouette Scores</figcaption></figure><p id="857d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图5中可以清楚地看出，最佳的聚类数是3，因为它获得了最高的分数。因此，利用这一点，我们将执行我们的k-均值聚类。</p><pre class="nm nn no np gt ns nt nu nv aw nw bi"><span id="72f4" class="nx lj iq nt b gy ny nz l oa ob"><strong class="nt ir">n_clusters = 3<br/>clf = KMeans(n_clusters=n_clusters, <br/>            max_iter=100, <br/>            init='k-means++', <br/>            n_init=1)<br/>labels = clf.fit_predict(X)</strong></span></pre><p id="c969" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变量‘n _ clusters’包含最佳数量的簇。该代码产生以下输出:</p><figure class="nm nn no np gt jr gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/e03813c6a0bdc1c1fab2be1bcb9a5494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*Z8b4N4BV-3lW5j_SRGKE8A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Figure 6: Clusters</figcaption></figure><p id="1dbd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图6显示我们已经成功地执行了k-means聚类。从图中可以清楚地看到，有三个聚类中心。</p><p id="8d2a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考资料和其他资源:</strong></p><ul class=""><li id="add6" class="mg mh iq kf b kg kh kk kl ko og ks oh kw oi la mn mo mp mq bi translated"><a class="ae kc" href="https://link.springer.com/chapter/10.1007/978-981-15-3369-3_9" rel="noopener ugc nofollow" target="_blank">https://link . springer . com/chapter/10.1007/978-981-15-3369-3 _ 9</a></li><li id="6eef" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><a class="ae kc" href="https://github.com/abhishek-924/Cohesion-Analysis-in-Email-Clustering-" rel="noopener ugc nofollow" target="_blank">https://github . com/abhishek-924/Cohesion-Analysis-in-Email-Clustering-</a></li><li id="8f5b" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><a class="ae kc" href="https://www.statisticssolutions.com/what-is-logistic-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/what-is-logistic-regression/</a></li><li id="d8fc" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><a class="ae kc" href="https://sites.google.com/site/dataclusteringalgorithms/k-means-clustering-algorithm" rel="noopener ugc nofollow" target="_blank">https://sites . Google . com/site/dataclusteringalgorithms/k-means-clustering-algorithms</a></li><li id="1948" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><a class="ae kc" href="https://www.sciencedirect.com/science/article/pii/S1319157814000573" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/pii/s 1319157814000573</a></li><li id="dea3" class="mg mh iq kf b kg mr kk ms ko mt ks mu kw mv la mn mo mp mq bi translated"><a class="ae kc" href="https://www.theaisorcery.com/post/linear-regression-for-beginners-a-mathematical-introduction" rel="noopener ugc nofollow" target="_blank">https://www . theaisorcery . com/post/linear-regression-for-初学者-数学入门</a></li></ul></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="2863" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">本人于2020年8月26日在我的网站(</em><a class="ae kc" href="https://www.theaisorcery.com/" rel="noopener ugc nofollow" target="_blank">【https://www.theaisorcery.com/】</a>)<em class="lb">发布。</em></p></div></div>    
</body>
</html>