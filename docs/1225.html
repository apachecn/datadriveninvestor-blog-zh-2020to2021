<html>
<head>
<title>Glue Crawler optimization/alternative | Athena use case</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">粘合爬虫优化/替代| Athena用例</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/glue-crawler-optimization-alternative-athena-use-case-15cc5bdd94d6?source=collection_archive---------2-----------------------#2020-03-09">https://medium.datadriveninvestor.com/glue-crawler-optimization-alternative-athena-use-case-15cc5bdd94d6?source=collection_archive---------2-----------------------#2020-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="eca6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇博客的目的是看看Glue Crawler的不同替代方案，以便使用AWS Athena查询来自S3的数据。在这里，我们将讨论一些可以避免爬虫的替代方案，但是这些可以根据用例进行调整。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/b1d59ad4a69e156aac3183c6909b5017.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*Unu8mdMDigHGXLhLPdeYyA.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Photo By <a class="ae kx" href="https://medium.com/@aliatakan" rel="noopener">Ali Atakan</a> on <a class="ae kx" href="https://medium.com/" rel="noopener">Medium</a></figcaption></figure><p id="9777" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“Amazon Athena是一项查询服务，用于查询驻留在AWS S3上的数据。Athena允许您运行标准SQL查询来查询来自S3的数据。”</p><p id="85a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“AWS <strong class="jp ir"> Glue crawler </strong>用于连接到数据存储，通过用于提取数据模式和其他统计数据的分类器的优先级列表进行处理，然后在元数据的帮助下填充<strong class="jp ir"> Glue </strong>数据目录。”</p><p id="deea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">需要爬虫来分析指定S3位置的数据，并生成/更新<strong class="jp ir">胶合数据目录</strong>，该目录基本上是实际数据的元存储(类似于<a class="ae kx" href="https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore" rel="noopener ugc nofollow" target="_blank"> Hive metastore </a>)。换句话说，它保存了关于数据的物理位置、模式、格式和分区的信息，这使得通过Athena查询实际数据或在Glue作业中加载数据成为可能。</p><p id="69b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，除了这些好处，它还有一个成本因素:<br/> * <strong class="jp ir"> $0.44每DPU小时，</strong>每秒计费，每次爬虫运行最少10分钟。<br/>有关定价的更多信息，请访问此<a class="ae kx" href="https://aws.amazon.com/glue/pricing/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="a2de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">示例:</strong> <br/>考虑一个大数据项目，当管道按要求调度或运行，大量数据驻留在S3时，大量使用爬虫。对于每一次流水线运行，都会形成新的数据分区，这些信息需要在Glue数据目录中更新，以便使用Athena查询这些分区上的数据。</p><p id="322f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们知道AWS为以上用例推荐的方式是运行<br/> <strong class="jp ir">胶水爬虫，还有哪些方式？</strong></p><p id="0e8f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Athena DDL语句，可以避开爬虫而创建外部表:<br/> <code class="fe ky kz la lb b">create external table Student(col1 string, col2 string) partitioned by (dept string) location 'ANY_S3_LOCATION';</code></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="5140" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">例如对于CSV文件:</strong></p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="f957" class="ln lo iq lb b gy lp lq l lr ls">CREATE EXTERNAL TABLE `student`(<br/> `code` string, <br/> `state` string)<br/>PARTITIONED BY ( <br/> `process` string)<br/>ROW FORMAT DELIMITED <br/> FIELDS TERMINATED BY ‘,’ <br/>STORED AS INPUTFORMAT <br/> ‘org.apache.hadoop.mapred.TextInputFormat’ <br/>OUTPUTFORMAT <br/> ‘org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat’<br/>LOCATION<br/> ‘s3://bucket/tablefolder/’<br/>TBLPROPERTIES ( <br/> ‘classification’=’csv’, <br/> ‘columnsOrdered’=’true’, <br/> ‘compressionType’=’none’, <br/> ‘delimiter’=’,’, <br/> ‘typeOfData’=’file’)</span></pre><p id="b962" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设当前它包含两个分区:</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="242a" class="ln lo iq lb b gy lp lq l lr ls">1) s3://bucket/student/process=1/</span><span id="0dcc" class="ln lo iq lb b gy lt lq l lr ls">2) s3://bucket/student/process=2/</span></pre><p id="5cd4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要添加新分区，只需运行alter table命令:</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="7376" class="ln lo iq lb b gy lp lq l lr ls">ALTER TABLE student ADD PARTITION (process = ‘3’) LOCATION ‘s3://bucket/student/process=3/’</span></pre><p id="2a70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更新分区信息的另一种方法是简单地加载分区，使用:</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="9ebc" class="ln lo iq lb b gy lp lq l lr ls">MSCK REPAIR TABLE student;</span></pre><p id="8440" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面给出的命令也可以在支持的编程环境中执行，例如python:</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="1c8e" class="ln lo iq lb b gy lp lq l lr ls"><em class="lu">athena = boto3.client(‘athena’)<br/>response = athena.start_query_execution(<br/> QueryString = “MSCK REPAIR TABLE </em>student<em class="lu">”,<br/> QueryExecutionContext={<br/> ‘Database’: ‘somedb’<br/> },<br/> ResultConfiguration = {<br/> ‘OutputLocation’: “s3://bucket/</em>student<em class="lu">/”<br/> })</em></span></pre><p id="91b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者，对于Alter Table操作，您可以获取分区列表并为新分区运行create _ partition/batch _ create _ partition。</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="cd04" class="ln lo iq lb b gy lp lq l lr ls">response = glue_client.get_partitions(<br/> DatabaseName=’hira-glue-database’,<br/> TableName=’glue_testa’<br/>)<br/>partition_list = response[‘Partitions’]</span></pre><p id="4c12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">创建新分区:</p><pre class="km kn ko kp gt lj lb lk ll aw lm bi"><span id="ab6f" class="ln lo iq lb b gy lp lq l lr ls">create_partition_response = client.batch_create_partition(<br/>        DatabaseName=l_database,<br/>        TableName=l_table,<br/>        PartitionInputList=each_input<br/>    )</span><span id="b2f8" class="ln lo iq lb b gy lt lq l lr ls">There is a limit of 100 partitions in a single api call Or for other queries like alter table.</span><span id="5d1d" class="ln lo iq lb b gy lt lq l lr ls">params = {</span><span id="9606" class="ln lo iq lb b gy lt lq l lr ls">‘region’: ‘eu-central-1’,</span><span id="b66a" class="ln lo iq lb b gy lt lq l lr ls">‘database’: ‘databasename’,</span><span id="f578" class="ln lo iq lb b gy lt lq l lr ls">‘bucket’: ‘your-bucket-name’,</span><span id="4059" class="ln lo iq lb b gy lt lq l lr ls">‘path’: ‘temp/athena/output’,</span><span id="59c0" class="ln lo iq lb b gy lt lq l lr ls">‘query’: ‘SELECT * FROM tablename LIMIT 100’</span><span id="a51e" class="ln lo iq lb b gy lt lq l lr ls">}</span><span id="4968" class="ln lo iq lb b gy lt lq l lr ls">response = client.start_query_execution(</span><span id="3f88" class="ln lo iq lb b gy lt lq l lr ls">QueryString=params[“query”],</span><span id="0b54" class="ln lo iq lb b gy lt lq l lr ls">QueryExecutionContext={ ‘Database’: params[‘database’] },</span><span id="06ec" class="ln lo iq lb b gy lt lq l lr ls">ResultConfiguration={ ‘OutputLocation’: ‘s3://’ + params[‘bucket’] + ‘/’ + params[‘path’] }</span><span id="9629" class="ln lo iq lb b gy lt lq l lr ls">)</span></pre><p id="2e27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注:</strong></p><p id="6258" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">" DDL查询或分区检测是免费的."</p><p id="4566" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“雅典娜并没有为这些请求单独向你收费，但S3得到的收费确实适用。此外，MSCK将扫描所有的分区。如果您正在同步分区，最好使用Alter Table命令。</p><p id="95de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“一旦你有很多分区，MSCK修复表就会变得非常慢。慢到你可能无法再运行它，因为你的查询被限制在30分钟内。”</p><p id="f1d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论:</strong></p><blockquote class="lv lw lx"><p id="3164" class="jn jo lu jp b jq jr js jt ju jv jw jx ly jz ka kb lz kd ke kf ma kh ki kj kk ij bi translated">简而言之，在我们有静态或不频繁变化的模式的用例中，为了成本效率，我们可以选择Athena DDL来代替Glue Crawlers，但是对于频繁变化的模式，Crawler在架构上将是更好的选择。</p></blockquote></div></div>    
</body>
</html>