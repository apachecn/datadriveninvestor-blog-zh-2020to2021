<html>
<head>
<title>Impacts of Alluxio Optimization on Kubernetes Deep Learning and Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Alluxio优化对Kubernetes深度学习和训练的影响</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/impacts-of-alluxio-optimization-on-kubernetes-deep-learning-and-training-b3d46da263c6?source=collection_archive---------10-----------------------#2020-09-08">https://medium.datadriveninvestor.com/impacts-of-alluxio-optimization-on-kubernetes-deep-learning-and-training-b3d46da263c6?source=collection_archive---------10-----------------------#2020-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/bebf50a33ad6767899c476ac886731c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*RPsDiz_TDfksQCye.png"/></div></figure><p id="e0a8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><em class="ks">车阳(阿里云高级技术专家)和古戎(南京大学副研究员，Alluxio核心开发者)</em></p><h1 id="c2a8" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">人工智能培训的新趋势:场外Kubernetes深度学习</h1><h2 id="39af" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">背景</h2><p id="9d31" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">近年来，<a class="ae mi" href="https://www.alibabacloud.com/product/machine-learning" rel="noopener ugc nofollow" target="_blank">深度学习等人工智能</a> (AI)技术发展迅速，正在广泛应用于各行各业。随着深度学习的广泛应用，越来越多的领域要求高效便捷的AI模型训练能力。此外，在云计算时代，容器和容器编排技术如<a class="ae mi" href="https://www.alibabacloud.com/product/kubernetes" rel="noopener ugc nofollow" target="_blank"> Docker和Kubernetes </a>在软件开发和O &amp; M期间应用服务的自动化部署方面取得了巨大的进步。Kubernetes社区对加速计算设备资源如<a class="ae mi" href="https://www.alibabacloud.com/product/gpu" rel="noopener ugc nofollow" target="_blank">图形处理单元</a>(GPU)的支持正在增加。由于云环境在计算成本和可扩展性方面的优势以及容器化在优化部署和敏捷迭代方面的优势，基于容器化的弹性基础设施和基于云的GPU实例的分布式深度学习模型训练现在是业界主要的AI模型生成趋势。</p><p id="3d9d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了确保资源扩展的灵活性，大多数云应用都采用将计算和存储分离的基本架构。<a class="ae mi" href="https://www.alibabacloud.com/product/oss" rel="noopener ugc nofollow" target="_blank">对象存储</a>常用于存储和管理海量训练数据，因为它大大降低了存储成本，提高了可扩展性。除了在云中存储数据，许多云平台用户出于安全合规性、数据所有权或遗留架构的考虑，还在私有数据中心存储大量数据。这类用户希望利用混合云构建AI训练平台，利用云平台的弹性计算能力进行AI商业模型训练。然而，本地存储和异地培训的结合加剧了计算-存储分离对远程数据访问性能的影响。基本的计算-存储分离架构使计算和存储资源的配置和扩展更加灵活。但是，就数据访问效率而言，如果在没有任何优化的情况下使用该体系结构，模型训练性能可能会由于有限的网络传输带宽而下降。</p><h2 id="0303" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">标准解决方案的数据访问挑战</h2><p id="f0fb" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">目前，场外深度学习模型训练的标准解决方案是手动准备数据。具体来说，将数据复制并分发到非易失性快速内存(NVMe)固态硬盘(SSD)等外部独立存储设备，或者分发到GlusterFS云并行文件系统(CPFS)等分布式高性能存储设备。当您手动或使用脚本准备数据时，可能会遇到以下问题:</p><p id="c129" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 1)更高的数据同步和管理成本:</strong>数据的持续更新需要定期从底层存储进行数据同步，这会产生更高的管理成本。<br/> <strong class="jw ir"> 2)更高的云存储成本:</strong>您需要为异地独立存储或分布式高性能存储支付额外费用。<br/> <strong class="jw ir"> 3)大规模扩展的复杂性更高:</strong>随着数据量的增长，很难将所有数据复制到异地独立存储。将所有数据复制到GlusterFS等CPFS也需要很长时间。</p><h1 id="77ad" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">基于容器和数据编排的模型训练体系结构</h1><p id="b718" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">为了解决上述问题，我们设计并实现了一个基于容器和数据编排的模型训练架构。图1显示了系统架构。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mj"><img src="../Images/10f54c5262c00ecdfdd66bb23c8aa15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TbQalAynK5GqkF3A.png"/></div></div></figure><h2 id="7cc3" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">系统架构中的核心组件</h2><p id="5eb6" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">1) <strong class="jw ir"> Kubernetes </strong>是一个流行的用于深度神经网络训练的容器集群管理平台。它通过容器和敏捷的按需扩展，支持灵活使用不同的机器学习框架。阿里云<a class="ae mi" href="https://www.alibabacloud.com/product/kubernetes" rel="noopener ugc nofollow" target="_blank">为Kubernetes </a> (ACK)提供的容器服务是阿里云提供的Kubernetes服务。使用它在阿里云的CPU、GPU、神经网络处理单元(NPUs)(汉光800芯片)和ECS裸机实例上运行Kubernetes工作负载。<br/> 2) <strong class="jw ir"> Kubeflow </strong>是一个开源的基于Kubernetes的云原生人工智能平台，用于开发、编排、部署和运行可扩展和可移植的机器学习工作负载。Kubeflow在两个TensorFlow框架中支持分布式训练:参数服务器和AllReduce。基于阿里云容器服务团队开发的<a class="ae mi" href="https://github.com/kubeflow/arena" rel="noopener ugc nofollow" target="_blank"> Arena </a>，使用这两个框架提交分布式训练工作负载。<br/> 3) <strong class="jw ir"> Alluxio </strong>是一个为混合云设计的开源数据编排和存储系统。在存储系统和计算框架之间添加了数据抽象层，以提供统一的挂载命名空间、分层缓存和各种数据访问接口。它支持在各种复杂环境中高效访问大规模数据，如私有云集群、混合云和公共云。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ms"><img src="../Images/98a93bfb3feac67d964a0904cfc28a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yGvwVayj6DOwn91t.png"/></div></div></figure><p id="a8ca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae mi" href="https://www.alluxio.io/" rel="noopener ugc nofollow" target="_blank"> Alluxio </a>起源于大数据时代，创建于加州大学伯克利分校Apache Spark的AMPLab。Alluxio系统旨在解决大数据处理管道中不同计算框架通过Hadoop分布式文件系统(HDFS)等磁盘文件系统交换数据时的分析性能瓶颈和I/O操作问题。Alluxio于2013年开源。经过七年的不断开发迭代，已经成为应用于大数据处理场景的成熟解决方案。随着近年来深度学习的兴起，Alluxio的分布式缓存技术正在成为行业内异地I/O性能问题的主流解决方案。此外，Alluxio推出了基于FUSE的POSIX文件系统接口，以便为异地人工智能模型培训提供高效的数据访问。</p><p id="82be" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了更好地将Alluxio整合到Kubernetes生态系统中，结合两者的优势，Alluxio团队和阿里云容器服务团队合作开发了<a class="ae mi" href="https://github.com/Alluxio/alluxio/tree/master/integration/kubernetes/helm-chart/alluxio" rel="noopener ugc nofollow" target="_blank"> Helm Chart解决方案</a>，极大地简化了Alluxio在Kubernetes中的部署和使用。</p><h1 id="e9ee" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">外部培训:Alluxio分布式缓存简介</h1><h2 id="57a5" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">深度学习的实验环境</h2><ul class=""><li id="91a7" class="mt mu iq jw b jx md kb me kf mv kj mw kn mx kr my mz na nb bi translated">这里，我们使用ResNet-50模型和ImageNet数据集。数据集大小为144 GB，其数据存储在TFRecords中，每个TFRecord大约130 MB。每个GPU的batch_size设置为256。</li><li id="5b35" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">模型训练用4个NVIDIA V100 GPUs(高配置GPU ECS . gn6v-c10g 1.20 xlarge)，总共32个GPU。</li><li id="2dc0" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">数据存储在阿里云<a class="ae mi" href="https://www.alibabacloud.com/product/oss" rel="noopener ugc nofollow" target="_blank">对象存储服务</a> (OSS)中。模型训练程序通过Alluxio读取数据，并将读取的数据自动缓存到Alluxio。内存配置在Alluxio缓存层。每台机器提供40 GB的内存用于存储，总的分布式缓存卷是160 GB。不使用预加载策略。</li></ul><div class="nh ni gp gr nj nk"><a href="https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">人工智能、深度学习和医疗实践|数据驱动的投资者</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">人工智能和深度神经学习的效用看起来可能是合法和有前途的，特别是…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny js nk"/></div></div></a></div><h2 id="803b" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">性能瓶颈</h2><p id="154d" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">根据我们的性能评估，GPU从英伟达P100升级到英伟达V100后，单个GPU的训练速度提高了300%以上。然而，计算性能的显著提高给数据存储和访问带来了更大的压力。这也对Alluxio的I/O性能提出了新的挑战。</p><p id="4679" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">下图比较了合成数据缓存和Alluxio缓存的性能。横轴表示GPU的数量，纵轴表示每秒处理的图像数量。合成数据是指由训练程序生成和读取的没有I/O开销的数据。这指定了模型训练性能的上限。Alluxio缓存是指训练程序从Alluxio系统中读取的数据。当GPU的数量为1或2时，合成数据和Alluxio缓存之间的性能差异是可以容忍的。但是，当GPU的数量增加到4个时，它们的性能差异就很明显了。Alluxio每秒处理的图像数量从4981幅下降到3762幅。当GPU数量为8时，Alluxio上模型训练的性能不到合成数据的30%。根据系统监测结果，系统的计算、内存和网络性能远远低于其极限，表明Alluxio无法高效地支持在单台主机上使用8个NVIDIA V100 GPUs进行训练。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0731531647cc6b416d8c78cc3ee266de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/0*0flQXPe73ZL91ts_.png"/></div></figure><p id="7dd8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了深入理解影响性能的因素以及为什么需要优化，我们需要分析一下Kubernetes中支持FUSE的Alluxio技术栈，如下图所示。</p><figure class="mk ml mm mn gt jr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8fb06b95adb8000ef086aaf0f080504d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*CzsFhvTEly_UJMWj.png"/></div></figure><h2 id="7a9f" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">原因分析</h2><p id="0a2b" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">通过对技术堆栈和Alluxio内核的深入分析，我们将性能影响的原因总结如下:</p><p id="e3a6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">1) Alluxio在文件操作中引入了多个远程过程调用(RPC ),并在训练场景中产生了性能开销。</p><p id="fb11" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Alluxio不仅仅是一个简单的缓存服务。它是一个分布式虚拟文件系统，提供全面的元数据管理、块数据管理、底层文件系统(UFS)管理和运行状况检查机制。它的元数据管理机制比很多ufs更强大。这些都是Alluxio的优势和特点，也构成了分布式系统产生的开销。例如，默认情况下，Alluxio客户端用于读取文件。即使文件数据已经缓存在本地Alluxio worker节点中，客户端也会对主节点进行多次RPC，以获取文件元数据并确保数据一致性。在传统的大数据场景中，完成整个读取操作的开销并不显著。然而，深度学习场景下的高吞吐量和低延迟要求很难满足。</p><p id="9de0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 2) Alluxio的数据缓存和驱逐策略频繁触发节点上的缓存抖动。</strong></p><p id="4494" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在深度学习场景中，冷热数据没有明确区分。因此，每个Alluxio worker节点都需要读取所有数据。但是，默认情况下，Alluxio首先在本地读取数据。即使数据存储在Alluxio集群中，Alluxio也会从其他缓存节点中提取数据，并保留一个本地副本。在我们的场景中，这个特性会导致额外的开销:</p><ul class=""><li id="afb6" class="mt mu iq jw b jx jy kb kc kf ob kj oc kn od kr my mz na nb bi translated">异步数据缓存的额外开销。</li><li id="0aa4" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">由于本地空间不足，触发了自动数据驱逐的开销。</li></ul><p id="3352" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">当一个节点的缓存快满时，性能开销是巨大的。</p><p id="ff9f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 3)基于FUSE的文件系统易于开发、部署和使用，但默认性能并不理想，原因如下:</strong></p><ul class=""><li id="5223" class="mt mu iq jw b jx jy kb kc kf ob kj oc kn od kr my mz na nb bi translated">熔丝读取操作效率不高。每个读取操作最多只能读取128 KB的数据，因此必须调用1000次读取操作才能读取128 MB的文件。</li><li id="9536" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">FUSE读操作是非阻塞行为，由libfuse非阻塞线程池处理。当并发请求的数量大于<code class="fe oe of og oh b">max_idle_threads</code>指定的数量时，会频繁触发线程创建和删除操作，从而影响读取性能。在FUSE中，max_idle_threads的默认值是10。</li><li id="63e3" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">频繁访问元数据会增加系统的压力，因为FUSE内核模块连接应用程序和Alluxio文件系统，并在每次读取文件或目录的inode和dentry时在Alluxio系统上运行。</li></ul><p id="60f4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">4)Alluxio与FUSE (AlluxioFUSE)的集成需要针对深度学习常见的多线程、高并发场景进行优化和深度定制。</strong></p><ul class=""><li id="c12e" class="mt mu iq jw b jx jy kb kc kf ob kj oc kn od kr my mz na nb bi translated">Alluxio在FUSE中只支持<code class="fe oe of og oh b">direct_io</code>模式，不能使用<code class="fe oe of og oh b">kernel_cache</code>模式通过页面缓存进一步提高I/O效率。这是因为Alluxio要求每个线程在多线程场景中使用自己的文件输入句柄(<code class="fe oe of og oh b">FileInputStream</code>)。但是，如果启用了页面缓存，AlluxioFUSE可能会提前并发读取缓存，从而导致错误。</li><li id="2c2a" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">Alluxio客户端读取的数据在进入FUSE之前会被复制多次。由于AlluxioFUSE使用的第三方Java库的API限制，数据被复制。</li><li id="a563" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">AlluxioFuse实现中使用的第三方库JNRFuse只能适应早期版本的FUSE，在高并发场景下会产生高性能负担。</li></ul><p id="7adf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 5) Kubernetes影响Alluxio的线程池。</strong></p><p id="da2b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Alluxio是基于Java 1.8实现的，其中一些线程池是基于<code class="fe oe of og oh b">Runtime.getRuntime().availableProcessors()</code>计算的。但在Kubernetes环境中，cpu_shares的值默认为2，Java虚拟机(JVM)中的cpu核数根据<code class="fe oe of og oh b">cpu_shares()/1024</code>计算为1。这会影响容器中Java进程的并发性。</p><h1 id="341d" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">场外模型训练的性能优化</h1><p id="b6f0" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">在分析了上述性能问题和因素后，我们设计了一系列性能优化策略来提高场外模型训练的性能。首先，很难同时确保数据访问的速度、质量和成本效益。相反，我们只关注在模型训练中加速对只读数据集的数据访问。优化的基本思想是以一些适应性为代价来确保高性能和数据一致性，例如同时读写和连续数据更新。</p><p id="c0ed" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">基于此，我们设计了符合以下核心原则的特定性能优化策略:</p><ul class=""><li id="311a" class="mt mu iq jw b jx jy kb kc kf ob kj oc kn od kr my mz na nb bi translated">找到资源限制，包括容器中线程池和JVM的配置。</li><li id="6e04" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">使用多层缓存，包括FUSE层的缓存和Alluxio元数据缓存。</li><li id="add1" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">避免额外的开销并减少不必要的跟踪。例如，避免不必要的元数据交互以及导致上下文切换的垃圾收集(GC)线程和编译器进程，并简化一些Alluxio操作。</li></ul><p id="a066" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们将从每层组件优化的角度来描述这些优化策略。</p><h1 id="9dfa" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">保险丝优化</h1><h2 id="3b7a" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">升级Linux内核版本</h2><p id="3902" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">fuse在两层实现:libfuse运行在用户态，FUSE内核运行在内核态。我们做了很多优化来融合最新版本的Linux内核。我们发现Linux内核4.19的读取性能比Linux内核3.10提升了20%。</p><h2 id="9383" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">优化保险丝参数</h2><p id="d357" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated"><strong class="jw ir"> 1)延长融合元数据的有效期。</strong></p><p id="47e6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Linux中的每个文件包含两种类型的元数据:<code class="fe oe of og oh b">struct dentry</code>和<code class="fe oe of og oh b">struct inode</code>。它们是内核中文件的基础。这两个结构必须在对文件进行所有操作之前获得。因此，每次获取文件或目录的inode和dentry时，FUSE内核都会从libfuse和Alluxio文件系统执行一次完整的操作，这在高延迟、高并发数据访问的场景下对Alluxio主节点造成了很大的压力。<code class="fe oe of og oh b">–o entry_timeout=T –o attr_timeout=T</code>可以进行优化设置。</p><p id="6831" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> 2)配置</strong> <code class="fe oe of og oh b"><strong class="jw ir">max_idle_threads</strong></code> <strong class="jw ir">避免频繁的线程创建和删除操作，产生CPU开销。</strong></p><p id="fdb4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这样做是因为FUSE开始在多线程场景中运行一个线程。当有两个以上的可用请求时，FUSE会自动生成额外的线程。每个线程一次处理一个请求。处理完一个请求后，每个线程检查线程的数量是否超过了<code class="fe oe of og oh b">max_idle_threads</code>指定的数量(默认为10)。如果是这样，多余的线程将被回收。该字段与用户进程中生成的活动I/O线程的数量相关，可以设置为读取线程的数量。但是max_idle_threads只支持libfuse3，而AlluxioFUSE只支持libfuse2。所以我们修改了libfuse2的代码来支持<code class="fe oe of og oh b">max_idle_threads</code>的配置。</p><h1 id="89af" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">Alluxio优化</h1><p id="9960" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">Alluxio和FUSE通过<code class="fe oe of og oh b">AlluxioFuse</code>流程进行整合。该进程调用嵌入式Alluxio客户端，以便在运行时与正在运行的Alluxio主节点和工作节点进行交互。针对深度学习场景，我们定制了<code class="fe oe of og oh b">AlluxioFuse</code>的Alluxio属性来优化性能。</p><h2 id="af5b" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">避免由频繁的缓存驱逐导致的缓存抖动</h2><p id="23c2" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">在深度学习训练场景中，每个训练迭代都是一个完整数据集的迭代。任何节点都不可能有足够的空间来缓存数TB的数据集。Alluxio的默认缓存策略是为冷热数据明显区分的大数据处理场景(比如查询)而设计的。数据缓存存储在Alluxio客户端的本地节点中，以确保下次读取的最佳性能。这些配置如下:</p><p id="6673" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">1) <code class="fe oe of og oh b">alluxio.user.ufs.block.read.location.policy</code>:默认值为<code class="fe oe of og oh b">alluxio.client.block.policy.LocalFirstPolicy</code>。这意味着Alluxio会不断地将数据保存到Alluxio客户端所在的本地节点。在这种情况下，当缓存接近满时，节点上的缓存会不断出现抖动。这大大降低了吞吐量和延迟，并给主节点带来了很大压力。因此，<code class="fe oe of og oh b">location.policy</code>需要设置为<code class="fe oe of og oh b">alluxio.client.block.policy.LocalFirstAvoidEvictionPolicy</code>，并且必须设置<code class="fe oe of og oh b">alluxio.user.block.avoid.eviction.policy.reserved.size.bytes</code>参数。此参数指定要保留的数据卷，以防止本地缓存被收回。通常，该参数的值必须大于节点缓存x的上限(100% -节点逐出的最大百分比)。<br/> 2) <code class="fe oe of og oh b">alluxio.user.file.passive.cache.enabled</code>:指定是否在Alluxio的本地节点上缓存额外的数据副本。默认情况下，该属性处于启用状态。因此，当Alluxio客户端请求数据时，客户端所在的节点会缓存来自其他工作节点的数据。您可以将该属性设置为false，以避免不必要的本地缓存。<br/> 3) <code class="fe oe of og oh b">alluxio.user.file.readtype.default</code>:默认值为<code class="fe oe of og oh b">CACHE_PROMOTE</code>。这种配置有两个潜在的问题:首先，数据可能会在同一节点上的不同缓存层之间移动。其次，对数据块的大多数操作都需要锁。但是，很多锁操作在Alluxio的源代码中是相当重要的。大量的加锁和解锁操作会在高并发时产生大量的开销，即使没有数据迁移。因此，可以将该参数设置为CACHE，而不是默认值<code class="fe oe of og oh b">CACHE_PROMOTE</code>，以避免moveBlock操作造成的锁定开销。</p><h2 id="9052" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">缓存元数据和节点列表</h2><p id="3466" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">在深度学习训练场景中，在每个训练任务开始之前，会列出所有训练数据文件，并读取元数据。运行训练任务的过程进一步读取训练数据文件。通过Alluxio访问文件时，默认情况下会完成以下操作:从主节点获取文件元数据和块元数据，从工作节点获取块元数据的位置，然后从获取的位置读取块数据。完整的操作包括多个RPC的开销，并导致显著的文件访问延迟。如果将数据文件的块信息缓存到客户端内存中，文件访问性能将会得到显著提高。</p><p id="42ea" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">1)当您将<code class="fe oe of og oh b">alluxio.user.metadata.cache.enabled</code>设置为true时，将在Alluxio客户端上启用文件和目录元数据缓存，因此您无需再次通过RPC访问元数据。根据分配给AlluxioFUSE的堆的大小，设置<code class="fe oe of og oh b">alluxio.user.metadata.cache.max.size</code>来指定缓存文件和目录的最大元数据量。还要设置<code class="fe oe of og oh b">alluxio.user.metadata.cache.expiration.time</code>来调整元数据缓存的有效期。此外，当您选择worker节点来读取数据时，Alluxio主节点会不断查询所有worker节点的状态，这在高并发情况下会产生额外的开销。<br/> 2)将<code class="fe oe of og oh b">alluxio.user.worker.list.refresh.interval</code>设置为2分钟或更长。<br/> 3)读取文件时最后一次访问时间不断更新，这给高并发场景下的Alluxio主节点带来很大压力。因此，我们在Alluxio代码中添加了一个开关，禁止更新上次访问时间。</p><h2 id="e76b" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">充分利用数据局部性</h2><p id="7c76" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">数据局部性表示数据计算在数据所在的节点上进行，以避免数据通过网络传输。在分布式并行计算环境中，数据局部性非常重要。容器环境支持两种短路读写模式:<a class="ae mi" href="https://en.wikipedia.org/wiki/Unix_domain_socket" rel="noopener ugc nofollow" target="_blank"> UNIX socket </a>和直接文件访问。</p><ul class=""><li id="bc48" class="mt mu iq jw b jx jy kb kc kf ob kj oc kn od kr my mz na nb bi translated">UNIX套接字模式提供了良好的隔离，因此Alluxio客户端和Alluxio工作节点不需要在同一网络、UNIX分时(UTS)系统和挂载命名空间上运行。但其性能低于直接文件访问模式，可能会出现Netty 的<a class="ae mi" href="https://github.com/Alluxio/alluxio/issues/9345" rel="noopener ugc nofollow" target="_blank"> OutOfDirectMemoryError。</a></li><li id="1cf7" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated">对于直接文件访问，您必须确保在同一台计算机上运行的Alluxio worker节点和AlluxioFUSE的主机名和IP地址相同，并且Alluxio客户端和worker节点共享相同的缓存目录。这种模式性能更好，也更稳定。但是，此模式要求Alluxio客户端和Alluxio工作节点共享同一个网络、UTS系统和装载命名空间。</li></ul><p id="b4f6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">目前，我们倾向于后一种解决方案。</p><h1 id="1568" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">Java和Kubernetes优化</h1><h2 id="898c" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">设定<code class="fe oe of og oh b">ActiveProcessorCount</code></h2><p id="99e5" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">该参数由<code class="fe oe of og oh b">Runtime.getRuntime().availableProcessors()</code>控制。如果使用Kubernetes部署一个容器，而没有指定CPU请求的数量，那么Java进程将从容器中的proc文件系统(procfs)中读取两个CPU份额。在这种情况下，<code class="fe oe of og oh b">availableProcessors()</code>根据<code class="fe oe of og oh b">cpu_shares()/1024</code>计算为1。这限制了容器中Alluxio的并发线程数量。Alluxio客户端是一个I/O密集型应用程序。因此，设置<code class="fe oe of og oh b">-XX:ActiveProcessorCount</code>来指定处理器的数量。基本原则是尽可能将<code class="fe oe of og oh b">ActiveProcessorCount</code>设置为较大的值。</p><h2 id="f97b" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">调整GC和JIT线程</h2><p id="c0c5" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated"><code class="fe oe of og oh b">-XX:ActiveProcessorCount</code>指定JVM的GC和实时(JIT)编译线程的默认数量。但是，您可以使用<code class="fe oe of og oh b">-XX:ParallelGCThreads</code>、<code class="fe oe of og oh b">-XX:ConcGCThreads</code>和<code class="fe oe of og oh b">-XX:CICompilerCount</code>参数将其设置为较小的值。这避免了这些进程的频繁抢占和故障转移以及由此导致的性能下降。</p><h2 id="3f6d" class="lr ku iq bd kv ls lt dn kz lu lv dp ld kf lw lx lh kj ly lz ll kn ma mb lp mc bi translated">优化结果</h2><p id="f0e9" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">Alluxio优化后，单主机8个GPU的ResNet-50训练性能提升236.1%，解决了可扩展性问题。培训过程可能会扩展到四台主机上的八个GPU。此外，与合成数据的性能相比，性能损失仅为3.29%(每秒31068.8幅图像对每秒30044.8幅图像)。在合成数据场景下，使用四台主机上的八个GPU完成训练需要63分钟，使用Alluxio需要65分钟。</p><h1 id="a707" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">摘要</h1><p id="fc5f" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">在本文中，我们总结了在高性能分布式深度学习模型训练场景中应用Alluxio的挑战以及我们在优化Alluxio方面的实践。我们还讨论了在高并发读取场景中提高AlluxioFUSE性能的多种方法。最后，我们基于我们的Alluxio优化实现了一个分布式模型训练解决方案，使用四台主机上的八个GPU验证了ResNet-50的性能。在这次测试中，我们的系统取得了良好的效果。</p><p id="f811" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Alluxio正在努力支持页面缓存，并确保在具有大量小文件的高吞吐量场景和高并发读取场景中的FUSE稳定性。阿里云<a class="ae mi" href="https://www.alibabacloud.com/product/kubernetes" rel="noopener ugc nofollow" target="_blank">容器服务</a>团队将继续与Alluxio开源社区和南京大学的教授(如戴海鹏和古戎)合作，以进一步改进。我们相信，通过行业、开源社区和学术界的共同努力和创新思想，我们可以逐步降低计算-存储分离场景中深度学习训练的数据访问成本和复杂性，进一步促进场外人工智能模型训练。</p><h1 id="c443" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">感谢</h1><p id="e771" class="pw-post-body-paragraph ju jv iq jw b jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn mh kp kq kr ij bi translated">Alluxio团队的范斌、邱璐、Calvin Jia和Chang Cheng为该解决方案的设计和优化提供了巨大帮助。他们通过Alluxio自身的能力对元数据缓存系统进行了重大改进，使Alluxio在AI场景中的应用成为可能。谢谢大家。</p><h1 id="72e0" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">关于作者</h1><ul class=""><li id="f1ed" class="mt mu iq jw b jx md kb me kf mv kj mw kn mx kr my mz na nb bi translated"><strong class="jw ir">车阳，</strong>阿里云高级技术专家，专门从事<a class="ae mi" href="https://www.alibabacloud.com/product/kubernetes" rel="noopener ugc nofollow" target="_blank"> Kubernetes和容器相关产品</a>的开发。他专注于如何使用云原生技术来构建机器学习平台和系统，并且是Kubernetes 中<a class="ae mi" href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" rel="noopener ugc nofollow" target="_blank"> GPU共享调度程序扩展器的主要作者和维护者。</a></li><li id="b2f1" class="mt mu iq jw b jx nc kb nd kf ne kj nf kn ng kr my mz na nb bi translated"><strong class="jw ir">古戎，</strong>南京大学副研究员，Alluxio的核心开发者，研究大数据处理。2016年获得南京大学博士学位。他曾是微软亚洲研究院(MSRA)、英特尔和百度大数据系统的研发实习生。</li></ul><h1 id="bb88" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">原始来源:</h1><div class="nh ni gp gr nj nk"><a href="https://www.alibabacloud.com/blog/impacts-of-alluxio-optimization-on-kubernetes-deep-learning-and-training_596534" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">Alluxio优化对Kubernetes深度学习和训练的影响</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">阿里巴巴集装箱服务2020年8月25日90车阳(阿里云高级技术专家)和古戎…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">www.alibabacloud.com</p></div></div><div class="nt l"><div class="oi l nv nw nx nt ny js nk"/></div></div></a></div><p id="62c4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">访问专家视图— </strong> <a class="ae mi" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>