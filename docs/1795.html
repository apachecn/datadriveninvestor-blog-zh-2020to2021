<html>
<head>
<title>Recurrent Neural Networks in Deep Learning — Part2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的递归神经网络——第二部分</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/recurrent-neural-networks-in-deep-learning-part2-ce9fe1770a31?source=collection_archive---------0-----------------------#2020-04-01">https://medium.datadriveninvestor.com/recurrent-neural-networks-in-deep-learning-part2-ce9fe1770a31?source=collection_archive---------0-----------------------#2020-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/33dd17daca0664061e449ff32fbf10c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-2dHCcasYihmwxtnd_pWw.png"/></div></div></figure><p id="3474" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">作者普里亚尔·瓦尔皮塔</strong></p><p id="33f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">阅读这篇文章将有助于你理解人工神经网络(ANN)的术语，ANN的缺点，RNN(递归神经网络)的架构观点，使用RNN优于ANN的优点，它们如何工作，以及如何构建一个系列模型和解决各种用例。有意地，我保持这篇文章基于理论和他们的解释，主要集中在递归神经网络(RNN)。</p><p id="b783" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这篇博文由两部分组成，这是第二部分。<a class="ae kw" href="https://medium.com/datadriveninvestor/recurrent-neural-networks-in-deep-learning-part-1-df3c8c9198ba" rel="noopener">第一部分介绍了RNN及其背后的理论。</a>本节将讨论RNN的类型和一些实际用法。</p><p id="c405" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt" rel="noopener ugc nofollow" target="_blank">注:本文基于吴恩达博士在Coursera </a>的演讲</p><h1 id="b9f2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">不同类型的递归神经网络(RNN) </strong></h1><p id="e687" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">简而言之，RNN有各种形式。它们如下:</p><ul class=""><li id="ffb6" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">一对一RNN</li><li id="a14b" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">一对多RNN</li><li id="b0fe" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">多对一RNN</li><li id="a9b4" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">多对多RNN</li></ul><p id="e632" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们定义了递归神经网络，也称为RNNs，是一类允许使用先前的输出作为输入同时具有隐藏状态的神经网络。它们主要用于自然语言处理和语音识别领域。让我们来看看各种类型:</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4d08b91f19783922518a6ca4519218c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*EQIzoFoF-8FcjYWs"/></div></figure><h2 id="1672" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">一对一的RNN </strong></h2><p id="14ed" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">一对一RNN(Tx= Ty=1)是神经网络最基本、最传统的形式，如上图所示，对单个输入给出单个输出。</p><h2 id="e1e4" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">一对多</strong></h2><p id="92ac" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">一对多(Tx=1，Ty&gt;1)是一种RNN架构，在单个输入提供多个输出的情况下实施。音乐生成将是其应用的一个参考例子。RNN模型用于音乐生成模型，从单个音符(单个输入)产生一段音乐(多个输出)。</p><h2 id="a94c" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">多对一</strong></h2><p id="274c" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">RNN (Tx&gt;1，Ty=1)的多对一架构通常被用作情感分析模型的常见示例。顾名思义，当需要多个输入来提供一个输出时，就使用这种类型的模型。</p><div class="nf ng gp gr nh ni"><a href="https://www.datadriveninvestor.com/2020/02/12/has-general-ai-exceeded-the-intellectual-capacity-of-humans/" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">AI将军是否已经超过了人类的智力容量？数据驱动的投资者</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">不仅在游戏中，而且在劳动力市场上，机器都比人类聪明。在今天的许多领域，使用…</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw jw ni"/></div></div></a></div><p id="836c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">比如:分析Twitter情绪的模型。该模型中的文本输入(作为多个输入的单词)给人一种固定的感觉(单一输出)。一个例子是电影分级模型，它使用评论文本作为输入来对电影进行分级，范围从1到5。</p><h2 id="e523" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">多对多</strong></h2><p id="28ea" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">多对多RNN体系结构(Tx&gt;1，Ty&gt;1)采用多个输入并提供多个输出，但是多对多模型可以有两种类型，如上所示:</p><ol class=""><li id="1867" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv nx mg mh mi bi translated"><strong class="ka ir"> Tx = Ty: </strong></li></ol><p id="318b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是它适用于输入和输出层具有相同厚度的情况的位置。它也可以解释为任何输出信息都可以在命名实体识别中找到。</p><p id="ae15" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2。Tx！= Ty : </strong></p><p id="b51e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">多对多架构也可以在输入和层大小不同的模型中表示，机器翻译展示了这种类型的RNN架构的最常见应用。结果，由于在上下文中操作的非等价的多对多RNN架构，机器翻译模型能够返回比输入字符串更多或更少的单词。</p><p id="d5e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">以下部分解释了不同类型的RNN的用法。</p><h1 id="801c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">RNN的用法</h1><h2 id="6bc4" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">语言建模和序列生成</strong></h2><h2 id="cfc1" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">什么是语言建模？</strong></h2><p id="37a2" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">语言建模是自然语言处理的基本活动之一。因此，您可以学习如何使用RNN构建语言模型。</p><p id="9c21" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">“语音识别”方法是语言模型中最常见的用例之一。假设您正试图创建一个语音识别程序。想象一下，就像这样，在一次演讲中，有一个词听起来很混乱，这就是我的意思</p><p id="3f6a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例句:“苹果和<strong class="ka ir">搭配</strong>沙拉。”</p><p id="fa44" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">"苹果和梨沙拉."</p><p id="66fe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，如果我们说单词“pair”或“pear”，语音识别系统不能清楚地理解但我是一个认为“梨”这个词会有意义的人，但问题是如何训练机器准确地做到这一点。</p><p id="67b3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">现在，一个成功的语音识别系统通过测量两个句子的可能性并选择更有可能出现的一个，使用语言模型来捕获正确的句子(在示例中为第二个)。</strong></p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/23d8b6cd105340e8dbcfca5f74f0f658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/0*f2msCuVbpwwZfxck"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/dbe14f5ffea83d0833802c1132d9a9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DlCwJ88QRNs_tYVn"/></div></div></figure><p id="de0b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以现在的问题是如何使用RNN创建这种语言模型。这些是已经采取的具体措施</p><ol class=""><li id="6a67" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv nx mg mh mi bi translated">我们需要的第一件事是一个训练集，它是一个给定语言的广泛的文本语料库(集)(我们以英语为例)。</li><li id="49a4" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv nx mg mh mi bi translated">然后，如果您的训练集中有一个句子，首先将该句子标记化，并将其表示为一个热编码向量(使用字典/词汇索引值)。仅仅作为一个例子，一个词汇可能是英语中的10，000个单词。</li><li id="ca1c" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv nx mg mh mi bi translated">使用一个额外的标记作为&lt; EOS &gt;在你的句子结束处添加一个额外的标记。</li><li id="f4d0" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv nx mg mh mi bi translated">如果你想把标点符号符号化或者丢弃，你会看到这一点。</li><li id="6ee1" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv nx mg mh mi bi translated">如果这样的单词(标记)没有出现在您的训练数据中正在使用的词汇表中，那么用一个特殊的标记来表示它们，如代表未知的&lt; UNK &gt;。</li><li id="986f" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv nx mg mh mi bi translated">然后用RNN来模拟不同序列的机会。</li></ol><h2 id="0568" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">语音识别的递归神经网络架构</strong></h2><p id="b49c" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">如果你仔细观察，使用Softmax函数，第一个RNN模块的第一步是尝试预测所有字典术语的可能性。在下一步中，它试图预测第二个单词“normal ”,前提是第一个单词“cats ”(条件概率)，并继续下去，直到到达句子标记的末尾。</p><p id="f25c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">所以，RNN正在学习一次从左到右预测一个术语。</strong></p><p id="c06b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，为了训练该神经网络，我们将定义成本函数，即在指定的时间阶段“t ”,如果真实单词是“y ”,而期望单词是“yhat”。</p><p id="123b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你在一个真正大的训练集上训练这只RNN，它将能够很好地预测下一个学期。RNN将能够预测“睡眠”十有八九是因为提供了过去的话作为“猫平均15小时”现在你已经在这个过程中训练了一个强大的语言模型。因此，当你得到一个逻辑和数学来源相同的新短语时，你可以预测句子中正确的术语。</p><p id="b76f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你在一个真正大的训练集上训练这只RNN，它将能够很好地预测下一个学期。RNN将能够预测“睡眠”十有八九是因为提供了过去的话作为“猫平均15小时”现在你已经在这个过程中训练了一个强大的语言模型。因此，当你得到一个逻辑和数学来源相同的新短语时，你可以预测句子中正确的术语。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/3aea2581003d0bc3a976cbf8ec3918e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BcgIuFizUVKiUFSZ"/></div></div></figure><p id="a8ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们之前开始的例子，正确的句子可以预测为-</p><p id="99cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">“苹果和梨沙拉”。</p><p id="a6ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您将在训练序列模型后对分布进行采样，以查看它实际学到了什么。</p><h2 id="7b9b" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">从训练好的语言模型中抽取新的序列</strong></h2><p id="ce10" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">当你有了一个学习语言的模型。试试看它训练得怎么样。一种简单的测试方法是从分布中取样，看看它是否有意义。</p><p id="e019" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">方法是通过使用用于训练的RNN，并使用softmax函数来估计字典中所有术语的可能性，然后随机选择一个单词(numpy.random.choice)。然后将该单词作为输入移动到第二时间步(一键编码的say ),并让RNN预测具有来自字典的新单词的条件可能性，该字典提供了来自上一时间步的单词。这一直持续到产生&lt; EOS &gt;令牌(或采样)为止。如果你的词汇中不包含这个标记，那么你可以抽取20个单词(通常是一个句子),一旦达到时间步数就停止</p><p id="3f2e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有时在这个特殊的类型过程中，它会生成一个未知的令牌<unk>。但是如果你确定它永远不会产生。然后你可以删除它，让其余的词汇再次测试。</unk></p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/6a7b50dcb7a6a7cc35eb1c799d346186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*55wi5GwhGrKlM48L"/></div></div></figure><p id="cdc5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是你的RNN语言模型如何产生一个随机选择的句子。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b2f27754f66d3b68ebcb0e56ece72bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/0*SMUFwpdOA3wasA6h"/></div></figure><p id="68de" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你也可以建立一个RNN字符级，其中你的词汇将是字母和一些特殊字符，如标点符号，数字0-9等。类似的结构也会随之而来。</p><p id="7283" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RNN角色等级的一个优点是你永远不会产生一个令牌&lt; UNK&gt;。但最大的缺点是，你要处理更长的序列和更长的时间，而且它们的计算训练成本更高。可能是你的词汇量非常有限，字符级的语言模型可能有用。</p><h2 id="d3ce" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">递归神经网络问题</strong></h2><p id="5c44" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">RNN的一个问题是它会遇到梯度消失的问题。让我们看看这是什么意思。有两句话是——</p><ul class=""><li id="9c2b" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">我阿姨开的这家餐馆提供正宗的中国菜。</li><li id="0188" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">这些由我的好朋友开的餐馆提供正宗的印度食物。</li></ul><p id="0b7e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在上面的例子中，你可以看到“餐馆”主语是如何影响“服务”动词的。这在英语中也被称为主语-动词一致，这是构造句子的主要原则之一。这样的规则建立了术语之间的长期依赖性(即，短语开头的单词影响短语结尾的单词)。</p><p id="4ee8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">基本的RNN架构不包含这些长期依赖关系。</strong></p><p id="0ebb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这类似于“消失梯度”问题，其中如果神经网络非常深(它有许多层)，那么当向后传播误差时，在道具开始时(最后几层)测量的梯度(导数)对最初几层的影响小得多。所以，我们说在某种程度上，梯度消失了，因为网络在这个过程中非常深入。因此，反过来，简单的RNN不能捕捉单词之间的长期依赖关系，因此它不能检测主语和动词相距甚远的主语-动词关系。</p><p id="e848" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">因此，基本的RNN建筑具有非常明显的地方影响</strong></p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/ad3afadaeb789f6e443a9b8af43caae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jV0oPhvelyf3CXc5"/></div></div></figure><p id="e5ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，这个“消失梯度”的问题是用一种特殊的RNNs来解决的，这种RNNs被称为门控循环单元，或者更普遍地被称为GRU。</p><h2 id="6b91" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">门控循环单元(GRU) </strong></h2><p id="44c3" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">它是RNN隐层的一个改进，因此它可以识别序列中的长程相关性和消失梯度问题。</p><p id="9729" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">RNN隐藏层单元</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/ad762a00e52e153e773a1b414abd3f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QLrcWCqmIJvMofEl"/></div></div></figure><p id="e68c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在让我们根据GRU改变这个设计，这样我们就可以捕捉到长期的依赖关系。当我们从左到右阅读句子时，我们将为“memory cell”添加另一个“c”变量，以记住依存关系，例如与主语动词的一致。在我们的例子中,“餐馆”是单数还是复数，以及这如何影响与之相连的动词。</p><p id="1e11" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">GRU的主要部分是根据决定是否检查内存单元的“更新门”用c更新“c”内存单元。在我们的例子中，“c”将被设置为1或0，这取决于它是单数还是复数，然后GRU将记住这个“Ct”值，直到在我们的例子中是动词“serve ”,并将确定它的形式。</p><p id="1d20" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在一些GRU架构中，我们经常使用另一个名为“相关性门”的门来更好地捕捉长期依赖关系(随着时间的推移而发展)。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/1569d59f88298266909a4b78ef7bd3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lQ9F1KFaAJR0Br1b"/></div></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/e585fa58278f3f91103586c63b9805e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K8t7IlBBJnKArSnV"/></div></div></figure><p id="b013" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这些建筑被用于我们现在要看的大门，也就是LSTM。</p><p id="ec03" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">长短期记忆(LSTM) </strong></p><p id="4fca" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">它捕捉长期依赖关系，就像序列中的GRUs一样，但比GRUs更有效。这是GRUs的简化版。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/bfb9c57a9b9482b936690c097f2b9c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D_x-cJXDM7Yzc8TY"/></div></div></figure><p id="fc9a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里“at”不等于“ct ”,即激活不等于存储单元。我们确实使用了两个门，一个是“更新”，一个是“忘记”，而不是我们在GRU中使用的减1更新门。</p><p id="2ce4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">GRU比lstm简单，因为它们对于较大的模型在计算上运行得更快，但是lstm更有效，因为它们是GRU的一般版本，并且有更多的门来控制保留什么和忘记什么。</p><p id="19cf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对两者的理解当然给了我们选择想要解决什么问题的权利。</p><h2 id="0189" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">双向递归神经网络(RNN) </strong></h2><p id="6e34" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">如果你还记得，当我们在讨论RNN的时候，我们注意到它从左到右开始读令牌。因此，它不会捕获下面的任何依赖项。</p><p id="5678" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的实体识别问题中，我们试图找出一个术语是否是一个人的名字？</p><p id="de87" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="oh">他说，“泰迪熊在打折！”。</em></p><p id="3c3c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">他说，“泰迪·罗斯福是一位总统！”。</p><p id="140b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在第一句话中“泰迪”是一个玩具，在第二句话中是一个名字的一部分。因此，当我们有一个在两个方向(从左到右和从右到左)检查术语的网络时，这种依赖性将不会被捕获。</p><p id="223d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，我们将描述一种称为“双向RNNs”的架构，它允许您在一个时间点从系列的早期和后期获取信息。现在，它也可能是RNN单元上的GRU或LSTM单元，但其想法是如何检查两边的序列，然后做出猜测(在我们的情况下，它是否是一个人)。</p><p id="0016" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以，这是一个无环图。由于输入序列，它将计算“a-forward 1”，然后使用“a-forward2”直到结束，同时计算“a-backward4”，“a-backward3”直到开始。在计算完所有这些秘密层之后，你可以预测Yhat-1，Yhat-2，直到最后。</p><p id="2e04" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">双向RNN的一个缺点是，你首先需要整个数据序列来做一些预测。因此，我们不能在语音识别等领域使用它，因为这意味着人需要先说点什么，然后我们的算法将开始识别不真实的语音。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h2 id="ce0c" class="mt ky iq bd kz mu mv dn ld mw mx dp lh kj my mz ll kn na nb lp kr nc nd lt ne bi translated"><strong class="ak">用一个深层网络把这些都包起来</strong></h2><p id="679c" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">从一开始到现在，我们已经看到了RNN、格鲁什和伊斯特姆斯的基本块。现在，我们需要将这些东西堆积起来，建立一个深度网络或深度RNNs来学习非常复杂的功能。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="3025" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">非常感谢你阅读这篇文章。如果您有任何问题，请在这里提问或通过电子邮件(priyal@priyal.ai)或从我的<a class="ae kw" href="https://www.linkedin.com/in/priyalwalpita/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="op oq l"/></div></figure></div></div>    
</body>
</html>