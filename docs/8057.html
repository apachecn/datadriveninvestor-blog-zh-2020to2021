<html>
<head>
<title>Reinforcement Learning — An Interactive Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——一种交互式学习</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/reinforcement-learning-an-interactive-learning-b1fa29166fc8?source=collection_archive---------6-----------------------#2020-12-31">https://medium.datadriveninvestor.com/reinforcement-learning-an-interactive-learning-b1fa29166fc8?source=collection_archive---------6-----------------------#2020-12-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/29f5575974110efaba6ae458e30c2dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JXGk2Q_32LRogD3C.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Reinforcement Learning Methods and working style</strong></figcaption></figure><div class=""/><p id="9194" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">互动学习</strong></p><p id="b008" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习是机器学习的一部分，代理通过与环境交互来自主学习。RL不需要数据集。</p><p id="d795" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习的历史和MDP由于它的流行和更先进的研究今天被更详细地覆盖，它适用于主要的应用，如自动驾驶汽车，机器人，未知和艰苦的环境。</p><p id="6734" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文通过例子探讨了动态编程的MDP &amp; RL方法。这篇文章特别适合初学者和那些想知道RL是如何进入画面的人。由于代理人行为的不确定性，在线学习是热门和必要的。在上面的标题图中，我更详细地介绍了绿色矩形框主题。</p><p id="1d62" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL不像监督学习或模仿数据集，因为它在没有任何干预的情况下，独自在<strong class="kf jh">已知</strong>和<strong class="kf jh">未知</strong>环境中表现出色。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lb"><img src="../Images/7999d38e2e2b0de9ed255a0ad1e8ade2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WhsKKU10mCX9FucI.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lg"><img src="../Images/d1616ff04afdd276c3a1c546d934ed8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NDzaxyQxNF9gC7o2.png"/></div></div></figure><p id="489e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习是一种机器学习，它与环境交互来学习，这种行为的组合产生了最有利的结果。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/f057bf04451c6b4efcbf4f54d41fb07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*UdmHT_VnkpF68dfB.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi li"><img src="../Images/3cc9ee5be3c9488febfb9bfc817eff88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9TYoE3C3tGzwDXpJ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Picture describe RL appearance in ML</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/ab09b8c43960598ee31484083a13e324.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*JR0IlLLdjRlWXnJv.png"/></div></figure><p id="abe4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了对环境中的主体进行推理，RL引入了两个新概念，即:状态和动作。世界在特定时间停止的状态称为<strong class="kf jh">“状态”</strong>。代理可以执行一个可能的<strong class="kf jh">【动作】</strong>来改变当前状态。</p><p id="36cc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了驱动代理执行动作，每个状态产生一个相应的<strong class="kf jh">奖励</strong>。一个代理计算每个状态的期望总报酬，称为一个状态的<strong class="kf jh">值</strong>。</p><p id="148f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL作品图片如下:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/30306fd2db7a2bcced8226116bc0a344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A0b94rY3QEhh5s4G.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Reinforcement Learning working style</figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/f8fa9877fd63b999121d6cd400b89dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/0*slKfQUpJyCnCOpNJ.png"/></div></figure><p id="7997" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个代理在环境中的时间“t”执行状态中的动作，并在时间t+1获得新的状态和该动作的奖励。</p><p id="084e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个步骤中，代理</p><blockquote class="lm"><p id="69c6" class="ln lo jg bd lp lq lr ls lt lu lv la dk translated"><em class="lw">执行一个动作</em></p><p id="86af" class="ln lo jg bd lp lq lr ls lt lu lv la dk translated"><em class="lw">观察新状态</em></p><p id="979f" class="ln lo jg bd lp lq lr ls lt lu lv la dk translated"><em class="lw">领取奖励</em></p></blockquote><figure class="ly lz ma mb mc is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lx"><img src="../Images/21e6cf653b854ab88d938202cccc1576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DwPdMoZ3ZrndQsFh.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Environment, State, Action,Agent,etc., described in Maze Example</strong></figcaption></figure><p id="a7a6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在强化学习中，我们不知道一系列行为的最终成本或回报，直到它被执行。代理人的目标是找到一系列能最大化<strong class="kf jh">回报</strong>的行动。基本强化学习处理最大化回报。</p><p id="259e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自主智能体应该学会在没有人类操作员任何指导的情况下，通过反复试验来执行任务。RL允许我们建造智能机器。它为行为提供了形式主义。RL不需要数据来训练智能代理。</p><p id="8c42" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL代理可以通过分层学习显著减少学习时间——首先解决初级学习问题，然后组合解决方案来解决一个复杂的问题。</p><p id="1985" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL通过对过去的记忆来处理非马尔可夫环境。</p><blockquote class="lm"><p id="4bda" class="ln lo jg bd lp lq lr ls lt lu lv la dk translated">强化学习不需要训练数据，而是适用于未知环境。</p></blockquote><figure class="ly lz ma mb mc is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi md"><img src="../Images/c5c490b56e5e994a692052991b06cfcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tZxLML9lR8MHOnE1.png"/></div></div></figure><p id="189a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些元素及其变体在马尔可夫决策过程中被很好地定义。在这篇文章中，我们将简要地去MDP。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi me"><img src="../Images/3f590a5c3dbc9cd4ac0765c615a56d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/0*8MMt57P5jmVwe3Tn.png"/></div></figure><p id="51c6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL动态地而非离线地学习马尔可夫决策过程(MDP)。它评估每个状态的值，无论哪个状态值最大，都将进入该状态。</p><p id="2769" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了下面的主题，还需要理解它的符号。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mf"><img src="../Images/2b70a8281dd106d38cf47ec806afd383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HBclgAsML3nvkKin.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Major Concepts used in Reinforcement Learning</strong></figcaption></figure><p id="47cf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">注意，动态规划、蒙特卡罗评估和时间差学习是迭代方法。</strong></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/b799aaf823abe40921bb65d9de57af4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/0*wVJOVyhdkaxhAoUH.png"/></div></figure><p id="7cf0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，不同的作者、大学和博客使用不同的符号，以下符号是基于理查·萨顿的《T2强化学习》这本书</p><p id="df3f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我试图探索不同的符号，以便读者熟悉这些符号。</p><p id="a50e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我试图探索不同的符号，以便读者熟悉这些符号。例如，以下3个符号是相同的。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/a7f72bff06cfdabcd83439ebef2a79c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/0*c17_-JNzTCEXg7LR.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mi"><img src="../Images/e53bd8b633cd625d8a78da5261d69897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZiG_pSyWbNHO75zn.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Notations used in Reinforcement Learning and Markov Decision Processes (MDPs)</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/aed0a5078f918f1e02d0a06639a41ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*IA0SA909TX1rViX2.png"/></div></figure><p id="6937" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始研究RL概念的例子之前，我们先考虑一下马尔可夫决策过程(MDPs ),它可以被视为离线规划，而RL则被视为在线规划。</p><p id="c982" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑状态和动作如何有助于<strong class="kf jh">问题解决</strong>、<strong class="kf jh"> MDPs </strong>和<strong class="kf jh">强化学习</strong>。</p><p id="2659" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在任何时候，代理都应该处于环境中任何可能的状态。</p><p id="9401" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">动作是确定性的和非确定性的。确定性动作在当前状态下执行并仅产生一个后继状态，而非确定性状态在当前状态下执行动作并产生多个后继状态。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/b23932f28d3b072e590124ef1a5aa506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RGoeTnvhUnlKg_mK.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/3f8061b444ec2899efc76cfab9b6c6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*2zD_a_c2QkauaUmx.png"/></div></figure><p id="1fad" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马尔可夫决策过程(MDPs)在非确定性搜索问题中起着至关重要的作用，它以一种有效的方式处理多个后继状态，它主要被认为是离线规划，并且主体对变迁和报酬(或环境)有充分的了解。MDP提供了通常提出RL问题的形式主义。</p><p id="b3ca" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马尔可夫决策过程由几个属性定义。马尔可夫决策过程是一个元组，它被定义为</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/ef76846f8f338ca27384a7e0c8a43c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/0*Yt5V6B6Z6-2tm39T.png"/></div></figure><p id="a313" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/4f44707b26884a9d35e94f9c1bb8fc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*WK2HknqqKfpEGS8E.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/2c44a8d86f06ea1ef8185addf4cadf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*NbOzo3gw1ZWrED1-.png"/></div></figure><p id="359b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴于现在，未来独立于过去。一个状态被称为马尔可夫的当且仅当:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ae1a5fdb6c71d1b8b5b519634371316f.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*V3RJF4Gaf6YikOf8.png"/></div></figure><p id="e733" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态从历史中获取所有相关信息。一旦状态是已知的，历史可以被丢弃，即状态是未来的充分统计。</p><p id="19bd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MDP可以被建模为国家行动奖励序列</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mq"><img src="../Images/209d8f762aebc9160f0bb878b5cee68b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tgGxbJHFym54u3aS.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6f083678c8805cef0ee1eeb7517e3eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/0*GJ0L3rG5MssutXa6.png"/></div></figure><p id="910d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">奖励函数被定义为，</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/b5efa4486ebd6b19ba352e75018c326b.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/0*NCe1HUG4VbYXWLkg.png"/></div></figure><p id="5155" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回报函数是从时间步长t开始的总折扣奖励，定义为</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/393d5f7d7e4b80622236e67487e14385.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/0*ia-lvwE04201SHyz.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ddd71dbd457da0dcec47b280c904b1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/0*c6s8zGdivQRc5X5Y.png"/></div></figure><p id="566c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">价值函数V(s)描述了状态s的长期价值。价值函数有不同的变体，下面简单介绍。</p><p id="0f6b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态值函数V(s)是从状态s开始的期望收益</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6f66dc45b842b3b8573497c44b8b32e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/0*-sG7sqXfCiozv-r7.png"/></div></figure><p id="ffb2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">价值函数可以分解为即时报酬和后继状态的贴现值。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/40533c5310cdf53965fe2df7506f424d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/0*XGQ8clQapqr3d8m1.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Value Function Decomposition</strong></figcaption></figure><p id="d108" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">继承国的直接回报和贴现值</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/afd2c286426ad6d39466ae8226a432d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/0*YOyq9FOSgeTLEZTa.png"/></div></figure><p id="5422" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是与被称为<a class="ae my" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态规划</a>的数学优化方法相关的最优性的必要条件。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/b5f625955aadaa633e2eddcb269390e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/0*PwuPpo4cmNae_gwq.png"/></div></figure><p id="b898" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DP通过以递归方式分解成简单的子问题来简化复杂的问题。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a83b1ff54568d9cc5a5d240e257d0bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/0*JfMby6YUuyQyiSjT.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Image from Wiki</strong></figcaption></figure><p id="f8d5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">最优子结构:</strong>一个复杂的问题可以通过分解成子问题，并使用递归概念寻找子问题的最优解而得到最优解决，那么它就是最优子结构。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a2527b97b49f6faaa4e6e07dfb1b9ddc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*W1V7x7HLoRUbe-ot.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">DP solves complex problem by decomposing into sub-problems and find optimal solution using recursive concept</strong></figcaption></figure><p id="2b29" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">子问题可以递归地嵌套在更大的问题中，因此DP方法是适用的，那么在更大的问题的值和子问题的值之间存在关系。在最优化文献中，这种关系被称为<strong class="kf jh">“贝尔曼方程”。</strong></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/747095edd052fb1415c5923b6c4d8d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/0*jwb_pax1w3yt6ynA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">This Relationship is called Bellman Equation</strong></figcaption></figure><p id="b7e4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贝尔曼方程可以用矩阵简明地表达，</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c6ddcddac8ef6d0e8bb8cc7ff746c5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/0*Hu3lFftqpocddhsA.png"/></div></figure><p id="7cce" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">价值函数具有需要策略的变量。它们是状态-价值函数和行动-价值函数，这将在政策定义之后讨论。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/402d8b002470328f8b8a4845e3417ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/0*Bv42gGOb3g9RX6mD.png"/></div></figure><p id="e244" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略是给定状态的动作分布，</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/61f1949497ea667f4f9f88083cba0ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*VfpPRgu-tyqZ4JCH.png"/></div></figure><p id="85be" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略完全定义了代理的行为。MDP策略取决于当前状态(而非历史)，即策略是固定的(与时间无关)</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ceb84024451e86db70368dafb901fcd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*Q9D369JpwRf4KYLe.png"/></div></figure><p id="2d86" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态转换和奖励功能与策略结合在一起，定义如下</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/98a3ebbd9e4da0fe9fadc8898a7ef2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-dvItHceL_6Txc-G.png"/></div></div></figure><p id="0eee" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态值函数是从状态s开始的期望收益，然后遵循策略</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ec7994a93d6ab10e3e63f87b17074b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/0*ulimubkg3-NbA8Dq.png"/></div></figure><p id="568c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">行动价值函数是从状态s开始的期望收益，采取行动a，然后遵循政策</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9505e7f3b7c360c709613923767264a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/0*xifUrob-LzHEvQkP.png"/></div></figure><p id="5caa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态-价值和行动-价值函数可以分解为直接回报加上后继状态的贴现值</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e42c053a6111c54153b084a68b8ef774.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*IXx6feMdPvqJIPFX.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">State-value Function</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/acf745785eefadedcb002795f89a5dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*rcS6WrWowh00EuXM.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Action-value Function</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9ceb38e9b879b3720240740ab784a715.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*ACp4kUsWRTKHNbEt.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a43cb25246e6ae3f003477db20e3d469.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*-Ofiqorr1SNJnMZ-.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f292abd66d15dd7530f9780ef1807964.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*hgnLeU7Y7-gKmnpp.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ff7487e410671b44e6a26d4163866bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/0*kmAb_AC1Dm_uh_C8.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/db0e827c0bb5b0d0f2d60a35dfde6db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ICxlLBPEAe4ALlt1.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/0f2802787b1e6d56f7228400e4663319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*REOwcdCD0f6WFXi6.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/5d7acdd48c0f26b7bcac3c7c52262e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dOCms6twO3kVIqNY.png"/></div></div></figure><p id="761a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">状态-价值函数的贝尔曼期望方程可以用矩阵形式表示如下</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/38fd4b7fad9bd1bf2341a8a94ec5a7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/0*opo9F1FXwtjnjMIm.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/8df63f83cd21b36bd23261d63247f7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/0*qfQgdLsEG8_-8UQw.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/3046d4833aaaf9d5aa2f5b44fa762c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/0*v5gzJOKIwto6wi_L.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Optimality for state-value, action-value and policy.</strong></figcaption></figure><p id="dfb5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">最佳状态值函数</strong></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/6c13c6344adc2439f38f257f665efc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7jxuI4jTQeiWemx0.png"/></div></div></figure><p id="e9d9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">最佳动作值函数</strong></p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/606923c7bcf7958ba15ff22c6bd83a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mYkA0wF8gtMBOrPL.png"/></div></div></figure><p id="3d69" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最佳值函数指定了可能的最佳性能，一旦知道，它将被标记为已在MDP解决。</p><p id="5ac7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">最优策略</strong></p><p id="6884" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">定义策略的偏序如下</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a5478089391939c061236b15fedfc8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/0*WK5FlUb6KrtcBE2V.png"/></div></figure><p id="9726" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最优策略可以通过最大化最优价值行动来找到</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/591cfa4bffe99f9e5fdbefea6ff83220.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*8vxADgpSq1B0Kd3v.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/790b5299f33736459f194baf787a5646.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/0*m4TVISV15n1vl-Io.png"/></div></figure><p id="5051" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最佳值函数通过贝尔曼最优方程递归关联:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/82312b32de3abb4e0d3db79cd6858975.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*ZUCAbCFMUFBXnxnB.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bd742d12b189f36a663bcc52b669d837.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*RsfBTJIejfSKU1k8.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3ba13a36621f84f61cb35a8c2293623e.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/0*wcGpXa2GTkAo4EKb.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4d3130652882ab6340100757fe634fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/0*YuR93YzmfEdwZGOW.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c1a89da1ca96deb386f9c5afb1e18f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*n_57rsLAO6bptKZe.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7c4beba35b01c54cac50004235bd39e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*U5KxoxyjPh6w9P5p.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/04f691bda3ed412333ecb7f051b354ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/0*Lgf3Rq5cRcVIOTot.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Bellman Optimality Equation for action-state function</strong></figcaption></figure><p id="6424" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL导出了算法的MDP的一些概念，正如已经陈述的，它提供了RL问题通常被提出的形式。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1c624d9538c9a781b22af1b6c7a0d1aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*amhdkV4eD22CcAFj.png"/></div></figure><p id="c63a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑一个汽车有三种状态的例子，并从这些状态中评估所需的功能。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/5eb7c9482038ad4d2ef368609354ee66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XfCjEP6o7hmgs5gm.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">State-space graph for a race car (search problem)</figcaption></figure><p id="5997" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你在这个例子中看到的，奖励用(+/-)符号表示，动作(慢、快)和过渡没有符号。基于这个图表，我们需要计算<strong class="kf jh">价值函数</strong>和<strong class="kf jh">策略</strong>。</p><div class="ip iq gp gr ir oh"><a href="https://www.datadriveninvestor.com/2020/11/19/how-machine-learning-and-artificial-intelligence-changing-the-face-of-ecommerce/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd jh gy z fp om fr fs on fu fw jf bi translated">机器学习和人工智能如何改变电子商务的面貌？|数据驱动…</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">电子商务开发公司，现在，整合先进的客户体验到一个新的水平…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ix oh"/></div></div></a></div><p id="e8b7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们构造转换函数和奖励函数表，并在价值和政策迭代中使用。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ow"><img src="../Images/a900e27951438f16e247f10504bb3a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aurmAqdRKOGeh_2g.png"/></div></div></figure><p id="c3e8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始评估或检查之前，我们需要价值迭代、策略提取和策略迭代。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f2c3b585a5a7fd716e88c50a78188fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/0*6K0fOQLp164jh79B.png"/></div></figure><blockquote class="lm"><p id="233f" class="ln lo jg bd lp lq oy oz pa pb pc la dk translated"><em class="lw">值迭代用于计算状态的最优值，通过迭代更新直到收敛。</em></p></blockquote><p id="9a5a" class="pw-post-body-paragraph kd ke jg kf b kg pd ki kj kk pe km kn ko pf kq kr ks pg ku kv kw ph ky kz la ij bi translated">值迭代是一种动态规划算法，它使用迭代更长的时间限制来计算时间限制值，直到收敛，即在时间<strong class="kf jh">‘k’</strong>和<strong class="kf jh">‘k+1’时，每个状态的V值是相同的。</strong>根据定义，它应该是</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/31360a7794c713c999506602f68d9fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*RLdqTTwZtoWYrDef.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ba28320409044ae3bc2aedfa96ec0686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*cP1zkceJbFY8TBGD.png"/></div></figure><p id="ed18" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它的操作如下</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/961d11e21e759fdbe277020c1ff2e0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dBwRe4AUblEg2-Cr.png"/></div></div></figure><p id="b3fd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">值迭代的计算:</strong></p><p id="b169" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">步骤1 </strong>:我们开始将初始状态设置为零，即，</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/c5c100e2ec3750ee30c0412adc5f060d.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*_dp2_97cfp-CKC2d.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/af6e3397922ceb2546fa932c923ae289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*g102Aj38j9ZF5dA-.png"/></div></figure><p id="c31a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">步骤2: </strong>在我们的第一轮更新中，我们可以计算第一状态(冷状态)</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="ab gu cl pn"><img src="../Images/31573030d1a65a61fbd5e375cc6c1f55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*b31hiO4ynbDLRrXWEFF4aQ.png"/></div></figure><p id="0e82" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算状态后，状态值如下:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi po"><img src="../Images/9b7347da414b012fc386d40fb35576ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*bEeEk3atppfOakW8.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pp"><img src="../Images/1a9d368091d42b0083d5bb07993e51d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cjSUWLmmx_jFDSUQ.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/dd2df5c0de151ce76cb4bcca931c3329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*4Xq2SorWzfhB2y5c.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e6b9ada41bdd0906a6bce51800fd97e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/0*-Fv3eQKjuc38C_FG.png"/></div></figure><p id="2a60" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">求解MDP的最终目标是确定一个最优策略，一旦使用一种叫做<strong class="kf jh">“策略提取”</strong>的方法确定了状态的所有最优值，就可以得到最优策略。</p><p id="89e0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，最佳策略是</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pr"><img src="../Images/f3d55d5b0af62e0ba2ccc0f587a2862d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UFwEB_Dz5br-oWtu.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/87720b4873c148577808e9abe772b6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/0*nrz67rO_IaR_hpH8.png"/></div></figure><p id="7a1b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略迭代是一种算法，它保持值迭代的最优性，同时提供显著的性能增益。</p><p id="9623" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略迭代需要策略评估和策略改进。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/5863b9a331c46d7df23cb6de01a6dab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/0*iOblDhyx2a429axD.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pu"><img src="../Images/a6f294f5711822c22179a464c66153df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hc3Wg_Q-Ik0u4W8C.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/ed82fe61fd3b6fba31a7f5bcb2cf6675.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/0*yyfgrK-R7nKFU6SE.png"/></div></figure><p id="ea4b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它封装了策略评估和策略提取</p><p id="93e4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略改进用于生成更好的策略。它对由策略评估生成的状态值使用策略提取来生成这个新的和改进的策略。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pw"><img src="../Images/593d88dfc5a942a4f9b6a6f7b4476206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fJdF24MbRlogUDP8.png"/></div></div></figure><p id="8363" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">算法如下:</p><ol class=""><li id="5b7e" class="px py jg kf b kg kh kk kl ko pz ks qa kw qb la qc qd qe qf bi translated">定义初始策略。</li><li id="a71a" class="px py jg kf b kg qg kk qh ko qi ks qj kw qk la qc qd qe qf bi translated">反复进行政策评估和政策改进，直至达到收敛。</li></ol><p id="cb7d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图像上看，政策改进如下:</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/b71c3c5a0b772729bb206bb27dcc2c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*a1C3G7pQ9J2rBwuY.png"/></div></figure><p id="887b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们为赛车示例计算策略迭代如下:</p><ol class=""><li id="6fd0" class="px py jg kf b kg kh kk kl ko pz ks qa kw qb la qc qd qe qf bi translated">定义初始策略。</li></ol><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/63724c766d482e73e9340dd3afc86b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/0*tAUS-UEe9_kQ8tHO.png"/></div></figure><p id="fdf5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">终端状态(过热)没有传出操作，没有策略可以为其赋值。因此，我们简单地为任何终端状态赋值0。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a0d4e81fabd1f571413e4264e1b39226.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*4R1nd-AqJby3YwWr.png"/></div></figure><p id="de5b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是对初始状态运行一轮策略评估。即，</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/b8ae78da71889e7a10ffd75f172a8a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:72/format:webp/0*MXyjPkDyt5WfNfsb.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qo"><img src="../Images/cdd5d1cae0fd703440152e1ca397d303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WGGr2ksAA7TxpjpI.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/125b436b26912a912a9a8fb3de825dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/0*udDdUklo1_bfS_yI.png"/></div></figure><p id="a01b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在可以使用这些值运行策略提取。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/d495bf162c3aa772f80f14923928be99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*mfzwoMWsp_1icjxa.png"/></div></figure><p id="33b6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略提取</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qr"><img src="../Images/23784dd7c9cf13e3c2532160992c0102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zZRK4nKOnsxNdAbO.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/277e4653bc87e65c6709d998d83ed51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*e0QH5r3C7KKbcXpN.png"/></div></figure><p id="d118" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个例子给出了策略迭代的真正威力，只需要2次迭代。</p><p id="bd9e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们在底层工作中完成了赛车示例的价值迭代和策略迭代。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/e5c5848660c6e23ae93a11b6ee9651c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/0*K2WqAAox5vYMXdBa.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qs"><img src="../Images/c1c82fbccff1b3b59bdca43dd943d929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PE_WcRmp8iwt5_8v.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Achievements of using Reinforcement Learning</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/da3f45f7a6b89f2df120f6adac7e7997.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/0*1ztfDzOLpgSs7xCs.png"/></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qu"><img src="../Images/98aec7aabf04daea3071be590d014595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8L24TMCcMrz2SYuQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Examples along with States , Actions and Rewards.</strong></figcaption></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/2d376df9415571572409b36bf0019525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*H6WiLG-hzd4hQAnZ.png"/></div></figure><p id="9c40" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有两种类型的强化学习，基于模型的学习和无模型的学习。都是用来做决策的。</p><p id="f483" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于模型的学习算法使用转移函数和奖励函数以及在控制探索期间获得的样本。。</p><p id="4e1d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无模型学习(Model-free learning)，试图直接估计状态的值或q值，而不使用转换和奖励。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qw"><img src="../Images/a8ddc6f9a3a65c5d6d5980e530dbcfec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G87voIevTXEtQuM5.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd jd">Types of RL Algorithms</strong></figcaption></figure><p id="a315" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图描述了强化学习算法。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qx"><img src="../Images/553b55592df8a0d8a545970aa8b1f052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ai4HwMbxD__GNRGW.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi qy"><img src="../Images/9f4fa06a0e7d07567e4cbcd0001b1b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RmwntMz2ocXv_aaB.jpeg"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/71d9b2468cfc946947794300b93afbd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/0*ksuVk49FKzqYkrJr.png"/></div></figure><p id="2a45" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习可以应用在艰苦的环境中，下图描述了它的典型应用。</p><figure class="lc ld le lf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ra"><img src="../Images/1412ae225c52dd3ab20f67c92ab26201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mGjO1sg0rJh5-BWf.png"/></div></div></figure><figure class="lc ld le lf gt is gh gi paragraph-image"><div class="gh gi rb"><img src="../Images/c2d44d83840bb25fd18ddfb91fc9377a.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/0*aOwZsvmzp7rn6V9t.png"/></div></figure><p id="a222" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习并不容易理解，它要求用户应该完全了解其方法的内部工作原理。虽然我们有3种不同的迭代方法，但本文涵盖了一种类型的迭代方法，即带有赛车示例的动态编程。虽然有许多主题需要涵盖，但它们将在以后的文章中得到证明。在机器人、自主车辆、机器、发动机等领域的应用中，虚拟现实是一个非常热门的研究领域。,</p><p id="6f8d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您阅读本文，感谢您的反馈、评论和分享。如果评论中有错误，请告诉我。</p><p id="39fa" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">进入专家视角— </strong> <a class="ae my" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kf jh">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>