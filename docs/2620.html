<html>
<head>
<title>Inpainting of Irregular holes using Partial Convolution: Paper summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用部分卷积修补不规则孔洞:论文摘要</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/inpainting-of-irregular-holes-using-partial-convolution-paper-summary-e836cd2c44ae?source=collection_archive---------0-----------------------#2020-05-08">https://medium.datadriveninvestor.com/inpainting-of-irregular-holes-using-partial-convolution-paper-summary-e836cd2c44ae?source=collection_archive---------0-----------------------#2020-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f7fe8b53a917dae180c85755099a2ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmd-RmzVqCeNgZXGaIOP4Q.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Courtesy: <a class="ae kc" href="https://unsplash.com/s/photos/abstract" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ea88" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的<a class="ae kc" href="https://medium.com/analytics-vidhya/image-inpainting-and-its-evolution-a-brief-discussion-ae1d42431308" rel="noopener"> <strong class="kf ir">博客</strong> </a>中，我谈到了修复的基础知识，以及修复方法的进步如何展示了修复的美好未来。上述博客最后介绍了基于部分卷积的图像修复，以及它如何解决过去模型面临的各种问题。在这里，我将详细讨论使用部分卷积<strong class="kf ir"/>来修复不规则孔洞的<strong class="kf ir"/><a class="ae kc" href="https://arxiv.org/pdf/1804.07723.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">图像的症结所在。</strong></a></p><h1 id="79ed" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是部分卷积？</h1><p id="9568" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">部分卷积执行输出的标准化，以调整丢失数据的部分。<strong class="kf ir">部分卷积层包括屏蔽和重新归一化的卷积操作，随后是屏蔽更新设置。</strong>卷积时不考虑图像的补丁像素。</p><p id="7fdf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">部分卷积定义为:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/7496d576fb8357ff070faf003d9d43bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AyZNlHDbqow5XtzMgnkNPQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Partial Convolution, Courtesy: <a class="ae kc" href="https://arxiv.org/pdf/1804.07723.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><p id="dfc7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mj"> W=内核权重<br/> X=当前滑动窗口的特征值<br/>M = X<br/>对应的二进制掩码(。)=逐元素乘法<br/> 1=所有元素为“1”且维数等于“M”的矩阵<br/> b=偏差</em></p><p id="2ec3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">举个例子会更好理解。考虑大图像的一小部分X:</p><p id="4947" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mj"> Xp0= X完全被空洞区域包围<br/> 1p0=类似于“M”，定义的值为“1”，修补区域为“0”<br/>1p 1 =类似于之前定义的</em> <strong class="kf ir"> <em class="mj"> 1 </em> </strong> <em class="mj"/></p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/74a651c771e044b694161288781e3b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*4YKuQGp2LR-nmVuNFXgPIg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Partial Convolution example, Courtesy: <a class="ae kc" href="https://arxiv.org/pdf/1811.11718.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><p id="7ece" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="mj"> x' = Wt( Xp0。1p 0)sum(1p 1)/sum(1p 0)+b</em></strong></p><ul class=""><li id="311c" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated">在第一次卷积中，仅考虑非补片像素，并且补片像素与卷积核权重的乘积被设置为“0”。</li><li id="6c73" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">在上面示例中的红框之后，<strong class="kf ir"> Xp0 </strong>与<strong class="kf ir"> 1p0 </strong>进行逐元素相乘，并在<strong class="kf ir"> 1p1的帮助下进行归一化。</strong></li><li id="5ffc" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">使用项<strong class="kf ir"> Sum(1)/Sum(M ),根据丢失像素的数量对输出和进行升级。</strong></li><li id="b232" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated">对于第一层之后的卷积，除了非面片特征的定义被更新为在其感受域中具有至少一个非面片输入像素的所有特征之外，遵循相同的事情。</li><li id="62e8" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kf ir"> <em class="mj">感受野是影响旋绕输出像素的输入像素数量。</em>T46】</strong></li></ul><h1 id="aec8" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">网络体系结构</h1><p id="5ba6" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kf ir"> UNet-like </strong>架构在原文中用于执行<strong class="kf ir">【完井网络】</strong>操作(此处<a class="ae kc" href="https://medium.com/analytics-vidhya/image-inpainting-and-its-evolution-a-brief-discussion-ae1d42431308" rel="noopener">解释的【完井网络】</a>)。不使用普通的CNN，部分卷积层与<strong class="kf ir">跳过链接</strong>一起使用。由于架构很长，为了产生更好的结果，在后面的层中还需要微小的细节。该信息通过将来自第<em class="mj">n</em>编码器层的图像和遮罩与等效的解码器层连接起来进行传输，这是一个跳过链接的功能。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/379463fb7245e6d5a195c2316ac4dfc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Tq1y3upBldGis0nrSjFqLw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Courtesy : <a class="ae kc" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">Paper</a></figcaption></figure><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/e4e99a2c071889bf2ddde41b25d4b888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*VIVbYwwvVe3M3dK6kjNKrw.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A UNet example from this <a class="ae kc" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="f109" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ReLU用于编码阶段，α= 0.2的LeakyReLU用于所有解码层之间。编码器包括跨距=2的八个部分卷积层。内核大小为7、5、5、3、3、3、3和3。通道的数量是64、128、256、512、512、512和512。</p><p id="5995" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mj">(我不会深入损失计算，因为它完全是数学和定量的)</em></p><h1 id="ef26" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">额外好处:填充部分卷积</h1><p id="5d4b" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">传统上接受的填充技术，如<strong class="kf ir">零填充、反射填充、</strong>和<strong class="kf ir">复制填充</strong>试图通过使用图像边界附近可用的数据值来填充。这些技术导致不真实的图像图案，因为仅使用了部分特征。为了解决这些问题，可以使用部分卷积进行填充，同时假设图像最外面的填充边界为一个补丁，并用加权数据值填充它。</p><ul class=""><li id="d4c0" class="ml mm iq kf b kg kh kk kl ko mn ks mo kw mp la mq mr ms mt bi translated"><em class="mj">你可以在Nvidia官方实施</em> <a class="ae kc" href="https://www.nvidia.com/research/inpainting/" rel="noopener ugc nofollow" target="_blank"> <em class="mj">网站</em> </a> <em class="mj">尝试在线演示。</em></li><li id="4b74" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated"><em class="mj">我已经尝试在我的</em><a class="ae kc" href="https://github.com/Bunnyyyyy/PConv-Keras/blob/master/Mytest.py" rel="noopener ugc nofollow" target="_blank"><em class="mj">Github</em></a><em class="mj">中实现代码。</em></li><li id="0ad5" class="ml mm iq kf b kg mu kk mv ko mw ks mx kw my la mq mr ms mt bi translated"><em class="mj">论文依次为:</em><a class="ae kc" href="https://arxiv.org/abs/1804.07723" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1804.07723</a><a class="ae kc" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1505.04597.pdf</a><a class="ae kc" href="https://arxiv.org/pdf/1611.07004.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.07004.pdf</a>，</li></ul><div class="nb nc gp gr nd ne"><a href="https://www.datadriveninvestor.com/2020/03/24/encoder-decoder-sequences-how-long-is-too-long/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ir gy z fp nj fr fs nk fu fw ip bi translated">编码器解码器序列:多长是太长？数据驱动的投资者</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在机器学习中，很多时候我们处理的输入是序列，输出也是序列。我们称这样的一个…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jw ne"/></div></div></a></div><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="nt nu l"/></div></figure></div></div>    
</body>
</html>