<html>
<head>
<title>How a ‘Lazy Learner’ be useful to you?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“懒惰的学习者”对你有什么用？</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/how-a-lazy-learner-be-useful-to-you-dad50a7710e3?source=collection_archive---------9-----------------------#2020-06-03">https://medium.datadriveninvestor.com/how-a-lazy-learner-be-useful-to-you-dad50a7710e3?source=collection_archive---------9-----------------------#2020-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8614d9682ee4081efe785d8fea7006da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cog4TvnMwIeg3eV0z5gKsQ.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">(<a class="ae jd" href="https://pixabay.com/photos/fox-sleeping-resting-relaxing-red-1284512/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><div class=""/><p id="189d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之KNN</p><p id="cb36" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">k-最近邻(或简称为KNN)是一种用于分类和回归问题的监督机器学习算法。这里是最近邻居的数量。</p><div class="ip iq gp gr ir lb"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="lc ab fo"><div class="ld ab le cl cj lf"><h2 class="bd jh gy z fp lg fr fs lh fu fw jf bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="li l"><h3 class="bd b gy z fp lg fr fs lh fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="lj l"><p class="bd b dl z fp lg fr fs lh fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="lk l"><div class="ll l lm ln lo lk lp ix lb"/></div></div></a></div><p id="18b7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">KNN被称为<strong class="kf jh">懒惰学习者(Instance-based learning) </strong>为<strong class="kf jh"> </strong>它<strong class="kf jh"> </strong>在训练阶段学不到多少东西。它不会从训练数据中导出任何判别函数。它存储训练数据集，并仅在进行实时预测时从中学习。</p><p id="efef" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，让我们了解KNN是如何工作的，如何使用数据集实现KNN，以及如何提高KNN的性能。</p><ol class=""><li id="75f2" class="lq lr jg kf b kg kh kk kl ko ls ks lt kw lu la lv lw lx ly bi translated">KNN是如何工作的？</li><li id="c9db" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">如何实现KNN？</li><li id="bc7d" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">如何提高KNN？</li><li id="bd3d" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">KNN的利与弊</li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="65c0" class="ml mm jg bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">KNN是如何工作的？</h1><p id="7080" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">让我们考虑分类的机器学习问题，其中我们必须将一个新的数据点分类，比如说，将X分类到一个类中。这个分类问题是用KNN分两步完成的。</p><ol class=""><li id="f0bf" class="lq lr jg kf b kg kh kk kl ko ls ks lt kw lu la lv lw lx ly bi translated">找出X的K个最近邻</li><li id="5d8b" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">应用多数表决</li></ol><h2 id="a691" class="no mm jg bd mn np nq dn mr nr ns dp mv ko nt nu mz ks nv nw nd kw nx ny nh nz bi translated">第一步:找出X的K个最近邻居</h2><p id="2107" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">为了找到X的k个最近邻，我们找到从X到这些点的距离。该距离可以通过以下方式计算:</p><ol class=""><li id="1def" class="lq lr jg kf b kg kh kk kl ko ls ks lt kw lu la lv lw lx ly bi translated">闵可夫斯基距离</li><li id="787f" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">曼哈顿距离</li><li id="37b5" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">欧几里得距离</li><li id="0575" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">汉明距离</li><li id="286f" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">余弦距离</li></ol><p id="7f99" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="oa">闵可夫斯基距离</em> </strong>是一种广义的距离度量。我们可以通过替换“p”来操纵下面的公式，从而以不同的方式计算两个数据点之间的距离，</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6c44a847a677bfd0f635535c1c799633.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*rdNNKVieXLjZM1eDoYv0Gg.png"/></div></figure><p id="bdda" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="oa">曼哈顿距离</em> </strong>是沿直角轴测量的两点之间的距离，由下式给出:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ed400ecc2e6ff9441d26384ecd03a647.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*DsDKFfxfi3r-ZrPN-03m6w.png"/></div></figure><p id="c120" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="oa">欧氏距离</em> </strong>是两点间的直线距离，由下式给出，</p><figure class="oc od oe of gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4bc92e730991b7a03cf85c0223274674.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*6RlyN-AyiunwV7GEsGXtLA.png"/></div></figure><p id="36a0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="oa">汉明距离</em> </strong>是两个二进制字符串之间不同的位数。<br/>对于A = " 1 0 1 1 0 0 1 0 1 1 "和B = " 1 0 0 1 0 1 0 1 0 1 0 "<br/>由于位置3和1 0的值不相似，A、B之间的汉明距离为2。</p><p id="5466" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh"> <em class="oa">余弦距离</em> </strong>和<strong class="kf jh"> <em class="oa"> </em> </strong>余弦相似性度量主要用于寻找两个数据点之间的相似性。余弦相似度由‘Cosθ’给出，余弦距离为(1- Cos θ)。点越靠近，余弦相似度越高，余弦距离越低。</p><p id="007a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在使用上面讨论的任何方法找到X与邻居的距离之后，让我们考虑与X最近的前K个邻居。</p><h2 id="72af" class="no mm jg bd mn np nq dn mr nr ns dp mv ko nt nu mz ks nv nw nd kw nx ny nh nz bi translated">第二步:实行多数投票制</h2><p id="c44b" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">从K个最近的邻居中，我们对类别标签进行多数投票以估计x的类别。为了更精确，可以考虑这些邻居的权重，使得更近的点获得更重的权重。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="b826" class="ml mm jg bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">如何实现KNN？</h1><p id="678f" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">让我们用Kaggle的<a class="ae jd" href="https://www.kaggle.com/brynja/wineuci" rel="noopener ugc nofollow" target="_blank">葡萄酒分类器</a>问题来说明KNN分类。</p><h2 id="b3b6" class="no mm jg bd mn np nq dn mr nr ns dp mv ko nt nu mz ks nv nw nd kw nx ny nh nz bi translated">关于数据集:</h2><p id="f26f" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">这些数据是对生长在意大利同一地区但来自三个不同品种的葡萄酒进行化学分析的结果。这项分析确定了三种葡萄酒中13种成分的含量。</p><p id="9c92" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个葡萄酒类别的实例数量</p><ul class=""><li id="17bd" class="lq lr jg kf b kg kh kk kl ko ls ks lt kw lu la oi lw lx ly bi translated">1级–59级</li><li id="49ba" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la oi lw lx ly bi translated">2级–71级</li><li id="d9dd" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la oi lw lx ly bi translated">3级–48级</li></ul><p id="be96" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特点:</p><p id="df5e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">酒精、苹果酸、灰分、灰分碱度、镁、总酚、类黄酮、非类黄酮酚、原花色素、颜色强度、色调、稀释葡萄酒的OD280/OD315、脯氨酸</p><p id="1709" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们实现KNN分类器，</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="dac2" class="no mm jg ok b gy oo op l oq or">import pandas as pd<br/>features = ['class','alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']</span><span id="7315" class="no mm jg ok b gy os op l oq or">wine_data = pd.read_csv("wine_class.csv",names=features)</span></pre><p id="0867" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="oa">使用pandas将</em>CSV(逗号分隔值)数据加载到数据帧中。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="dcb9" class="no mm jg ok b gy oo op l oq or">wine_data.head()</span></pre><p id="8b0d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:</p><figure class="oc od oe of gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/f416308dcf3271841a1fa555be1967ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tx-kS13JVtKr4C7vYeAOSA.png"/></div></div></figure><p id="950c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Head()给出了数据帧中的前5个数据点。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="1559" class="no mm jg ok b gy oo op l oq or">wine_data.shape</span></pre><p id="f1d5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:(178，14)</p><p id="b4ce" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着葡萄酒数据有178个数据点和14个特征，包括类。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="36a9" class="no mm jg ok b gy oo op l oq or">wine_target = wine_data['class']<br/>wine_features = wine_data.drop(['class'],axis=1)</span></pre><p id="e037" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="oa">将独立特征</em>与从属特征分离。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="5fb2" class="no mm jg ok b gy oo op l oq or">from sklearn.model_selection import train_test_split</span><span id="58bd" class="no mm jg ok b gy os op l oq or">X_train, X_test, y_train, y_test = train_test_split(wine_features, wine_target, test_size=0.3,random_state=0)</span></pre><p id="c4f1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="oa">将数据</em>拆分为训练数据和测试数据，其中测试数据占总数据的30%，训练数据占总数据的70%。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="3e3c" class="no mm jg ok b gy oo op l oq or">#Import knearest neighbors Classifier model <br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="014d" class="no mm jg ok b gy os op l oq or">#Create KNN Classifier <br/>knn = KNeighborsClassifier(n_neighbors=7)</span><span id="3be4" class="no mm jg ok b gy os op l oq or">#Train the model using the training sets <br/>knn.fit(X_train, y_train)</span></pre><p id="0e9e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的，不管数据大小如何，实现KNN分类器只需要3行代码。</p><p id="5e70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经<em class="oa">生成了模型，</em>让我们来看看模型的准确性。</p><pre class="oc od oe of gt oj ok ol om aw on bi"><span id="aab5" class="no mm jg ok b gy oo op l oq or">#Predict the response for test dataset <br/>y_pred = knn.predict(X_test)</span><span id="beeb" class="no mm jg ok b gy os op l oq or">#Import scikit-learn metrics module for accuracy calculation <br/>from sklearn import metrics</span><span id="97de" class="no mm jg ok b gy os op l oq or"># Model Accuracy, how often is the classifier correct? print("Accuracy:",metrics.accuracy_score(y_test, y_pred))</span></pre><p id="0314" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出:<br/>精度:0.77777777</p><p id="4387" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经实现了一个大约77%精度的K最近邻分类器模型。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="ac8c" class="ml mm jg bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">如何提高KNN？</h1><p id="d31a" class="pw-post-body-paragraph kd ke jg kf b kg nj ki kj kk nk km kn ko nl kq kr ks nm ku kv kw nn ky kz la ij bi translated">为了获得更好的性能，可以从以下几个方面改进KNN。</p><ul class=""><li id="34b0" class="lq lr jg kf b kg kh kk kl ko ls ks lt kw lu la oi lw lx ly bi translated">建议对数据进行标准化(范围为0-1)并处理缺失值，因为它对噪声很敏感</li><li id="eb75" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la oi lw lx ly bi translated">使用加权距离代替非加权距离进行多数表决可以提高精确度</li><li id="1ff7" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la oi lw lx ly bi translated">降维更适合于高维数据，因为它最适合于低维数据</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="866e" class="ml mm jg bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">优点:</h1><ol class=""><li id="b4b3" class="lq lr jg kf b kg nj kk nk ko ou ks ov kw ow la lv lw lx ly bi translated">KNN易于理解、实施和解释</li><li id="0916" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">通过对模型进行微小的修改，KNN可用于分类和回归问题</li><li id="7ab9" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">不管数据的线性如何，KNN都是有用的</li><li id="9aac" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">由于其基于实例的学习，KNN在训练阶段比其他算法工作得更快</li></ol><h1 id="7ed5" class="ml mm jg bd mn mo ox mq mr ms oy mu mv mw oz my mz na pa nc nd ne pb ng nh ni bi translated">缺点:</h1><ol class=""><li id="320b" class="lq lr jg kf b kg nj kk nk ko ou ks ov kw ow la lv lw lx ly bi translated">在测试阶段，KNN比许多其他算法都要慢</li><li id="e88f" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">KNN在更高维度上工作效率不高</li><li id="1d16" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">由于时间和空间复杂度较高，KNN不适合处理海量数据</li><li id="d45c" class="lq lr jg kf b kg lz kk ma ko mb ks mc kw md la lv lw lx ly bi translated">KNN对异常值和噪音很敏感</li></ol><p id="9a41" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，KNN对于具有较小数据点的低维数据可能是有效的。在这篇文章之后，我将使用各种数据集为您的KNN模型确定最佳“K”。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="3fea" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谢谢你的阅读。以后我会写更多初学者友好的帖子。请在<a class="ae jd" href="https://medium.com/@ramyavidiyala" rel="noopener">媒体</a>上关注我，以便了解他们。我欢迎反馈，可以通过Twitter <a class="ae jd" href="https://twitter.com/ramya_vidiyala" rel="noopener ugc nofollow" target="_blank"> ramya_vidiyala </a>和LinkedIn <a class="ae jd" href="https://www.linkedin.com/in/ramya-vidiyala-308ba6139/" rel="noopener ugc nofollow" target="_blank"> RamyaVidiyala </a>联系我。快乐学习！</p><figure class="oc od oe of gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure></div></div>    
</body>
</html>