# 控制的诞生

> 原文：<https://medium.datadriveninvestor.com/the-birth-of-control-13291b9ba84d?source=collection_archive---------9----------------------->

## **当人工智能出错时会发生什么**

## **艾的伦理**

> **作者亚提什·拉姆霍利**
> 
> 改变和适应不是软弱。灵活性本身就是一种优势。事实上，这种灵活性与力量的结合将使我们变得坚韧和不可阻挡。~马库斯·奥勒留，《沉思》，8.16

![](img/9591a1459223487a4ccd85b718699764.png)

众所周知，人工智能是一个由数学算法、数学概率和统计数据组成的系统，旨在从训练数据中学习，以便能够在现实世界的应用中进行预测。但是，数学概率永远不可能完全准确，而且因为数学概率不可能 100%准确，所以有时必然会导致灾难性的失败。

尽管如果你的笔记本电脑崩溃或被黑客攻击可能只是一个小麻烦，但如果一个人工智能系统控制你的汽车、飞机、起搏器、自动交易系统或电网，那么它做你希望它做的事情就变得更加重要。另一个短期挑战是防止致命自主武器的毁灭性军备竞赛。

那么，当一个人工智能算法出现可怕的错误时，会发生什么？

最新的美国人口普查数据显示，黑人和西班牙裔人口在历史上一直处于弱势。要让 AI 学习，必须给它输入数据。如果数据显示某些人群被拒绝贷款的频率更高，它可能会错误地“认识到”这些人群的信贷风险更大，从而导致恶性循环。

[](https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/) [## 人工智能、深度学习和医疗实践|数据驱动的投资者

### 人工智能和深度神经学习的效用看起来可能是合法和有前途的，特别是…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/06/24/disclosure-and-resolution-program-wont-prevent-physicians-from-practicing-defensive-medicine/) 

另一个例子是，亚马逊在 2018 年放弃了一种招聘算法，因为它忽略了女性申请人，而偏向于男性申请技术职位。原因很简单——学习项目已经输入了过去的申请人和雇员的数据，其中大多数是男性。如果 AI 只考虑过去的数据，未来永远不会改变。

早在 2016 年春天，微软就陷入了一场公关噩梦，当时它的 Twitter 聊天机器人——一个名叫 Tay 的实验性人工智能角色——完全偏离了信息，并开始发表辱骂性言论甚至纳粹情绪。“希特勒是对的，”可怕的聊天机器人在推特上写道。还有:“911 是内鬼干的。”

公平地说，Tay 本质上是鹦鹉学舌，模仿其他(人类)用户故意试图激怒她的攻击性言论。该聊天机器人针对令人垂涎的 18 至 24 岁人群，旨在模仿千禧一代女性的语言模式，最初在多个社交媒体平台上发布。通过机器学习和自适应算法，Tay 可以通过处理输入的短语和融入其他相关数据来近似对话。唉，像今天的许多年轻人一样，Tay 发现自己与错误的人群混在一起。

3 月 28 日，在亚利桑那州坦佩市，一辆优步自动驾驶 SUV 撞死了一名女性行人，这是已知的第一起在公共道路上发生的与自动驾驶汽车相关的行人死亡事件。优步汽车处于自动模式，由一名人类安全驾驶员驾驶。

发生了什么事？优步发现，在汽车传感器检测到行人后，其自动驾驶软件决定不采取任何行动。根据美国国家运输安全委员会对事故的初步报告，优步的自动模式使沃尔沃工厂安装的自动紧急制动系统失效。

悲剧发生后，优步暂停了在北美城市的自动驾驶测试，英伟达和丰田也停止了在美国的自动驾驶道路测试。事故发生八个月后，优步宣布计划在匹兹堡恢复自动驾驶道路测试，尽管该公司的自动驾驶未来仍不确定。

无人驾驶汽车是保险行业最紧迫的人工智能相关考虑因素，谷歌、优步和沃尔沃等公司的最新进展使它们有可能在未来十年内主宰道路。今年 6 月，英国保险公司 Adrian Flux 开始提供第一份专门针对自动驾驶和部分自动驾驶汽车的保单。该政策涵盖了典型的汽车保险项目，如损坏、火灾和盗窃，以及特定于人工智能的事故——由于汽车无人驾驶系统故障、黑客侵入汽车操作系统的干扰、未能安装汽车软件更新和安全补丁、卫星故障或中断影响导航系统，或者制造商的汽车操作系统或其他授权软件故障而导致的损失或损坏。

这是向前迈出的重要一步，表明该行业终于开始处理这个问题。

# 可解释的人工智能

可解释的人工智能意味着询问一个人工智能应用程序为什么会做出这样的决定。国防部高级研究计划局(DARPA)是国防部的一个机构，目前正在致力于一个名为[可解释人工智能项目](https://www.darpa.mil/program/explainable-artificial-intelligence)的项目，以开发技术，使系统不仅能够解释他们的决策，还能洞察他们思维的强弱部分。可解释的 AI 帮助我们知道在多大程度上依赖结果，以及如何帮助 AI 改进。

可审计的人工智能要求第三方测试系统的思维，给它一系列不同的查询，并测量结果，以寻找非故意的偏见或其他有缺陷的思维。

人工智能先驱、前谷歌高管、斯坦福大学以人为中心的人工智能研究所联合主任费-李非认为，[另一种有助于消除偏见](https://www.wired.com/story/fei-fei-li-artificial-intelligence-humanity/)的方法，特别是在性别和种族歧视领域，是让更多的女性和有色人种参与开发人工智能系统。虽然这并不是说程序员在人工智能中实施偏见是错误的，但简单地让更广泛的人参与进来将消除无意识的倾向，并揭示被忽视的问题。

# 几个供我们大家考虑的问题

毫无疑问，人工智能已经对我们的生活产生了重大影响——很多时候我们甚至没有意识到这一点。对于它可能对你、你认识的人或你服务的人产生的影响，你有什么问题或顾虑？如果您的组织在决策过程中使用某种形式的人工智能，您将采取什么措施来确保偏见不会意外地蔓延到图片中？欢迎在下面的评论区分享你的想法。

**访问专家视图—** [**订阅 DDI 英特尔**](https://datadriveninvestor.com/ddi-intel)