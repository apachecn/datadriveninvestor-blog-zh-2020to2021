<html>
<head>
<title>t-SNE (T-distributed Stochastic Neighbourhood Embedding)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">T-SNE(T-分布式随机邻域嵌入)</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/t-sne-t-distributed-stochastic-neighbourhood-embedding-4112e9c1232f?source=collection_archive---------2-----------------------#2020-05-03">https://medium.datadriveninvestor.com/t-sne-t-distributed-stochastic-neighbourhood-embedding-4112e9c1232f?source=collection_archive---------2-----------------------#2020-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4b627d0b59b27204761737eafa5f8ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*heSgd1ePChL8NAXc"/></div></div></figure><p id="6e15" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还记得你上一次处理有高维数据的<strong class="ka ir">机器学习</strong>问题或任何常见的NLP任务时，你对文本进行了矢量化处理，得到了大维度。<strong class="ka ir">你刚刚做了什么？</strong>大多数人会说他们只是降低了维度。他们只是在谷歌上搜索了StackOverflow或机器学习大师的3-4行代码，然后在没有正确了解它的情况下使用了它。</p><p id="83ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是理解工作是<strong class="ka ir">专家做的事情。仅仅使用代码并不是每次都能奏效，尤其是当你要从事任何基于研究的项目时。</strong></p><p id="6abf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我的上一篇博客中，我写了降维及其必要性。我还解释了主成分分析，即降维的基本算法之一。所以这篇博客期望对降维有一个总体的了解，它的需求和所有的一切。</p><p id="8355" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">今天我将介绍<strong class="ka ir">T-分布式随机邻居嵌入(t-SNE) </strong>，这是一种最先进的降维算法。</p><p id="040b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但首先，让我们了解PCA和t-SNE之间的基本区别。</p><h1 id="efb2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">PCA和t-SNE的区别。</h1><p id="0a43" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">PCA是起源于1901年的一种相当基本和古老的技术。而SNE霸王龙是2008年才出现的新生事物。</p><p id="86dc" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">PCA的目的之一是降低你的维度，捕捉你的数据的全局结构。然而，从PCA目标函数中，我们知道PCA只能捕获特征中的线性结构。t-SNE算法以非常不同的方式工作，并且专注于在到低维数据的一些映射中保持高维数据的局部距离。</p><div class="ma mb gp gr mc md"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jw md"/></div></div></a></div><p id="3c82" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">PCA将无法找到这条非线性(实线)路径，但是如果我们只关注沿着最近的点，看起来您会找到实线所示的路径。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e5c1e8d6aa8b51bda8039219b899c619.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/0*qOZrfJ2ThUna5LDh"/></div></figure><p id="7d8c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以重要的是-</p><p id="cef5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">→ PCA试图<strong class="ka ir">保留数据的全局形状</strong>，而t-SNE则更注重保留数据的局部结构<strong class="ka ir"/>。</p><p id="689b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">→全局只是指任何图形的完整形状。而局部主要指主图中的小簇。</p><blockquote class="mx"><p id="18b0" class="my mz iq bd na nb nc nd ne nf ng kv dk translated">在讨论这个算法之前，我想你必须对t分布有所了解。</p></blockquote><h1 id="959d" class="kx ky iq bd kz la lb lc ld le lf lg lh li nh lk ll lm ni lo lp lq nj ls lt lu bi translated">什么是t分布？</h1><p id="82b7" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">也称为学生t分布，它与正态分布一样，是对称的钟形分布，但尾部较重，这意味着它更容易产生远离平均值的值。尾部的粗细由称为自由度的参数决定，较小的值表示尾部较重，较大的值使t分布看起来类似于均值为0、标准差为1的标准正态分布。</p><div class="mt mu mv mw gt ab cb"><figure class="nk jr nl nm nn no np paragraph-image"><img src="../Images/7dfde9602793ec69364a7567e1ac9aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*4scE5e8STt4Qiz6N.png"/></figure><figure class="nk jr nl nm nn no np paragraph-image"><img src="../Images/fd1da01c59f918a25bd91262cb716439.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*URnu0AKxT6KYNpF_.png"/><figcaption class="nq nr gj gh gi ns nt bd b be z dk nu di nv nw">Probability density and Cumulative distributed function for Student’s t</figcaption></figure></div><h1 id="edaa" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">什么是SNE霸王龙？</h1><p id="a05e" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><strong class="ka ir">T-分布式随机邻居嵌入(t-SNE) </strong>是由<a class="ae kw" href="https://en.wikipedia.org/w/index.php?title=Laurens_van_der_Maaten&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank"> Laurens van der Maaten </a>和<a class="ae kw" href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" rel="noopener ugc nofollow" target="_blank"> Geoffrey Hinton </a>开发的用于<a class="ae kw" href="https://en.wikipedia.org/wiki/Data_visualization" rel="noopener ugc nofollow" target="_blank">可视化</a>的<a class="ae kw" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法。这是一种<a class="ae kw" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">非线性降维</a>技术，非常适合在二维或三维的低维空间中嵌入用于可视化的高维数据。具体而言，它通过二维或三维点对每个高维对象进行建模，以这种方式，相似的对象通过附近的点进行建模，而不相似的对象通过远处的点以高概率进行建模。</p><p id="8131" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这就是维基百科页面上关于SNE霸王龙的内容。该算法有两个要点。</p><ol class=""><li id="dc19" class="nx ny iq ka b kb kc kf kg kj nz kn oa kr ob kv oc od oe of bi translated">这是一种非线性降维算法。</li><li id="6c59" class="nx ny iq ka b kb og kf oh kj oi kn oj kr ok kv oc od oe of bi translated">它的工作方式是，相似的对象由附近的点建模，不相似的对象由远处的点建模的概率很高。</li></ol><p id="21e8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以它的名字SNE霸王龙听起来可能有点太专业了。但是我们会很直观的理解。这是一种专门为可视化目的而构建的降维算法。并且在无监督学习中被大量使用。还有其他技术，如多维标度(MDS)、Sammon映射等。</p><h1 id="a385" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">-邻居，嵌入？</h1><p id="62f5" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><strong class="ka ir">邻域</strong>直观地说，<strong class="ka ir">点</strong>的<strong class="ka ir">邻域</strong>是包含那个<strong class="ka ir">点</strong>的一组<strong class="ka ir">点</strong>，在这里可以从那个<strong class="ka ir">点</strong>向任意<strong class="ka ir">方向移动一定量，而不会离开<strong class="ka ir">集合</strong>。在这里，您可以将集合引用到不同的集群，这些集群引用任何分类任务中的不同类。</strong></p><p id="e2f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">插入邻居的图像</p><p id="4f1c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">嵌入是一个特殊的术语，简单地说就是将输入投射到另一个更方便的表示空间。</p><p id="51f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">想象一下，我们在d维空间中有任何数据，我们只想将其可视化为2d。</p><p id="6b0c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">设d维中的点表示为x(i ),因此对于d维空间中的每个点x(i ),我们必须在2d空间中的x(i)虚线中找到它的对应点。这样的事情叫做嵌入。因此，基本上在高维空间中选取一个点，并将其放置在低维空间中，这就是所谓的嵌入。</p><h1 id="4e87" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">几何直觉</h1><p id="8285" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">该算法背后的几何直觉是，它专注于保持一个点的邻居的距离，而不在邻居中的点可以被放置在低维空间中的任何地方。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/2c50ca6f7bd8f0b57b3d8c3166c10885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVv8ihztLE2bYk-JoEgZsw.jpeg"/></div></div></figure><p id="79f7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如上图所示，我们可以看到两个局部聚类，t-SNE算法更注重保持局部聚类的距离。就像它保留了邻域1和邻域2中的点之间的<strong class="ka ir">距离</strong>，但是当一个点在第一个邻域中而另一个点在第二个邻域中时，它不会保留两个点之间的距离<strong class="ka ir">。</strong></p><p id="21d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="om">我们可以说它产生了邻域保持嵌入</em> </strong>。</p><h1 id="900a" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">拥挤问题</h1><p id="0e0f" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">因为SNE霸王龙试图保持邻居之间的距离，但这也可能产生问题。</p><blockquote class="mx"><p id="837b" class="my mz iq bd na nb nc nd ne nf ng kv dk translated">你自己先试着想想？？🤔</p></blockquote><p id="1e4a" class="pw-post-body-paragraph jy jz iq ka b kb on kd ke kf oo kh ki kj op kl km kn oq kp kq kr or kt ku kv ij bi translated">让我们尝试将一个二维集群映射到一维集群。把二维地图当成正方形的角。设正方形有a，b，c，d 4个角。当你试图保持邻里距离时。如果你为a-b，b-c，c-d这样做，那么你将不能为a-d保存它。这里出现了一个问题，叫做<strong class="ka ir">拥挤问题。</strong></p><p id="188a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更正式地说，当我们从高维映射到低维时，不可能保持所有的邻域距离，这被称为<strong class="ka ir">拥挤问题</strong></p><blockquote class="mx"><p id="70fd" class="my mz iq bd na nb nc nd ne nf ng kv dk translated">那么什么有助于解决拥挤问题呢？？</p></blockquote><p id="bd0e" class="pw-post-body-paragraph jy jz iq ka b kb on kd ke kf oo kh ki kj op kl km kn oq kp kq kr or kt ku kv ij bi translated">答案是<strong class="ka ir"> t </strong>。是的，<strong class="ka ir"> t分布函数</strong>。这有助于在创建嵌入时保持最大可能的邻域距离。它是如何工作的超出了这篇博文的范围。我会包括很多优化和统计操作。</p><h1 id="58c4" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">它是如何工作的？</h1><p id="780a" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">为了更直观地了解它的工作原理，我建议您访问一下这里的<a class="ae kw" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"/>并尝试不同的参数，熟悉它的工作原理。</p><p id="813e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">t-SNE是一种迭代算法，最终，它希望达到嵌入的最佳阶段，保留最大可能的距离。两个主要参数是<strong class="ka ir">步长</strong>和<strong class="ka ir">困惑度</strong>。</p><p id="c338" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">步长</strong>由于t-SNE是一种迭代算法，因此步长是控制最大迭代次数的参数。默认情况下，它是1000。当与SNE霸王龙一起工作时，你必须尝试几个值才能得到好的结果。</p><p id="b201" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">困惑度</strong>指的是您希望保持这些点之间距离的邻居的数量。也就是说，如果我们假设困惑度= 5，那么对于每个点，该算法将保持其前5个邻域点的距离，并将留下其他点。你必须在训练时尝试不同数量的困惑值，但是<strong class="ka ir">永远不会</strong>让它<strong class="ka ir">等于数据点的数量。</strong></p><h1 id="26ed" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">密码</h1><p id="93fc" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">TSNE是一种迭代算法，在给定的数据上进行训练需要相当长的时间。所以首先需要的是耐心。</p><p id="b103" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面给出的代码很好理解。你不能做的一件事是只尝试一个困惑和迭代值。你需要一次又一次地和它们一起玩，并把数据可视化，以充分利用SNE霸王龙。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="df68" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">n_components是指要减少的维数</p><h1 id="2931" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">结论</h1><p id="b033" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">t-SNE如此受欢迎是有原因的:它非常灵活，经常能找到其他降维算法找不到的结构。不幸的是，正是这种灵活性使得解释起来很棘手。在用户看不到的地方，算法会进行各种调整，整理其可视化效果。</p><blockquote class="mx"><p id="4aad" class="my mz iq bd na nb nc nd ne nf ng kv dk translated">希望你理解得很好😊</p></blockquote><p id="24f4" class="pw-post-body-paragraph jy jz iq ka b kb on kd ke kf oo kh ki kj op kl km kn oq kp kq kr or kt ku kv ij bi translated">在那之前，祝你学习愉快，保持安全！！</p><p id="876e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果你喜欢，请鼓掌👏👏。</p><p id="9c75" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">也请看看我以前的博文。</strong></p><p id="28e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://medium.com/analytics-vidhya/principal-component-analysis-for-dimensionality-reduction-432e718beed?source=your_stories_page---------------------------" rel="noopener"> <strong class="ka ir">主成分分析降维</strong> </a></p><p id="a2af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://medium.com/analytics-vidhya/yolo-object-detection-made-easy-7b17cc3e782f?source=your_stories_page---------------------------" rel="noopener"> <strong class="ka ir"> Yolo物体检测变得容易</strong> </a></p><p id="7a0e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://medium.com/@2017167/p-value-t-test-chi-square-test-anova-when-to-use-which-strategy-32907734aa0e?source=your_stories_page---------------------------" rel="noopener"> <strong class="ka ir"> P值、T检验、卡方检验、ANOVA，什么时候使用哪种策略？</strong> </a></p><p id="19e9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://medium.com/@2017167/understanding-hypothesis-testing-for-data-science-df952bbc1ef9?source=your_stories_page---------------------------" rel="noopener"> <strong class="ka ir">了解数据科学的假设检验</strong> </a></p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="ou ot l"/></div></figure></div></div>    
</body>
</html>