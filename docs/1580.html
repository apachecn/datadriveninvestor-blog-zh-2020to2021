<html>
<head>
<title>Artificial Neural Network in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的人工神经网络</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/artificial-neural-network-in-python-704fae2e23?source=collection_archive---------1-----------------------#2020-03-24">https://medium.datadriveninvestor.com/artificial-neural-network-in-python-704fae2e23?source=collection_archive---------1-----------------------#2020-03-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f7fccc481c7d88a9fab0fc297b85c6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-F6O7dgtohSUuaZd2IUXAA.png"/></div></div></figure><h1 id="d5c9" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是人工神经网络？</h1><p id="6f47" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">人工神经网络或连接主义系统是受构成动物大脑的生物神经网络启发但不完全相同的计算系统。这种系统通过考虑例子来“学习”执行任务，通常没有用特定于任务的规则来编程。</p><p id="aa67" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">人工神经网络是一组算法，大致模仿人脑，用于识别模式。他们通过一种机器感知、标记或聚类原始输入来解释感官数据。它们识别的模式是数字的，包含在向量中，所有现实世界的数据，无论是图像、声音、文本还是时间序列，都必须转换成向量。</p><p id="062b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在不进行大脑类比的情况下，我发现简单地将神经网络描述为将给定输入映射到期望输出的数学函数更容易。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/6b659fa8aaf4e000112bec972f38af42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*flQwZoO3IJyUeQRGlbZl8A.png"/></div></div></figure><p id="f1a6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">神经网络由以下组件组成</p><ul class=""><li id="5054" class="me mf iq ky b kz lu ld lv lh mg ll mh lp mi lt mj mk ml mm bi translated">一个<strong class="ky ir">输入层</strong>、<strong class="ky ir">T3、x</strong></li><li id="ed7b" class="me mf iq ky b kz mo ld mp lh mq ll mr lp ms lt mj mk ml mm bi translated">任意数量的<strong class="ky ir">隐藏层</strong></li><li id="bd21" class="me mf iq ky b kz mo ld mp lh mq ll mr lp ms lt mj mk ml mm bi translated">一个<strong class="ky ir">输出层</strong>，<strong class="ky ir">，<em class="mn"> ŷ </em>，</strong></li><li id="d6d5" class="me mf iq ky b kz mo ld mp lh mq ll mr lp ms lt mj mk ml mm bi translated">一组<strong class="ky ir">权重</strong>和<strong class="ky ir">在各层之间偏向</strong>，<strong class="ky ir"> <em class="mn"> W和b </em> </strong></li><li id="a1e9" class="me mf iq ky b kz mo ld mp lh mq ll mr lp ms lt mj mk ml mm bi translated">一个选择<strong class="ky ir">激活功能</strong>用于每个隐藏层，<strong class="ky ir"><em class="mn"/></strong>。在本教程中，我们将使用一个Sigmoid激活函数。</li></ul><p id="a633" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">下图显示了2层神经网络的架构(<em class="mn">注意，在计算神经网络的层数时，输入层通常被排除在外</em></p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/03805513a4d26eab170cc27d7f5e8a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bvGxacDBssy78fq0WNKt5w.png"/></div><figcaption class="mu mv gj gh gi mw mx bd b be z dk">Architecture of a 2-layer Neural Network</figcaption></figure><h1 id="abb5" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">用Python创建人工神经网络类很容易</h1><pre class="ma mb mc md gt my mz na nb aw nc bi"><span id="a7c6" class="nd jz iq mz b gy ne nf l ng nh">class NeuralNetwork:<br/>    def __init__(self, x, y):<br/>        self.input      = x<br/>        self.weights1   = np.random.rand(self.input.shape[1],4) <br/>        self.weights2   = np.random.rand(4,1)                 <br/>        self.y          = y<br/>        self.output     = np.zeros(y.shape)</span></pre><h1 id="678a" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">训练神经网络</h1><p id="603b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">简单2层神经网络的输出<strong class="ky ir"> <em class="mn"> ŷ </em> </strong>为:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/625deacf9fb4ea5aaabc3b1f8dfab0eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*Np25ZmEshapHPEKC.png"/></div></figure><p id="0d7f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">您可能会注意到，在上面的等式中，权重<strong class="ky ir"> <em class="mn"> W </em> </strong>和偏差<strong class="ky ir"> <em class="mn"> b </em> </strong>是影响输出<strong class="ky ir"> <em class="mn"> ŷ.的唯一变量</em>T45】</strong></p><p id="8375" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">自然，权重和偏差的正确值决定了预测的强度。从输入数据中微调权重和偏差的过程称为<strong class="ky ir">训练神经网络。</strong></p><p id="83d9" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">训练过程的每次迭代包括以下步骤:</p><ul class=""><li id="0c34" class="me mf iq ky b kz lu ld lv lh mg ll mh lp mi lt mj mk ml mm bi translated">计算预测输出<strong class="ky ir"> <em class="mn"> ŷ </em> </strong>，称为<strong class="ky ir">前馈</strong></li><li id="25e7" class="me mf iq ky b kz mo ld mp lh mq ll mr lp ms lt mj mk ml mm bi translated">更新权重和偏差，称为<strong class="ky ir">反向传播</strong></li></ul><p id="21d5" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">下面的序列图说明了这个过程。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/6c43d7f07ab59736d71411f313ea3158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifnAiRSABlhNlxEdosWXuw.png"/></div></div></figure><h1 id="b987" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">前馈</h1><p id="ccbd" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">正如我们在上面的序列图中看到的，前馈只是简单的微积分，对于一个基本的2层神经网络，神经网络的输出是:</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2c730c239571630ae3c7a766965c3015.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*c-3z3lw-qHGZ6-mN.png"/></div></figure><p id="38e8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们在python代码中添加一个前馈函数来做到这一点。注意，为了简单起见，我们假设偏差为0。</p><pre class="ma mb mc md gt my mz na nb aw nc bi"><span id="c43e" class="nd jz iq mz b gy ne nf l ng nh">class NeuralNetwork:<br/>    def __init__(self, x, y):<br/>        self.input      = x<br/>        self.weights1   = np.random.rand(self.input.shape[1],4) <br/>        self.weights2   = np.random.rand(4,1)                 <br/>        self.y          = y<br/>        self.output     = np.zeros(self.y.shape)</span><span id="17e3" class="nd jz iq mz b gy nk nf l ng nh">def feedforward(self):<br/>        self.layer1 = sigmoid(np.dot(self.input, self.weights1))<br/>        self.output = sigmoid(np.dot(self.layer1, self.weights2))</span></pre><p id="d012" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">然而，我们仍然需要一种方法来评估我们预测的“好性”(即我们的预测有多远)？损失函数允许我们这样做。</p><h1 id="f5a6" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">损失函数</h1><p id="9f7b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">有许多可用的损失函数，我们问题的性质决定了我们对损失函数的选择。在本教程中，我们将使用一个简单的<strong class="ky ir">平方和误差</strong>作为我们的损失函数。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c9fb527ffc5d3aa1f4f5a071348d0d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*fVOU8vtUPesGZeuT.png"/></div></figure><p id="fbd6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">也就是说，平方和误差就是每个预测值和实际值之间的差值之和。差值是平方的，因此我们可以测量差值的绝对值。</p><p id="1d3e" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">我们训练的目标是找到使损失函数最小化的最佳权重和偏差集。</strong></p><h1 id="0a19" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">反向传播</h1><p id="3e0b" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">既然我们已经测量了预测的误差(损失)，我们需要找到一种方法来<strong class="ky ir">传播</strong>误差回来，并更新我们的权重和偏差。</p><p id="bfe6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">为了知道调整权重和偏差的合适量，我们需要知道损失函数相对于权重和偏差的<strong class="ky ir">导数。</strong></p><p id="05cb" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">回想一下微积分，函数的导数就是函数的斜率。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/90854b6f029cf84f8c9c55b9f61138b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QvMJm1gOSLSV3uVr.png"/></div></div></figure><h1 id="427f" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">梯度下降算法</h1><p id="399f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如果我们有导数，我们可以简单地通过增加/减少来更新权重和偏差(参考上图)。这被称为<strong class="ky ir">梯度下降</strong>。</p><p id="3cdb" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">然而，我们不能直接计算损失函数关于权重和偏差的导数，因为损失函数的方程不包含权重和偏差。因此，我们需要<strong class="ky ir">链式法则</strong>来帮助我们计算。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/e729c7134320f4a638dc71d86db4b399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c5FizvxAichTfjBs.png"/></div></div></figure><p id="98b1" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">计算损失函数相对于权重的导数的链式法则。注意，为了简单起见，我们只显示了假设一层神经网络的偏导数。</p><p id="a6c3" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">唷！这很难看，但它让我们得到了我们需要的东西——损失函数相对于权重的导数(斜率)，这样我们就可以相应地调整权重。</p><p id="b19d" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在我们有了这个，让我们把反向传播函数添加到我们的python代码中。</p><pre class="ma mb mc md gt my mz na nb aw nc bi"><span id="5586" class="nd jz iq mz b gy ne nf l ng nh">class NeuralNetwork:<br/>    def __init__(self, x, y):<br/>        self.input      = x<br/>        self.weights1   = np.random.rand(self.input.shape[1],4) <br/>        self.weights2   = np.random.rand(4,1)                 <br/>        self.y          = y<br/>        self.output     = np.zeros(self.y.shape)</span><span id="3913" class="nd jz iq mz b gy nk nf l ng nh">def feedforward(self):<br/>        self.layer1 = sigmoid(np.dot(self.input, self.weights1))<br/>        self.output = sigmoid(np.dot(self.layer1, self.weights2))</span><span id="99c6" class="nd jz iq mz b gy nk nf l ng nh">def backprop(self):<br/>        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1<br/>        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))<br/>        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))</span><span id="ab4f" class="nd jz iq mz b gy nk nf l ng nh"># update the weights with the derivative (slope) of the loss function<br/>        self.weights1 += d_weights1<br/>        self.weights2 += d_weights2</span></pre><h1 id="eebe" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">把所有的放在一起</h1><p id="6f20" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">现在我们已经有了完整的python代码来进行前馈和反向传播，让我们将我们的神经网络应用于一个例子，看看它做得有多好。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f06829221394c445b89452d55dfee967.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*zZwt6DvOpoTxzFuk.png"/></div></figure><p id="d315" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们的神经网络应该学习一组理想的权重来表示这个函数。请注意，仅仅通过检查来计算重量对我们来说并不简单。</p><p id="6f87" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们训练神经网络1500次迭代，看看会发生什么。查看下面的每次迭代损失图，我们可以清楚地看到损失<strong class="ky ir">朝着最小值单调递减。</strong>这与我们之前讨论的梯度下降算法一致。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9078104471cf8b2ec849ec43c252eab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/0*ysj1HiJ15S24BQGt.png"/></div></figure><p id="a4f4" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">让我们看看神经网络经过1500次迭代后的最终预测(输出)。</p><figure class="ma mb mc md gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/45cad04a24130546eef66064e2925eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*fgxmNwB6-d0flTo8.png"/></div></figure><p id="39b8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">1500次训练迭代后的预测</p><p id="03b2" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们做到了！我们的前馈和反向传播算法成功地训练了神经网络，并且预测收敛于真实值。</p><div class="nr ns gp gr nt nu"><a href="https://www.datadriveninvestor.com/2020/03/04/on-artificial-intelligence-and-surveillance-capitalism/" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">人工智能和监督资本主义|数据驱动的投资者</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">大科技，总是现在:人工智能推动的大科技，已经使购物，搜索，在你的…</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="od l"><div class="oe l of og oh od oi jw nu"/></div></div></a></div><p id="65b7" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">请注意，预测值和实际值之间略有不同。这是可取的，因为它防止<strong class="ky ir">过度拟合</strong>，并允许神经网络<strong class="ky ir">更好地将</strong>推广到看不见的数据。</p><h1 id="926a" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">最终想法</h1><figure class="ma mb mc md gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/157020ab5cb10ad2007faa068d14cb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvhgF5zduk8HWJmz8YHJKg.jpeg"/></div></div></figure><p id="922c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">尽管TensorFlow和Keras等深度学习库可以在不完全了解神经网络内部工作原理的情况下轻松构建深度网络，但我发现，对有抱负的数据科学家来说，更深入地了解神经网络是有益的。</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="ok ol l"/></div></figure></div></div>    
</body>
</html>