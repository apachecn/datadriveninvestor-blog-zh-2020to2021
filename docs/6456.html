<html>
<head>
<title>Image manipulation detection using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的图像操纵检测</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/image-manipulation-detection-using-deep-learning-dedcb7a84d06?source=collection_archive---------2-----------------------#2020-10-27">https://medium.datadriveninvestor.com/image-manipulation-detection-using-deep-learning-dedcb7a84d06?source=collection_archive---------2-----------------------#2020-10-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/298b3d83ad5ae02bb422ca34562e2524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onbwHeN-Y1Y9O3HuEPLnzw.jpeg"/></div></div></figure><h1 id="617b" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">我们试图解决的问题是什么？</h1><p id="2db3" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这个软件功能强大的时代，我们可以轻松地编辑任何图像。最强大的软件之一是Adobe photoshop，使用它我们可以轻松地改变、裁剪、删除或对图像的任何部分做更多的事情。</p><p id="ecca" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">这些图像可能是假新闻的一部分，这是当今各大社交媒体平台的一个主要问题。这个问题也存在于使用容易操作的文档的各种领域中。</p><p id="bb62" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">IEEE信息取证与安全技术委员会(IFS-TC)发起了检测和定位取证挑战赛，这是2013年第一个解决这一问题的图像取证挑战赛。</p><p id="6277" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">关于该问题的进一步信息:</p><p id="18cd" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><a class="ae lz" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/ Zhou_Learning_Rich_Features_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ma">研究论文1</em></strong>T5】</a></p><p id="7ac1" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><a class="ae lz" href="https://arxiv.org/pdf/1903.02495.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ma">研究论文2 </em> </strong> </a></p><p id="13d3" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><a class="ae lz" href="https://signalprocessingsociety.org/newsletter/2013/06/ifs-tc-image-forensics-challenge" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ma"> IEEE挑战赛</em> </strong> </a></p><h1 id="8727" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="a43d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为了解决这个问题，首先我们将建立一个模型来检测图像是真实的还是被篡改的。如果图像被操纵，那么我们将尝试预测图像的操纵区域。</p><p id="3438" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><em class="ma">处理图像的不同方式:</em></p><ol class=""><li id="b9ae" class="mb mc iq ky b kz lu ld lv lh md ll me lp mf lt mg mh mi mj bi translated">图像拼接:从真实图像中复制区域并粘贴到其他图像中。</li><li id="9e22" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">复制-移动:在同一个图像中复制和粘贴区域。</li><li id="5bf7" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt mg mh mi mj bi translated">去除:从真实图像中去除区域，然后进行修复。</li></ol><h1 id="cfbf" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">关于数据集</h1><ul class=""><li id="a6a0" class="mb mc iq ky b kz la ld le lh mp ll mq lp mr lt ms mh mi mj bi translated"><em class="ma">对于分类任务，在CASIA2和IEE IFS-TC数据集上训练模型。</em></li><li id="118f" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><em class="ma">对于掩码预测，仅使用IEEE IFS-TC数据集来训练模型。</em></li></ul><p id="6a04" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> CASIA2 </strong>:包含7408张真实图像和5123张虚假图像。</p><p id="46a5" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">IEEE IFS-TC数据集:它包含1050个真实的450个处理过的图像以及它们的450个遮罩。</p><p id="5fc8" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">伪图像的遮罩是描述伪图像的拼接区域的黑白(非灰度)图像。蒙版中的黑色像素代表在源图像中执行操作以获得伪造图像的区域，具体来说，它代表拼接区域。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/7fdbeaf5ed3a4985a79545ef394dca7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDR8tuu_gUYepO28xA_oKg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Fake image with corresponding mask</figcaption></figure><h1 id="3beb" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">体系结构</h1><p id="050a" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我们将在分类和掩码预测任务中使用迁移学习，因为这种技术在我的实验中给出了最好的结果。</p><p id="d6a3" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><em class="ma">分类模型</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/cc2b1c7c2cf09617c726350e68da2a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*Jk7ula0ym31LlRmxp3HPCA.png"/></div></figure><p id="8813" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><em class="ma">掩模预测模型</em></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/2326b8e0b644b16ab37b89f1260648e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Be-Z4DULlPQLozZfbwflQ.png"/></div></div></figure><blockquote class="ne nf ng"><p id="e6bb" class="kw kx ma ky b kz lu lb lc ld lv lf lg nh lw lj lk ni lx ln lo nj ly lr ls lt ij bi translated"><strong class="ky ir"><em class="iq">【UNET】</em></strong><a class="ae lz" href="https://segmentation-models.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="iq">分割模型库</em> </strong> </a> <strong class="ky ir"> <em class="iq">与不同的骨干模型一起用于此任务</em> </strong></p></blockquote><h1 id="a19e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">关于遮罩预测模型中使用的过滤器</h1><p id="8b18" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="ma"> SRM滤波器:</em> SRM特征收集基本噪声特征。SRM对这些滤波器的输出进行量化和截断，并提取附近的同现信息作为最终特征。从该过程获得的特征可以被视为局部噪声描述符。我们直接使用噪声特征作为噪声流网络的输入。噪声流的骨干卷积网络架构与RGB流相同。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d940972e58f1809a475c294be7437cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*p7rjreA8uHQBkNnHhV46wg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The three SRM filter kernels used to extract noise features.</figcaption></figure><p id="cd4c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><em class="ma"> ELA滤波器:误差等级分析(ELA)允许识别图像中处于不同压缩等级的区域。对于JPEG图像，整个画面应该处于大致相同的水平。如果图像的一部分处于显著不同的误差水平，那么它可能指示数字修改。</em></p><p id="c3af" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><em class="ma"> ELA强调了JPEG压缩率的差异。与高对比度边缘相比，具有均匀颜色的区域(如纯蓝天空或白墙)可能具有较低的ELA结果(较暗的颜色)。</em></p><p id="8a4a" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">查看图片，识别不同的高对比度边缘、低对比度边缘、表面和纹理。将这些区域与ELA的结果进行比较。如果有显著的差异，那么它识别可能已经被数字改变的可疑区域。</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/602d09ec6075fff9a40890f8f45c05d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNAkHcg99GIf86vVm2OT5w.png"/></div></div></figure><blockquote class="nm"><p id="28fa" class="nn no iq bd np nq nr ns nt nu nv lt dk translated">从实验中，我发现ELA滤波效果更好，所以它被用于最终的掩模预测模型</p></blockquote></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="f7ac" class="jy jz iq bd ka kb od kd ke kf oe kh ki kj of kl km kn og kp kq kr oh kt ku kv bi translated">探索性数据分析</h1><ul class=""><li id="f15a" class="mb mc iq ky b kz la ld le lh mp ll mq lp mr lt ms mh mi mj bi translated"><em class="ma">关于CASIA2数据集</em></li></ul><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/0ccb880df88e4d67396377edde17e6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgxrgXgA65aG7mcUzSdciw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Information about Authentic images in CASIA2 dataset</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/258d94f4d9cf1d0c571e513447669404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfap5tXS_DJa8yU1a56Aig.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Information about Manipulated images in CASIA2 dataset</figcaption></figure><ul class=""><li id="238b" class="mb mc iq ky b kz lu ld lv lh md ll me lp mf lt ms mh mi mj bi translated">关于<em class="ma"> IEEE IFS-TC图像取证挑战数据集</em></li></ul><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/505286f8d22162a164257934c6276741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sdO5hF04cbVVAHag3AaRg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Information about Authentic images in IEEE dataset</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/a7f241fdc14ffdfac4193b90f79419b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKQvxcduEcKGWvFHSHGBCA.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Information about Manipulated images in IEEE dataset</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/dc0303b2250107804fbc6076ce67616f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LroIalZYtetwmwbycbQjhg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Information about Masks of manipulated images in IEEE dataset</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ad5ea996943bc325ecc7f49be096601e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*tbkoa9fSfVzPat99sXSRGw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Distribution of images in IEEE dataset</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/41b8ed04aff54839eef9ccedfb4fc632.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Ua4X3SHb2fsOwAS0elTBpA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Distribution of channels in authentic images</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/363c3a43784f3dcb92d318d8bf73ed17.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*A2mRbAqAxP0KeVUm0dqCqg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Distribution of channels in fake images</figcaption></figure><ul class=""><li id="f102" class="mb mc iq ky b kz lu ld lv lh md ll me lp mf lt ms mh mi mj bi translated">我们可以看到一些4通道图像也存在于真实和伪造的图像中。</li></ul><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/48578f0c6a625e56b9ee804ce4aaee2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*_0YJKfWOpjURoCk0MTA6qg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Image height distribution of Authentic images</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/97e431a3845d6a9d67aecbd68d2183a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*0SLMLYODiCX0pFOOPPTiRA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Image height distribution of Fake images</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/02a46ddcfd59ff3ebdfd0e3a7e23c3e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*dCS3lvUSHc11EzvZN5bJrA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Image width distribution of Authentic images</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f0d8587df9ea012d96527b3855231dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*ICDvexPYiLiATJfiT5GDcQ.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Image width distribution of Fake images</figcaption></figure><ul class=""><li id="3025" class="mb mc iq ky b kz lu ld lv lh md ll me lp mf lt ms mh mi mj bi translated">假图像比原始图像具有更多高度更大的图像。</li><li id="aacd" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated">几乎所有的原始图像都具有相同的宽度，而伪图像也具有接近类似范围的高密度。</li></ul></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><blockquote class="ne nf ng"><p id="5863" class="kw kx ma ky b kz lu lb lc ld lv lf lg nh lw lj lk ni lx ln lo nj ly lr ls lt ij bi translated">我在两个不同的笔记本上做了不同的实验，在所有的实验之后，我从这两个笔记本中选择了最好的模型。这些实验可以在以下链接中找到:</p></blockquote><ul class=""><li id="12e8" class="mb mc iq ky b kz lu ld lv lh md ll me lp mf lt ms mh mi mj bi translated"><a class="ae lz" href="https://github.com/sankalp-chawla/Image-manipulation-detection-using-Deep-Learning/blob/master/Experiment1_Project2.pdf" rel="noopener ugc nofollow" target="_blank">笔记本1 </a></li><li id="5bf2" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://github.com/sankalp-chawla/Image-manipulation-detection-using-Deep-Learning/blob/master/Experiment2_Project2.pdf" rel="noopener ugc nofollow" target="_blank">笔记本2 </a></li></ul><blockquote class="ne nf ng"><p id="b049" class="kw kx ma ky b kz lu lb lc ld lv lf lg nh lw lj lk ni lx ln lo nj ly lr ls lt ij bi translated">我添加了图像放大以及假图像，因为图像的数量非常少，以训练一个神经网络。</p></blockquote><pre class="mu mv mw mx gt om on oo op aw oq bi"><span id="4ae3" class="or jz iq on b gy os ot l ou ov">#Resizing and augmenting images</span><span id="0ebc" class="or jz iq on b gy ow ot l ou ov">def agument(aug,image,mask,ela):</span><span id="2f97" class="or jz iq on b gy ow ot l ou ov">augmented = aug(image=image, mask=mask,ela=ela)</span><span id="0c2c" class="or jz iq on b gy ow ot l ou ov">return augmented['image'],augmented['mask'],augmented['ela']</span><span id="bda6" class="or jz iq on b gy ow ot l ou ov">def resize(image,mask,ela):</span><span id="07d5" class="or jz iq on b gy ow ot l ou ov">aug = Resize(height=256,width=256,p=1)</span><span id="ac10" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="0387" class="or jz iq on b gy ow ot l ou ov">def horizontalFlip(image,mask,ela):</span><span id="e7bb" class="or jz iq on b gy ow ot l ou ov">aug = HorizontalFlip(p=1)</span><span id="b29b" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="db02" class="or jz iq on b gy ow ot l ou ov">def verticalFlip(image,mask,ela):</span><span id="c12c" class="or jz iq on b gy ow ot l ou ov">aug = VerticalFlip(p=1)</span><span id="fd21" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="2c9d" class="or jz iq on b gy ow ot l ou ov">def transpose(image,mask,ela):</span><span id="3d69" class="or jz iq on b gy ow ot l ou ov">aug = Transpose(p=1)</span><span id="41dc" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="509b" class="or jz iq on b gy ow ot l ou ov">def hueSaturationValue(image,mask,ela):</span><span id="59ab" class="or jz iq on b gy ow ot l ou ov">aug = HueSaturationValue(p=1,hue_shift_limit=100, sat_shift_limit=100, val_shift_limit=50)</span><span id="7465" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="bb4e" class="or jz iq on b gy ow ot l ou ov">def elasticTransform(image,mask,ela):</span><span id="d2a8" class="or jz iq on b gy ow ot l ou ov">aug = ElasticTransform(p=1)</span><span id="c11d" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="b705" class="or jz iq on b gy ow ot l ou ov">def opticalDistortion(image,mask,ela):</span><span id="4923" class="or jz iq on b gy ow ot l ou ov">aug = OpticalDistortion(p=1, distort_limit=3, shift_limit=0.4)</span><span id="443d" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span><span id="8c69" class="or jz iq on b gy ow ot l ou ov">def randomBrightnessContrast(image,mask,ela):</span><span id="5d51" class="or jz iq on b gy ow ot l ou ov">aug =RandomBrightnessContrast(p=1,brightness_limit=0.5, contrast_limit=0.4)</span><span id="cdd8" class="or jz iq on b gy ow ot l ou ov">return agument(aug,image,mask,ela)</span></pre><blockquote class="ne nf ng"><p id="7973" class="kw kx ma ky b kz lu lb lc ld lv lf lg nh lw lj lk ni lx ln lo nj ly lr ls lt ij bi translated">我还在蒙版上使用高斯模糊来平滑边缘。在图像处理中，高斯模糊是通过高斯函数模糊图像的结果。这是图形软件中广泛使用的效果，通常用于减少图像噪声和细节。</p></blockquote><pre class="mu mv mw mx gt om on oo op aw oq bi"><span id="00cb" class="or jz iq on b gy os ot l ou ov">#Adding gaussian blur to masks to reduce noise and converting to single channel</span><span id="90f0" class="or jz iq on b gy ow ot l ou ov">os.mkdir(base_path+'binary_masks')</span><span id="6c64" class="or jz iq on b gy ow ot l ou ov">bin_masks =[]</span><span id="1baf" class="or jz iq on b gy ow ot l ou ov">for mask in tqdm.tqdm(fake_names_intersection,position=0, leave=True):</span><span id="09fd" class="or jz iq on b gy ow ot l ou ov">mask_img = cv2.imread(base_path+'fake'+'/'+mask+'.mask.png')[:,:,:1]</span><span id="6ab2" class="or jz iq on b gy ow ot l ou ov">blur = cv2.GaussianBlur(mask_img,(5,5),0) #Adding gaussian blur</span><span id="4df6" class="or jz iq on b gy ow ot l ou ov">ret,bin_mask = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) #modifing the values to either 0 or 255</span><span id="ba99" class="or jz iq on b gy ow ot l ou ov">cv2.imwrite(base_path+'binary_masks/'+mask+'.mask.png',bin_mask)</span><span id="4c7f" class="or jz iq on b gy ow ot l ou ov">bin_masks.append(bin_mask)</span></pre><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ox"><img src="../Images/fcc03e22ac5f4a2067c85645085b1a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByHymHEFHUVBDcWhVikWLQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Manipulated image vs Image mask without Gaussian Blur vs Image mask with Gaussian Blur</figcaption></figure></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="e008" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> <em class="ma">笔记本1中的分类型号</em> </strong></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oy"><img src="../Images/498947f9e3bb779a89ae4c37b6a0a992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-J8w_KTmJouxMd3ZYm5fg.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Classification models in Notebook 1</figcaption></figure><p id="b815" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> <em class="ma">笔记本1中的掩码预测模型</em> </strong></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/916e6554f3153b59a90f79303be35a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dx_8kO-B4IF1dk6mxs1vGQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Mask prediction models in Notebook 1</figcaption></figure><p id="fb3f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> <em class="ma">笔记本2中的分类型号</em> </strong></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pa"><img src="../Images/bfba858424d0d4843c0c5a170cce9754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vuj2e1z26uuZ0adb-zhLWQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><strong class="bd ka"><em class="pb">Classification models in Notebook 2</em></strong></figcaption></figure><p id="9ef0" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> <em class="ma">笔记本2中的掩码预测模型</em> </strong></p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pc"><img src="../Images/625355642192f0215f0f1b77cc4b84a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50EUYNw80Ls5DQHh-C1Pjw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><strong class="bd ka"><em class="pb">Mask Prediction models in Notebook 2</em></strong></figcaption></figure><h1 id="8281" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">最佳模特</strong></h1><blockquote class="nm"><p id="d350" class="nn no iq bd np nq pd pe pf pg ph lt dk translated"><strong class="ak">用于分类:ResNet50与imagenet权重一起使用，并在来自CASIA2数据集以及来自IEEE IFS-TC图像取证挑战的数据集的图像上进行训练。</strong></p><p id="107f" class="nn no iq bd np nq pd pe pf pg ph lt dk translated">对于掩模预测:具有图像网权重的Resnet101高斯模糊+来自两个流的串联输出+ ELA滤波+在IEEE IFS-TC图像取证挑战数据集上训练的增强</p></blockquote><h1 id="5852" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj pi kl km kn pj kp kq kr pk kt ku kv bi translated"><strong class="ak">使用的公制</strong></h1><p id="5971" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">F值(Dice系数)可以解释为精确度和召回率的加权平均值，其中F值在1时达到最佳值，在0时达到最差值。<code class="fe pl pm pn on b"><strong class="ky ir">precision</strong></code>和<code class="fe pl pm pn on b"><strong class="ky ir">recall</strong></code>对F1分数的相对贡献相等。F分数的公式是:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi po"><img src="../Images/2837c37906c62326eb0828e26602042a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wmm9XmHxbHcTeRwqQ40yXA.png"/></div></figure><h1 id="657c" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">构建分类模型</h1><pre class="mu mv mw mx gt om on oo op aw oq bi"><span id="a980" class="or jz iq on b gy os ot l ou ov">#Method to generate ELA of images</span><span id="cb91" class="or jz iq on b gy ow ot l ou ov">def generate_ela(path,quality):</span><span id="1986" class="or jz iq on b gy ow ot l ou ov">temp_file = 'temp_file.jpg'</span><span id="ef14" class="or jz iq on b gy ow ot l ou ov">image = Image.open(path).convert('RGB')</span><span id="762a" class="or jz iq on b gy ow ot l ou ov">image.save(temp_file, 'JPEG', quality = quality)</span><span id="591d" class="or jz iq on b gy ow ot l ou ov">temp_image = Image.open(temp_file)</span><span id="df00" class="or jz iq on b gy ow ot l ou ov">ela_img = ImageChops.difference(image, temp_image)</span><span id="b622" class="or jz iq on b gy ow ot l ou ov">extrema = ela_img.getextrema()</span><span id="0229" class="or jz iq on b gy ow ot l ou ov">max_diff = max([ex[1] for ex in extrema])</span><span id="397d" class="or jz iq on b gy ow ot l ou ov">if max_diff == 0:</span><span id="9a8e" class="or jz iq on b gy ow ot l ou ov">max_diff = 1</span><span id="cc57" class="or jz iq on b gy ow ot l ou ov">scale = 255.0 / max_diff</span><span id="03a5" class="or jz iq on b gy ow ot l ou ov">ela_img = ImageEnhance.Brightness(ela_img).enhance(scale)</span><span id="8368" class="or jz iq on b gy ow ot l ou ov">return ela_img</span><span id="cae8" class="or jz iq on b gy ow ot l ou ov">model2 = Sequential()</span><span id="b506" class="or jz iq on b gy ow ot l ou ov">model2.add(ResNet50(include_top = False, pooling = 'avg', weights = 'imagenet'))</span><span id="3972" class="or jz iq on b gy ow ot l ou ov">model2.add(Dense(256, activation = 'relu'))</span><span id="d157" class="or jz iq on b gy ow ot l ou ov">model2.add(Dropout(0.1))</span><span id="6392" class="or jz iq on b gy ow ot l ou ov">model2.add(Dense(128, activation='relu'))</span><span id="4129" class="or jz iq on b gy ow ot l ou ov">model2.add(Dense(64, activation='relu'))</span><span id="60d6" class="or jz iq on b gy ow ot l ou ov">model2.add(Dropout(0.2))</span><span id="907a" class="or jz iq on b gy ow ot l ou ov">model2.add(Dense(32, activation='relu'))</span><span id="b503" class="or jz iq on b gy ow ot l ou ov">model2.add(Dropout(0.3))</span><span id="d57c" class="or jz iq on b gy ow ot l ou ov">model2.add(Dense(2, activation = 'softmax'))</span></pre><p id="9f33" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">结果:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/9ae737da025f88a85e148a49e1df33ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*OkrO-CrGo5oRWVyK6_u3yA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Results for Classification model</figcaption></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/f5ce142a2f8132e209d5ad8808df117a.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*04lO-gwo9h0Y52dn0_H9xg.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Results for Classification model</figcaption></figure><h1 id="8a00" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">建立掩模预测模型</h1><pre class="mu mv mw mx gt om on oo op aw oq bi"><span id="5f62" class="or jz iq on b gy os ot l ou ov">path_img = Unet(backbone_name='resnet101', encoder_weights='imagenet', activation='sigmoid',classes=3,input_shape=(256,256,3),decoder_use_batchnorm=True)</span><span id="abb3" class="or jz iq on b gy ow ot l ou ov">path_img._name = 'path_1'</span><span id="dc8b" class="or jz iq on b gy ow ot l ou ov">out1 = Conv2D(3,(1,1), activation='sigmoid')(path_img.output)</span><span id="a297" class="or jz iq on b gy ow ot l ou ov">path_filter = Unet(backbone_name='resnet101', encoder_weights='imagenet', activation='sigmoid',classes=3,input_shape=(256,256,3),decoder_use_batchnorm=True)</span><span id="bfa3" class="or jz iq on b gy ow ot l ou ov">path_filter._name = 'path_2'</span><span id="7685" class="or jz iq on b gy ow ot l ou ov">out2 = Conv2D(3,(1,1), activation='sigmoid')(path_filter.output)<br/></span><span id="f05c" class="or jz iq on b gy ow ot l ou ov">for layer in path_img.layers:</span><span id="7b54" class="or jz iq on b gy ow ot l ou ov">layer._name = layer.name + str("_img")</span><span id="7abd" class="or jz iq on b gy ow ot l ou ov">combined = concatenate([out1, out2])</span><span id="c0ec" class="or jz iq on b gy ow ot l ou ov">final = Conv2D(1,(1,1),activation='sigmoid')(combined)</span><span id="ac6f" class="or jz iq on b gy ow ot l ou ov">model3 = Model(inputs=[path_img.input,path_filter.input], outputs=[final])</span><span id="05ec" class="or jz iq on b gy ow ot l ou ov">metrics = [metric]</span><span id="62f9" class="or jz iq on b gy ow ot l ou ov">log_dir = "logs_mask3/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span><span id="f73b" class="or jz iq on b gy ow ot l ou ov">tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)</span><span id="05a9" class="or jz iq on b gy ow ot l ou ov">early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)</span><span id="24f0" class="or jz iq on b gy ow ot l ou ov">reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.22, patience = 1, verbose = 1, min_delta = 0.0001)</span><span id="9f77" class="or jz iq on b gy ow ot l ou ov">model3.compile(tf.keras.optimizers.Adam(0.0001), 'binary_crossentropy',metrics)</span><span id="1eaa" class="or jz iq on b gy ow ot l ou ov">model3.fit([X_train1,X_train2], [Y_train],validation_data=([X_val1,X_val2], [Y_val]),epochs=10, batch_size=1,callbacks=[reduce_lr,early_stop,tensorboard_callback],verbose=1)</span></pre><p id="e93c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">结果:</p><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pr"><img src="../Images/330165d18a8f033a14be85a6400c090a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNHFYz4C3GnKDFsnmyZf0A.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Mask prediction results</figcaption></figure><h1 id="4533" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">未来作品</h1><ul class=""><li id="1e34" class="mb mc iq ky b kz la ld le lh mp ll mq lp mr lt ms mh mi mj bi translated">由于计算的限制，我不能添加更多的增强，我们可以添加更多的图像增强，以进一步改善。</li><li id="10c3" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated">也可以使用具有3个输入的更复杂的网络:伪图像+ ELA滤波图像+ SRM滤波图像。</li><li id="ecf6" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated">许多新的研究论文也可以参考，以调整架构和进一步提高模型的准确性。</li></ul><h1 id="a2ab" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">参考</h1><ul class=""><li id="f173" class="mb mc iq ky b kz la ld le lh mp ll mq lp mr lt ms mh mi mj bi translated"><a class="ae lz" href="https://gist.github.com/ewencp/3356622" rel="noopener ugc nofollow" target="_blank">参考文献1 </a></li><li id="e495" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://github.com/shurain/ela/blob/master/ela.py" rel="noopener ugc nofollow" target="_blank">参考2 </a></li><li id="c2b3" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://pypi.org/project/imageio/" rel="noopener ugc nofollow" target="_blank">参考文献3 </a></li><li id="bb2f" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://towardsdatascience.com/reshaping-numpy-arrays-in-python-a-step-by-step-pictorial-tutorial-aed5f471cf0b" rel="noopener" target="_blank">参考文献4 </a></li><li id="fa0a" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://stackoverflow.com/questions/48991077/reshape-1d-array-to-3d-array-numpy" rel="noopener ugc nofollow" target="_blank">参考文件5 </a></li><li id="5fa6" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://github.com/Zulko/moviepy/issues/219" rel="noopener ugc nofollow" target="_blank">参考文献6 </a></li><li id="08a8" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2#:~:text=3.-,Dice%20Coefficient%20(F1%20Score),of%20union%20in%20section%202" rel="noopener" target="_blank">参考文献7 </a></li><li id="d262" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://gist.github.com/cirocosta/33c758ad77e6e6531392" rel="noopener ugc nofollow" target="_blank">参考文献8 </a></li><li id="bc21" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://www.kaggle.com/alexanderliao/image-augmentation-demo-with-albumentation" rel="noopener ugc nofollow" target="_blank">参考文献9 </a></li><li id="1361" class="mb mc iq ky b kz mk ld ml lh mm ll mn lp mo lt ms mh mi mj bi translated"><a class="ae lz" href="https://towardsdatascience.com/image-forgery-detection-2ee6f1a65442" rel="noopener" target="_blank">参考文献10 </a></li></ul><p id="094f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我已经将我所有的笔记本上传到我的<a class="ae lz" href="https://github.com/sankalp-chawla/Image-manipulation-detection-using-Deep-Learning" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> Github资源库</em> </a> <em class="ma"> </em>中，你可以在这些笔记本中找到很多关于所有方法的深入分析。</p><p id="e0d6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我已经尽力包含尽可能多的信息。请分享您的宝贵反馈。</p><p id="5cec" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">也可以通过<a class="ae lz" href="https://www.linkedin.com/in/sankalp-chawla-22996484/" rel="noopener ugc nofollow" target="_blank"><em class="ma">LinkedIn</em></a><em class="ma">联系我。</em></p><p id="0d75" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">感谢阅读！！！</p></div></div>    
</body>
</html>