<html>
<head>
<title>Getting Started with Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习入门</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/getting-started-with-deep-reinforcement-learning-5e973fec1e28?source=collection_archive---------0-----------------------#2020-04-26">https://medium.datadriveninvestor.com/getting-started-with-deep-reinforcement-learning-5e973fec1e28?source=collection_archive---------0-----------------------#2020-04-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="dc54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你刚刚开始深度强化学习(或正在考虑)，这里有一个快速提示，告诉你要注意什么，以及如何最好地度过这个过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/34776e7038c2937fb2ee5533e86b31b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kJUNBkCg1tZjJ8q-"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Receiving a reward is always nice. Especially if you’re a Reinforcement Learning agent. (Photo by <a class="ae le" href="https://unsplash.com/@freestocks?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">freestocks</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a>)</figcaption></figure><p id="e835" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如在计算机科学中经常出现的那样，对一个主题有两种主要的方法。科学家的方法是试图通过熟悉基础理论来深入理解问题，而工程师的方法是:找出哪些库做了你想做的事情，并把它们投入使用。</p><p id="f5e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在不要误解我的意思，这两种方法本身都不是“最好的”。事实上，在你可能发现自己所处的每一个场景中，这两者都是不可能的。然而对我们来说不幸的是，在DRL唯一可行的方法更难:像科学家一样去做。</p><h1 id="8106" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">该理论</h1><p id="2d1b" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated"><em class="lf">我真的需要— </em>是的！你知道。虽然你会在网上找到很多关于如何编写自己的DRL代理的教程，但是这些教程非常有限。他们讨论了具有特定网络架构的单个RL算法，用于解决单个(并且在大多数情况下相当琐碎)问题。</p><p id="a466" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">去吧，给他们一个机会！实际上，它们工作得很好。但是一旦你试图将它们归纳到你自己的问题中，如果你碰壁了，不要感到惊讶。</p><p id="9df8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实是，你需要明白你在做什么才能让DRL运转起来。如果你不知道什么是回报和回报，以及它们如何影响你的代理人，你就迷失了。不知道TD(λ)是什么？最好弄清楚。因为一旦你坐在那里，看着你的经纪人在每一个训练步骤中变得越来越差，你就需要知道如何解释正在发生的事情以便解决它。</p><div class="mj mk gp gr ml mm"><a href="https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">AI和机器学习有什么区别？数据驱动的投资者</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">这两个主题背后有很多令人兴奋的东西，所以这是一个快速指南，介绍了它们是什么以及它们有什么…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na ky mm"/></div></div></a></div><p id="d335" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谢天谢地，现在已经有大量的文献了。这方面的书<strong class="js iu">对强化学习做了非常全面的总结:<a class="ae le" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">强化学习:介绍</em> —萨顿&amp;巴尔托</a>。<a class="ae le" href="https://www.davidsilver.uk/teaching/" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗的讲座</a>对这个话题也很有帮助。</strong></p><h2 id="431a" class="nb lh it bd li nc nd dn lm ne nf dp lq kb ng nh lu kf ni nj ly kj nk nl mc nm bi translated">别担心。</h2><p id="b019" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">你不需要读一整本书就能知道发生了什么。但是要知道:每当你试图让你的代理人学习一些东西时，最好的选择是不要跳到StackOverflow上，希望有人也有类似的问题。即使他们这样做了，他们的问题的解决方案可能也不会帮助你解决你的问题。</p><p id="5403" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过仔细检查(1)错误和错误的代码库，(2)你的算法和它是否适合你的问题，以及(3)超参数，你的时间更好地用于定位你痛苦的原因。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="9891" class="lg lh it bd li lj nu ll lm ln nv lp lq lr nw lt lu lv nx lx ly lz ny mb mc md bi translated">如何开始——以及如何坚持下去</h1><p id="c4f6" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">假设你已经知道RL(奖励、政策、价值观)和深度学习(神经网络、损失函数)背后的基本理论，那么在开始实施时，你需要注意一些陷阱。</p><h2 id="f409" class="nb lh it bd li nc nd dn lm ne nf dp lq kb ng nh lu kf ni nj ly kj nk nl mc nm bi translated">1.慢慢开始</h2><p id="a214" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">登上月球是诱人的，但特别是在当前的研究领域，困难的问题往往还没有一个规范、直截了当的解决方案。爬得太高可能会让你离太阳太近。</p><p id="4018" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最好先解决简单的问题。在开始你自己的定制环境之前，尝试一些OpenAI的更简单的健身房问题。这些基准的优势在于，它们有明确的回报(允许高效学习并且不会太稀疏)，并且用许多当前的RL算法(举两个最好的例子，<a class="ae le" href="https://arxiv.org/abs/1910.00177" rel="noopener ugc nofollow" target="_blank"> AWR </a>和<a class="ae le" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"> SAC </a>)来破解相当简单。</p><p id="384a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">顺便说一下:如果你在你的项目中使用了<a class="ae le" href="http://pytorch.org" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>，请查看一下AWR的<a class="ae le" href="https://github.com/lukDev/awr_pytorch" rel="noopener ugc nofollow" target="_blank">我自己的实现</a>。</p><h2 id="bbe1" class="nb lh it bd li nc nd dn lm ne nf dp lq kb ng nh lu kf ni nj ly kj nk nl mc nm bi translated">2.从他人那里获得灵感</h2><p id="423a" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">不要害怕寻求帮助。如果你和我一样，你会有一种强烈的冲动，想出去按自己的方式做事，甚至不看任何外界的建议。不要犯那个错误。</p><p id="4307" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">很多人都有一种误解，认为要真正称之为<em class="lf">你的</em>工作，你需要自己去做。每个人(尤其是计算机科学家)都应该知道，最值得一提的成就是不看原件就能重现已经存在了很久的东西，而是能够快速理解艺术的状态，并能够在此基础上继续发展。</p><p id="2b60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">没有人会因为你能够从头开始编写深度Q学习代理而受益。如果您可以使用这样一个代理的现有实现，并将它们转换成解决一个新问题的东西，那么它们会带来好处。</p><p id="fd9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另外:如果你不确定如何解释论文中的某些陈述或评论(相信我，你有时会这样)，作者通常很乐意回答你的问题。毕竟，是他们对这篇论文的主题充满热情，花了几个月的时间来创作。只是事先查查有没有一些关于你这个问题的论坛帖子；你不想用他们已经回答了一千次的问题来烦他们。</p><h2 id="7611" class="nb lh it bd li nc nd dn lm ne nf dp lq kb ng nh lu kf ni nj ly kj nk nl mc nm bi translated">3.不要气馁</h2><p id="e6db" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">如果在DRL有一个保证成功的策略，那就是:永远不要放弃。</p><p id="8dff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">很有可能你的第一个、第二个甚至第十个方法都不会达到你想要的结果。这主要是因为DRL太难了。真的没有其他方式来表达。</p><p id="10ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如安德烈·卡帕西在他的<a class="ae le" href="https://news.ycombinator.com/item?id=13519044" rel="noopener ugc nofollow" target="_blank">对黑客新闻</a>的评论中恰当地指出的，问题是:</p><blockquote class="nz oa ob"><p id="572b" class="jq jr lf js b jt ju jv jw jx jy jz ka oc kc kd ke od kg kh ki oe kk kl km kn im bi translated">监督学习想要工作。【……】强化学习必须强制起作用。</p></blockquote><p id="0fad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">他的意思是，监督学习允许你把细节搞砸一点。选择稍微不完美的超参数，你仍然会得到收敛，尽管没有那么快。如果你在强化学习中犯了最微小的细节错误，你将受到惩罚。毫不犹豫。</p><p id="5800" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">举例来说，这可能意味着，你的代理人(可以完美地解决你给它的问题)将无法学到任何东西，仅仅是因为温度参数设置不当，奖励没有适当调整。RL没有显示“生命迹象”，也就是说，没有迹象表明你在正确的轨道上。当且仅当你把一切都做对了，它才会起作用。</p><p id="0bd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有这一切的寓意并不是说DRL是不可思议的困难；那就是，你应该相信自己的直觉，知道什么是对的，什么是可行的，并遵循直觉，即使一开始看起来你错了。只要你再改变一个超参数，一切都可能逆转。</p><h2 id="44ef" class="nb lh it bd li nc nd dn lm ne nf dp lq kb ng nh lu kf ni nj ly kj nk nl mc nm bi translated">4.知道什么意味着什么</h2><p id="dc3c" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">正如我前面提到的:解决DRL(以及其他许多地区)问题的最好方法是努力理解潜在的原因。是奖励没有按比例适当吗？奖励是不是太稀疏了？我的网络是不足还是过度？</p><p id="a153" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">深度学习的诀窍通常不是为你已经确定的问题找到解决方案，而是首先确定问题。众所周知，欠拟合需要更强大的网络架构，而过拟合可以通过一些调整来缓解。但是你怎么知道这些现象是否存在呢？</p><p id="4996" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这同样适用于DRL，如果不是更多的话。如果你的批评者不稳定意味着什么？或者你的演员不仅没有进步反而变得越来越差？</p><p id="b2bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦回答了这些问题，就可以找到解决方案。例如，如果状态值估计得不好，参与者在训练后可能表现得更差。也许在TD(λ)估计的实现中有一个bug？还是奖励值太大，导致训练评论家时数值不稳定？</p><p id="0c14" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">许多事情都可能导致错误的价值评估，但是我们需要知道(或者至少怀疑)这些价值是问题的根源，否则我们甚至不会考虑寻找可能的解决方案。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="817e" class="lg lh it bd li lj nu ll lm ln nv lp lq lr nw lt lu lv nx lx ly lz ny mb mc md bi translated">勇往直前去征服吧！</h1><p id="4a2f" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">你知道他们怎么说开始是最难的部分吗？他们是对的。所以就这么做吧，不会比这更难的了！如果你坚持下去，我相信你会取得一些学习进步，就像这里一样:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="of og l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">A randomly initialized neural network agent performing badly on <a class="ae le" href="https://gym.openai.com/envs/Pendulum-v0/" rel="noopener ugc nofollow" target="_blank">Pendulum-v0</a>.</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="of og l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">The same neural network after being trained for 200 epochs using <a class="ae le" href="https://github.com/lukDev/awr_pytorch" rel="noopener ugc nofollow" target="_blank">my implementation of AWR</a>.</figcaption></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh og l"/></div></figure></div></div>    
</body>
</html>