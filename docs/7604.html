<html>
<head>
<title>5 Trending AI/ML Research Papers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5篇趋势人工智能/人工智能研究论文</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/5-trending-ai-ml-research-papers-60f4b255f05d?source=collection_archive---------13-----------------------#2020-12-15">https://medium.datadriveninvestor.com/5-trending-ai-ml-research-papers-60f4b255f05d?source=collection_archive---------13-----------------------#2020-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1242" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">惊人的论文…</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b0b080c653910bce46b5fdf223acbf37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qWM_pNe6cQEas45v.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Pic credits : Pinterest</figcaption></figure><h1 id="1211" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">Qlib:面向人工智能的量化投资平台</h1><p id="31a5" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mj">肖扬、刘、董舟、、</em></p><h1 id="92fe" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak"> <em class="mk">摘要— </em> </strong></h1><p id="1626" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">量化投资的目标是在一系列金融工具的连续交易期内实现回报最大化和风险最小化。最近，受人工智能技术在量化投资方面产生显著创新的快速发展和巨大潜力的启发，越来越多的人采用人工智能驱动的工作流进行量化研究和实际投资。在丰富量化投资方法论的同时，AI技术对量化投资体系提出了新的挑战。特别是，量化投资的新学习范式要求升级基础设施，以适应更新的工作流程；此外，人工智能技术的数据驱动性质确实表明需要具有更强大性能的基础设施；此外，应用人工智能技术来解决金融场景中的不同任务存在一些独特的挑战。为了应对这些挑战，弥合人工智能技术和量化投资之间的差距，我们设计和开发了Qlib，旨在实现潜力，授权研究，并在量化投资中创造人工智能技术的价值。</p><p id="a726" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">论文可以在这里找到:</em> </strong></p><p id="4975" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><a class="ae mq" href="https://arxiv.org/pdf/2009.11189v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.11189v1.pdf</a></p><p id="4464" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">代码可以在这里找到:</em> </strong></p><div class="mr ms gp gr mt mu"><a href="https://github.com/microsoft/qlib" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">微软/qlib</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">Qlib是一个以人工智能为导向的量化投资平台，旨在实现潜力，授权研究，并…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni kp mu"/></div></div></a></div></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="7e93" class="kv kw iq bd kx ky nq la lb lc nr le lf jw ns jx lh jz nt ka lj kc nu kd ll lm bi translated">自我关注生成对抗网络</h1><p id="5373" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mj">作者:张寒、伊恩·古德菲勒、迪米特里斯·梅塔克萨斯、奥古斯都·奥登纳</em></p><h1 id="396c" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="80ee" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">自我注意生成对抗网络(SAGAN ),它允许为图像生成任务进行注意力驱动的远程依赖性建模。传统的卷积gan仅根据较低分辨率特征图中的空间局部点生成高分辨率细节。在SAGAN中，可以使用来自所有特征位置的线索来生成细节。此外，鉴别器可以检查图像的远处部分中的高度详细的特征是否彼此一致。此外，最近的研究表明，发电机调节会影响GAN的性能。利用这一认识，我们将频谱归一化应用于GAN发生器，并发现这改善了训练动态。提出的SAGAN实现了最先进的结果，在具有挑战性的ImageNet数据集上，将发表的最佳初始得分从36.8提高到52.52，并将Frechet初始距离从27.62减少到18.65。注意力层的可视化显示，生成器利用对应于对象形状的邻域，而不是固定形状的局部区域。</p><p id="418f" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">论文可以在这里找到:</em> </strong></p><p id="f638" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated">【https://arxiv.org/pdf/1805.08318v2.pdf T4】</p><p id="22c6" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">代码可以在这里找到:</em> </strong></p><div class="mr ms gp gr mt mu"><a href="https://github.com/brain-research/self-attention-gan" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">大脑研究/自我关注</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">用Tensorflow实现自注意生成对抗网络</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="nv l nf ng nh nd ni kp mu"/></div></div></a></div></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="7272" class="kv kw iq bd kx ky nq la lb lc nr le lf jw ns jx lh jz nt ka lj kc nu kd ll lm bi translated">自适应信念优化器:根据观测梯度的信念调整步长</h1><p id="6236" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作者:庄，汤米唐，，塞克哈塔提孔达，尼查德沃内克，色诺芬帕帕德米特里斯，詹姆斯s邓肯</p><h1 id="34a6" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="426d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">最流行的深度学习优化器可以大致分为自适应方法(如Adam)和加速方案(如带动量的随机梯度下降(SGD))。对于许多模型，如卷积神经网络(CNN)，自适应方法通常比SGD收敛得更快，但泛化能力更差；对于复杂的设置，如生成性敌对网络(GANs)，自适应方法通常是默认的，因为它们的稳定性。我们提出AdaBelief来同时实现三个目标:快速收敛，如自适应方法，良好的推广，如SGD，和训练的稳定性。AdaBelief的直觉是根据当前梯度方向上的“信念”来调整步长。将含噪梯度的指数移动平均(EMA)视为下一时间步梯度的预测，如果观测梯度与预测偏差较大，则不信任当前观测，采取小步；如果观察到的梯度接近预测，我们就相信它，并迈出一大步。我们在大量的实验中验证了AdaBelief的有效性，结果表明，它在图像分类和语言建模方面的收敛速度快，准确率高，优于其他方法。具体来说，在ImageNet上，AdaBelief实现了与SGD相当的准确性。此外，在Cifar10上训练GAN时，与经过良好调整的Adam优化器相比，AdaBelief表现出了高稳定性，并提高了生成样本的质量。</p><p id="3da6" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">论文可以在这里找到:</em> </strong></p><p id="5b94" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated">【https://arxiv.org/pdf/2010.07468v4.pdf T4】</p><p id="8200" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">代码可以在这里找到:</em> </strong></p><div class="mr ms gp gr mt mu"><a href="https://github.com/juntang-zhuang/Adabelief-Optimizer" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">庄/阿大信念优化器</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">在adabelief-pytorch的下一个版本中，我们将修改几个参数的缺省值，以适应…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="nw l nf ng nh nd ni kp mu"/></div></div></a></div></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="f42a" class="kv kw iq bd kx ky nq la lb lc nr le lf jw ns jx lh jz nt ka lj kc nu kd ll lm bi translated">基于全卷积网络的端到端目标检测</h1><p id="dafb" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mj">由王建峰、宋琳、李则明、孙宏斌、孙健、郑南宁</em></p><div class="mr ms gp gr mt mu"><a href="https://www.datadriveninvestor.com/2020/11/19/how-machine-learning-and-artificial-intelligence-changing-the-face-of-ecommerce/" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">机器学习和人工智能如何改变电子商务的面貌？|数据驱动…</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">电子商务开发公司，现在，整合先进的客户体验到一个新的水平…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="nd l"><div class="nx l nf ng nh nd ni kp mu"/></div></div></a></div><h1 id="90ac" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="edff" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">基于全卷积网络的主流物体检测器已经取得了令人印象深刻的性能。而它们中的大多数仍然需要手工设计的非最大抑制(NMS)后处理，这妨碍了完全的端到端训练。在本文中，我们给出了丢弃NMS的分析，结果表明一个合适的标签分配起着至关重要的作用。为此，对于完全卷积检测器，我们引入了预测感知的一对一(POTO)标签分配用于分类，以实现端到端检测，这获得了与NMS相当的性能。此外，提出了一种简单的3D Max滤波方法，利用多尺度特征提高局部区域卷积的可分辨性。通过这些技术，我们的端到端框架在COCO和CrowdHuman数据集上实现了与NMS的许多最先进的检测器相竞争的性能。</p><p id="49b6" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">论文可以在这里找到:</em> </strong></p><p id="3895" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated">【https://arxiv.org/pdf/2012.03544v1.pdf T4】</p><p id="022e" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">代码可以在这里找到:</em> </strong></p><div class="mr ms gp gr mt mu"><a href="https://github.com/Megvii-BaseDetection/DeFCN" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">基于旷视科技的检测/定义</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">本项目在PyTorch上实现了“全卷积网络的端到端目标检测”。</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="ny l nf ng nh nd ni kp mu"/></div></div></a></div></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="87ae" class="kv kw iq bd kx ky nq la lb lc nr le lf jw ns jx lh jz nt ka lj kc nu kd ll lm bi translated">注意力液体扭曲GAN:人体图像合成的统一框架</h1><p id="c625" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mj">由、朴智信、涂、、罗、、高胜华</em></p><h1 id="d32e" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">抽象—</h1><p id="40b0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们在一个统一的框架内处理人体图像合成，包括人体运动模仿、外观转换和新颖的视图合成。这意味着模型一旦被训练，就可以用来处理所有这些任务。现有的特定任务方法主要使用2D关键点来估计人体结构。然而，它们仅表达位置信息，没有能力表征人的个性化形状和模拟肢体旋转。在本文中，我们建议使用一个三维身体网格恢复模块来解开姿态和形状。它不仅可以模拟关节的位置和旋转，还可以表征个性化的体形。为了保留源信息，如纹理、风格、颜色和人脸身份，我们提出了一种带有注意力液体扭曲块的注意力液体扭曲GAN(AttLWB ),它将图像和特征空间中的源信息传播到合成参考。具体地，源特征由去噪卷积自动编码器提取，用于很好地表征源身份。此外，我们提出的方法可以支持来自多个源的更灵活的弯曲。为了进一步提高对未知源图像的泛化能力，采用了一次/少次对抗学习。具体来说，它首先在广泛的训练集中训练一个模型。然后，它以自我监督的方式通过一张/几张未拍摄的图像对模型进行微调，以生成高分辨率(512 x 512和1024 x 1024)的结果。此外，我们建立了一个新的数据集，即iPER数据集，用于评估人体运动模仿，外观转移和新的视图合成。大量的实验证明了我们的方法在保持人脸身份、形状一致性和衣服细节方面的有效性。</p><p id="6391" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">论文可以在这里找到:</em> </strong></p><p id="2602" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated">【https://arxiv.org/pdf/2011.09055v2.pdf T4】</p><p id="195e" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated"><strong class="lp ir"> <em class="mj">代码可以在这里找到:</em> </strong></p><div class="mr ms gp gr mt mu"><a href="https://github.com/iPERDance/iPERCore" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">iPERDance/iPERCore</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">一个统一的人体图像合成框架，包括人体运动模拟…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">github.com</p></div></div><div class="nd l"><div class="nz l nf ng nh nd ni kp mu"/></div></div></a></div></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="c24a" class="pw-post-body-paragraph ln lo iq lp b lq ml jr ls lt mm ju lv lw mn ly lz ma mo mc md me mp mg mh mi ij bi translated">参考文献和致谢—</p><div class="mr ms gp gr mt mu"><a href="https://arxiv.org/" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd ir gy z fp mz fr fs na fu fw ip bi translated">arXiv.org</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">arXiv是一个免费的分发服务和开放存取的档案库，包含1，721，837篇学术文章，涉及领域包括…</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">arxiv.org</p></div></div></div></a></div><h2 id="8e07" class="oa kw iq bd kx ob oc dn lb od oe dp lf lw of og lh ma oh oi lj me oj ok ll ol bi translated">访问专家视图— <a class="ae mq" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank">订阅DDI英特尔</a></h2></div></div>    
</body>
</html>