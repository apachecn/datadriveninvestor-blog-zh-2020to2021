# HDFS 的大数据和小文件问题

> 原文：<https://medium.datadriveninvestor.com/big-data-small-files-problem-in-hdfs-ba8707ec46b?source=collection_archive---------6----------------------->

![](img/b104ff0d9e614ca271c83eafa2c017d0.png)

Lost person looking for answers. Image by [VisionPic .net](https://www.pexels.com/@freestockpro)

如今，每个人都在谈论“大数据”，但每个人都知道是什么让大数据变大了吗？试试看维基百科 [上](https://en.wikipedia.org/wiki/Big_data};)[“大数据”这个词的定义就知道了；](https://en.wikipedia.org/wiki/Big_data)“一个……处理太大或太复杂而无法处理的数据集的领域……”，你在任何地方都找不到让数据变得太大或太复杂的数字。

**定义数据中的“大”**

作为一个成立一年的大数据团队的团队领导，我们没有理解大的定义，我们的组织没有！

该项目的目的是处理来自不同来源的日志数据，以提高我们网络的安全性。日志数据主要由事件组成，平均为 500 字节。有的甚至是 200 字节。

之前，我们在 Hadoop HDFS 中遇到了[稳定性问题，因为我们忽略了一个基本假设；HDFS 是为大文件(1 GB 及以上)而设计的。当不稳定袭击我们时，我们开始重新思考是什么让我们成为大数据！毕竟，我们来自一个没有大数据的世界(和组织)，过去和将来都很难建立一个坚实的知识库。](https://medium.com/datadriveninvestor/solving-stability-problems-in-hadoop-cluster-big-data-with-small-data-ce2989d91425)

我们配置 Flume(我们的摄取工具)以 5 分钟为一批摄取数据，并将这些数据写到 HDFS，每个事件 500 字节；输入介于 15 个事件/秒到 10，000 个事件/秒之间，我们有许多 8kb 到 140kb 的文件。当考虑来自所有输入的数据时，我们得到大约 55GB/小时的输入速率，这足以被认为是大数据。

在我们困惑地寻找任何绝对答案的路上，我偶然发现了这个[博客](https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html)，虽然它语言粗糙，但它很简洁，在考虑做大(数据)时会为你节省宝贵的时间和金钱。

该页面特别讨论了采用 Hadoop 的生态系统作为您的大数据平台。尽管文章提供了一些数字，让我们有了更清晰的了解，但从不同的角度来看，这些数字是不准确的，应该予以考虑。

首先，页面写于 2013 年，当时 Hadoop 在 2.2.0 版本。简单地说，今天我们是在 3.2 版本，所以事情发生了变化。

其次，如果我们用维基百科中的定义替换页面上的数字，我们可以说作者认为 5TB+是一个太大\太复杂而无法处理的数据集。5TB 当然很大，但大和复杂是相对于数字世界中的其他数据和我们的处理能力而言的形容词。以下预测将明确这些变化很快:

1.仅在一年内，累积的世界数据将增长到 44 zettabytes(即 44 万亿千兆字节)！相比之下，今天大约是 4.4 zettabytes。从[hostingtribunal.com](https://hostingtribunal.com/blog/big-data-stats/)取回。

2.摩尔定律称计算能力每两年翻一番(保持到 3nm)。从[维基百科](https://en.wikipedia.org/wiki/Moore's_law)检索。

总之，今天的 5TB 可能超过 20TB(使用最新技术，不考虑软件限制)。

此外，在大多数情况下，您可能会流式传输新数据(或每 X 次加载增量)，因此您的数据会有一个增长因子。这与考虑增长是否反映在您或您的客户查询的数据集中有关。

数据集的增长有几个来源。当您希望扩展查询的时间范围时，可以形成这种情况，另一个增长源可以从相同的输入(发送消息的速率)发展起来，最后，通过添加您希望处理的附加数据输入

理解数据集的定义是很重要的。

当我们更好地了解客户的数据集时，我们发现各种研究问题需要基于几天、几个月甚至几年的数据。这些数据集处理的数据从 100MB 到 100TB 不等。所以这是大事！

不幸的是，对于 Hadoop 来说，小于 1GB 的文件是滥用，因此，我们存储数据的方式仍然存在问题。

即使问题仍然存在，回答这个大问题是实质性的，需要修改以验证我们的需求没有改变。

**为 HDFS 解决小文件**

在项目的这一点上，已经有活跃的客户端使用存储在 Hadoop 上的数据，我们无法迁移到 Cassandra(它比 Hadoop 更好地处理小文件——我认为选择 Hadoop 而不是 Cassandra 不是最适合我们最初需求的决定，但这都是事后诸葛亮)。因此，我们需要找到一个更简单的解决方案，基于与我们当前目录结构向后兼容的相同架构。

那时，我们有了一个想法，以每小时的方式聚合数据，以充分利用当前的索引惯例(查看[以前的故事](https://medium.com/datadriveninvestor/solving-stability-problems-in-hadoop-cluster-big-data-with-small-data-ce2989d91425))，在每小时拥有我们可能拥有的最大的单个文件。

这是一个简单的程序，它从一个特定的(过去的)小时读取 Flume 写的数据，并将它们写入另一个并行目录结构中的一个 Parquet 文件。这种优化为我们在 HDFS 节省了大量空间和文件数量，从而降低了节点的 RAM 使用率，进而增强了 HDFS 的性能和稳定性。你可以查看不同格式之间的这些[基准](https://conferences.oreilly.com/strata/strata-ny-2016/public/schedule/detail/51952)，这是我们这些数据工程师必须知道的基础知识！

这种简单的策略运行得非常好，这就是为什么我们扩展了它的功能，以支持不同的流应用程序输出，并通过输入/输出目录结构变得更加灵活。

我们估计这种解决方案只能维持一两年，但事实证明它的寿命超出了我们的预期。此外，由于原始数据和发布数据格式的分离，它为我们提供了灵活性。我们从纯文本和 JSON 格式开始，后来，我们转移到 Parquet 作为我们客户的最终发布数据格式，同时保持原始数据格式的灵活性(我们使用更适合输入数据类型的格式)，应用程序在两者之间进行转换。后来，当我们添加新格式时，在聚合应用程序中实现转换非常简单。

**重新思考你项目的假设和基础有利于增强你对所采取的行动和所做的决定的信念。请记住，在做这些决定时，要站在最简单最容易的一方，以保持未来的灵活性。**