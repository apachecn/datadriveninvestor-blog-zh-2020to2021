<html>
<head>
<title>Feature Selection Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择技术</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/feature-selection-techniques-1a99e61da222?source=collection_archive---------2-----------------------#2020-07-14">https://medium.datadriveninvestor.com/feature-selection-techniques-1a99e61da222?source=collection_archive---------2-----------------------#2020-07-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/c75ba61895044ba1d7735351b14b32ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*CHnUGGi0rPzwyUlT0pphcw.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Image source: kdnuggets</figcaption></figure><h1 id="4c6e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是特征选择？</h1><p id="5179" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">你们都看过数据集。有时它们很小，但通常它们的尺寸非常大。处理非常大的数据集变得非常具有挑战性，这些数据集至少大到足以导致处理瓶颈。</p><p id="e275" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">机器学习算法的训练时间和性能在很大程度上取决于数据集中的特征。理想情况下，我们应该只保留数据集中那些实际上帮助我们的机器学习模型学习一些东西的特征。拥有太多的特征会带来众所周知的维数灾难问题。</p><p id="4554" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">不必要的和冗余的特征不仅减慢算法的训练时间，而且它们还影响算法的性能。选择最适合训练机器学习模型的特征的过程称为“特征选择”。</p><p id="2775" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在进行特征选择之前，我们需要进行数据预处理。你可以<a class="ae lz" href="https://medium.com/@yashjoshilko/data-pre-processing-before-model-training-7e741e9cb253" rel="noopener">检查这个</a></p><h1 id="dea1" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">执行特征选择的好处:</h1><p id="e607" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在训练机器学习模型之前执行特征选择有几个优点，其中一些列举如下:</p><ul class=""><li id="ee8b" class="ma mb iq ky b kz lu ld lv lh mc ll md lp me lt mf mg mh mi bi translated">特征数量少的模型具有更高的解释能力</li><li id="d5e1" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">实现特征减少的机器学习模型更容易</li><li id="ec0b" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">特征越少，泛化能力越强，从而减少了过度拟合</li><li id="a7cd" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">特征选择消除了数据冗余</li><li id="47c3" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">具有较少特征的模型的训练时间明显更短</li><li id="97ce" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">特征越少的模型越不容易出错</li></ul><h1 id="cf22" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">特征选择技术:</h1><p id="d460" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">已经开发了几种方法来为机器学习算法选择最佳特征。</p><p id="bf7d" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">注意:</strong>在本文中，我们将讨论被广泛采用的方法。<strong class="ky ir"> </strong>所有的技术都是相互独立实现的，不是连续的</p><ol class=""><li id="f9b4" class="ma mb iq ky b kz lu ld lv lh mc ll md lp me lt mo mg mh mi bi translated"><strong class="ky ir">过滤方法。</strong></li><li id="6aea" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mo mg mh mi bi translated"><strong class="ky ir">包装方法。</strong></li><li id="4407" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mo mg mh mi bi translated"><strong class="ky ir">嵌入法(收缩法)。</strong></li></ol><h2 id="1021" class="mp jz iq bd ka mq mr dn ke ms mt dp ki lh mu mv km ll mw mx kq lp my mz ku na bi translated">过滤方法:</h2><p id="09e3" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">过滤方法可以大致分为两类:<strong class="ky ir">单变量过滤方法</strong>和<strong class="ky ir">多变量过滤方法</strong>。</p><p id="bb3b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">单变量过滤方法是一种根据特定标准对单个特征进行分级的方法。然后选择前N个特征。</p><p id="4388" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">统计测试可用于选择那些与输出变量关系最密切的特征。<strong class="ky ir">互信息、ANOVA F检验</strong>和<strong class="ky ir">卡方</strong>是一些最流行的单变量特征选择方法。</p><p id="8358" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">scikit-learn库提供:</p><p id="0b2b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> SelectKBest </strong>:保留了前k的评分特征。</p><p id="211f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> SelectPercentile </strong>:保留用户指定百分比的顶级特性。</p><p id="898c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">必须注意的是，chi只能用于本质上非负的数据。</p><p id="cc7b" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">以下示例使用非负特征的chi统计测试从移动价格范围预测数据集中选择10个最佳特征。</p><p id="0907" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">可以下载</strong> <a class="ae lz" href="https://www.kaggle.com/iabhishekofficial/mobile-price-classification#train.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">数据集</strong> </a> <strong class="ky ir"> : </strong></p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/643ddac5b149ccf890ede013af2f6f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeLNpV0tYGLuZmrQC8z_-w.png"/></div></div></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nk"><img src="../Images/e7c792f3fe00d7fda222989c24758fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uflizS-5HuYxcfsPq3kuTA.png"/></div></div></figure><p id="9016" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在我们将看到如何在Python的帮助下<strong class="ky ir">从我们的数据集中移除方差非常低的要素</strong>和<strong class="ky ir">相关要素</strong>。</p><p id="6536" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如果特征具有非常低的方差(即非常接近于0)，则它们接近于常数，因此根本不会给我们的模型增加任何价值。摆脱它们并因此降低复杂性是很好的。请注意，方差也取决于数据的缩放比例。Scikit-learn有一个方差阈值的实现，可以精确地做到这一点。</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nl"><img src="../Images/33c15626199c46512a34464700cf2e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PT2etwu1jzauYx0O8p6p3Q.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">All columns with variance less than 0.1 will be removed</figcaption></figure><p id="9b80" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">输出观察值和输入特征之间的相关性非常重要，应保留这些特征</strong>。但是，如果两个或两个以上的特征相互关联，它们会向模型传递冗余信息。</p><p id="9085" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们可以删除高度相关的特征。请注意，我们将使用<strong class="ky ir">皮尔森相关性</strong>来计算不同数字特征之间的相关性。</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nm"><img src="../Images/d3476f2eee502a00d3dc222f4e9ae4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZsiFTWwkDexWMWBEUMFXPg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Importing Libraries</figcaption></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nn"><img src="../Images/ffea29edfc616fdead2d6fd06f9f0f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QfqdNxhmemFVzjF2vohv_g.png"/></div></div></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi no"><img src="../Images/1a936959899232e8ecf4f3e59bdf92b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2pH8NYNKzsi-t1Hb20cMQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.</figcaption></figure><p id="8b8f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们看到特性MedInc_Sqrt与MedInc具有非常高的相关性。因此，我们可以删除其中一个。</p><p id="c7e6" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">现在，你可能会说，为什么不通过直觉或仅仅查看热图来移除不相关的特征呢？</p><p id="0510" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">总的来说，不被个人的偏见或直觉所影响是明智的。</p><p id="9e34" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在现实生活中，我们必须处理3个以上的特征(通常从几百到几千个)。因此，逐一检查并决定是否保留它是不可行的。此外，变量之间可能存在不容易被人眼发现的关系，即使通过精确的分析也不容易发现。</p><p id="8d08" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">但是，在某些情况下，您可能希望使用特定的机器学习算法来训练您的模型。在这种情况下，通过过滤方法选择的特征可能不是特定算法的最佳特征集。还有另一类特征选择方法，为特定算法选择最佳特征。这样的方法被称为<strong class="ky ir">包装器方法</strong>。</p><h2 id="2da1" class="mp jz iq bd ka mq mr dn ke ms mt dp ki lh mu mv km ll mw mx kq lp my mz ku na bi translated"><strong class="ak">包装方法:</strong></h2><p id="d95f" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">包装方法使用变量组合来确定预测能力。它们基于贪婪的搜索算法。包装器方法将找到变量的最佳组合。包装器方法实际上是根据测试模型来测试每一个特性，然后用它们来评估结果。</p><p id="9913" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">在所有这三种方法中，这种方法的计算量非常大。不建议在大量特征上使用此方法，如果不正确使用此特征选择，甚至可能会过度拟合模型。</p><p id="4b73" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">常见的包装方法有:<strong class="ky ir">逐步</strong> / <strong class="ky ir">子集选择、向前逐步、向后逐步(RFE)。</strong></p><p id="f89a" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">这里我提到了要遵循的基本步骤:</strong></p><ul class=""><li id="1127" class="ma mb iq ky b kz lu ld lv lh mc ll md lp me lt mf mg mh mi bi translated">训练基线模型。</li><li id="c386" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">使用特征选择技术确定最重要的特征</li><li id="ad81" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">创建仅包含这些要素的新“有限要素”数据集</li><li id="b5c2" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">在这个新数据集上训练第二个模型</li><li id="c0fe" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mf mg mh mi bi translated">将“全功能”(基准)模型的准确性与“有限功能”(新)模型的准确性进行比较</li></ul><p id="40fb" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">正向选择:</strong></p><ol class=""><li id="098c" class="ma mb iq ky b kz lu ld lv lh mc ll md lp me lt mo mg mh mi bi translated">确定最佳变量(例如，基于模型准确性)</li><li id="7d4f" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mo mg mh mi bi translated">将下一个变量添加到模型中</li><li id="faf1" class="ma mb iq ky b kz mj ld mk lh ml ll mm lp mn lt mo mg mh mi bi translated">等等，直到满足某个预定的标准</li></ol><p id="4630" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">逐步</strong> / <strong class="ky ir">子集选择:</strong></p><p id="a021" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">类似于正向选择过程，但是如果某个变量在经过一定数量的步骤后被认为不再有用，它也可以被丢弃。</p><h2 id="abe4" class="mp jz iq bd ka mq mr dn ke ms mt dp ki lh mu mv km ll mw mx kq lp my mz ku na bi translated">现在让我们实现各种特征选择技术</h2><h2 id="5085" class="mp jz iq bd ka mq mr dn ke ms mt dp ki lh mu mv km ll mw mx kq lp my mz ku na bi translated">1.<strong class="ak">向后逐步(</strong>递归特征消除(RFE))</h2><p id="7b36" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">递归=重复发生的事情</p><p id="be42" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">顾名思义，递归特征消除通过递归(重复)移除特征并在保留的特征上构建模型来工作。</p><p id="308e" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">以下示例使用RFE和线性回归算法来选择前3个要素。算法的选择并不重要，我们可以使用任何其他算法来代替线性算法。</p><p id="ad9f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">我们使用sklearn库中的特征选择模块来应用递归特征消除(RFE)</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi np"><img src="../Images/6b33e4b8559fda19701903034252f901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXdiPEbFAlXhvlvDeHVUNA.png"/></div></div></figure><p id="58a7" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">Scikit learn还提供了<strong class="ky ir"> SelectFromModel </strong>，帮助您直接从给定的模型中选择特性。如果需要，还可以指定系数或要素重要性的阈值以及要选择的最大要素数。</p><p id="d95a" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir"> 3。嵌入法(收缩)。</strong></p><p id="f722" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">嵌入式方法是内置变量选择方法。在这种方法中，我们不选择或拒绝预测因子或变量。这控制参数的值，即不太重要的预测值被赋予非常低的权重(接近零)，这也被称为<strong class="ky ir">正则化。</strong></p><p id="663c" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">使用具有<strong class="ky ir"> L1(套索)惩罚</strong>的模型进行特征选择。当我们对正则化进行L1惩罚时，大多数系数将是0(或接近0)，并且我们选择具有非零系数的特征。</p><p id="934f" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated"><strong class="ky ir">【岭】罚</strong>，这个<strong class="ky ir"> </strong>加一个罚，等于系数大小的平方。所有系数都以相同的因子收缩(因此没有预测值被消除)。</p><p id="d010" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">最后，我想说的是，特征选择是机器学习管道中决定性的一部分:过于保守意味着引入不必要的噪音，而过于激进意味着丢弃有用的信息。</p><p id="666d" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如果你想了解缺失值处理，那么<a class="ae lz" href="https://medium.com/@dikshabellani.2803/handling-missing-values-bb8b549364cc" rel="noopener">看看这个。</a></p><p id="3311" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">如果你发现这篇文章有用，给它一个<strong class="ky ir">掌声</strong>和<strong class="ky ir">与他人分享</strong>。</p><p id="9f78" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">——<em class="nq">快乐学习</em></p><p id="6278" class="pw-post-body-paragraph kw kx iq ky b kz lu lb lc ld lv lf lg lh lw lj lk ll lx ln lo lp ly lr ls lt ij bi translated">— <em class="nq">谢谢</em></p></div></div>    
</body>
</html>