<html>
<head>
<title>Reinforcement Learning — Monte-Carlo for policy evaluation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习-用于政策评估的蒙特卡罗方法。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/reinforcement-learning-monte-carlo-for-policy-evaluation-312fd2e8331d?source=collection_archive---------0-----------------------#2020-05-13">https://medium.datadriveninvestor.com/reinforcement-learning-monte-carlo-for-policy-evaluation-312fd2e8331d?source=collection_archive---------0-----------------------#2020-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4124" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae kl" href="https://medium.com/datadriveninvestor/reinforcement-learning-planning-dynamic-programming-45f81a3cc9fa" rel="noopener">之前的博客文章</a>中，我们已经使用动态编程为代理已知环境模型的问题找到了解决方案。然而，在现实世界的问题中，模型并不总是已知的。例如，这就是为什么在股票市场上赚钱如此困难的原因。我们没有关于环境的完整信息，事先也不知道转移概率。</p><p id="0c02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本帖中，我们将讨论如何使用蒙特卡洛来解决预测/评估问题。通过在没有MDP的情况下获得价值函数。</p><h1 id="3ef8" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">蒙特卡罗方法</h1><p id="3a91" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在本帖中，我们将仅使用蒙特卡洛来评估政策<em class="lp"> π </em>。<em class="lp"> </em>这意味着我们正在寻找状态值函数v_π(s)给定策略<em class="lp"> π。它也适用于动作值函数q_π(s ),但是为了简单起见，我们在这里集中于v_π(s)。</em></p><p id="f63a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请记住，状态值只不过是从遵循策略<em class="lp"> π </em>的状态<em class="lp"> s </em>开始到最后一个时间步长T的结束状态的贴现累积奖励:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/00d88f123832b72bb98c420c24eb7af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*KfV8tnoTz3qjMdF2JUckeQ.png"/></div></figure><p id="dc93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<strong class="jp ir">事件</strong>是从初始状态<em class="lp"> s </em>到结束状态的所有状态的序列。它是<strong class="jp ir"> <em class="lp">而不是</em> </strong>一个连续的infit序列。此外，对于每一集，我们得到不同的返回G_t。视频游戏的一集的例子可以是所有状态的序列，即从开始到游戏结束的帧。</p><p id="1749" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们选择我们的一个状态作为初始状态<em class="lp">s _ I。</em>我们通过遵循策略π经历一个完整的插曲，并最终到达结束状态<em class="lp"> s_e </em>。</p><p id="438c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们重复这个很多很多次。在每次迭代中，根据随机策略<em class="lp"> π，最终状态可能以不同的方式达到。</em>这就导致每一集的回报<em class="lp"> G_t </em>都不一样。对这些回报求平均，可以得到初始状态s_i的真实状态值的近似表示。</p><p id="bcce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们对所有状态进行这一过程，其中我们总是选择不同的状态作为初始状态，那么我们获得每个状态的状态值的良好近似<strong class="jp ir">,并因此获得环境模型的状态值函数。这是蒙特卡罗背后的主要思想。</strong></p><div class="ly lz gp gr ma mb"><a href="https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd ir gy z fp mg fr fs mh fu fw ip bi translated">AI和机器学习有什么区别？数据驱动的投资者</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">这两个主题背后有很多令人兴奋的东西，所以这是一个快速指南，介绍了它们是什么以及它们有什么…</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp lw mb"/></div></div></a></div><p id="6d30" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过考虑未来的回报和未来状态的值，而不是使用动态规划和贝尔曼方程来计算时间步长t时状态s的精确值，我们通过运行许多集来获得真实值函数的估计，然后观察我们为每个集获得的回报，并对所有集进行平均。不需要模型。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/18117f639af1d234a06df7a3983a9484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XjxgGdkp_1tbmMuo"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Monte Carlo follows the policy and ends up with different samples for each episode. The underlining model is approximated by running many episodes and averaging over all samples. Dynamic Programming, on the other hand, would consider all future actions and future states from every state.</figcaption></figure><p id="2c7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有两种不同的方法来评估政策使用蒙特卡罗。首次就诊和每次就诊。对于这两种情况，我们为每个状态S保留一个计数器N(s ),并将每个状态的每集不同回报的总和保存在S(s)中。</p><p id="4825" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">N(s)和S(s)累加值，直到完成所有K集。在每一集之后，它们不会被设置为零。</p><p id="5196" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们仔细看看这两种方法。</p><h2 id="672f" class="mz kn iq bd ko na nb dn ks nc nd dp kw jy ne nf la kc ng nh le kg ni nj li nk bi translated"><strong class="ak">每次访问蒙特卡罗政策评估</strong></h2><p id="be05" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们迭代K集。在每集中，每当我们到达状态s时，我们就将该状态的计数器N(s)加1。有可能在同一个情节中多次达到相同的状态。对于所有这些访问，计数器增加。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/84a261720df5ffbf6191d09cb90f0d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*ktqPCg89TTOF5sifoNhjQQ.png"/></div></figure><p id="5494" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在所有这些访问中，我们还通过添加当前集k的返回G_t来更新当前S(s)值，从状态S开始到结束状态。对于每次访问，G_t可以是不同的。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/03df227cde268387c620b175a8885992.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*9WBI9OsmRYG2cav8J-6qKA.png"/></div></figure><p id="407a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完成后，我们通过执行以下操作来更新状态的当前值:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a1af46d91792fb0987c1b2e4f9fcc2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*77xnbcpbvKufiqNj7s11qA.png"/></div></figure><p id="8860" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这只是到目前为止所有剧集的平均回报。</p><p id="9215" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该算法如下所示:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi no"><img src="../Images/4863bb572e2b31df990c082d93567449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL5JcVJYqGJkwkfRfk06Sg.png"/></div></div></figure><h2 id="f0ad" class="mz kn iq bd ko na nb dn ks nc nd dp kw jy ne nf la kc ng nh le kg ni nj li nk bi translated">首次访问蒙特卡罗政策评估</h2><p id="d10f" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们做的和每次蒙特卡罗策略评估一样。这里唯一的区别是，在同一集内，我们仅在状态S第一次访问时更新N(s)和S(s ),在我们的算法中，我们将检查S是否在该集中第一次被访问，然后才更新它们。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi no"><img src="../Images/b4a0cdfe9cf33993b3958502c166fbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRNgomTUMdxUHW4BK7T83Q.png"/></div></div></figure><h2 id="7b7d" class="mz kn iq bd ko na nb dn ks nc nd dp kw jy ne nf la kc ng nh le kg ni nj li nk bi translated"><strong class="ak">增量蒙特卡罗</strong></h2><p id="7589" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">现在，我们在每集之后递增地更新V(s ),而不是先浏览所有集，最后只更新所有状态的V(s)。我们不再需要探访了。</p><p id="0f31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们计算剧集G_t的实际输出和我们的估计V(s)之间的误差项。记住G_t总是从当前状态开始计算。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d4727a45fb46cc3c1a85c8d70cc9bd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*eU9iO46VcI5NQiv8fSb3bA.png"/></div></figure><p id="da76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们按照平均误差的方向更新V(s ):</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9a2694d08108f3e52d4705fdba55c9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*Rlo9Ua8rbg7RNS-lsQvkqQ.png"/></div></figure><p id="4bea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看看每集之后的更新:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1f77767987be4b1152c9601562fb563e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*gGgD9qrw5hmiFDAxu1lGtw.png"/></div></figure><p id="e304" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在每次迭代中，我们都向我们看到的样本靠近一点，并且越来越接近平均值。</p><p id="c1fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">算法是:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi ns"><img src="../Images/a0efaba9962676472c526851c0fad78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGSS7mC0bN6OTo1zfX51PQ.png"/></div></div></figure><p id="0467" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们不用N(s)来考虑所有的状态，而是用一个常数α。这样，我们只关心固定的步数，允许忘记一些状态。</p><p id="1d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这在更灵活的环境中，在非平稳问题中是有意义的。这实际上几乎是真实世界的情况。机器人不需要考虑它曾经走过的每一步来决定下一步。只是一些过去的步骤很重要。缺点是通过使用固定步长α而不是均值，我们会对V(s)进行欠校正或过校正。</p><p id="7fea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于我们的更新部分，这意味着我们只需用α替换1/N(s)。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f8ea84e4b293810e8679bec157b29dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*yb3xo8noYgkeOD0Du_sOEQ.png"/></div></figure><p id="1886" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这也使得我们上面的算法简单了一点，因为我们不再需要跟踪N(s)。</p><h2 id="0866" class="mz kn iq bd ko na nb dn ks nc nd dp kw jy ne nf la kc ng nh le kg ni nj li nk bi translated">蒙特卡洛的问题</h2><ul class=""><li id="0fa5" class="nu nv iq jp b jq lk ju ll jy nw kc nx kg ny kk nz oa ob oc bi translated">仅适用于小插曲，不适用于连续或无限的问题</li><li id="1ba2" class="nu nv iq jp b jq od ju oe jy of kc og kg oh kk nz oa ob oc bi translated">我们需要先完成一集，然后才能更新所有值。这对于自动驾驶汽车来说是不好的，如果使用蒙特卡罗，只有在事故发生后，即事件结束时，才会更新状态值。那有点太晚了。</li></ul><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oi oj l"/></div></figure></div></div>    
</body>
</html>