<html>
<head>
<title>Writing a Neural Machine Translation Model: Seq2Seq model with Attention.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">写一个神经机器翻译模型:Seq2Seq模型注意。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/writing-a-neural-machine-translation-model-seq2seq-model-with-attention-d3ec3ac8e9e4?source=collection_archive---------1-----------------------#2020-06-09">https://medium.datadriveninvestor.com/writing-a-neural-machine-translation-model-seq2seq-model-with-attention-d3ec3ac8e9e4?source=collection_archive---------1-----------------------#2020-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d97e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">内部人工智能</h2><div class=""/><div class=""><h2 id="2762" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个可以像谷歌翻译一样将文本从一种语言翻译成另一种语言的模型。</h2></div><p id="2051" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">谷歌翻译总是让我惊讶于它从一种语言翻译到另一种语言的能力。不仅仅是短句，它在大句上也表现得很好。在本文中，我将尝试通过创建一个可以将西班牙语的任何句子翻译成英语的模型来模仿Google Translate。您可以在任何数据集上训练该模型，以便在任何两种语言之间进行翻译。但是对于本文，我将使用西班牙语到英语的数据集。我们还将使用<a class="ae ln" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau注意力</a>来解释大句子。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/707bc72deae7bb0c422d5091564d7097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qSo9tnRzixQaXrRW"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">credits: OLYMPUS IMAGING CORP.</figcaption></figure><blockquote class="me mf mg"><p id="34c8" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated"><strong class="kt jd"> <em class="it">注意:如果你似乎没有理解文章中讨论的任何观点或想法，只需试着把整篇文章读一遍，然后回来再读一遍那个观点。希望这能有点意义，因为一切都是相互关联的。</em> </strong></p></blockquote><p id="d226" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">目录:- </strong></p><ol class=""><li id="bc7a" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm mq mr ms mt bi translated"><strong class="kt jd">启动前的一些说明</strong></li><li id="f088" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">数据集</strong></li><li id="7c28" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">库</strong></li><li id="5e07" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">预处理<br/></strong>——预处理的可选步骤</li><li id="c2c9" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">创建模型<br/> </strong> -编码器<br/> -巴丹瑙注意<br/> -解码器</li><li id="e052" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">训练模型</strong></li><li id="b084" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">评估或测试模型</strong></li><li id="3624" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">结论</strong></li><li id="aa6f" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">作者注</strong></li></ol><blockquote class="me mf mg"><p id="7176" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated"><strong class="kt jd">【已编辑:本文还包括模型的代码，分为不同的部分，在每一节的末尾给出。代码由GitHub托管。如果您没有看到任何，只需刷新页面。】</strong></p></blockquote></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="45d9" class="ng nh it bd ni nj nk nl nm nn no np nq ki nr kj ns kl nt km nu ko nv kp nw nx bi translated">开始前的一些说明:-</h1><ul class=""><li id="0a30" class="ml mm it kt b ku ny kx nz la oa le ob li oc lm od mr ms mt bi translated">使用的建筑:GRU的</li><li id="4860" class="ml mm it kt b ku mu kx mv la mw le mx li my lm od mr ms mt bi translated">编程语言、库、框架:Python 3.x、NumPy、Matplotlib、sklearn、TensorFlow 2.x、<strong class="kt jd">Keras【tensor flow后端】、</strong> unicodedata、os、io、time</li><li id="b6c3" class="ml mm it kt b ku mu kx mv la mw le mx li my lm od mr ms mt bi translated">专业水平:中等水平，需要一些序列对序列模型和矢量化序列的知识(例如，以矢量化形式表示句子)。)</li></ul><p id="6737" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> <em class="mh">注:-</em></strong><em class="mh"><br/></em><strong class="kt jd"><em class="mh">我知道你们大多数人都没有接触过功能强大的机器。别担心。我们将从谷歌获得帮助。一如既往！</em> </strong> <em class="mh">你可能熟悉google colab，它为你提供了一个可以运行你的jupyter笔记本的环境。Colab笔记本在谷歌的云服务器上执行代码，这意味着你可以利用谷歌硬件的力量，包括GPU和TPU。</em></p><p id="d44e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="mh">在继续实施该模型之前，如果您希望在本地机器上运行该模型，请确保您的系统/计算机/笔记本电脑满足以下要求。<br/> - GPU <br/> -大约8GB内存<br/> -大约10 GB空闲空间</em></p><p id="afcf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">所以，你不用担心这个。就去<a class="ae ln" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"><em class="mh">colab.research.google.com</em></a><em class="mh">。</em>加载或创建一个新笔记本，您就可以开始了。你可以通过<a class="ae ln" href="https://www.youtube.com/watch?v=inN8seMm7UI" rel="noopener ugc nofollow" target="_blank">谷歌</a>观看这个视频开始。</p><blockquote class="me mf mg"><p id="7392" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated"><strong class="kt jd"> <em class="it">注:强烈建议打开我的笔记本再看如何实施。你可以在这里找到笔记本</em> </strong> <a class="ae ln" href="https://github.com/Mahyar-Ali/Neural-Machine-Translation/blob/master/Neural_Machine_Translation.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="it">原文(Github)。</em> </strong> </a> <strong class="kt jd"> <em class="it">和</em> </strong> <a class="ae ln" href="https://colab.research.google.com/drive/1ZQy3GKKu9cioluUMz69BSo7LjLwC4yyA" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="it"> Colab版</em> </strong> </a> <strong class="kt jd"> <em class="it">【推荐】。</em> </strong></p></blockquote><div class="oe of gp gr og oh"><a href="https://colab.research.google.com/github/Mahyar-Ali/Neural-Machine-Translation/blob/master/Neural_Machine_Translation.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd jd gy z fp om fr fs on fu fw jc bi translated">谷歌联合实验室</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">编辑描述</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">colab.research.google.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ly oh"/></div></div></a></div><div class="oe of gp gr og oh"><a href="https://github.com/Mahyar-Ali/Neural-Machine-Translation/blob/master/Neural_Machine_Translation.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd jd gy z fp om fr fs on fu fw jc bi translated">mah yar-Ali/神经机器翻译</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">github.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov ly oh"/></div></div></a></div></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="24cd" class="ng nh it bd ni nj nk nl nm nn no np nq ki nr kj ns kl nt km nu ko nv kp nw nx bi translated">数据集:-</h1><p id="e0f6" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated"><a class="ae ln" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"/></a><strong class="kt jd"/>是目前最好的拥有最多制表符分隔的双语句子对的网站。它有超过30种不同语言的制表符分隔的双语句子对。数据集包含格式为:- <br/>“英语+ TAB +其他语言+TAB+attribute”<br/>的语言翻译对，本网站仅提供“英语和其他语言”之间的翻译数据集。<a class="ae ln" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank"><strong class="kt jd"/></a><strong class="kt jd"/>manythings.org不是唯一的网站，还有很多其他网站可以下载你选择的数据集。</p><h1 id="5ae0" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">图书馆:-</h1><p id="cd52" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">为了创建这种类型的模型，我们必须使用不同的库进行预处理、创建模型和评估其性能。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h1 id="bd33" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">预处理:-</h1><p id="8214" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">本文的目的是向您介绍开发nmt模型的过程。因此，在本文中，我不会解释准备数据集的所有步骤。你可以从我的<a class="ae ln" href="https://github.com/Mahyar-Ali/Neural-Machine-Translation" rel="noopener ugc nofollow" target="_blank"> GitHub库获得所有预处理的代码。</a></p><h2 id="2b89" class="ph nh it bd ni pi pj dn nm pk pl dp nq la pm pn ns le po pp nu li pq pr nw iz bi translated">可选:-</h2><p id="0fbf" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">下面是预处理步骤的一点解释。<br/> <a class="ae ln" href="https://github.com/Mahyar-Ali/Neural-Machine-Translation/blob/master/preprocess.py" rel="noopener ugc nofollow" target="_blank"> preprocess.py </a>包含了load_dataset，tokenize，create_tensorflow_dataset对象，预处理句子的所有必要函数。预处理句子涉及:- <br/> 1。增加一个<code class="fe ps pt pu pv b">&lt;start&gt;</code>和<code class="fe ps pt pu pv b">&lt;end&gt; toke.</code> <br/> 2。删除特殊字符。<br/> 3。从word → id和id →word <br/> 4创建字典映射。以矢量化的形式表示每个句子。<br/> 5。将每个句子填充到最大长度。</p><blockquote class="me mf mg"><p id="9acd" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">注意:如果您不理解预处理步骤，也不用担心。你可以随时使用我的代码进行预处理，直接开始关注机器学习部分。</p></blockquote><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/f6c009969e711f2464fc0f87411fd599.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*vom4LCbCasrtyh6BOhN4iw.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Initial Output</figcaption></figure><p id="88ce" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们将创建一个<a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly" rel="noopener ugc nofollow" target="_blank"> TensorFlow dataset对象</a>，这样我们就可以在训练时方便地访问它，然后将它分成几批。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h1 id="05ab" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">创建模型:-</h1><p id="c936" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">因为这是一项涉及自然语言处理的任务。因此，我们将使用RNN架构，因为RNNs在这类任务中提供了最佳结果。此外，由于这是一个翻译任务，我们将输入一个句子，模型将输出另一个句子(在这种情况下是一个翻译的句子)。这种类型的RNN被称为序列到序列模型。它的输入是一个序列，输出也是一个序列。</p><div class="oe of gp gr og oh"><a href="https://www.datadriveninvestor.com/2020/02/19/cognitive-computing-a-skill-set-widely-considered-to-be-the-most-vital-manifestation-of-artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd jd gy z fp om fr fs on fu fw jc bi translated">认知计算——一套被广泛认为是……</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">作为它的用户，我们已经习惯了科技。这些天几乎没有什么是司空见惯的…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oq l"><div class="px l os ot ou oq ov ly oh"/></div></div></a></div><p id="bdc0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">门控循环单元是RNN的扩展，它提供了比简单RNN更好的结果。如果您是门控循环<br/>单元(GRUs)的新用户，请参考<a class="ae ln" href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" rel="noopener" target="_blank"> <strong class="kt jd">和</strong> </a>。简而言之，gru类似于简单的rnn，只是增加了内存。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi py"><img src="../Images/b479c91a0758bc903bc5ba412c73fde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aVEICTj1fuZYXx0t.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">credits: <a class="ae ln" href="https://commons.wikimedia.org/" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/</a></figcaption></figure><p id="17d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将建立的模型是一个具有扩展注意力的编码器-解码器模型。我们先来了解编码器和解码器部分。</p><blockquote class="me mf mg"><p id="1871" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">为了简单起见，我将整个模型分成了三个子模型。<br/> 1。编码器<br/> 2。巴丹瑙注意<br/> 3。解码器</p></blockquote><h2 id="7932" class="ph nh it bd ni pi pj dn nm pk pl dp nq la pm pn ns le po pp nu li pq pr nw iz bi translated">1.编码器:-</h2><p id="678d" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">模型的这一部分使用了最简单的架构类型。我们将使用一个嵌入层，然后是GRUs。我们将使用自定义Keras层。如果您没有使用过定制层，请参考<a class="ae ln" href="https://www.tensorflow.org/tutorials/customization/custom_layers" rel="noopener ugc nofollow" target="_blank">这个</a>。只需15分钟即可开始。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/5de0e21914f7919bfbc8aba87038c2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7ZYKzAydTCIOyiZz"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">credits:<a class="ae ln" href="https://6chaoran.wordpress.com/" rel="noopener ugc nofollow" target="_blank">https://6chaoran.wordpress.com/</a></figcaption></figure><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="538e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这将为我们创建编码器层:-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="b552" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这个模型的工作方式是，我们在编码器部分输入一个西班牙语句子。编码器RNN对句子进行编码，并将其传递给解码器RNN，后者输出其英语翻译。为了得到更好的结果，我们在编码器部分使用了一个<a class="ae ln" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">嵌入层</a>来嵌入输入的句子。我们不需要担心嵌入层，因为Tensorflow会处理它。最重要的是模型架构。上图是一个简单的编码器和解码器模型。我们可以使用简单的编码器和解码器模型来完成我们的任务，但是这些类型的架构对于长句来说不太好用。我们也许可以用短句得到很好的结果，但是随着句子长度的增加，模型的准确性会降低。所以为了解决这个问题，我们将使用Bahdanau注意力。</p><h2 id="404f" class="ph nh it bd ni pi pj dn nm pk pl dp nq la pm pn ns le po pp nu li pq pr nw iz bi translated">2.巴丹瑙注意:-</h2><p id="cf7d" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">注意力模型的工作方式非常有趣。我们知道，当我们想考虑以前的信息时，也使用rnn。使得RNN考虑先前的信息来做出决定。但是随着序列长度的增加，由于消失梯度效应，句子起始单词的影响开始减小。必须阅读消失渐变<a class="ae ln" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">这里</a>。从技术上讲，这是有一定道理的。随着序列长度的增加，序列开头的单词的影响开始减小。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi qa"><img src="../Images/b05a1f1190853c46226ba1472d718f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXdhUVVTQwOHZz72Rc7vzA.png"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">credits: Andrew Ng</figcaption></figure><p id="a086" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是格鲁前来救援的地方。他们可以在一定程度上减少这种影响。随着消失梯度问题，还有一个非常重要的事情要考虑。在做出决定时，RNNs通常认为前一个单词是决定下一个单词的最重要的单词。这并不总是有帮助的。考虑下面的句子。“在花园里玩耍的两只猫在我的花园里。”那么，应该用什么来代替空白呢？我们可以很容易地说“are”代替了空格。但是现在考虑一下RNN会如何做决定。它将考虑最近的单词来做出决定。在这种情况下,“在花园里玩耍”,它没有给出任何应该发生的事情的意义，即“是，是，是，是，是”？<br/>做决定最重要的词是“两只猫”。所以我们必须教会模型在做决定时注意某些词。这就是Bahdanau注意力的概念派上用场的地方。它要做的是学习如何在做决定时注意句子的不同部分。<br/>这些想法在制作翻译模型时很有用，因为模型可以很容易地决定在翻译时关注句子的哪一部分。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/6f9f58bb82c918baf741b99da0020461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6zZx4itauarBMbqyRtQJFg.png"/></div></figure><p id="2220" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">答:所有gru的输出都在这一点上收集。输出将是<strong class="kt jd">(批量大小，序列长度，字典大小)</strong>的形状。<br/>在这种情况下，序列的长度是3。字典的大小是在预处理期间选择的。<br/> B:隐藏状态的输出就收集在这一点上。其形状为<strong class="kt jd"> (batch_size，hidden_units)。</strong>这是一个二维矢量，但我们必须将其转换成三维矢量，以处理“A”部分。所以我们将只扩展维度，即<strong class="kt jd"> (batch_size，1，hidden_units)。<br/> STEP-1: </strong>之后，A和B都被传递到一个密集层中，这个密集层带有隐藏单元"<strong class="kt jd"> h_u" </strong>，它会将A转换成<strong class="kt jd"> (batch_size，length_of_sequence，h_u) </strong>并将B转换成<strong class="kt jd"> (batch_size，1，h_u)。</strong>之后，A和B相加，得到形状<strong class="kt jd"> (batch_size，length_of_sequence，h_u)。然后，这个向量再次被传递到一个只有一个感知器的密集层，所以我们得到了形状为<strong class="kt jd"> (batch_size，length_of_sequence，1)的最终输出。<br/>第二步:</strong>在这之后，还有一个softmax层，给我们注意力权重。这些注意力权重存储了每个单词应该获得多少注意力的信息。<br/> <strong class="kt jd">步骤-3: </strong>然后将这些注意力权重乘以解码器的原始输出“所有gru的输出”，然后沿着“序列长度”轴相加，以获得可在解码器处用于生成翻译的上下文向量。</strong></p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="7a00" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这将创建巴达瑙注意层:-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">h_u Is 10</figcaption></figure><h2 id="2a0f" class="ph nh it bd ni pi pj dn nm pk pl dp nq la pm pn ns le po pp nu li pq pr nw iz bi translated">3.解码器:-</h2><p id="3c8b" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">现在，我们必须创建解码器模型，将编码文本翻译成英语。解码器与编码器相同，但有一些变化。解码器模型的行为将根据它是处于“训练过程”还是“测试过程”而改变。首先，让我们关注培训部分。正如在预处理步骤中提到的，每个句子都以一个<code class="fe ps pt pu pv b">&lt;start&gt;</code>标记开始。所以不管我们是训练还是测试，解码器模型的第一个输入将是一个<code class="fe ps pt pu pv b">start</code>令牌。考虑下图:-</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/4abee4bef6eb29ea92c1ab7280e8ccff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*MxDmjFveCpOK5BXFci7R8g.png"/></div></figure><p id="3e88" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是我们模型的架构。你可能想知道为什么我们给解码器模型一个输入。请记住，我们正在考虑“训练阶段”,在训练期间，我们不使用采样，而是向解码器提供输入，以便它可以根据真实输入和注意力向量预测下一个单词。然后根据模型生成的单词计算误差。像这样:-</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/b309337d16b01ed36c8a2e81d82f796d.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*rXQGWkG2AfuUyh2WLXeeMQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">This is the decoder part only.credits:tensorflow.org</figcaption></figure><p id="cfeb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">记住解码器的第一个字是<code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌。让我把这个分成几部分。</p><ol class=""><li id="35f3" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm mq mr ms mt bi translated">给编码器一个西班牙语句子。</li><li id="f86f" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">编码后的句子被传递给注意机制。</li><li id="04f5" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">注意力机制会生成一个向量，对句子进行完整编码。</li><li id="0261" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌传递到解码器的嵌入层。</li><li id="6212" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">来自嵌入层的输出与来自步骤3的编码矢量连接。</li><li id="6026" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">然后，连接的向量被传递给GRU，以预测下一个单词。</li><li id="3c82" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将预测单词与真实单词进行比较以计算误差。</li><li id="c0aa" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">如果“Estoy Bien”被输入，并且它的基本事实是“我很好”，那么step-7应该预测单词“我”。如果有任何其他字，那么将计算误差。</li><li id="f51a" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">之后，“I”被传递到解码器，该解码器通过嵌入层，然后在与来自步骤3的向量连接之后，被传递到试图预测下一个单词的GRU。这就是训练的方式。这也叫<strong class="kt jd"><em class="mh">老师逼死</em></strong></li><li id="361a" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated"><strong class="kt jd">记住:我们在训练时向解码器提供真实输入，但在测试期间，我们不提供任何输入。我们使用抽样的方法。我稍后会谈到这一点。</strong></li></ol><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="0f09" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这将创建解码器层:-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h1 id="54c2" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">训练模型:-</h1><p id="f5f4" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">我们将使用Adam Optimizer和<code class="fe ps pt pu pv b"><a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy?version=nightly" rel="noopener ugc nofollow" target="_blank">SparseCategoricalCrossEntropy</a></code>损失。我们提供的输入是用整数表示的单词，但是解码器的输出不是单个整数，而是一个大小等于字典大小的向量。我们从输出的字典中选择单词的索引，使得它具有最高的概率。计算损失时，我们必须处理整部词典。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="af94" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在大型数据集上训练大型模型时，重要的是我们应该在训练时存储模型进度。Tensorflow为sus提供了一个功能，可以存储训练时不同层的状态。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="c2de" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一切都准备好了。让我们继续训练。为了训练，我们将使用Tensorflow提供的<a class="ae ln" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank">梯度带</a>来更新权重。这里是培训的一个快速概述。</p><ol class=""><li id="4637" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm mq mr ms mt bi translated">定义历元数并初始化编码器模型的隐藏状态。</li><li id="5cc4" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">初始化总损失(初始为零)。</li><li id="ae4e" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">从数据集中获取大量数据。</li><li id="93e8" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将该数据传递给编码器。编码器将返回编码后的句子及其隐藏状态。</li><li id="127e" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将隐藏状态和编码输出与真实输出一起传递给解码器。即原文翻译。</li><li id="8397" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将隐藏状态和编码输出传递给注意机制，以生成新的编码向量。</li><li id="2245" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将<code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌传递给解码器。注意:由于我们使用批量数据进行训练，所以我们必须将<code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌传递给每一批数据。这可以通过复制等于批次sie的<code class="fe ps pt pu pv b">&lt;start&gt;</code>来完成。</li><li id="4e89" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">步骤6和7在解码器层同时完成。然后，解码器层生成预测及其隐藏状态。</li><li id="3fc8" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">计算预测的误差。当我们计算整批的损失时，我们必须通过将损失除以序列的长度来说明。这只是一个惯例。</li><li id="a22c" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">现在，得到所有可训练的变量。加权并计算这些变量相对于损失的梯度(导数)。最后，通过应用渐变来更新变量。</li></ol><p id="3cd3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就是这样。这就是你如何训练一个神经机器翻译模型。为了方便起见，我把它分成了两个功能。一个用于步骤1-3，另一个用于步骤4-10。<br/>步骤4-10的功能:-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="f74f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了训练模型:-</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h1 id="5448" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">评估/测试模型:-</h1><p id="32cc" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">如果你已经完成了这篇文章的这一部分，相信我，你已经准备好收获你的努力和耐心的果实了。在大约2分钟内，你将有一个可以从西班牙语翻译成英语的模型。您可以在任何双语句子对数据集上训练该模型，并在不同语言之间进行翻译。<br/>在进入模型测试之前，有一些事情需要了解。唯一的区别在于解码器模型的工作方式。记得我提到过解码器模型的行为。它根据阶段的不同而不同。现在我们正处于测试阶段。在训练期间，我们向解码器提供了实际输出，以便它能够学习。但是现在我们想把西班牙语文本转换成英语。我们没有英文翻译。我们希望模型为我们提供英语翻译。但是解码器模型需要前一个字来预测下一个字。这就是取样的由来。如果你不知道取样。看看<a class="ae ln" href="https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f#:~:text=In%20the%20example%2C%20first%20the%20word%20The%20is%20the%20most%20likely.&amp;text=However%2C%20when%20learning%2C%20the%20output,This%20is%20called%20sampling%20." rel="noopener">这首</a>。</p><p id="2a24" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是我做的一个小图表</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/d85ea86de31a9622baecc52aad49f1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*0k9xgP6E7MawwelwKzQx4w.png"/></div></figure><p id="9b29" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们所做的是，首先，我们将<code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌传递给解码器。然后它预测下一个单词。这个词不再用于计算误差。事实上，这个单词将被用作下一个GRU(或时间步长)的输入。在下一个时间步，模型生成了另一个单词。我们通过将单词追加到一个空字符串来跟踪所有的单词。那么我们什么时候停止创造新词呢？<br/>当达到输出序列的最大长度或一旦模型预测下一个字为<code class="fe ps pt pu pv b">&lt;end&gt;</code>时，我们将停止向RNN输入。这就是ML的妙处。它会自动学习句子应该在哪里结束。太神奇了。</p><p id="ab22" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里有一个简单的概述</p><ol class=""><li id="0845" class="ml mm it kt b ku kv kx ky la mn le mo li mp lm mq mr ms mt bi translated">用西班牙语输入句子。</li><li id="e3dd" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">对句子进行预处理。</li><li id="65f0" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">将句子拆分成单词，并在句子长度小于RNN可接受的长度的末尾填充零。</li><li id="9f80" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">把它转换成张量。</li><li id="b806" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">初始化编码器的隐藏状态，将句子输入编码器，得到编码输出。</li><li id="5493" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">在解码器中输入<code class="fe ps pt pu pv b">&lt;start&gt;</code>令牌。</li><li id="705d" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">开始一个循环，逐字预测给定句子的翻译。</li><li id="7d85" class="ml mm it kt b ku mu kx mv la mw le mx li my lm mq mr ms mt bi translated">在每一步，GRU将输出一个大小等于字典大小的向量。我们从向量(指最可能的值)中选取最高值的索引，使用字典将其转换为相应的单词，将该单词附加到输出字符串，将该单词的索引再次传递给解码器模型以预测下一个单词。采样！</li></ol><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pf pg l"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">u stands for Unicode</figcaption></figure><p id="e38b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">和输出:-</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/a0e4900461d735c50e7eb8d07176740b.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*i2Qsr2Tnqrf5xynXUuo0NQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Perfect Translation.</figcaption></figure><h1 id="1528" class="ng nh it bd ni nj pa nl nm nn pb np nq ki pc kj ns kl pd km nu ko pe kp nw nx bi translated">结论:-</h1><p id="5a7f" class="pw-post-body-paragraph kr ks it kt b ku ny kd kw kx nz kg kz la ox lc ld le oy lg lh li oz lk ll lm im bi translated">仅此而已。你可以在任何西班牙语句子上测试它，希望你会看到一些惊人的结果。该模型可以通过调整一些参数和对大量时期的整个数据集进行训练来实现最先进的性能。十个纪元算不了什么，但还是取得了不错的成绩。这就是注意力机制的力量。甚至google Translator也有相同的架构，只是增加了堆叠的LSTMs，而不是单个GRU。您可以通过更改该模型中的一些内容来开发与Google Translate相同的架构，但这需要大量的计算来训练。无论如何，你现在可以说，你可以开发一个语言翻译模型与国家的艺术表演。</p><p id="e846" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">非常感谢大家有耐心等了这么久看到一些好的结果。有注意力的神经机器翻译一开始真的是一个很难抓住的概念。如果你不太理解这篇文章。考虑再读一遍。相信我，手里有一个神经机器翻译模型，真的是一大进步。考虑再读一遍这篇文章，看看我在文章中提供的所有链接。</p><p id="fdb2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">再次感谢！</p><p id="84f7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">米（meter的缩写））马赫亚尔·阿里</p><p id="8f51" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">进入专家视角— </strong> <a class="ae ln" href="https://datadriveninvestor.com/ddi-intel" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">订阅DDI英特尔</strong> </a></p></div></div>    
</body>
</html>