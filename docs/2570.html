<html>
<head>
<title>Data Science : K-Nearest Neighbor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学:K-最近邻</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/data-science-k-nearest-neighbor-b7843b285300?source=collection_archive---------3-----------------------#2020-05-06">https://medium.datadriveninvestor.com/data-science-k-nearest-neighbor-b7843b285300?source=collection_archive---------3-----------------------#2020-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/36b9dac92e9faf8c0b52f57d3261bb4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAw-gwAZBGmQkOI7WwWHxw.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image:ComplexDiscovery</figcaption></figure><div class=""/><h1 id="bcd7" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍</h1><p id="b895" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">k-最近邻也称为KNN，是一种用于分类和回归问题的监督机器学习算法。最近邻分类器背后的思想很简单。</p><p id="6e8d" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">如果它像鸭子一样走路，像鸭子一样嘎嘎叫，那么它很可能是一只鸭子</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi md"><img src="../Images/42b58c8839c0799deb77b0a6782f0173.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*DTzjv4aykFtJVRkUUFAv6g.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image: rashmee.com</figcaption></figure><h1 id="7793" class="kc kd jf bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">直觉:</strong></h1><p id="3ae0" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">KNN是非常简单的机器学习算法。该算法使用K-最近邻对新数据点进行分类。</p><p id="9f6d" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">这里，我们讨论的是不同类的数据点，我们的新数据点的类是基于各种类的最近K个数据点的类来决定的。</p><p id="1430" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">它也被称为基于实例的分类器。基于实例的分类器有两种类型。</p><ol class=""><li id="098b" class="mi mj jf lc b ld ly lh lz ll mk lp ml lt mm lx mn mo mp mq bi translated">死记硬背的学生</li><li id="e844" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">最近邻(使用K最近邻)</li></ol><p id="60ab" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">让我们用一个简单的图表来理解这一点:</strong></p><p id="8969" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">我们可以看到红色和浅蓝色各种数据点，我们创建了一个二维空间(X和Y坐标)。</p><p id="e8ab" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">当一个新数据点出现时(如图所示)，我们需要对其进行分类(考虑到我们有红色和浅蓝色两个类别)。在KNN，我们使用K-最近邻对这个新数据点的类别进行分类。</p><p id="0832" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">希望现在你已经清楚了K-最近邻的定义以及它将如何解决这里的分类问题。</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/8a5b8feaf15ccb7f3f309521083d7440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*piqTGBzjmYZUI-gs1uJeCg.jpeg"/></div></div></figure><p id="ed6f" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">现在，由于我们必须找到K个最近的邻居，问题是<strong class="lc jg">我们应该考虑</strong>多少个邻居来预测新数据点的类别，以及<strong class="lc jg">如何测量哪个邻居是最近的</strong>。</p><h2 id="0de6" class="mx kd jf bd ke my mz dn ki na nb dp km ll nc nd kq lp ne nf ku lt ng nh ky ni bi translated">我们需要三样东西来创建一个KNN算法</h2><ol class=""><li id="fe52" class="mi mj jf lc b ld le lh li ll nj lp nk lt nl lx mn mo mp mq bi translated">存储的记录集(基本上是训练数据集)</li><li id="e056" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">计算数据点之间距离的距离度量。</li><li id="8235" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">K的正确值，最近邻的数目。</li></ol><h2 id="da98" class="mx kd jf bd ke my mz dn ki na nb dp km ll nc nd kq lp ne nf ku lt ng nh ky ni bi translated"><strong class="ak">如何衡量哪个邻居最近？</strong></h2><p id="8d65" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated"><strong class="lc jg">为了测量最近的邻居，我们使用距离度量</strong>。这些距离度量使用各种距离度量来寻找新数据点和最近的K个邻居之间的距离，并且基于大多数邻居，我们对新数据点的类别进行分类。</p><p id="9b5f" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">基础数学定义(来源维基百科)，</p><blockquote class="nm nn no"><p id="ebab" class="la lb np lc b ld ly lf lg lh lz lj lk nq ma ln lo nr mb lr ls ns mc lv lw lx ij bi translated"><em class="jf">距离度量使用距离函数，该函数提供数据集中每个元素之间的关系度量。</em></p></blockquote><p id="6c74" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">距离指标有多种类型。主要问题如下</strong></p><ol class=""><li id="5b42" class="mi mj jf lc b ld ly lh lz ll mk lp ml lt mm lx mn mo mp mq bi translated">欧几里得距离</li><li id="892a" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">曼哈顿距离</li><li id="e0aa" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">闵可夫斯基距离</li></ol><p id="d374" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">欧几里德距离:</strong></p><p id="18d0" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">欧几里德距离表示两点之间的最短距离。</p><p id="b4ec" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">这里(X2，y2)和(X1，Y1)是二维空间中的两个点，我们使用给定的公式计算两个点之间的距离d。</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/282e87eebd076b084bb54854acad1ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*Tt6a01VofqCLd8YGJvgNKA.png"/></div></figure><p id="9bee" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">曼哈顿距离:</strong></p><p id="b903" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">曼哈顿距离是所有维度上的点之间的绝对差之和。曼哈顿距离也被称为出租车几何，城市街区距离等。</p><p id="b7eb" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">在点1位于(x1，y1)且点2位于(x2，y2)的平面中。</p><p id="4f62" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">曼哈顿距离= |x1 — x2| + |y1 — y2|</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5a5f9ce703e631038f024259fe3ff786.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*GnQGpVdjgAlGLDhg9yez8w.png"/></div></figure><p id="5c9c" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">闵可夫斯基距离:</strong></p><p id="804f" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">闵可夫斯基距离是欧几里德距离和曼哈顿距离的推广形式。</p><p id="e239" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">两点间的闵可夫斯基距离序<em class="np"> p </em></p><figure class="me mf mg mh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/afe1aecdeafee7dc8c56ee1d2630085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6d6Ck98Tm8eQipz78FMHQ.jpeg"/></div></div></figure><p id="08c5" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">我们将使用欧几里德距离作为我们的KNN分类器。</strong></p><h2 id="35c1" class="mx kd jf bd ke my mz dn ki na nb dp km ll nc nd kq lp ne nf ku lt ng nh ky ni bi translated"><strong class="ak">寻找K的正确值:</strong></h2><p id="e013" class="pw-post-body-paragraph la lb jf lc b ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ij bi translated">k是分类时要考虑的邻居/数据点的总数。</p><p id="1b2d" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">为了选择最适合您的数据的K，我们使用不同的K值运行KNN算法几次，并选择减少我们遇到的错误数量的K，同时保持算法在给定以前从未见过的数据时准确做出预测的能力。</p><p id="7ed6" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">带有图片的Python代码将清除这一点。取误差为常数的K值(对于多个点)。或者，我们可以使用GridSearchCV来寻找最佳参数。</p><p id="2b5a" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">这里我们可以看到，从K=16开始，误差是恒定的，因此我们将选择K=16。</p><figure class="me mf mg mh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/0d4552308c8b9e79ab63089d0d71143e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCP1vK84mFd4uMQ5goMcJw.png"/></div></div></figure><p id="4bd5" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">创建KNN模型时的重要注意事项:</strong></p><p id="e592" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">KNN需要缩放/标准化，因为它使用距离作为两个数据点之间的度量。对于一个好的模型，要求所有的特征都具有相同的比例，这样一个特征就不能支配另一个特征。</p><p id="aa1d" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">一旦我们找到K的最佳值并最终确定要使用的距离度量，我们将获得一个模型，该模型将基于其最近邻对新数据点进行分类。</p><p id="c09c" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">在图片中，由于选择了K=1的值，新数据点的类别为<strong class="lc jg">A类(红星)</strong></p><figure class="me mf mg mh gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d5375d01afdfaec87fe155531cab7338.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*nzPHgvsf4S7ylD9_Xv-c_w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">image:KDnuggets</figcaption></figure><p id="c8ec" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">请找到正在逐步实现KNN算法的python代码。</p><figure class="me mf mg mh gt is"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="62f1" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">KNN的优势:</strong></p><ol class=""><li id="e6b5" class="mi mj jf lc b ld ly lh lz ll mk lp ml lt mm lx mn mo mp mq bi translated">算法简单，易于实现。</li><li id="2239" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">它适用于线性和非线性数据集。</li><li id="4a30" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">对输入数据中的噪声具有鲁棒性。</li><li id="8c58" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">算法是通用的。它可用于分类、回归和搜索。</li></ol><p id="4649" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">缺点:</strong></p><ol class=""><li id="e4fe" class="mi mj jf lc b ld ly lh lz ll mk lp ml lt mm lx mn mo mp mq bi translated">它是懒惰的，也叫做懒惰的学习者。必须执行每个KNN模型来寻找新的看不见的数据点的类别。</li><li id="7b0e" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">没有办法确定数据集是线性的还是非线性的。我们只能尝试检查性能，如果模型不能为线性数据集提供良好的准确性，我们可以认为数据集是非线性的。</li><li id="75c8" class="mi mj jf lc b ld mr lh ms ll mt lp mu lt mv lx mn mo mp mq bi translated">由于它是懒惰的学习者，人们避免使用它，因为计算成本很高。</li></ol><p id="7760" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated"><strong class="lc jg">结论</strong> : KNN是一种简单的算法，对非线性数据集也有很好的表现，可以在数据量较低时使用。随着数据量的增加，算法的计算成本增加。</p><div class="ip iq gp gr ir oa"><a href="https://www.datadriveninvestor.com/2020/02/19/five-data-science-and-machine-learning-trends-that-will-define-job-prospects-in-2020/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd jg gy z fp of fr fs og fu fw je bi translated">将定义2020年就业前景的五大数据科学和机器学习趋势|数据驱动…</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">数据科学和ML是2019年最受关注的趋势之一，毫无疑问，它们将继续发展…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ix oa"/></div></div></a></div><p id="ce7c" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">我希望你喜欢我的文章。如果你喜欢并希望我激励你写更多，请点击<strong class="lc jg">拍手</strong>。也与你的朋友分享。</p><p id="0cae" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">想要连接:</p><p id="4286" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">链接地:【https://www.linkedin.com/in/anjani-kumar-9b969a39/ T2】</p><p id="4983" class="pw-post-body-paragraph la lb jf lc b ld ly lf lg lh lz lj lk ll ma ln lo lp mb lr ls lt mc lv lw lx ij bi translated">如果你喜欢我在Medium上的帖子，并希望我继续做这项工作，请考虑在<a class="ae op" href="https://www.patreon.com/anjanikumar" rel="noopener ugc nofollow" target="_blank">上支持我</a></p><figure class="me mf mg mh gt is"><div class="bz fp l di"><div class="oq nz l"/></div></figure></div></div>    
</body>
</html>