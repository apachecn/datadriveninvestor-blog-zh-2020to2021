<html>
<head>
<title>The Basics of Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树的基础</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/the-basics-of-decision-trees-e5837cc2aba7?source=collection_archive---------0-----------------------#2020-03-08">https://medium.datadriveninvestor.com/the-basics-of-decision-trees-e5837cc2aba7?source=collection_archive---------0-----------------------#2020-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="9f04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">决策树算法-第1部分</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/27fdec77df0fe8728984c7cee8011216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tci16Hnihl0ej8k_B_uO3w.jpeg"/></div></div></figure><h2 id="a9e5" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><strong class="ak"> 1。简介</strong></h2><p id="3766" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">决策树是非参数监督学习方法，可以应用于回归和分类问题。按照树的类比，决策树实现了一个连续的决策过程。从根节点开始，评估一个特征并选择两个节点(分支)中的一个，树中的每个节点基本上是一个决策规则。重复这个过程，直到到达最后一片叶子，这通常代表目标。如果我们关心可解释性，决策树也是有吸引力的模型。</p><h2 id="4c84" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><strong class="ak"> 2。各种决策树算法</strong></h2><p id="95ae" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">创建决策树的算法有:</p><ul class=""><li id="981f" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/ID3_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> ID3 </strong> </a> <strong class="jp ir">(迭代二分法3) </strong>由罗斯·昆兰于1986年开发。该算法创建一个多路树，为每个节点(即，以贪婪的方式)寻找将为分类目标产生最大信息增益的分类特征。树生长到最大尺寸，然后通常应用修剪步骤来提高树概括看不见的数据的能力<a class="ae me" href="https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart" rel="noopener ugc nofollow" target="_blank">【1】</a>。</li><li id="af2a" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir"> C4.5 </strong>由Ross Quinlan于1993年开发，是ID3的后续版本，通过动态定义一个离散属性(基于数值变量)将连续属性值划分为一组离散区间，取消了要素必须分类的限制。C4.5将经过训练的树(即ID3算法的输出)转换成if-then规则集。然后，评估每个规则的准确性，以确定应用它们的顺序。如果在没有规则的前提条件的情况下规则的准确性提高了，则通过移除规则的前提条件来进行修剪<a class="ae me" href="https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart" rel="noopener ugc nofollow" target="_blank">【1】</a>。</li><li id="af22" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir"> C5.0 </strong>是昆兰在专有许可下发布的最新版本。与C4.5相比，它使用更少的内存，构建更小的规则集，同时更加准确<a class="ae me" href="https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart" rel="noopener ugc nofollow" target="_blank">【1】</a>。</li><li id="1623" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir"> CART(分类和回归树)</strong>与C4.5非常相似，但不同之处在于它支持数值目标变量(回归)，不计算规则集。CART使用在每个节点产生最大信息增益的特征和阈值构建二叉树<a class="ae me" href="https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart" rel="noopener ugc nofollow" target="_blank">【1】</a>。</li></ul><p id="4454" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> scikit-learn </strong>使用优化版本的<strong class="jp ir"> CART算法</strong></p><h2 id="8eb0" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><strong class="ak"> 3。决策树术语</strong></h2><p id="c0b8" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">为了与树的类比保持一致，该术语采用了树的术语<a class="ae me" href="https://gdcoder.com/decision-tree-regressor-explained-in-depth/" rel="noopener ugc nofollow" target="_blank">【2】</a>。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mk"><img src="../Images/d3810fb18f5b96e6ce381ec8a98e9a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hvhzPjR1nU52chzu9FDWg.png"/></div></div></figure><ul class=""><li id="6ada" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated"><strong class="jp ir">根节点</strong>:决策树的第一个节点</li><li id="1456" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">分割</strong>:从根节点开始，将节点分割成两个或两个以上子节点的过程</li><li id="9225" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">节点</strong>:将根节点的结果拆分成子节点，并将子节点拆分成子节点</li><li id="dcfa" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">叶节点或终端节点</strong>:节点的末端，因为节点不能再分割</li><li id="dc97" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">剪枝</strong>:是一种通过删除决策树的子节点来减少决策树大小的技术。目标是降低复杂性，以提高预测准确性，并避免过度拟合</li><li id="d86b" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">分支/子树</strong>:整个树的一个子部分称为分支或子树。</li><li id="57da" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir">父节点和子节点</strong>:被划分为子节点的节点称为子节点的父节点，子节点是父节点的子节点。</li></ul><h2 id="250d" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><strong class="ak"> 4。决策树直觉</strong></h2><p id="2f35" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">让我们考虑下面的例子，其中决策树对决策树预测棒球运动员的工资情况:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/bd0de449ca6a8facf784682e38750ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BnKk1aLzpglWGBEqiTUNNg.png"/></div></figure><p id="ced4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用Hitters数据集，根据年数(他在大联盟中打球的年数)和击球数(他在上一年击球的次数)来预测棒球运动员的工资(平均对数工资)。</p><p id="bc73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于这些特征，决策树模型从树的顶部(根节点)开始学习一系列分裂规则。</p><ol class=""><li id="b448" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk mm mb mc md bi translated">被分割成子节点根节点被分配给右分支，观察规则具有年份&lt;4.5 to the left branch, which means the players in dataset with Years&lt;4.5 having mean log salary is 5.107 and we make a prediction of e5.107 thousands of dollars, i.e. $165,174 for these players</li><li id="ea3d" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">Players with Years&gt; =4.5，然后该组被进一步细分</li><li id="1404" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">在这种情况下，可以看到决策树将一个片段分成三个区域，其中该区域确定棒球运动员的工资，并且可以说该区域是决策边界[ <a class="ae me" href="http://faculty.marshall.usc.edu/gareth-james/ISL/" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]。</li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/29846cd89233b4a6a3bf2dc5fde7623b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMyXvVpaQLzj52NzRzsswQ.png"/></div></div></figure><p id="8da4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这三个区域可以写成</p><p id="11d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> R1 </strong> ={X |年&lt; 4.5 }</p><ul class=""><li id="4dbe" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk ma mb mc md bi translated"><strong class="jp ir"> R2 </strong> ={X |年&gt; =4.5，点击量&lt; 117.5 }</li><li id="9f32" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated"><strong class="jp ir"> R3 </strong> ={X |年&gt; =4.5，命中数&gt; =117.5 }。</li><li id="a867" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk ma mb mc md bi translated">从这种直觉有一个过程，决策树如何分裂特征形成一个区域，可以预测棒球运动员的工资。这个过程将在下一篇文章(决策树算法-第2部分)中详细解释。</li></ul><p id="74d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak"> 5。决策树中的分裂</strong></p><h2 id="cd48" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">为了使用决策算法在信息最丰富的特征处分割节点，我们从树根开始，并在导致最大信息增益(IG)的特征上分割数据。这里，目标函数是最大化每次分裂的信息增益(IG ),我们定义如下:</h2><p id="678a" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated"><em class="mp"> f </em>是执行拆分的特征，<em class="mp"> Dp </em>和<em class="mp"> Dj </em>是父节点的数据集，<em class="mp"> j </em>第个子节点，<em class="mp"> I </em>是我们的杂质度量，<em class="mp"> Np </em>是父节点的样本总数，<em class="mp"> Nj </em>是第<em class="mp"> j </em>子节点的样本数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/0929fecafcab151aa886699ec941c9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*qpUAC2VxA1KFMAyrEINREQ.png"/></div></figure><p id="2b92" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，信息增益就是父节点杂质和子节点杂质之和的差——子节点杂质越低，信息增益越大。然而，为了简化和减少组合搜索空间，大多数库(包括scikit-learn)都实现了二叉决策树。这意味着每个父节点被分成两个子节点，<em class="mp"> D-left </em>和<em class="mp"> D-right。</em></p><p id="7855" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">杂质测量实现二元决策树，二元决策树中常用的三种杂质测量或分裂标准是<em class="mp">基尼杂质(IG) </em>、<em class="mp">熵(IH)和误分类误差(IE)</em>【4】</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/40da7d2eb798266129dc6f1dccbd438a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*R_GDx8NhSZ_p27EN8Wh6EQ.png"/></div></figure><p id="e3f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak"> 5.1基尼杂质</strong></p><h2 id="ca0d" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">根据<a class="ae me" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">维基百科</a>【5】，</h2><p id="8768" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated"><em class="mp">由CART(分类和回归树)算法用于分类树</em>，<em class="mp"> Gini杂质是根据标签在子集中的分布随机标记的集合中随机选择的元素被错误标记的频率的度量。</em></p><p id="cd2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数学上，我们可以把基尼系数写成如下</p><p id="db53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中<em class="mp"> j </em>是节点中存在的类的数量，而<em class="mp"> p </em>是节点中类的分布。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f46a77ff7a110ae775f43d270a49efb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*V5NKEredkTuDnoQPyjmyOw.png"/></div></figure><p id="4878" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用有303行和13个属性的<a class="ae me" href="https://archive.ics.uci.edu/ml/datasets/Heart+Disease" rel="noopener ugc nofollow" target="_blank">心脏病数据集</a>进行简单模拟。目标包括138个值0和165个值1</p><p id="1d3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了从数据集构建决策树并确定哪种分离是最好的，我们需要一种方法来测量和比较每个属性中的基尼系数杂质。第一次迭代中最低的Gini杂质值将是根节点。我们可以将等式3写成:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d0554aa76af20d0e8218c428bec257db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/0*UxGWvrRifh3dqWOQ"/></div></figure><p id="30bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在此模拟中，仅使用性别、fbs(空腹血糖)、exang(运动诱发的心绞痛)和目标属性。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mt"><img src="../Images/65b113d8d66f19a217c81bc29b852e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRptCs22opsiVSglWd5yGg.png"/></div></div></figure><p id="2b62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何测量性别属性中的基尼杂质</strong></p><p id="799d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Gini杂质—左侧节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/70cd1aefad0d7322f408ecf796e08acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*2ndE048b-mZsOW7jGhQm2w.png"/></div></figure><p id="6c93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">吉尼杂质—右节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c32e8407d3edc159dae94e1e7a62da75.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*duP-RZz_uqPsGCto83LMqw.png"/></div></figure><p id="8f97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们已经测量了两个叶节点的基尼系数。我们可以用加权平均来计算总基尼系数。左侧节点代表138名患者，而右侧节点代表165名患者</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/ccc987877de321a9b216ff228387b653.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*vZoY-xvtohHlgME4wxTXQQ.png"/></div></figure><p id="f353" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总基尼系数杂质—叶节点</p><p id="cd1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何测量Fbs(空腹血糖)中的基尼杂质</strong> <strong class="jp ir">属性</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c6abf24ce4486e42f6a2110af610ca16.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*Ms5_6AyOi88KRHqtNcsr5g.png"/></div></figure><p id="5870" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Gini杂质—左侧节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f2e4269305384982ba432585b52942ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*nd8JBT_sNcRmfoPmE7qu2g.png"/></div></figure><p id="abc1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">吉尼杂质—右节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/38b526ab80d472bf3810a453d650ee21.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*737ks4tN95RmntxNoeAvPQ.png"/></div></figure><p id="3fe4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总基尼系数杂质—叶节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c75cdbf9647f73d728137a3ad0c5c4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*us9J-6tAkkl8AuXBVfYqnw.png"/></div></figure><p id="a7ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何测量Exang(运动诱发心绞痛)属性中的Gini杂质</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi na"><img src="../Images/34930ca170c91d999f8f19f3b79821c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*rS2R__ScmkXav7R0o11XSw.png"/></div></figure><p id="f4a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Gini杂质—左侧节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0442b4560f9d270cdbb81423b720929e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*imlmEAq5Pff0TQwbT1qPEw.png"/></div></figure><p id="a678" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">吉尼杂质—右节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f22a4f1cc4d57b3dd1eab8c18b74f98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*KiKMK1f3wiWN9eY0HOBOww.png"/></div></figure><p id="128e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总基尼系数杂质—叶节点</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3a0bfee8d3291f4c522b9b891b42aca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*AdCnWqLMT3seOgMnKxT_zQ.png"/></div></figure><p id="4f9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Fbs(空腹血糖)的基尼杂质最低，所以我们要在根节点使用它</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1ec59b27129ff489ebaae5adbe62b270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*28cOoKNFYJUOv4we5aNFHQ.png"/></div></figure><p id="f97a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak"> 5.2熵</strong></p><h2 id="e914" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">由ID3、C4.5和C5.0树生成算法使用。信息增益基于熵的概念，熵的度量被定义为[ <a class="ae me" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]:</h2><p id="2b85" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">其中<em class="mp"> j </em>是节点中存在的类的数量，而<em class="mp"> p </em>是节点中类的分布。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a3bac6a10952a686de1a81c173a401e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*coWr5c4M7IQVEo9OzJanvw.png"/></div></figure><p id="4380" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在同一个案例和同一个<a class="ae me" href="https://archive.ics.uci.edu/ml/datasets/Heart+Disease" rel="noopener ugc nofollow" target="_blank">数据集</a>中，我们需要一种方法来度量和比较每个属性中的熵。第一次迭代的最高熵值将是根节点。</p><p id="485e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们首先需要计算目标属性的熵</p><p id="3d86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何测量性别属性中的熵</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ea124f38a2730b64935da1104b86bc3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KFgTicw0qBw60p1juuaRzQ.png"/></div></figure><p id="9363" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵—性别= 0</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/70cd1aefad0d7322f408ecf796e08acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*2ndE048b-mZsOW7jGhQm2w.png"/></div></figure><p id="2a0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵—性别= 1</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b0f7891c66001e49d35c00c3ba17ad48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*Jrc501ITDFcoQtrE533Pjw.png"/></div></figure><p id="3d33" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们已经测量了两个叶节点的熵。我们再次取加权平均值来计算总熵值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f568af801f4f06fcb06897e3385bd3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*hABWmggHDGxCYao19t_vDA.png"/></div></figure><p id="01d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵——性</p><p id="3394" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何测量Fbs属性中的熵</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nh"><img src="../Images/cd76910db8e049ab5555d77d76e573dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kbv0juuzWksRIKBfNr2edQ.png"/></div></div></figure><p id="1c6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Fbs = 0</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f2e4269305384982ba432585b52942ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*nd8JBT_sNcRmfoPmE7qu2g.png"/></div></figure><p id="2652" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Fbs = 1</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3146fd2ede87f4fae1d029680b6308fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*nb1zrw6St72BWPgiV9Ir0w.png"/></div></figure><p id="b8c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Fbs</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9d858ab6bb9a34ecdc8f87ecca0c9b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*g7vBJnaaqH9eZlevjizY1Q.png"/></div></figure><p id="90c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">如何度量Exang属性中的熵</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nk"><img src="../Images/9e6ae510f53f9e1ad2d93f510a3ff21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yy2UDKaOIkjYpSheeC5LtA.png"/></div></div></figure><p id="161a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Exang = 0</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0442b4560f9d270cdbb81423b720929e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*imlmEAq5Pff0TQwbT1qPEw.png"/></div></figure><p id="7b76" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Exang = 1</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/caedc95c08909fac8c6f93ad9fc7d735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*4aoH9A5C-slBy0dfyY8Nwg.png"/></div></figure><p id="418a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵— Exang</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4b9d8bfc838e6c53166958785f64fbcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*igHmyuWwQzbB0k7ui8Vy9A.png"/></div></figure><p id="9128" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> Fbs(空腹血糖)具有最高的基尼系数，因此我们将在根节点使用它，与我们从基尼系数得到的结果完全相同。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nn"><img src="../Images/17a1c72f68a76f734811dc285caec1c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kFkZKlMtzjuI2m2XY0NQg.png"/></div></div></figure><p id="c9e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak"> 5.3误分类杂质</strong></p><h2 id="6b51" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">另一个杂质测量是错误分类杂质，数学上，我们可以写错误分类杂质如下</h2><p id="8591" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">就质量性能而言，该指数不是最佳选择，因为它对不同的概率分布并不特别敏感(这可以很容易地使用基尼系数或熵将选择驱动到细分)[6]。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/43ca8c8eb70ac235ef021adec3ec9297.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*z7vOj6jpq8ZOmXfA0ISW3w.png"/></div></figure><p id="ea3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6.树木的优点和缺点</p><h2 id="70a0" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><strong class="jp ir">优势</strong></h2><p id="98a2" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated">树很容易向人们解释。事实上，它们甚至比线性回归更容易解释！</p><ol class=""><li id="7190" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk mm mb mc md bi translated">一些人认为决策树比之前章节中看到的回归和分类方法更能反映人类的决策。</li><li id="9444" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">树可以用图形显示，即使非专家也很容易理解(特别是如果它们很小的话)。</li><li id="7aa7" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">树可以轻松处理定性预测，而不需要<br/>创建虚拟变量【3】。</li><li id="d0be" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated"><strong class="jp ir">缺点</strong></li></ol><p id="c1ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，树通常不具有与其他回归和分类方法相同的预测准确性水平[3]。</p><ol class=""><li id="7c7f" class="lv lw iq jp b jq jr ju jv jy lx kc ly kg lz kk mm mb mc md bi translated">继续学习— <strong class="ak">决策树中的分裂过程</strong></li></ol><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi np"><img src="../Images/de76b3a3df5c6b80e31261b619165557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPRnFd7prTjoyafxmsLyyw.jpeg"/></div></div></figure><h2 id="b168" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><a class="ae me" href="https://medium.com/@arifromadhan19/classification-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-8e5f5228b11e" rel="noopener">决策树中的分类——一步一步的CART(分类和回归树)——第二部分)</a></h2><ol class=""><li id="9cb2" class="lv lw iq jp b jq lq ju lr jy nq kc nr kg ns kk mm mb mc md bi translated"><a class="ae me" href="https://medium.com/@arifromadhan19/regrssion-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-196c6ac9711e" rel="noopener">决策树中的回归——一步一步的CART(分类和回归树)——第三部分)</a></li><li id="4204" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">* sci kit-learn包将CART实现为其默认决策树。</li></ol><p id="0157" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak">关于我</strong></p><h2 id="0937" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated">我是一名数据科学家，专注于机器学习和深度学习。你可以通过<a class="ae me" href="https://medium.com/@arifromadhan19" rel="noopener">媒体</a>和<a class="ae me" href="https://www.linkedin.com/in/arif-romadhan-292116138/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我</h2><p id="4fee" class="pw-post-body-paragraph jn jo iq jp b jq lq js jt ju lr jw jx jy ls ka kb kc lt ke kf kg lu ki kj kk ij bi translated"><strong class="jp ir">我的网站:</strong><a class="ae me" href="https://komuternak.com/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">https://komuternak.com/</strong></a></p><p id="290f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="ak">参考</strong></p><h2 id="da47" class="kx ky iq bd kz la lb dn lc ld le dp lf jy lg lh li kc lj lk ll kg lm ln lo lp bi translated"><a class="ae me" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank"> Sklearn -决策树</a></h2><ol class=""><li id="376a" class="lv lw iq jp b jq lq ju lr jy nq kc nr kg ns kk mm mb mc md bi translated"><a class="ae me" href="https://gdcoder.com/decision-tree-regressor-explained-in-depth/" rel="noopener ugc nofollow" target="_blank">https://gdcoder . com/decision-tree-regressor-explained-in-deep/</a></li><li id="d206" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated"><a class="ae me" href="http://faculty.marshall.usc.edu/gareth-james/ISL/" rel="noopener ugc nofollow" target="_blank">统计学习简介</a></li><li id="fcf0" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">拉什卡，塞巴斯蒂安。Python机器学习</li><li id="8b94" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">https://en.wikipedia.org/wiki/Decision_tree_learning<a class="ae me" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank"/></li><li id="c3b9" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi translated">博纳科索朱塞佩。机器学习算法</li><li id="f32f" class="lv lw iq jp b jq mf ju mg jy mh kc mi kg mj kk mm mb mc md bi">Bonaccorso, Giuseppe. Machine Learning Algorithm</li></ol></div></div>    
</body>
</html>