<html>
<head>
<title>Natural Language Processing — Things you need to Know.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理-你需要知道的事情。</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/natural-language-processing-things-you-need-to-know-43eeb87da177?source=collection_archive---------2-----------------------#2020-03-16">https://medium.datadriveninvestor.com/natural-language-processing-things-you-need-to-know-43eeb87da177?source=collection_archive---------2-----------------------#2020-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0812" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个故事向你解释了几乎所有你需要知道的关于自然语言处理和人工智能的事情。</p><p id="1699" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们简单地过一遍。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/57c023127d7cd4e32dcc7130172d1228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pie03olFwl498TeBCPdLzg.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Artificial Intelligence | Machine Learning | Deep Learning</figcaption></figure><h2 id="73a9" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">人工智能</h2><p id="4a0a" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">人工智能是指机器对人类智能的刺激。人工智能的具体应用包括专家系统、自然语言处理(NLP)、语音识别和机器视觉。</p><h2 id="18b3" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">机器学习</h2><p id="3078" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">人工智能是人工智能的一个分支，机器可以分析数据，系统可以从数据中学习，识别模式，并在最少或没有人工干预的情况下做出决策。</p><h2 id="4e72" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">深度学习</h2><p id="026e" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">同样，深度学习是人工智能中机器学习的子集，具有能够学习的人工神经网络，也称为深度神经网络或深度神经学习。深度学习是一种受人脑结构启发的ML，就深度学习而言，这种结构被称为人工神经网络。</p><div class="lz ma gp gr mb mc"><a href="https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">DDI编辑推荐:5本让你从新手变成专家的机器学习书籍|数据驱动…</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">机器学习行业的蓬勃发展重新引起了人们对人工智能的兴趣</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kv mc"/></div></div></a></div><p id="fdc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">深度学习和机器学习的基本区别在于，当你希望系统学习如何对两个对象进行分类时，在机器学习中，你必须输入允许机器学习的数据特征，并输出分类。但是在深度学习中，人工神经网络本身使用人工神经网络学习特征，代价是拥有大量数据。</p><h2 id="90eb" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">人工神经网络是如何工作的？</h2><p id="49b4" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">考虑人类书写数字的分类(0-9 ),这些数字很容易被人类理解。深度学习的动作来了。在上面提到的图像中，您可以看到图像中的一个数字由28*28 = 784个像素组成。信息处理发生的神经元，图像中的784个像素中的每一个都作为每个神经元的输入，这是输入层。有一个包含输出值0–9的输出图层。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mr"><img src="../Images/2962d12f57d19d2ef9c1618f17a4b27f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCtz7v8xOOqOZVV911b2dg.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Working of artificial neural networks</figcaption></figure><p id="a9e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入层和输出层通过连接通道经由不同层连接。每个通道都有一个值，称为权重，隐藏层中的每个神经元都有一个与之相关的偏差。该偏差与输入一起被添加到权重中，然后被应用于被称为激活函数的函数。激活函数的结果决定了神经元是否被激活。每个被激活的神经元都包含关于输入的信息。这个过程一直持续到倒数第二层和输出层中激活的一层对应于输出。权重和偏差得到调整和更新，以获得训练有素的网络。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0122f55e7ca5c0d6784ad0a552b4a62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*1h9L7znh-eIvyy0T7LAqJw.gif"/></div></figure><h1 id="9f8b" class="mt lc iq bd ld mu mv mw lg mx my mz lj na nb nc lm nd ne nf lp ng nh ni ls nj bi translated"><strong class="ak">自然语言处理</strong></h1><p id="b2d0" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">人工智能领域的自然语言处理，赋予机器阅读、理解和从人类语言中获取含义的能力。</p><p id="3bc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NLP的类别包括:</p><ol class=""><li id="91fd" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk np nq nr ns bi translated">文本分类:将文本分类到不同的类别。垃圾邮件过滤</li><li id="57e2" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">语言建模:问题是从给定的先前输入单词中预测下一个单词。</li><li id="71a8" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">语音识别:将文本的声音转换为人类可读文本的过程。</li><li id="0220" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">标题生成:如果你有一个数字图像，获得它的文本描述属于这一类。</li><li id="1a01" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">机器翻译:将源文本从一种语言转换成另一种语言，而不丢失上下文。</li><li id="edd6" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">文档摘要:创建文档摘要。当我们想从一个给定的文档中创建一个摘要或标题时，这就起作用了。</li><li id="0e7d" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">问答:给定一个文本或文档，获取特定查询的结果。</li><li id="3df3" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">聊天机器人:创建一个用户友好的助手，可以集成自定义操作，也可以聊天。</li></ol><p id="f6e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们来讨论一下NLP中的一些重要算法:</p><p id="4627" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一)<strong class="jp ir">袋字</strong>(鞠躬)</p><p id="c52e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个允许你计算一段文本中所有单词的模型。它为句子或文档创建一个出现矩阵，不考虑语法和词序。这些词频或出现次数然后被用作训练分类器的特征。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ny"><img src="../Images/e06469d5f6e6fca2a982e6f69a5c28c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgHtaqBPmKZR-aYLi-nofQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">BOW implementation</figcaption></figure><p id="40b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">B) <strong class="jp ir">标记化</strong></p><p id="3e4d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它是将连续文本分割成句子和单词的过程。本质上，它的任务是将文本切割成称为<em class="nz">记号的片段。</em></p><ul class=""><li id="c5b6" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk oa nq nr ns bi translated">nltk . tokenize . whitespace tokenizer</li><li id="50f0" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated">按标点符号分割:使用标点符号</li><li id="5a57" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated">由一组规则分割:ntlk。Tokenize . TreebankWordTokenizer</li></ul><p id="f965" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">C) <strong class="jp ir">停止字</strong></p><p id="dc4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个过程中，一些看起来对NLP目标提供很少或没有价值的非常常见的单词被过滤并从要处理的文本中排除</p><p id="3829" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">D) <strong class="jp ir">令牌规范化:</strong></p><blockquote class="ob oc od"><p id="d24a" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated">词干化和词汇化</p><p id="d0a2" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated"><em class="iq">词条解析将单词解析为其字典形式(称为</em>词条<em class="iq">)。</em></p><p id="9929" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated">指的是为了去除词缀(词根的词汇附加物)而对单词的结尾或开头进行切片的过程。</p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oh"><img src="../Images/48a8e562ee6db1c18539b3413a78f03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BvguWKorDONwTrEGslccng.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Stemming and Lemmatization</figcaption></figure><p id="b573" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">E) <strong class="jp ir"> TF-IDF </strong></p><p id="6f7b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该值适用于提取特征。我们可以找到文档中的术语计数(术语频率)乘以语料库中文档总数的对数的倒数乘以出现术语t的文档总数。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/cf74fea2c2524eb2c5a8ecc664d0cc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3veGpGM1td9tBnd50OcqXA.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">TF-IDF implementation</figcaption></figure><p id="22e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f)<strong class="jp ir">NER——命名实体识别:</strong></p><p id="762d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">命名实体识别是信息提取的一个子任务，它试图定位非结构化文本中提到的命名实体并将其分类成预定义的类别，例如人名、组织、位置、医疗代码、时间表达式、数量、货币值、百分比等。</p><p id="1680" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">G) <strong class="jp ir"> N-Grams </strong></p><p id="dfbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">N元语法指的是出于表示目的将邻近单词组合在一起的过程，其中N表示要组合在一起的单词的数量。</p></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><p id="2d2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们讨论了人工智能、机器学习、深度学习。然后是NLP以及与之相关的各种算法。</p><p id="4a2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们来讨论一下<strong class="jp ir">自然语言处理</strong>的<strong class="jp ir">各种框架</strong>。</p><ol class=""><li id="35d5" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk np nq nr ns bi translated"><strong class="jp ir"> spaCy </strong>:一个Python包，设计用于提高速度、完成任务，并与其他深度学习框架互操作。它是在精心管理内存的Cython中从头开始编写的。2015年的独立研究发现spaCy是世界上最快的。它可以与TensorFlow、PyTorch、scikit-learn、Gensim和Python的其他出色的人工智能生态系统无缝互操作。</li></ol><p id="97a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">空间示例</p><p id="4694" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.<strong class="jp ir"> Gensim: </strong> Gensim是针对大型语料库的<em class="nz">主题建模</em>、<em class="nz">文档索引</em>和<em class="nz">相似度检索</em>的Python库。目标受众是自然语言处理(NLP)和信息检索(IR)社区。</p><p id="cb1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个软件依赖于科学计算的两个Python包<a class="ae oq" href="http://www.scipy.org/Download" rel="noopener ugc nofollow" target="_blank"> NumPy和Scipy </a>。您必须在安装gensim之前安装它们。</p><pre class="km kn ko kp gt or os ot ou aw ov bi"><span id="f99b" class="lb lc iq os b gy ow ox l oy oz"><strong class="os ir">&gt;&gt;&gt; from</strong> <strong class="os ir">gensim.summarization.summarizer</strong> <strong class="os ir">import</strong> summarize<br/><strong class="os ir">&gt;&gt;&gt; </strong>text = '''Rice Pudding - Poem by Alan Alexander Milne<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>She's crying with all her might and main,<br/><strong class="os ir">... </strong>And she won't eat her dinner - rice pudding again -<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>I've promised her dolls and a daisy-chain,<br/><strong class="os ir">... </strong>And a book about animals - all in vain -<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>She's perfectly well, and she hasn't a pain;<br/><strong class="os ir">... </strong>But, look at her, now she's beginning again! -<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>I've promised her sweets and a ride in the train,<br/><strong class="os ir">... </strong>And I've begged her to stop for a bit and explain -<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?<br/><strong class="os ir">... </strong>She's perfectly well and she hasn't a pain,<br/><strong class="os ir">... </strong>And it's lovely rice pudding for dinner again!<br/><strong class="os ir">... </strong>What is the matter with Mary Jane?'''<br/><strong class="os ir">&gt;&gt;&gt; print</strong>(summarize(text))<br/>And she won't eat her dinner - rice pudding again -<br/>I've promised her dolls and a daisy-chain,<br/>I've promised her sweets and a ride in the train,<br/>And it's lovely rice pudding for dinner again!</span></pre><p id="19d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">gensim摘要生成器的一个例子。</p><p id="6f15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.Fasttext:  fastText是一个高效学习单词表示和句子分类的库。FastText是一个开源、免费、轻量级的库，允许用户学习文本表示和文本分类器。它在标准的通用硬件上工作。模型可以缩小尺寸，甚至适合移动设备。</p><p id="e089" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">4.<strong class="jp ir">基于TensorFlow </strong>:</p><ul class=""><li id="687e" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk oa nq nr ns bi translated">SyntaxNet:一个自然语言理解的工具包(NLU)。</li></ul><p id="bc79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SyntaxNet是一个在学术界被称为<a class="ae oq" href="https://en.wikipedia.org/wiki/Parsing" rel="noopener ugc nofollow" target="_blank"> <em class="nz">语法解析器</em> </a>的框架，它是许多NLU系统中的第一个关键组件。给定一个句子作为输入，它用描述单词的句法功能的词性(POS)标签来标记每个单词，并且它确定句子中单词之间的句法关系，用依存解析树来表示。</p><ul class=""><li id="c672" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk oa nq nr ns bi translated"><strong class="jp ir"> textsum </strong>(用于文本摘要):一种用于文本摘要的序列到序列的注意模型。</li></ul><p id="a696" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文本摘要问题有许多有用的应用。如果你经营一个网站，你可以为用户生成的内容创建标题和简短摘要。如果你想读很多文章，但没有时间，你的虚拟助手可以为你总结这些文章的要点。</p><ul class=""><li id="968b" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk oa nq nr ns bi translated"><strong class="jp ir">跳跃思维向量</strong>:“跳跃思维向量”或简称“跳跃思维”是一个简单的神经网络模型的名称，用于学习任何自然语言中句子的固定长度表示，无需任何标记数据或监督学习。Skip-Thoughts使用的唯一监督/训练信号是自然语言语料库中句子的排序。</li><li id="1b5f" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated"><strong class="jp ir"> ActiveQA </strong>:在传统QA中，<a class="ae oq" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督学习技术</a>与标记数据结合使用，训练一个回答任意输入问题的系统。虽然这是有效的，但它缺乏像人类一样处理不确定性的能力，通过重新制定问题，发布多次搜索，评估和汇总响应。受人类“提出正确问题”能力的启发，ActiveQA引入了一个反复咨询QA系统的代理。在此过程中，代理可能会多次重新表述原始问题，以便找到可能的最佳答案。我们称这种方法为主动的，因为代理参与了与QA系统的动态交互，目的是提高返回答案的质量。</li><li id="fb9d" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated"><strong class="jp ir"> BERT: </strong> BERT使用Transformer，这是一种学习文本中单词(或子单词)之间上下文关系的注意力机制。一般来说，Transformer包括两个独立的机制——一个读取文本输入的编码器和一个为任务生成预测的解码器。由于BERT的目标是生成一个语言模型，所以只有编码器机制是必要的。谷歌的<a class="ae oq" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中描述了Transformer的详细工作原理。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi pa"><img src="../Images/fe2b0ad76bffa2741f59799051242099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15ESzOSJIdYo71kTJ0jg7A.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">BERT Language Model</figcaption></figure><p id="b805" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与顺序读取文本输入(从左到右或从右到左)的方向模型相反，Transformer编码器一次读取整个单词序列。因此，它被认为是双向的，虽然说它是非定向的会更准确。这一特性允许模型基于单词的所有周围环境(单词的左侧和右侧)来学习单词的上下文。</p><h2 id="a92e" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">5.<strong class="ak">建立在PyTorch上</strong></h2><p id="0fab" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">a) <strong class="jp ir"> PyText </strong> : PyText是一个基于深度学习的NLP建模框架，构建于PyTorch之上。PyText解决了快速实验和大规模模型服务这两个经常发生冲突的需求。它通过为模型组件提供简单和可扩展的接口和抽象，以及使用PyTorch的导出模型的功能，通过优化的Caffe2执行引擎进行推理，来实现这一点。我们在脸书使用PyText快速迭代新的建模想法，然后无缝地大规模交付它们。</p><p id="1402" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b) <strong class="jp ir"> AllenNLP </strong> : AllenNLP可以轻松设计和评估新的深度学习模型，用于几乎任何NLP问题，以及在云中或笔记本电脑上轻松运行它们的基础架构。</p><p id="d119" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">c) <strong class="jp ir"> Flair </strong>:强大的NLP库。Flair允许您将我们最先进的自然语言处理(NLP)模型应用于您的文本，如命名实体识别(NER)、词性标注(PoS)、词义消歧和分类。</p><p id="13aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">d) <strong class="jp ir"> FairSeq </strong> : Fairseq(-py)是一个序列建模工具包，允许研究人员和开发人员训练自定义模型，用于翻译、摘要、语言建模和其他文本生成任务。</p><p id="72e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">e)<strong class="jp ir">Fastai</strong>:Fastai库的<code class="fe pb pc pd os b"><a class="ae oq" href="https://docs.fast.ai/text.html#text" rel="noopener ugc nofollow" target="_blank">text</a></code>模块包含了定义适合各种NLP(自然语言处理)任务的数据集和快速生成您可以使用的模型的所有必要函数。具体来说:</p><ul class=""><li id="7328" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk oa nq nr ns bi translated"><code class="fe pb pc pd os b"><a class="ae oq" href="https://docs.fast.ai/text.transform.html#text.transform" rel="noopener ugc nofollow" target="_blank">text.transform</a></code>包含预处理数据的所有脚本，从原始文本到令牌id，</li><li id="2744" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated"><code class="fe pb pc pd os b"><a class="ae oq" href="https://docs.fast.ai/text.data.html#text.data" rel="noopener ugc nofollow" target="_blank">text.data</a></code>包含了<code class="fe pb pc pd os b"><a class="ae oq" href="https://docs.fast.ai/text.data.html#TextDataBunch" rel="noopener ugc nofollow" target="_blank">TextDataBunch</a></code>的定义，它是NLP中需要的主类，</li><li id="3d6b" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk oa nq nr ns bi translated"><code class="fe pb pc pd os b"><a class="ae oq" href="https://docs.fast.ai/text.learner.html#text.learner" rel="noopener ugc nofollow" target="_blank">text.learner</a></code>包含快速创建语言模型或RNN分类器的辅助函数</li></ul><p id="5ba4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">f)<strong class="jp ir">Transformer Model</strong>:<strong class="jp ir">Transformer</strong>是2017年推出的一款<a class="ae oq" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度</a> <a class="ae oq" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>模型，主要用于<a class="ae oq" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a> (NLP)像<a class="ae oq" href="https://en.wikipedia.org/wiki/Recurrent_neural_networks" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNNs)，Transformer被设计为处理有序的数据序列，如自然语言，用于各种任务，如<a class="ae oq" href="https://en.wikipedia.org/wiki/Statistical_machine_translation" rel="noopener ugc nofollow" target="_blank">机器翻译</a>和<a class="ae oq" href="https://en.wikipedia.org/wiki/Automatic_summarization" rel="noopener ugc nofollow" target="_blank">文本摘要</a>。然而，与rnn不同，转换器不要求按顺序处理序列。因此，如果所讨论的数据是自然语言，转换器在处理句子结尾之前不需要处理句子的开头。由于这个特性，Transformer在训练期间允许比RNNs更多的<a class="ae oq" href="https://en.wikipedia.org/wiki/Parallel_computing" rel="noopener ugc nofollow" target="_blank">并行化</a>。</p><p id="251a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自推出以来，变压器已成为NLP中大多数最先进架构的基本构建模块，在许多情况下取代了门控循环神经网络模型，如<a class="ae oq" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a> (LSTM)。由于Transformer体系结构在训练计算过程中促进了更多的并行化，因此它能够对比引入它之前多得多的数据进行训练。这导致了<a class="ae oq" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">预训练系统</a>的开发，例如<a class="ae oq" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>(来自变压器的双向编码器表示)和<a class="ae oq" href="https://en.wikipedia.org/wiki/OpenAI#GPT-2" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>，它们在发布之前已经过大量通用语言数据的训练，然后可以针对特定语言任务进行微调训练</p><p id="fb1d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> GPT-2 </strong> (OpenAI): OpenAI发布了<a class="ae oq" href="https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b" rel="noopener" target="_blank">生成式预训练模型</a> (GPT)，在2018年的多个NLP任务中取得了最先进的结果。GPT利用transformer来执行无监督学习和有监督学习，以学习NLP下游任务的文本表示。</p><p id="a4e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">GPT-2使用无监督学习方法来训练语言模型。与其他车型如<a class="ae oq" href="https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f" rel="noopener" target="_blank">埃尔默</a>和<a class="ae oq" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">不同，伯特</a>需要两个阶段的培训，即预培训和微调阶段。GPT-2没有微调阶段。</p><p id="d1b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有GPT-2的定制培训。OpenAI不发布训练GPT-2的源代码(截至2019年2月15日)。因此，我们只能将训练好的模型用于研究或采用。</p><p id="8294" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> ELMo </strong> : ELMo使用双向语言模型(biLM)来学习单词(例如，句法和语义)和语言上下文(例如，对多义性建模)。在预训练之后，向量的内部状态可以被转移到下游的NLP任务。</p><p id="ff25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与传统的单词嵌入不同，ELMo针对不同的场景，为每个单词生成多个单词嵌入。较高层捕获单词嵌入的上下文相关方面，而较低层捕获语法的模型方面。在最简单的情况下，我们只使用ELMo的顶层(只有一层),同时我们也可以将所有层合并成一个矢量。</p><h2 id="1b9c" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">递归神经网络(RNN)</h2><p id="9090" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">RNN是在自然语言处理中大量使用的神经网络的一个非常重要的变体。</p><blockquote class="ob oc od"><p id="2fa4" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated"><em class="iq">从概念上讲，它们不同于标准神经网络，因为RNN中的标准输入是一个单词，而不是标准神经网络中的整个样本。这使得网络可以灵活地处理不同长度的句子，这是标准神经网络</em>  <em class="iq">由于其固定的结构而无法实现的。它还提供了</em> <strong class="jp ir"> <em class="iq">共享跨文本</em> </strong> <em class="iq">的不同位置学习的特征的额外优势，这在标准神经网络中是无法获得的。</em></p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/839b1f9b6d09c6b5141cc4729ca73036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*6goUwZL3kmx6V0A8UQEFBw.jpeg"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">RNN architecture</figcaption></figure><p id="a77a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">RNN将句子中的每个单词视为在时间“t”出现的独立输入，并且还使用在“t-1”的激活值，作为除了在时间“t”的输入之外的输入。下图显示了RNN架构的详细结构。</p><blockquote class="ob oc od"><p id="aa48" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated"><strong class="jp ir"> GRU </strong>:</p><p id="56fe" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated">这是对基本循环单元的修改，有助于捕捉长程相关性，也有助于解决消失梯度问题。</p><p id="5990" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated">GRU由一个额外的存储单元组成，通常称为更新门或复位门。 <strong class="jp ir"> <em class="iq">除了通常的具有sigmoid函数和softmax输出的神经单元外，它还包含一个附加单元，具有tanh作为激活函数</em> </strong> <em class="iq">。使用Tanh是因为它的输出可以是正的也可以是负的，因此可以用于放大和缩小。然后，该单元的输出与激活输入相结合，以更新存储单元的值。</em></p></blockquote><p id="2fcb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> LSTM </strong>:</p><p id="0497" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在LSTM体系结构中，不是像GRU那样只有一个更新门，而是有一个更新门和一个遗忘门。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi pf"><img src="../Images/f0a81ff0bda30b4ab50fa297bdeb57d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqkZ_mXi_5oy4MOjBdbSlg.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">LSTM Architecture</figcaption></figure><p id="45ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种结构给存储单元一个选择，即在时间t-1保持旧值，并在时间t将值加到其上</p><p id="a34e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">双向RNN: </strong></p><p id="c768" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以前的架构只考虑以前的值，但是BRNN考虑以前和以后的输入。</p><blockquote class="ob oc od"><p id="57bc" class="jn jo nz jp b jq jr js jt ju jv jw jx oe jz ka kb of kd ke kf og kh ki kj kk ij bi translated"><em class="iq">双向RNN由一个前向和一个后向递归神经网络组成，在任何给定时间t，结合两个网络的结果进行最终预测，如图所示。</em></p></blockquote><h2 id="ba8c" class="lb lc iq bd ld le lf dn lg lh li dp lj jy lk ll lm kc ln lo lp kg lq lr ls lt bi translated">生成性对抗网络:</h2><p id="4223" class="pw-post-body-paragraph jn jo iq jp b jq lu js jt ju lv jw jx jy lw ka kb kc lx ke kf kg ly ki kj kk ij bi translated">观察GAN的最简单方式是将其视为一个<em class="nz">发生器网络</em>，通过引入一个对手(即<em class="nz">鉴别器网络</em>)对其进行训练以产生真实样本，鉴别器网络的工作是检测给定样本是“真”还是“假”。我喜欢的另一种方式是，鉴别器是一个动态更新的评估指标，用于发电机的调谐。发生器和鉴别器都在不断改进，直到达到一个平衡点:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi pg"><img src="../Images/467e198409710b5a13c91a5713979df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBefP0o3x8A6HsLMxQYbuw.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">GAN Architecture</figcaption></figure><ol class=""><li id="cb47" class="nk nl iq jp b jq jr ju jv jy nm kc nn kg no kk np nq nr ns bi translated">生成器在接收到关于其生成的样本如何设法欺骗鉴别器的反馈时会有所改进。</li><li id="31a7" class="nk nl iq jp b jq nt ju nu jy nv kc nw kg nx kk np nq nr ns bi translated">鉴别器得到了改进，不仅显示了生成器生成的“假”样本，还显示了从真实分布中提取的“真”样本。通过这种方式，它可以了解生成的样本是什么样的，真实的样本是什么样的，从而能够向生成器提供更好的反馈。</li></ol></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><p id="0cc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考资料:</p><div class="lz ma gp gr mb mc"><a href="https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f" rel="noopener follow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">ELMo有助于进一步改善您的单词嵌入</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">语言模型嵌入(ELMo)</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ml l"><div class="ph l mn mo mp ml mq kv mc"/></div></div></a></div><div class="lz ma gp gr mb mc"><a href="https://becominghuman.ai/generative-adversarial-networks-for-text-generation-part-1-2b886c8cab10" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">文本生成的生成性对抗网络——第一部分</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">用于文本生成的GANs的问题以及解决这些问题的方法</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">becominghuman.ai</p></div></div><div class="ml l"><div class="pi l mn mo mp ml mq kv mc"/></div></div></a></div><p id="0bef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">维基百科(一个基于wiki技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书</p><div class="lz ma gp gr mb mc"><a href="https://github.com/keon/awesome-nlp" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">keon/awesome-nlp</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">致力于自然语言处理的精选资源列表请用英语、繁体中文阅读此内容…ediu</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">github.com</p></div></div><div class="ml l"><div class="pj l mn mo mp ml mq kv mc"/></div></div></a></div><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="pk pl l"/></div></figure></div></div>    
</body>
</html>