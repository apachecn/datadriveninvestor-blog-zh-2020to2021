# 深度强化学习入门

> 原文：<https://medium.datadriveninvestor.com/getting-started-with-deep-reinforcement-learning-5e973fec1e28?source=collection_archive---------0----------------------->

如果你刚刚开始深度强化学习(或正在考虑)，这里有一个快速提示，告诉你要注意什么，以及如何最好地度过这个过程。

![](img/34776e7038c2937fb2ee5533e86b31b9.png)

Receiving a reward is always nice. Especially if you’re a Reinforcement Learning agent. (Photo by [freestocks](https://unsplash.com/@freestocks?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral))

正如在计算机科学中经常出现的那样，对一个主题有两种主要的方法。科学家的方法是试图通过熟悉基础理论来深入理解问题，而工程师的方法是:找出哪些库做了你想做的事情，并把它们投入使用。

现在不要误解我的意思，这两种方法本身都不是“最好的”。事实上，在你可能发现自己所处的每一个场景中，这两者都是不可能的。然而对我们来说不幸的是，在 DRL 唯一可行的方法更难:像科学家一样去做。

# 该理论

*我真的需要—* 是的！你知道。虽然你会在网上找到很多关于如何编写自己的 DRL 代理的教程，但是这些教程非常有限。他们讨论了具有特定网络架构的单个 RL 算法，用于解决单个(并且在大多数情况下相当琐碎)问题。

去吧，给他们一个机会！实际上，它们工作得很好。但是一旦你试图将它们归纳到你自己的问题中，如果你碰壁了，不要感到惊讶。

事实是，你需要明白你在做什么才能让 DRL 运转起来。如果你不知道什么是回报和回报，以及它们如何影响你的代理人，你就迷失了。不知道 TD(λ)是什么？最好弄清楚。因为一旦你坐在那里，看着你的经纪人在每一个训练步骤中变得越来越差，你就需要知道如何解释正在发生的事情以便解决它。

[](https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/) [## AI 和机器学习有什么区别？数据驱动的投资者

### 这两个主题背后有很多令人兴奋的东西，所以这是一个快速指南，介绍了它们是什么以及它们有什么…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/) 

谢天谢地，现在已经有大量的文献了。这方面的书**对强化学习做了非常全面的总结: [*强化学习:介绍* —萨顿&巴尔托](http://incompleteideas.net/book/the-book-2nd.html)。[大卫·西尔弗的讲座](https://www.davidsilver.uk/teaching/)对这个话题也很有帮助。**

## 别担心。

你不需要读一整本书就能知道发生了什么。但是要知道:每当你试图让你的代理人学习一些东西时，最好的选择是不要跳到 StackOverflow 上，希望有人也有类似的问题。即使他们这样做了，他们的问题的解决方案可能也不会帮助你解决你的问题。

通过仔细检查(1)错误和错误的代码库，(2)你的算法和它是否适合你的问题，以及(3)超参数，你的时间更好地用于定位你痛苦的原因。

# 如何开始——以及如何坚持下去

假设你已经知道 RL(奖励、政策、价值观)和深度学习(神经网络、损失函数)背后的基本理论，那么在开始实施时，你需要注意一些陷阱。

## 1.慢慢开始

登上月球是诱人的，但特别是在当前的研究领域，困难的问题往往还没有一个规范、直截了当的解决方案。爬得太高可能会让你离太阳太近。

最好先解决简单的问题。在开始你自己的定制环境之前，尝试一些 OpenAI 的更简单的健身房问题。这些基准的优势在于，它们有明确的回报(允许高效学习并且不会太稀疏)，并且用许多当前的 RL 算法(举两个最好的例子， [AWR](https://arxiv.org/abs/1910.00177) 和 [SAC](https://arxiv.org/abs/1801.01290) )来破解相当简单。

顺便说一下:如果你在你的项目中使用了 [PyTorch](http://pytorch.org) ，请查看一下 AWR 的[我自己的实现](https://github.com/lukDev/awr_pytorch)。

## 2.从他人那里获得灵感

不要害怕寻求帮助。如果你和我一样，你会有一种强烈的冲动，想出去按自己的方式做事，甚至不看任何外界的建议。不要犯那个错误。

很多人都有一种误解，认为要真正称之为*你的*工作，你需要自己去做。每个人(尤其是计算机科学家)都应该知道，最值得一提的成就是不看原件就能重现已经存在了很久的东西，而是能够快速理解艺术的状态，并能够在此基础上继续发展。

没有人会因为你能够从头开始编写深度 Q 学习代理而受益。如果您可以使用这样一个代理的现有实现，并将它们转换成解决一个新问题的东西，那么它们会带来好处。

另外:如果你不确定如何解释论文中的某些陈述或评论(相信我，你有时会这样)，作者通常很乐意回答你的问题。毕竟，是他们对这篇论文的主题充满热情，花了几个月的时间来创作。只是事先查查有没有一些关于你这个问题的论坛帖子；你不想用他们已经回答了一千次的问题来烦他们。

## 3.不要气馁

如果在 DRL 有一个保证成功的策略，那就是:永远不要放弃。

很有可能你的第一个、第二个甚至第十个方法都不会达到你想要的结果。这主要是因为 DRL 太难了。真的没有其他方式来表达。

正如安德烈·卡帕西在他的[对黑客新闻](https://news.ycombinator.com/item?id=13519044)的评论中恰当地指出的，问题是:

> 监督学习想要工作。【……】强化学习必须强制起作用。

他的意思是，监督学习允许你把细节搞砸一点。选择稍微不完美的超参数，你仍然会得到收敛，尽管没有那么快。如果你在强化学习中犯了最微小的细节错误，你将受到惩罚。毫不犹豫。

举例来说，这可能意味着，你的代理人(可以完美地解决你给它的问题)将无法学到任何东西，仅仅是因为温度参数设置不当，奖励没有适当调整。RL 没有显示“生命迹象”，也就是说，没有迹象表明你在正确的轨道上。当且仅当你把一切都做对了，它才会起作用。

所有这一切的寓意并不是说 DRL 是不可思议的困难；那就是，你应该相信自己的直觉，知道什么是对的，什么是可行的，并遵循直觉，即使一开始看起来你错了。只要你再改变一个超参数，一切都可能逆转。

## 4.知道什么意味着什么

正如我前面提到的:解决 DRL(以及其他许多地区)问题的最好方法是努力理解潜在的原因。是奖励没有按比例适当吗？奖励是不是太稀疏了？我的网络是不足还是过度？

深度学习的诀窍通常不是为你已经确定的问题找到解决方案，而是首先确定问题。众所周知，欠拟合需要更强大的网络架构，而过拟合可以通过一些调整来缓解。但是你怎么知道这些现象是否存在呢？

这同样适用于 DRL，如果不是更多的话。如果你的批评者不稳定意味着什么？或者你的演员不仅没有进步反而变得越来越差？

一旦回答了这些问题，就可以找到解决方案。例如，如果状态值估计得不好，参与者在训练后可能表现得更差。也许在 TD(λ)估计的实现中有一个 bug？还是奖励值太大，导致训练评论家时数值不稳定？

许多事情都可能导致错误的价值评估，但是我们需要知道(或者至少怀疑)这些价值是问题的根源，否则我们甚至不会考虑寻找可能的解决方案。

# 勇往直前去征服吧！

你知道他们怎么说开始是最难的部分吗？他们是对的。所以就这么做吧，不会比这更难的了！如果你坚持下去，我相信你会取得一些学习进步，就像这里一样:

A randomly initialized neural network agent performing badly on [Pendulum-v0](https://gym.openai.com/envs/Pendulum-v0/).

The same neural network after being trained for 200 epochs using [my implementation of AWR](https://github.com/lukDev/awr_pytorch).