<html>
<head>
<title>Reinforcement Learning — Planning &amp; Dynamic Programming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——规划和动态规划</h1>
<blockquote>原文：<a href="https://medium.datadriveninvestor.com/reinforcement-learning-planning-dynamic-programming-45f81a3cc9fa?source=collection_archive---------6-----------------------#2020-04-07">https://medium.datadriveninvestor.com/reinforcement-learning-planning-dynamic-programming-45f81a3cc9fa?source=collection_archive---------6-----------------------#2020-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="87f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上一篇<a class="ae kl" href="https://medium.com/@wlaurito2/reinforcement-learning-mdp-639aecec6da4" rel="noopener">帖子</a>中，我们讨论了强化学习和MDP的一些基础知识。现在，我们将描述如何通过使用动态规划寻找最优策略来求解MDP。</p><h1 id="558a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">规划问题</h1><p id="6fb8" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">一个<em class="lp">规划问题</em>的环境完全可以被一个代理观察到。因此，一个完全可观测的MDP是解决规划问题的基础。</p><p id="aa6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，在扑克游戏中，代理看到所有对手的牌，并且知道一副牌中所有牌的顺序。</p><p id="3824" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">规划问题有两种情况:</p><h2 id="226f" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated">预言；预测；预告</h2><p id="1df4" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">其中之一就是<em class="lp">预测题</em>。这里的输入是一个完全可观测的MDP和一个策略π。作为输出，我们获得了<em class="lp">状态值函数v_π。</em>有了价值函数，我们现在就能够评估给定政策的未来<em class="lp"> π </em>。我们可以通过遵循政策<em class="lp"> π来说一个状态会有多好。</em></p><p id="8bd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，在我们的扑克游戏中，我们可以预测下一张抽的是哪张牌，以及根据策略π，代理将出哪张牌。我们感兴趣的是预测玩家的每个回合会有多好。我们通过获得<em class="lp">状态值函数v_π来实现。</em></p><h2 id="a924" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated">控制</h2><p id="20d6" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">计划的另一个例子是<em class="lp">控制。</em>输入再次是完全可观测的MDP，但是现在没有给出策略<em class="lp">π</em><em class="lp">。</em>在这种情况下，我们所寻找的，是在我们的MDP中为每个州获得最大价值的最佳行为方式<em class="lp">。</em>因此，我们在寻找最优策略<em class="lp"> π* </em>和最优状态值函数<em class="lp"> v* </em>。</p><p id="59a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">赢得扑克游戏归结为找到最佳策略的行为。因为代理人像不朽的宙斯一样知道和看到一切，所以它只是一个计划如何在每个回合进行的问题。</p><div class="mc md gp gr me mf"><a href="https://www.datadriveninvestor.com/2020/01/22/whats-the-difference-between-ai-and-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">AI和机器学习有什么区别？数据驱动的投资者</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">这两个主题背后有很多令人兴奋的东西，所以这是一个快速指南，介绍了它们是什么以及它们有什么…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">www.datadriveninvestor.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt mu mf"/></div></div></a></div><h1 id="6b7b" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">策略迭代</h1><p id="0bdf" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了实际找到最优策略并求解我们的MDP，我们可以使用预测方法来评估策略<em class="lp"> π </em>以获得状态值函数v_ <em class="lp"> π。我们称这个过程为政策评估。</em></p><p id="47de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于这个状态值函数，我们现在可以通过贪婪地行动并选择具有最大行动值的行动来计算新的、更好的策略<em class="lp">π’</em>。基于上述控制问题的方法，该步骤被称为<strong class="jp ir">策略改进</strong>。</p><p id="030d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们重复评估过程，这次使用策略π’来获得<em class="lp">v _π’。</em></p><p id="55d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> <em class="lp">通过迭代评估和策略改进两个过程，我们最终得到最优策略π*和最优状态值函数v*。</em>T3】</strong></p><h2 id="f1f8" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated"><strong class="ak">迭代政策评估</strong></h2><p id="41f9" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">为了评估一项政策，我们将使用稍微修改过的<em class="lp">贝尔曼期望方程</em>。这就是为什么我们需要先检查原始方程:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi mv"><img src="../Images/48d7b8ba6199735eb95cab1c6d2af5fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYwMkS8GntuhWTAjg8Lscg.png"/></div></div></figure><p id="32be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上一篇文章中，我已经描述了v_ <em class="lp"> π的Bellman <em class="lp">最优性</em>方程。</em>您可能还记得，<em class="lp">s’</em>代表从状态s(在时间步长t)采取动作<em class="lp"> a </em>后的下一步(在时间步长t+1)。</p><p id="fd35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的主要区别是，我们现在不仅仅考虑在一种状态下给予我们最大回报的行动，而是考虑一个主体通过遵循随机策略<em class="lp"> π </em>可以选择的<em class="lp">所有</em>行动，因此我们现在通过选择行动来总结所有可能的状态s’。根据贝尔曼期望方程，我们基于代理可能达到的所有状态来更新每个状态值。</p><p id="02a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来看看贝尔曼期望方程的修正版:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ng"><img src="../Images/28c43d19b33d40569612e03c7372194e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_aqeUOPx87vneGKW4m35ig.png"/></div></div></figure><p id="e066" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你能看出不同之处吗？我们用右侧的<em class="lp">v _ k(s’)</em>代替了v _<em class="lp">π(s’)</em>，等式的左侧从v_ <em class="lp"> π(s) </em>变为v_k+1(s)。干得好！为你击掌！</p><p id="a975" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个修改版本告诉我们如何更新我们的状态值，并被称为<em class="lp">迭代策略评估。</em>为了得到下一次迭代的状态值函数的每个状态的值<em class="lp"> v_k+1 </em>，我们根据上一次迭代的状态值函数<em class="lp"> v_k </em>计算新函数的每个状态。换句话说，我们使用“旧”状态<em class="lp">s’，</em>s的后继状态的值，来计算在<em class="lp"> v_k+1(s) </em>的时间步长<em class="lp"> t </em>的“新”状态<em class="lp"> s </em>的值。</p><p id="cd10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目前，我们正在使用所谓的<strong class="jp ir">同步备份</strong>，因为下一个状态值函数的所有状态都被一次性更新，并且只有当所有状态都被更新时，我们才得到新的v_k+1。</p><p id="438e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更新了所有状态后，通过应用修改后的<em class="lp">贝尔曼期望方程</em>，我们现在有了一个基于π <em class="lp">的新的价值函数<em class="lp"> v_π </em>。我们现在知道遵循这个特殊的政策是多么有价值。</em></p><h2 id="93cd" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated">政策改进</h2><p id="d033" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们现在可以使用这个新计算的状态值函数来改进我们的策略。我们通过创建一个新策略<em class="lp">π’</em>来做到这一点，该策略选择具有最大动作值的动作<em class="lp"> a </em>，从而最大化MDP中每个状态<em class="lp"> s </em>的值。通过这样做，我们最终得到一个新的状态值函数v_π'(s)，它大于或等于v_π(s)。</p><p id="8c8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">选择具有最大动作值的动作a意味着新策略π'贪婪地作用于<em class="lp"> q_π(s，a): </em></p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b5111684ac63106f7936a78dbdb9cdad.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*cZgzshpUnHKCwUTbhkgKUg.png"/></div></figure><p id="38da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">新政策只是忽略了旧政策会做的事情，而是总是选择给我们带来最大回报的行动。</p><p id="8a15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过证明策略改进定理，我们可以证明贪婪行为的策略π'总是比原始策略π好。</p><h2 id="c9b2" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated"><strong class="ak">政策改进定理</strong></h2><p id="953e" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">让我们考虑一个用于<em class="lp"> v_π </em>的确定性策略<em class="lp"> π </em>。为什么是决定论？在策略改进的一次迭代之后贪婪地行动使得策略无论如何都是确定的，因为贪婪策略<em class="lp">π’</em>为每个状态<em class="lp"> s </em>选择一个动作，该动作最大化该状态的值。这意味着我们可以假设从确定性策略开始。</p><p id="cb7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在对每个状态遵循这个特定的确定性策略<em class="lp"> π </em>之后，我们最终得到:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0ed4696a3c615f3b8fc620b5b309c90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*_C3pT9r1CwJ3_Lpnqe52sw.png"/></div></figure><p id="c95e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是因为每个状态<em class="lp"> s </em>的值取决于所采取的动作<em class="lp"> a </em>，因此对于每个状态<em class="lp"> s </em>，状态值<em class="lp"> v_π </em>和动作值<em class="lp"> q_π </em>相等。</p><p id="3738" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们要展示为什么遵循贪婪策略<em class="lp">π’</em>会导致<em class="lp">v _π’(s)</em>大于或等于原来的<em class="lp"> v_π(s)。</em></p><p id="c939" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">政策改进定理</strong>如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b4cdc0fecf184b0d8656b8282475d04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*0-cGGlYYT7cErT6K0Fnnow.png"/></div></figure><p id="c209" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它基本上说:遵循确定性贪婪策略<em class="lp"> π' (s) = a </em>，而不是<em class="lp"> π(s) = a，</em>对于所有状态导致q_ <em class="lp"> π(s，π')</em>大于或等于v_ <em class="lp"> π(s) </em>并且因此也在</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d4ff24106d7f12f0ddc0ca75524584f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*fKbDHZ7WNJ6NAuE6n2wLow.png"/></div></figure><p id="1959" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们先证明从<em class="lp"> s </em>到后继状态<em class="lp">s’</em>的一步。如前所述<em class="lp"> π'(s) </em>选择动作值最大的动作<em class="lp"> q_π(s，a) </em>。通过遵循<em class="lp">π’(s)</em>而不是<em class="lp"> π(s) </em>用于该单个状态，然后继续原始策略π <em class="lp">，</em>，则我们最终得到<em class="lp">v _π’(s)</em>大于或至少等于<em class="lp"> v_π(s) </em>。这意味着我们的政策<em class="lp"> π'(s) </em>确实优于或等于<em class="lp"> π(s) </em>，如果这仅适用于一个州的话。我们可以把<em class="lp"> q_π(s，π')</em>写成:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nl"><img src="../Images/698cf2a995528f43bad11517483d26cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PB77-4hveelA0j4TKR7M4Q.png"/></div></div></figure><p id="7358" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这告诉我们，q_ <em class="lp"> π </em> (s，<em class="lp">π’(S)</em>就是通过跟随<em class="lp">π’</em>得到的期望即时报酬<em class="lp"> R_t+1 </em>加上通过跟随π <em class="lp">得到的下一个状态<em class="lp"> S_t+1 </em>的贴现值所代表的未来报酬。</em></p><p id="562f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们现在知道<em class="lp"> π' (s) </em>对于特定的状态一定更好。我们现在可以将<em class="lp"> v_π(s) ≤ q_π(s，π')</em>应用到<em class="lp"> γv_π(S_t+1) </em>来表明这对于下一个状态也是成立的:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nl"><img src="../Images/a9098ecf23255ad00134c231b7672669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5TLwuG_Zbt_vLcwFJSWIw.png"/></div></div></figure><p id="2927" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这告诉我们，q_ <em class="lp"> π </em> (s，<em class="lp">π’(S)</em>就是通过跟随<em class="lp">π’</em>得到的期望即时报酬<em class="lp"> R_t+1和</em> R_t+2加上通过跟随π <em class="lp">得到的下一个状态<em class="lp"> S_t+3 </em>的贴现值所代表的未来报酬。</em></p><p id="93fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们对所有状态重复应用<em class="lp"> v_π(s) ≤ q_π(s，π')</em>，我们最终将得到:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nm"><img src="../Images/f5f5f40b4d3ae7b8e04a9d9814b98321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g5CIDb5HLrjrSHUhtCHeig.png"/></div></div></figure><p id="140c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也就是<strong class="jp ir"> q_ <em class="lp"> π(，π'(s)) = v_π'(s) </em> </strong></p><p id="b6c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此证明了</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f715bd3ec9485ab7169403690bcdcf86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*aDsuhx1I14j7WwCk31bRCQ.png"/></div></figure><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d4ff24106d7f12f0ddc0ca75524584f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*fKbDHZ7WNJ6NAuE6n2wLow.png"/></div></figure><p id="313a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">是真的，因为策略<em class="lp"> π' (s) </em>对于每个状态来说确实比原始策略<em class="lp"> π(s) </em>更好。</p><h1 id="926a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">价值迭代</h1><p id="42ab" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">值迭代是策略迭代的特例，策略评估只有一次单次迭代<em class="lp"> k=1 </em>。我们可以使用稍加修改的贝尔曼最优方程，因此将策略评估和策略改进结合在一个更新规则中:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi no"><img src="../Images/94bde9185b763e0f7d07b8641227e304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHN8ytIsMromxtKXlZXbXw.png"/></div></div></figure><p id="62e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们再次使用时间步长k的先前价值函数的状态来获得<em class="lp"> k+1 </em>的价值函数的状态。<strong class="jp ir">但是现在我们直接选择奖励最大化的动作</strong>。通过反复应用这一规则，我们获得了最优价值函数，并由此获得了我们在上一篇文章中看到的最优策略。</p><p id="b072" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">举例:最短路径</strong></p><p id="f79e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们有以下状态和动作(见下图)。所有动作都有奖励<em class="lp"> R=-1 </em>。结束状态是一个正方形，位于右下角。对于<em class="lp"> k=0 </em>中的所有值状态，我们从<em class="lp"> 0 </em>开始。</p><p id="a0f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于<em class="lp"> v_0 </em>，我们得到如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2e23a72c3739da1ed6e71c5fa8483d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*SCo0kClakXqZQ18Ut9bSlA.png"/></div></figure><p id="8706" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们同步迭代应用修正的贝尔曼最优方程。首先对于<em class="lp"> k=1 </em>的每个状态得到<em class="lp"> v_1 </em>。例如，对于左上角的状态，我们称之为A，我们计算状态值如下:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nq"><img src="../Images/4d6a14ce308dd7a8c7cab0d681cb779f.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*NpLSGl-Cci00Ry8zX56jJQ.png"/></div></div></figure><p id="7d35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将修改的贝尔曼最优方程应用于每个状态，并得到具有以下状态值的新的值函数<em class="lp">v1</em>:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8b07c92eca34d723b7da69d0c5f9210b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*oewB8GbG-1sn14HdybGkBw.png"/></div></figure><p id="3655" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们为v2再做一次:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/163e4ae6873803d19953e28c2deeedc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*kqmxTXI7rFN5yh95dgm4vg.png"/></div></figure><p id="f149" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们仔细看看标有红色F的状态和左上角的状态A:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/04d8239331fe226699e8ca13064b8c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*WdMlefuL2rHo9L-3C8b82w.png"/></div></figure><p id="fe1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你现在看到的，一条清晰的路径由状态和它们的值来表示。</p><p id="07ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过继续这个过程，对于k=4，我们将得到如下结果:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/76ff796dcd60ea05e22d0ad3e1a1b9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*DiK6_7H0luQHcOZFt2KMKg.png"/></div></figure><p id="41f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在每一步中，我们通过选择那些引导我们达到最高价值状态的行动来获得我们的最优政策。在这种情况下，最优策略有不同的路径。</p><p id="e26f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了帮助你更好地理解价值迭代是如何工作的，你可以从最终状态开始考虑到“最接近”的状态，例如我们上面的状态F。实际上，不一定要有结束状态，MDP可以有循环。值迭代仍然可以工作，但是想象一个结束状态仍然有助于理解它是如何工作的，以及它是如何在每次迭代中从该状态向后“传播”到所有状态的。</p><p id="2a58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们把这个问题分成子问题。我们从直接指向最终状态的行动中选择最佳行动，这给了我们最高的回报。我们对每个“最接近”最终状态的状态都这样做。一旦我们完成了这些，我们就可以进行下一次迭代，来解决从指向上一次迭代状态的状态中找到动作的最高值的子问题，依此类推。在动态规划中，这叫做<strong class="jp ir"> <em class="lp">最优性原理</em> </strong>。我们通过寻找子问题的解来获得完整问题的解。</p><h1 id="82dd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">异步动态编程</strong></h1><p id="b99c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在同步动态编程中，我们需要为旧的值函数保存一个数组，为新的值函数保存一个数组。然后，我们使用旧值函数的状态来计算新值函数中的每个状态。只有计算完新值函数中的所有值，我们才能删除旧函数，然后使用新计算的函数进行下一次迭代。新函数在下一次迭代中成为旧函数，过程再次开始。由于这在现实世界中不是非常有效和实用，我们可以使用<strong class="jp ir">异步动态编程</strong>。</p><h2 id="7136" class="lq kn iq bd ko lr ls dn ks lt lu dp kw jy lv lw la kc lx ly le kg lz ma li mb bi translated">就地动态编程</h2><p id="2b36" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">一种技术是就地动态编程。这里的想法是不用保存两个数组，一个用于旧的，另一个用于新的值函数，我们只使用一个数组，直接就地更新状态。我们直接使用更新的状态。只要我们连续到达所有的状态，我们最终总是得到一个更新的值函数。</p><p id="7da6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当然还有更多像<strong class="jp ir">实时动态编程</strong>和<strong class="jp ir">优先级扫描</strong>，这里我就不描述了。</p><h1 id="69e6" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">结论</strong></h1><p id="2562" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">感谢阅读。在下一篇文章中，我将写关于无模型预测。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nv nw l"/></div></figure></div></div>    
</body>
</html>